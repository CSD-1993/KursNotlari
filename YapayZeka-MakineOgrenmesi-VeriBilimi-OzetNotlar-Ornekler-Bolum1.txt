#----------------------------------------------------------------------------------------------------------------------------
                                    
                                        Yapay Zeka, Makine öğrenmesi ve Veri Bilimi
                                                        Kursu
            
                                        Sınıfta Yapılan Örnekler ve Özet Notlar
                                                        1. Bölüm

                                                   Eğitmen: Kaan ASLAN
                                        
        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:           

        Aslan, K. (2025), "Yapay Zekâ, Makine Öğrenmesi ve Veri Bilimi Kursu", Sınıfta Yapılan Örnekler ve Özet Notlar, 
            C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları okurken editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                            Son Güncelleme: 01/12/2025 - Pazartesi

#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
                                                1. Ders - 23/12/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Katılımcılarla tanışıldı ve kursun tanıtımı yapıldı.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                2. Ders - 24/12/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Python Programlama Dilinin gözden geçirilmesine başlandı.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                3. Ders - 06/01/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Python Programlama Dilinin gözden geçirilmesine devam edildi.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                4. Ders - 07/01/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Python Programlama Dilinin gözden geçirilmesine devam edildi.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                5. Ders - 13/01/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Python Programlama Dilinin gözden geçirilmesine devam edildi.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                6. Ders - 14/01/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Python Programlama Dilinin gözden geçirilmesine devam edildi.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                7. Ders - 20/01/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    NumPy kütüphanesinin gözden geçirilmesine başlandı.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                8. Ders - 21/01/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    NumPy kütüphanesinin gözden geçirilmesine devam edildi ve Pandas kütüphanesinin gözden geçirilmesine başlandı. Matplotlib
    kütüphanesi gözden geçirildi.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzda Python kodlarında yazım stili olarak PEP 8 ("Style Guide for Python Code") kullanmayacağız. (Örneğin hangi 
    fonksiyonların ve sınıfların hangi modüllerin içerisinde olduğunun daha kolay kavranabilmesi için import işlemlerini 
    dosyanın başında yapmak yerine kullanım yerine yakın bir yerde yapacağız.) Bu nedenle kursumuzdaki notlarda PEP 8’e uygun 
    olmayan kodlar gördüğünüzde bunun kasten yapıldığını düşünmelisiniz. Tabii bu kodları kolay bir biçimde PEP 8'e uygun hale 
    getirebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                9. Ders - 27/01/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun ilk bölümünde zeka, yapay zeka, öğrenme, makine öğrenmesi ve veri bilimi kavramlarının ne anlama geldiğini 
    açıklayacağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Zeka kapsamı, işlevleri ve yol açtığı sonuçları bakımından karmaşık bir olgudur. Zekanın ne olduğu konusunda psikologlar 
    ve bilişsel nörobilimciler arasında tam bir fikir birliği bulunmamaktadır. Çeşitli kuramcılar ve araştırmacılar tarafından 
    zekanın çeşitli tanımları yapılmıştır. Bu kuramcıların ve araştırmacıların bazıları yaptıkları tanımdan hareketle zekayı 
    ölçmek için çeşitli araçlar da geliştirmeye çalışmışlardır. 

    Charles Spearman zekayı "g" ve "s" biçiminde iki yeteneğin birleşimi olarak tanımlamıştır. Spearman'a göre "g" faktörü akıl 
    yürütme ve problem çözmeyle ilgili olan "genel zekayı" belirtir. "s" faktörü ise müzik, sanat, iş yaşamı gibi özel alanlara 
    yönelik "spesifik zekayı" belirtmektedir. İnsanlar "zeka" denildiğinde daha çok genel zekayı kastetmektedirler. 

    Howard Gardner tarafından kuramsal hale getirilen "çoklu zeka (multiple intelligence)" zeka teorisinde Gardner zekayı 
    "sözel/dilbilimsel (verbal/linguistic), müzikal (musical), mantıksal/matematiksel (logical/mathematical), görsel/uzamsal 
    (visual/spatial), kinestetik (kinesthetic), kişilerarası (interpersonal), içsel (intrapsersonal)" olmak üzere yedi türe 
    ayırmıştır. Sonra bu yedi türe "doğasal (naturalistic), varoluşsal (existentialist)" biçiminde iki tür daha ekleyerek dokuza 
    çıkarmıştır.

    Robert Sternberg'e göre ise "analitik (analytical), pratik (practical) ve yaratıcı (creative)" olmak üzere üç tür zeka vardır.
    Analitik zeka problemi parçalara ayırma, analiz etme ve çözme ile ilgili yetileri içermektedir. Bu yetiler aslında zeka 
    testlerinin ölçmeye çalıştığı yetilerdir. Pratik zeka yaşamı sürdürmek için gerekli olan pratik becerilerle ilgilidir. Yaratıcı 
    zeka ise yeni yöntemler bulmak, problemleri farklı biçimlerde çözebilmek, yenilikler yapabilmekle ilgili yetilerdir.

    Catell-Horn-Carroll (CHC) Teorisi diye isimlendirilen çok katmanlı zeka teorisi üzerinde en çok durulan zeka teorisidir. 
    (Raymond Catell aslında Spearman'ın John Horn ise Catell'in öğrencisidir.) Catell zekayı "kristalize zeka (crystalized 
    intelligence)" ve "akıcı zeka (fluid intelligence)" biçiminde ikiye ayırmıştır. Kristalize zeka öğrenilmiş ve oturmuş bilgi 
    ve becerilerle ilgili iken akıcı zeka problem çözme ve yeni durumlara uyum sağlama becerileriyle ilgilidir. John Horn ise 
    Catell'in bu iki tür zekasını genişleterek ona görsel işitsel yetileri, belleğe erişimle ilgili yetileri, tepki zamanlarına 
    ilişkin yetileri, niceliksel işlemlere yönelik yetileri ve okuma yazma becerilerini de eklemiştir. Nihayet John Carroll zeka 
    ile ilgili 460 yeteneği faktör analizine sokarak üç katmanlı bir zeka teorisi oluşturmuştur. CHC teorisi Stanford Binet ve 
    Wechsler zeka testlerinin ileri sürümlerinin benimsediği zeka anlayışıdır. 

    Bu bilgilerin eşliğinde kuramsal farklılıklara değinmeden zekanın genel bir tanımı şöyle yapılabilir: "Zeka yeni 
    durumlara uyum sağlamak ve problem çözmek için deneyimlerden öğrenme, bilgi edinme ve kaynakları etkin bir biçimde kullanma" 
    becerisidir.

    Yapay zeka ise -ismi üzerinde- insan zekası ile ilgili bilişsel süreçlerin makineler tarafından sağlanmasına yönelik süreçleri 
    belirtmektedir. Yapay zeka terimi ilk kez 1955 yılında John McCarthy tarafından kullanılmıştır. Doğal zekada olduğu gibi 
    yapay zekanın da farklı kişiler tarafından pek çok tanımı yapılmaktadır. Ancak bu terim genel olarak "insana özgü nitelikler 
    olduğu varsayılan akıl yürütme, anlam çıkartma, genelleme ve geçmiş deneyimlerden öğrenme gibi yüksek zihinsel süreçlerin
    makineler tarafından gerçekleştirilmesi" biçiminde tanımlanabilir. Yapay zekanın diğer bazı tanımları şunlardır:

    - Yapay zeka insan zekasına ilişkin "öğrenme", "akıl yürütme", "kendini düzeltme" gibi süreçlerin makineler tarafından simüle 
    edilmesidir.
    - Yapay zeka zeki makineler yaratma amacında olan bilgisayar bilimlerinin bir alt alanıdır.
    - Yapay Zeka bilgisayarların insanlar gibi davranmasını sağlamayı hedefleyen bilgisayar bilimlerinin bir alt dalıdır.
    
    Yapay zekanın simüle etmeye çalıştığı bilişsel süreçlerin bazılarının şunlar olduğuna dikkat ediniz:

    - Bir şeyin nasıl yapılacağını bilme (knowledge)
    - Akıl yürütme (reasoning)
    - Problem çözme (problem solving)
    - Algılama (perception)
    - Öğrenme (learning)
    - Planlama (planning)
    - Doğal dili anlama ve konuşma
    - Uzmanlık gerektiren alanlarda karar verme
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ile ilgili düşünceler ve görüşler çok eskiye kadar götürülebilir. Ancak modern yapay zeka çalışmalarının 1950’li 
    yıllarda başladığı söylenebilir. Şüphesiz yapay zeka alanındaki gelişmeleri de aslında başka alanlardaki gelişmeler tetiklemiştir. 
    Örneğin bugün kullandığımız elektronik bilgisayarlar olmasaydı yapay zeka bugünkü durumuna gelemeyecekti. İşte aslında pek 
    çok bilimsel ve teknolojik gelişmeler belli bir noktaya gelmiş ve yapay zeka dediğimiz bu alan 1950’lerde ortaya çıkmaya 
    başlamıştır. 

    Yapay zeka çalışmalarının ortaya çıkmasına yol açan önemli gelişmeler şunlar olmuştur:

    - Mantıktaki Gelişmeler: Bertrand Russell ve Alfred North Whitehead tarafından 1913 yılında yazılmış olan "Principia Mathematica" 
    adlı üç cilt kitap "biçimsel mantıkta (formal logic)" devrim niteliğinde etki yapmıştır.

    - Matematikteki Gelişmeler: 1930’larda Alonzo Church "Lambda Calculus" denilen biçimsel sistemi geliştirmiş ve özyinelemeli 
    fonksiyonel notasyonla hesaplanabilirliği araştırmış ve sorgulamıştır. Yine 1930’larda Kurt Gödel "biçimsel sistemler (formal 
    systems)" üzerindeki çalışmalarıyla teorik bilgisayar bilimlerinin öncülüğünü yapmıştır.

    - Turing Makineleri: Alan Turing’in henüz elektronik bilgisayarlar gerçekleştirilmeden önce 1930’lu yılların ortalarında 
    (ilk kez 1936) tasarladığı teorik bilgisayar yapısı olan "Turing Makineleri" bilgisayar bilimlerinin ve yapay zeka kavramının 
    ortaya çıkmasında etkili olmuştur. (Turing mekinelerinin çeşitli modelleri vardır. Bugün hala Turing makineleri algoritmalar 
    dünyasında algoritma analizinde ve algoritmik karmaşıklıkta teorik bir karşılaştırma aracı olarak kullanılmaktadır.)
    
    - Elektronik Bilgisayarların Ortaya Çıkması: 1940’lı yıllarda ilk elektronik bilgisayarlar gerçekleştirilmeye başlanmıştır. 
    Bilgisayarlar yapay zeka çalışmalarının gerçekleştirilmesinde en önemli araçlar durumundadır.

    Yapay Zeka (Artificial Intelligence) terimi ilk kez John McCarthy tarafından 1955 yılında uydurulmuştur. John McCarthy, 
    Marvin Minsky, Nathan Rocheste ve Claude Shannon tarafından 1956 yılında Dartmauth College’de bir konferans organize edilmiştir. 
    Bu konferans yapay zeka kavramının ortaya çıkışı bakımından çok önemlidir. Bu konferans yapay zekanın doğumu olarak kabul 
    edilmektedir.  Yapay zeka terimi de bu konferansta katılımcılar tarafından kabul görmüştür. John McCarthy aynı zamanda 
    dünyanın ilk programlama dillerinden biri olan Lisp’i de 1958 yılında tasarlamıştır. Lisp hala yapay zeka çalışmalarında 
    kullanılmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                10. Ders - 28/01/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka çalışmalarının "yaz dönemleri" ve "kış dönemleri" olmuştur. Buradaki "yaz dönemleri" konuya ilginin arttığı, 
    finansman sıkıntısının azaldığı, çeşitli kurumların yapay zeka çalışmaları için fonlar ayırdığı dönemleri belirtmektedir. 
    "Kış dönemleri" ise yaz dönemlerinin tersine konuya ilginin azaldığı, finansman sıkıntısının arttığı, kurumların yapay zeka 
    çalışmaları için fonlarını geri çektiği dönemleri belirtmektedir.

    1956-1974 yapay zekanın altın yılları olmuştur. Bu yıllar arasında çeşitli algoritmik yöntemler geliştirilmiş ve pek çok 
    uygulama üzerinde çalışılmıştır. Örneğin arama (search) yöntemleri çeşitli problemlere uygulanmış ve arama uzayı (search 
    space) sezgisel (heuristic) yöntemlerle daraltılmaya çalışılmıştır. Yine bu yıllarda doğal dili anlamaya yönelik ilk çalışmalar 
    gerçekleştirilmiştir. Bu ilk çalışmalardan elde edilen çeşitli başarılar yapay zeka alanında iyimser bir hava estirmiştir. 
    Örneğin:

    - 1958 yılında Simon ve Newell "10 yıl içinde dünya satranç şampiyonunun bir bilgisayar olacağını" iddia etmişlerdir. 
    (Halbuki bu durum 90'lı yılların ikinci yarısında gerçekleşmeye başlamıştır.)

    - 1970 yılında Minsky 3 yıldan 8 yıla kadar makinelerin ortalama bir insan zekasına sahip olabileceğini iddia etmiştir. 

    1974-1980 yılları arasında yapay zeka alanında kış dönemine girilmiştir. Daha önce yapılan tahminlerin çok iyimser olduğu 
    görülmüş bu da biraz hayal kırıklığına yol açmıştır. Bu yıllarda yapay sinir ağları çalışmaları büyük ölçüde durmuştur. 
    Yeni projeler için finansman elde edilmesi zorlaşmıştır.

    1980'li yıllarla birlikte yapay zeka çalışmalarında yine yükseliş başlamıştır. 80'li yıllarda en çok yükselişe geçen yapay 
    zeka alanı "uzman sistemler" olmuştur. Japonya bu tür projelere önemli finansman ayırmaya başlamıştır. Ayrıca Hopfield ve 
    Rumelhart'ın çalışmaları da "yapay sinir ağlarına" yeni bir soluk getirmiştir. 

    1987-1993 yılları arasında yine yapay zeka çalışmaları kış dönemine girmiştir. Konuya ilgi azalmış ve çeşitli projeler için 
    finans kaynakları da kendilerini geri çekmiştir.

    1993 yılından itibaren yapay zeka alanı yeniden canlanmaya başlamıştır. Bilgisayarların güçlenmesi, Internet teknolojisinin 
    gelişmesi, mobil aygıtların gittikçe yaygınlaşması sonucunda veri analizinin önemi artmış ve bu da yapay zeka çalışmalarına 
    yeni bir boyut getirmiştir. 1990'lı yılların ortalarından itibaren veri işlemede yeni bir dönem başlamıştır. Veri madenciliği 
    bir alan olarak kendini kabul ettirmiştir. Özellikle 2011 yılından başlayarak büyük veri (big data) analizleri iyice yaygınlaşmış, 
    yapay sinir ağlarının bir çeşidi olan "derin öğrenme (deep learning)" çalışmaları hızlanmış, IOT uygulamaları da yapay zekanın 
    önemini hepten artırmıştır.

    2017 yılından itibaren "dönüştürücülerin (transformers)" ortaya atılması ve "büyük dil modellerinin (large language models)" 
    geliştirilmesi ile birlikte yapay zeka alanında yeni bir döneme girilmiştir. Bu yeni dönem yaşamın her alanında devrim 
    niteliğinde etkiler oluşturmaya başlamıştır. Artık insan uygarlığı yeni bir çağa adım atmıştır ve hiçbir şey artık eskisi 
    gibi olmayacaktır. Bu yapay zeka devrimi ekonomiyi, üretim ilişkilerini, meslekleri, kurumları, sosyal yaşamı, kısacası her 
    şeyi etkileyip dönüştürecek bir potansiyele sahip gibi görünmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka aslında pek çok alt konuya ayrılabilen bir alandır. Yapay zekanın önemli alt alanları şunlardır:

    - Makine Öğrenmesi
    - Yapay Sinir Ağları ve Derin Öğrenme (belli bir süredir "Makine Öğrenmesinin" bir alt konusu olarak da ele alınmaktadır)
    - Robotik Sistemlerin Tasarımı ve Gerçekleştirilmesi
    - Bulanık Sistemler (Fuzzy Logic Systems)
    - Evrimsel Yöntemler (Genetic Algoritmalar, Differential Evoluation, Neuroevolution vs. )
    - Üst Sezgisel (Meta Heuristic) Yöntemler (Karınca Kolonisi, Particle Swarm Optimization)
    - Olasılıksal (Probabilistic) Yöntemler (Bayesian Network, Hidden Markov Model, Kalman Filter vs.)
    - Uzman Sistemler (Expert Systems)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zekanın yukarıdaki uygulama alanlarından bazıları zaman içerisinde bazı kesimler tarafından artık yapay zekanın bir 
    konusu olarak görülmemeye başlanmıştır. Örneğin OCR işlemleri eskiden her kesim tarafından bir yapay zeka faaliyeti olarak 
    görülürdü. Ancak zamanla OCR işlemleri o kadar bilindik ve rutin bir hale geldi ki artık bu işlemler geniş bir kesim tarafından 
    bir yapay zeka faaliyeti olarak ele alınmıyor. Benzer biçimde satrançta bir büyük ustayı yenecek programın yazılması eskiden 
    bir yapay zeka faaliyeti olarak görülüyorken artık bu faaliyet de bazı kesimler tarafından bir yapay zeka faaliyeti olarak 
    ele alınmamaktadır. İşte zaman içerisinde "yapay zeka olarak görülen bir süreci makineler algoritmik olarak başardıkça onun 
    yapay zeka alanından çıkartılması" gibi bir durum oluşmaya başlamıştır. Eskiden yapay zeka faaliyeti olarak adlandırılan bazı 
    faaliyetlerin yapay zeka kapsamından çıkarılmasına "Yapay Zeka Etkisi (AI Effect)" denilmektedir. Günümüzde de yapay zeka 
    faaliyeti olarak ele aldığımız bazı faaliyetlerin zamanla yapay zeka faaliyeti olmaktan çıkabileceğine de dikkatinizi çekmek 
    istiyoruz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de makine öğrenmesinin ne anlama geldiği üzerinde duracağız. Ancak makine öğrenmesinin ne olduğundan önce öğrenmenin 
    ne olduğunu ele almak gerekir. Psikolojide öğrenme "davranışta göreli biçimde kalıcı değişiklikler oluşturan süreçler" 
    biçiminde tanımlanmaktadır. Bu tanımdaki davranış (behavior) klasik davranışçılara göre "gözlemlenebilen devinimleri" 
    kapsamaktadır. Ancak daha sonra "radikal davranışçılar" bu davranış tanımını zihinsel süreçleri de kapsayacak biçimde
    genişletmiştir.

    Psikolojide öğrenme kabaca dört bölümde ele alınmaktadır:

    1) Klasik Koşullanma (Classical Conditioning)
    2) Edimsel Koşullanma (Operant Conditioning)
    3) Sosyal Bilişsel Öğrenme (Social Cognitive Learning)
    4) Bilişsel Öğrenme (Cognitive Learning)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            11. Ders - 03/02/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Organizmada doğal bir tepki oluşturan uyaranlara "koşulsuz uyaranlar (unconditioned stimuli)", koşulsuz uyaranlara karşı 
    organizmanın verdiği tepkilere de "koşulsuz tepkiler (unconditioned responses)" denilmektedir. Örneğin "gök gürültüsü" 
    koşulsuz uyarana, gök gürültüsüne karşı verilen tepki de koşulsuz tepkiye örnek verilebilir. İşte klasik koşullanmada 
    başlangıçta organizmada bir tepkiye yol açmayan "nötr bir uyaran (neutral stimulus)" koşulsuz bir uyaranla (unconditioned 
    stimulus) zamansal bakımdan eşleştirildiğinde artık bu nötr uyaran organizmada koşulsuz uyaranın oluşturduğu tepkiye 
    benzer bir tepki oluşturmaya başlamaktadır. Nötr uyaranın zamanla koşulsuz uyaranla benzer tepkilere yol açması durumunda 
    artık bu nötr uyarana "koşullu uyaran (conditioned stimulus)", organizmanın da bu koşullu uyarana verdiği tepkiye "koşullu 
    tepki (conditioned response)" denilmektedir. Bu süreci şekilsel olarak aşağıdaki gibi ifade edebiliriz:

    Koşulsuz Uyaran  --->   Koşulsuz Tepki    (Doğal Durum)

    Nötr Uyaran      --->   Koşulsuz Uyaran   (Klasik Koşullanma Süreci)
    Nötr Uyaran      --->   Koşulsuz Uyaran   (Klasik Koşullanma Süreci)
    ...              --->   ...               (Klasik Koşullanma Süreci)

    Nötr uyaran artık koşullu uyaran haline gelmiştir:

    Koşullu Uyaran   --->   Koşullu Tepki     (Öğrenilmiş Yeni Durum)

    Klasik koşullanma sürecinin gerçekleşmesi için önce nötr uyaranın sonra koşulsuz uyaranın zamansal bakımdan peş peşe uygulanması 
    gerekmektedir. Burada iki uyaran arasındaki zaman uzarsa (örneğin 5 saniyeden büyük olursa) organizmanın onları ilişkilendirmesi 
    güçleşmektedir. Pek çok çalışma 0.5 saniye civarındaki bir zaman aralığının klasik koşullanma için ideal bir zaman aralığı 
    olduğunu göstermektedir. 

    Klasik koşullanma ilk kez Ivan Pavlov tarafından fark edilmiş ve tanımlanmıştır. Pavlov'un köpek deneyi klasik koşullanmaya 
    tipik bir örnektir. Bu deneyde Pavlov köpeğe yemek gösterdiğinde (koşulsuz uyaran) köpek salya salgılamaktadır. Yemeği 
    görünce köpeğin salya salgılaması doğal olan koşulsuz bir tepkidir. Daha sonra Pavlov köpeğe bir metronum sesinden (nötr 
    uyaran) sonra yemeği göstermiş ve bu işlemi bir süre tekrarlamıştır.  Bu süreç sonunda artık köpek yalnızca metronom sesini 
    duyduğunda (artık nötr uyaran koşullu uyaran haline gelmiştir) salgı salyalar hale gelmiştir. Köpeğin metronom sesini duyduğunda 
    salgı salgılaması daha önce yapmadığı koşullu bir tepkidir.

    Klasik koşullanma süreci pek çok hayvan üzerinde denenmiştir. Hayvanların çok büyük çoğunluğu klasik koşullanma ile 
    öğrenebilmektedir. Birinci kuşak davranışçılar pek çok davranışın nedenini klasik koşullanmayla açıklamışlardır. Gerçekten 
    de fobilerin çoğunda klasik koşullanmanın etkili olduğu görülmektedir. Özellikle olumsuz birtakım sonuçlar doğuran uyaranlar
    çok kısa süre içerisinde klasik koşullanmaya yol açabilmektedir. Örneğin karanlık bir sokakta saldırıya uğrayan bir kişi 
    yeniden karanlık bir sokağa girdiğinde klasik koşullanma etkisiyle yeniden saldırıya uğrayacağı hissine kapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Edimsel koşullanma (operant conditioning) en önemli öğrenme yollarından biridir. Pek çok süreç edimsel koşullanma yoluyla 
    öğrenilmektedir. Klasik koşullanmada önce uyaran sonra tepki gelmektedir. Halbuki edimsel koşullanmada önce tepki sonra 
    uyaran gelir. Organizma bir faaliyette bulunur. Bunun sonucunda hoşa giden bir durum (buna ödül de denilmektedir) oluşursa 
    bu davranış tekrarlanır ve böylece öğrenme gerçekleşir. Yani özetle organizmada hoşa giden sonuçlar doğuran davranışlar 
    tekrarlanma eğilimindedir.

    Edimsel koşullanma bir süreç olarak ilk kez Edward Thorndike tarafından fark edilmiştir. Thorndike "puzzle box" ismini 
    verdiği bir kafes düzeneği ile yaptığı deneylerden "organizmada olumlu sonuçlar doğuran tepkilerin tekrarlanma eğiliminde
    olduğu" sonucunu çıkarmıştır ve bu durumu "etki yasası (law of effect)" terimiyle ifade etmiştir. Her ne kadar edimsel 
    koşullanmayı gerçek anlamda ilk kez Thorndike fark ettiyse de bunu genişleterek kuramsal bir öğrenme modeli haline getiren 
    asıl kişi B.F. Skinner olmuştur. Bu konuda kullanılan terminoloji de büyük ölçüde Skinner tarafından oluşturulmuştur.

    Edimsel koşullanmada ödül oluşturan uyaranlara "pekiştireç (reinforcer)" denilmektedir. Davranış ne kadar pekiştirilirse 
    o kadar iyi öğrenilmektedir. Pekiştireçler "pozitif" ve "negatif" olmak üzere ikiye ayrılmaktadır. Pozitif pekiştireçler 
    doğrudan organizmanın hoşuna gidecek uyaranlardır. Negatif pekiştireçler ise organizmanın içinde bulunduğu hoş olmayan
    durumu ortadan kaldırarak dolaylı ödül oluşturan uyaranlardır. Edimsel koşullanma için bazı örnekler şöyle verilebilir:

    - Ödevini yapan öğrenciye öğretmenin ödül vermesi ödev yapma davranışını artırmaktadır (pozitif pekiştireç).
    - Maddenin bunaltıyı (anxiety) ortadan kaldırması kişiyi madde kullanımına teşvik etmektedir (negatif pekiştireç).
    - Arabada emniyet kemeri bağlı değilken rahatsız edici bir ses çıkmaktadır. Bu sesi ortadan kaldırmak için sürücü ve yolcular 
    emniyet kemerini bağlarlar (negatif pekiştireç).
    - Ağlayan çocuğun isteklerini ebeveynin karşılaması istekleri karşılanmayan çocukta ağlama davranışını artırabilmektedir 
    (pozitif pekiştireç).

    Bugün psikolojide edimsel koşullanma davranışı değiştirmede ve şekillendirmede en önemli araçlardan biri olarak kabul 
    edilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sosyal bilişsel öğrenmeye (social cognitive learning) taklit yoluyla öğrenme" ya da "model alarak öğrenme" de denilmektedir. 
    Biz başkalarını taklit ederek de davranışlarımızı değiştirebilmekteyiz. Sosyal bilişsel öğrenme büyük ölçüde Albert Bandura 
    tarafından kuramsal hale getirilmiştir. Aslında sosyal bilişsel öğrenmede de bir bakıma pekiştirmeler söz konusudur. Ancak 
    bu pekiştirmeler doğrudan değil dolaylı (vicarious) biçimde olmaktadır. Sosyal bilişsel öğrenmede birtakım bilişsel süreçlerin 
    de devreye girdiğine dikkat ediniz.  Çünkü bu süreçte kişinin başkalarının yaptığı davranışları izleme, izlediklerini bellekte 
    saklama ve onlardan sonuçlar çıkartma gibi süreçler de işin içine karışmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biliş (cognition) organizmanın bilgi işlem faaliyetlerini anlatan bir terimdir. Biliş denildiğinde düşünme, bellek, dikkat, 
    bilinç, akıl yürütme gibi faaliyetler anlaşılmaktadır. Araştırmacılar hiç pekiştireç olmadan öğrenmenin insanlarda ve bazı 
    hayvanlarda mümkün olduğunu göstermişlerdir. Yani biz klasik koşullanma, edimsel koşullanma ve sosyal öğrenme süreçleri 
    olmadan yalnızca bilişsel etkinliklerle de öğrenebilmekteyiz. 

    Halk arasında öğrenme denildiğinde genellikle sürecin davranışsal boyutu göz ardı edilmekte yalnızca bilişsel tarafı 
    değerlendirilmektedir. Halbuki her türlü öğrenmede açık ya da örtük göreli bir biçimde kalıcı bir davranışın ortaya çıkması 
    beklenir. Ancak "davranış (behavior)" sözcüğünün tanımı konusunda da tam bir anlaşma bulunmamaktadır. Yukarıda da belirttiğimiz 
    gibi ilk davranışçılar yalnızca gözlemlenebilen süreçleri davranış olarak tanımlarken radikal davranışçılar zihinsel süreçleri 
    de davranış tanımının içine katmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    O halde makine öğrenmesi (machine learning) nedir? Aslında psikolojideki öğrenme kavramı makine öğrenmesinde de geçerlidir. 
    Biz makinenin (makine demekle donanımı ve yazılımı kastediyoruz) bir biçimde davranışını arzu edilen yönde değiştirmesini 
    isteriz. Yani makinenin davranışı bizim istediğimiz yönde ve istediğimiz hedefleri gerçekleştirme anlamında değişmelidir. 
    İşte makine öğrenmesi kabaca geçmiş bilgilerden ve deneyimlerden faydalı birtakım sonuçlar (davranışlar) ortaya çıkartan 
    algoritmalar ve yöntemler topluluğudur. Makine öğrenmesinde üç bileşen vardır: Deneyim, Görev ve Performans. Deneyim 
    canlılarda olduğu gibi makine öğrenmesinde de en önemli öğelerdendir. Makine öğrenmesinde deneyim verilerin işlemelere 
    sokularak  onlardan faydalı birtakım sonuçların çıkartılması sürecidir. Görev makinenin yapmasını istediğimiz şeydir. 
    Görevler düşük bir deneyimle düşük bir performansla gerçekleştirilebilirler. Deneyim arttıkça görevin yerine getirilme 
    performansı da artabilir. İşte makine öğrenmesi temelde bunu hedeflemektedir. O halde makine öğrenmesinde bir veri grubu 
    incelenir, analiz edilir, bundan sonuçlar çıkartılır, sonra hedeflenen görev yerine getirilmeye çalışılır. Bu görevin yerine 
    getirilmesi de gitgide iyileştirilir. Bu süreç çeşitli algoritmalarla ve yöntemlerle değişik biçimlerde ve yaklaşımlarla 
    yürütülmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesi genel olarak üç bölümde ele alıp incelenmektedir:

    1) Denetimli Öğrenme (Supervised Learning)
    2) Denetimsiz Öğrenme (Unsupervised Learning)
    3) Pekiştirmeli Öğrenme (Reinforcement Learning) 

    Denetimli (supervised) öğrenmede makineye (yani algoritmaya) biz daha önce gerçekleşmiş olan olayları ve sonuçları girdi 
    olarak veririz. Makine bu olaylarla sonuçlar arasında bağlantı kurar. Daha sonra biz yeni bir olayı makineye verdiğimizde 
    onun sonucunu makineden kestirmesini isteriz. Denetimsiz öğrenmede biz makineye yalnızca olayları veririz. Makine bunların 
    arasındaki benzerliklerden ve farklılıklardan hareketle bizim istediğimiz sonuçları çıkartmaya çalışır. Örneğin biz makineye 
    resimler verip bunların elma mı armut mu olduğunu söyleyelim. Ve bunu çok miktarda yapalım. Sonra ona bir elma resmi 
    verdiğimizde o daha önceki deneyimlerden hareketle bunun elma olduğu sonucunu çıkartabilecektir. İşte bu denetimli öğrenmeye 
    bir örnektir. Şimdi biz makineye elma ve armut resimlerini verelim ama bunların ne olduğunu ona söylemeyelim. Ondan bu 
    resimleri ortak özelliklerine göre iki gruba ayırmasını isteyelim. Bu da denetimsiz öğrenmeye örnektir. Pekiştirmeli öğrenmede 
    ise tıpkı edimsel koşullanmada olduğu gibi hedefe yaklaşan durumlar ödüllendirilerek makinenin öğrenmesi sağlanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka, makine öğrenmesi ve veri bilimi alanlarının diğer pek çok disiplinle yakın ilgisi vardır. Bu ilişkileri açıklamak 
    istiyoruz.

    Bilgisayar Bilimleri (Computer Science): Bilgi işlem ve programlama etkinlikleriyle ilgili geniş kapsamlı bir bilim dalıdır.
    Bilgisayar bilimlerindeki teknikler ve yöntemler kullanılmadan makine öğrenmesi uygulamaları gerçekleştirilememektedir. 

    İstatistik: İstatistik temelde iki bölüme ayrılmaktadır:

    - Betimsel istatistik (descriptive statistics)
    - Çıkarımsal istatistik (inferential statistics)

    Betimsel istatistik verilerin gruplanması, özetlenmesi, karakteristiklerinin betimlenmesi ve gösterilmesi ile ilgilidir. 
    Yani betimleyici istatistik "zaten var olan durumu" betimlemektedir. Çıkarımsal istatistik ise "kestirim yapmakla" ilgilidir. 
    Makine öğrenmesi ve veri bilimi istatistiğin kestirimsel yöntemlerini açıkça kullanmaktadır. Örneğin regresyon analizi, 
    kümeleme analizi, karar ağaçları, faktör analizi gibi pek çok makine öğrenmesi yöntemi aslında istatistiğin bir konusu 
    olarak ortaya çıkmıştır. Ancak bu istatistiksel yöntemler makine öğrenmesi temelinde genişletilmiş ve dinamik bir biçime 
    dönüştürülmüştür. 

    Matematik: Makine öğrenmesi ve veri bilimi uygulamalarında pek çok matematiksel yöntemlerden de faydalanılmaktadır. Örneğin 
    makine öğrenmesinde matematiğin çok değişkenli fonksiyonlar, limit, türev gibi konuları optimizasyon süreçlerinde sıkça 
    kullanılmaktadır. 

    Veri Madenciliği (Data Mining): Veri madenciliği verilerin içerisinden çeşitli faydalı bilgilerin bulunması, onların çekilerek 
    elde edilmesi ve bunlarla ilişkin süreçlerle ilgilenmektedir. Şüphesiz bu süreçler istatistiksel birtakım bilgilerin yanı 
    sıra yazılımsal uygulamaları da bünyesinde barındırmaktadır. (Gerçek madenciliğin "zaten var olan olan değerli bir madeni 
    çıkarmakla" ilgili olduğuna dikkat ediniz. Veri madenciliği de "verinin içinde zaten var olan değerli unsurların" açığa 
    çıkarılması ile ilgilidir. Bu nedenle veri madenciliğindeki "madencilik" teriminin süreci anlatmak için uygun bir sözcük 
    olduğu söylenebilir.) 

    İlgili Konudaki Özel Bilgiler (Domain Specific Knowledge): Şüphesiz her türlü algoritmik yöntem için bir biçimde hedeflenen 
    konuda belli bir bilgi birikiminin var olması gerekir. Örneğin ne kadar iyi programlama ve istatistik bilirseniz bilin 
    görüntüsel verilerle çalışmak için bir görüntünün (resmin) nasıl bir organizasyona sahip olduğunu bilmeniz gerekir. Ya da 
    örneğin hiç muhasebe bilmeyen iyi bir programcının bir muhasebe programı yazabilmesini bekleyebilir miyiz?
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Veri bilimi (data science) ve makine öğrenmesi uygulamaları için pek çok dil kullanılabilmektedir. Bu bağlamda ilk akla 
    gelen dil şüphesiz Python'dur. Ancak C++, Java, C# gibi popüler programlama dilleri de bu amaçla gittikçe daha fazla kullanılır 
    hale gelmektedir. Python Programlama Dili son on yıldır bir atak yaparak dünyanın en popüler ilk üç dili arasına girmiştir.  
    Python dilinin özellikle veri bilimi ve makine öğrenmesi konusunda popülaritesinin neden bu kadar arttığına ilişkin görüşlerimiz 
    şöyledir:

    - Son yıllarda veri işleme ve verilerden kestirim yapma gereksinimi gittikçe artmıştır ve Python dili de veri bilimi için 
    iyi bir araç olarak düşünülmektedir.    

    - Veri bilimi ve makine öğrenmesi için Python dilinden kullanılabilecek pek çok kütüphane vardır. (Bu konudaki kütüphaneler 
    diğer dillerden -şimdilik- daha fazladır.)

    - Python nispeten basit bir dildir. Bu basitlik ana hatları veri analizi olan konularda uygulamacılara kolaylıklar sunmaktadır. 
    Bu nedenle Python diğer disiplinlerden gelip de veri bilimi ve makine öğrenmesi uygulaması yapmak isteyenler için nispeten 
    daha kolay bir araç durumundadır. 

    - Python genel amaçlı bir programalama dili olmasının yanı sıra aynı zamanda matematiksel alana da yakın bir programlama 
    dilidir. Yani Python'un matematiksel alana yönelik ifade gücü (expressivity) popüler diğer programlama dillerinden daha 
    yüksektir. 

    - Python dilinin çeşitli prestijli üniversitelerde "programlamaya giriş (introduction to programming)" gibi derslerde kullanılmaya 
    başlanmış olması onun popülaritesini artırmıştır. Ayrıca Python özellikle 3'lü versiyonlarla birlikte dikkate değer biçimde 
    iyileştirilmiştir.

    - Python dilinin veri bilimi için diğer dillere göre daha erken yola çıktığı söylenebilir. Bu alanda algoritma geliştiren 
    araştırmacılar algoritmalarını daha çok Python kullanarak gerçekleştirmişlerdir. 

    Peki Python dilinin veri bilimi ve makine öğrenmesi konusunda hangi dezavantajları vardır? Bu dezavantajları da şöyle 
    sıralayabiliriz:

    - Python nispeten yavaş bir dildir. Bu yavaşlık büyük ölçüde Python dilinin dinamik tür sistemine sahip olmasından, Python 
    programlarının yorumlayıcı yoluyla çalıştırılmasından ve dilin seviyesinin yüksek olmasından kaynaklanmaktadır. Her ne kadar 
    veri analizi ve makine öğrenmesinde kullanılan kütüphaneler (NumPy, Pandas, SciPy, scikit-learn, TensorFlow, Pytorch gibi) 
    asıl olarak C ve C++ programlama dilleri ile yazılmış olsalar da bu C ve C++ rutinlerinin Python'dan çağrılması ve diğer 
    birtakım işlemler yavaşlığa yol açmaktadır.

    - Python yüksek seviyeli bir dil olduğu için dilin olanakları ince birtakım işlemlerin yapılabilmesine olanak sağlamamaktadır. 

    Peki Python genel olarak yavaş bir dilse bu durum veri bilimi ve makine öğrenmesi uygulamalarında bir sorun oluşturmaz mı? 
    İşte Python'un yavaşlığı ve yorumlayıcılarla çalışılan (interpretive) bir dil olması bazı projelerde Python'u uygun bir dil 
    olmaktan çıkartabilmektedir. Makine öğrenmesi konusunda çalışmalar yapan şirket ve kurumlardan bazıları önceleri Python'u
    ana dil olarak kullanırken daha sonra bir prototip dil olarak kullanmaya başlamıştır. (Burada prototip dil olarak kullanmak 
    demekle projeyi Python'da hızlı bir biçimde gerçekleştirildikten sonra ürün aşamasında onu yeniden C++, Java ve C# gibi 
    dillerle kodlamayı kastediyoruz.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bilgisayar teknolojisinin gelişmesine paralel olarak veri toplama ve depolama olanaklarının artmasıyla birlikte veri 
    analizinde yeni bir dönemin başladığı söylenebilir. Özellikle 2000'li yıllardan itibaren insanlık eskiden olduğundan çok 
    daha fazla miktarda veriyle karşılaşmıştır. Bu bakımdan insanlığın bir veri bombardımanına maruz kaldığını söyleyebiliriz.
    İşte "veri bilimi (data science)" 2000'li yılların başlarında böyle bir bağlamda kullanılmaya başlanmış bir terimdir. Veri 
    biliminin tanımı konusunda tam bir fikir birliği bulunmamaktadır. Ancak veri bilimi "verilerden birtakım faydalı bilgilerin 
    ve içgörülerin (insights) elde edilmesine yönelik çalışmaların yapıldığı bir alan" olarak tanımlanabilir. Veri bilimi ile 
    uğraşan uygulamacılara "veri bilimcisi (data scientist)" denilmektedir. (Bu bağlamda veri bilimcisinin bir bilim insanı gibi 
    ele alınmadığını vurgulamak istiyoruz.)

    Aslında veri bilimi istatistik biliminin uygulamalı ve dinamik bir alt alanı gibi de düşünülebilir. Bazı bilim insanlarına 
    göre "veri bilimi" terimi gereksiz biçimde uydurulmuş bir terimdir ve aslında bu alan "uygulamalı istatistikten" başka bir 
    şey değildir. Ancak ne olursa olsun son 30 senedir verilerin işlenmesi ve bunlardan faydalı birtakım sonuçların çıkarılması 
    için yapılan işlemlerin klasik istatistiksel çalışma ile örtüşmediği de açıktır. Veri bilimi terimi -bazı çevreler tarafından 
    eleştiriliyor olsa da- kendini kabul ettirmiş ve yaygınlık kazanmış bir terim gibi görünmektedir.  

    İstatistik ile veri bilimi arasındaki farklılıkları birkaç cümleyle şöyle özetleyebiliriz: Veri bilimi klasik istatistikten 
    farklı olarak disiplinler arası bir niteliğe sahiptir. Veri bilimi yazılımı çok daha yoğun kullanmaktadır. Veri bilimi 
    örneklemlerden ziyade çok daha büyük verilerle uğraşma eğilimindedir. İstatistiksel çalışmalar daha çok hipotezleri doğrulamaya 
    odaklanırken veri bilimi daha çok faydalı hipotezler oluşturmaya odaklanmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                 12. Ders - 04/02/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve özellikle de makine öğrenmesi ile ilgili çalışmalar yapacak kişilerin belli düzeyde istatistiksel bilgilere 
    sahip olması gerekmektedir. Şüphesiz istatistik pek çok alt alanı olan geniş bir bilim dalıdır. Bu nedenle istatistiksel 
    konulara ilişkin pek çok ayrıntı vardır. Biz bu bölümde temel bilgiler vermekle yetineceğiz. Çeşitli ayrıntılar ilgili 
    konuların anlatıldığı bölümde gerektiğinde açıklanacaktır. (Örneğin "kümeleme analizi (cluster analysis)" aslında istatistikte 
    çok uzun süredir incelenen bir konudur. Ancak son yıllarda makine öğrenmesi bağlamında konunun önemi çok daha fazla artmış 
    ve bu bağlamda pek çok algoritmik yöntem geliştirilmiştir. Dolaysıyla örneğin kümeleme analizi çok değişkenli istatistiğin 
    bir konusu olduğu halde biz bu tekniğin ayrıntılarını "denetimsiz öğrenme (unsupervised learning)" içerisinde ele alacağız.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistikte ölçülen ya da ölçülmüş olan değerlerin sınıflarına genel olarak "ölçek (scale)" denilmektedir. Pek çok kişi 
    ölçeklerin yalnızca sayısal olduğunu sanmaktadır. Halbuki ölçekler başka biçimlerde de karşımıza çıkabilmektedir. İstatistikte 
    ölçekler tipik olarak dört sınıfa ayrılmaktadır:

    Kategorik (Nominal) Ölçekler: Bu ölçeklerde söz konusu kümenin elemanları kategorik olgulardır. Örneğin cinsiyet, renk, 
    coğrafi bölge gibi. Bu ölçekteki ölçülen ya da ifade edilen değerlerin sayısal karşılıkları yoktur. Örneğin "kadınlarla 
    erkekler arasında sigara içme miktarı arasında anlamlı bir fark olup olmadığını" anlamak için gerçekleştirilen bir araştırmada 
    ölçülmesi istenen değişkenlerden "cinsiyet" kategorik (nominal) bir ölçeğe ilişkindir. Benzer biçimde kişilerin renk tercihleriyle 
    ilgili bir araştırmada renkler (siyah, beyaz, kırmızı gibi) kategorik bir ölçekle ifade edilirler. 

    Sırasal (Ordinal) Ölçekler: Bu ölçeklerdeki değerler de birer kategori belirtmekle birlikte bu kategoriler arasında büyüklük 
    küçüklük ilişkisi söz konusudur. Örneğin eğitim durumu için kategorik değerler "ilköğretim", "lise", "üniversite" olabilir 
    ve bunlar arasında sıra ilişkisi vardır. Bu nedenle "eğitim durumu" bir sıralı ölçek belirtmektedir. 

    Aralıklı (Interval) Ölçekler: Aralıklı ölçekler sayısal bilgi içerirler. Bu tür ölçeklerde iki puan arasındaki fark aynı 
    miktar uzaklığı ya da yakınlığı ifade eder. Örneğin bir testte 20 puan alan 10 puan alandan beli miktarda daha iyidir. 
    30 puan alan da 20 puan alandan aynı miktar kadar daha iyidir. Bu tür ölçeklerde mutlak sıfır noktası yoktur. Başka bir 
    deyişle bu tür ölçeklerde sıfır "yokluğu" ya da "mevcut olmamayı" belirtmemektedir. Alınan puanlar her zaman belli bir 
    göreli orijine göre anlamlıdır. Örneğin aslında sınavlardan alınan puanlar böyle bir ölçek türündedir. Sınavdan sıfır 
    alınabilir. Ancak bu sıfır o kişinin o konu hakkında hiçbir şey bilmediği anlamına gelmez. Yani mutlak sıfır değildir. 
    Ya da örneğin ısı belirten "derece (celcius)" bir aralıklı ölçeği belirtmektedir. 50 derece ile 40 derece arasındaki ısı 
    farkı 40 derece ile 30 derece arasındaki fark kadardır ancak sıfır derece ısının olmadığı anlamına gelmez. Aralıklı 
    ölçeklerde oran oluşturmak anlamlı olmayabilmektedir. Örneğin 20 derecelik ısı ile 10 derecelik ısı arasında iki kat 
    bir oran vardır. Ancak biz 20 derecenin 10 dereceden iki kat daha sıcağı belirttiğini söyleyemeyiz. 

    Oransal (Ratio) Ölçekler: Bu ölçekler de sayısal bilgi içerirler. Oransal ölçekler aralık ölçeklerin tüm özelliklerine 
    sahiptirler. Ancak ek olarak oransal ölçeklerde mutlak bir sıfır noktası da vardır. Dolayısıyla puanlar arasındaki oranlar 
    mutlak olarak anlamlıdır. Örneğin uzunluk, kütle gibi temel fiziksel özellikler oransal ölçek türlerindendir. Bir nesnenin 
    uzunluğunun sıfır olması onun uzunluğunun olmadığı, kütlesinin sıfır olması da onun kütlesinin olmadığı anlamına gelmektedir. 
    Örneğin kişinin yaşı da oransal br ölçek belirtir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistikte verilerin merkezine ilişkin bilgi veren ölçülere "merkezi eğilim ölçüleri (measures of central tendency)" 
    denilmektedir. Merkezi eğilim ölçülerinin en yaygın kullanılanı "aritmetik ortalamadır". Aritmetik ortalama (mean) değerlerin
    toplanarak değer sayısına bölünmesiyle elde edilmektedir. 

    Aritmetik ortalama hesaplamak için çeşitli kütüphanelerde çeşitli fonksiyonlar hazır olarak bulunmaktadır. Örneğin Python'un 
    standart kütüphanesindeki statistics modülünde bulunan mean fonksiyonu aritmetik ortalama hesaplamaktadır. 

    >>> import statistics
    >>> a = [1, 2, 7, 8, 1, 5]
    >>> statistics.mean(a)
    4

    mean fonksiyonu herhangi bir dolaşılabilir nesneyi parametre olarak alabilmektedir. 

    NumPy kütüphanesindeki mean fonksiyonu eksen (axis) temelinde (yani satırsal ve sütunsal biçimde) ortalama hesaplayabilmektedir. 
    Örneğin:

    >>> import numpy as np
    >>> a = np.array([[1, 2, 3], [5, 6, 7], [8, 9, 10]])
    >>> np.mean(a, axis=0)
    array([4.66666667, 5.66666667, 6.66666667])

    NumPy'da mean fonksiyonu aynı zamanda ndarray sınıfının metodu biçiminde de bulunmaktadır. Örneğin:

    >>> import numpy as np
    >>> a = np.array([[1, 2, 3], [5, 6, 7], [8, 9, 10]])
    >>> a.mean(axis=0)
    array([4.66666667, 5.66666667, 6.66666667])

    Pandas kütüphanesinde Series ve DataFrame sınıflarının mean metotları aritmetik ortalama hesabı yapmaktadır. DataFrame 
    sınıfının mean metodunda default axis 0 biçimindedir. Yani sütunsal ortalamalar elde edilmektedir. Örneğin:

    >>> import pandas as pd
    >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    >>> df
    0  1  2
    0  1  2  3
    1  4  5  6
    2  7  8  9
    >>> df.mean()
    0    4.0
    1    5.0
    2    6.0
    dtype: float64

    Aritmetik ortalama aralıklı (interval) ve oransal (ratio) ölçeklere uygulanabilir. Aritmetik ortalama O(N) karmaşıklıkta 
    hesaplanabilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Diğer bir merkezi eğilim ölçüsü de "medyan (median)" denilen ölçüdür. Medyan küçükten büyüğe sıraya dizilmiş olan sayıların 
    ortasındaki değerdir. Ancak sayılar çift sayıda ise sayıların tam ortasında bir değer olmadığı için ortadaki iki değerin 
    aritmetik ortalaması medyan olarak alınmaktadır. Medyan işlemi uç değerlerden (outliers) etkilenmez. Ancak medyan işlemi 
    aritmetik ortalamadan daha fazla zaman alan bir işlemdir. Çünkü medyan için önce değerlerini sıraya dizilmesi gerekmektedir. 
    Dolayısıyla medyan işlemi O(N log N) karmaşıklıkta bir işlemdir. Medyan işlemi Python'un standart kütüphanesinde statistics 
    modülü içerisindeki median isimli fonksiyonla yapılabilmektedir. Örneğin:

    >>> import statistics
    >>> a = [1, 23, 56, 12, 45, 21]
    >>> statistics.median(a)
    22.0

    NumPy kütüphanesinde medyan işlemi eksensel biçimde median fonksiyonuyla yapılabilmektedir. Örneğin:

    >>> import numpy as np
    >>> a = np.random.randint(1, 100, (10, 10))
    >>> a
    array([[ 8, 26, 42, 26, 10, 66, 94, 91,  3, 97],
        [31, 62, 82, 86, 45, 73, 38, 29, 43, 62],
        [78, 49, 22, 32, 74, 15, 54, 59, 37, 87],
        [ 5, 75, 34, 82, 58, 63, 84, 40, 92, 20],
        [57, 21,  2, 65, 69, 37, 78,  9, 57,  9],
        [57,  6, 72, 17, 39, 13, 25, 49, 85, 46],
        [57, 47, 57, 47, 25, 40, 20, 72, 10, 16],
        [12, 83, 35, 89, 86, 84, 66, 54, 50, 38],
        [90, 88, 65, 82, 29, 18, 86, 37, 60, 70],
        [38, 17, 40, 81, 18, 89,  4, 22, 59, 65]])
    >>> np.median(a, axis=0)
    array([47.5, 48. , 41. , 73. , 42. , 51.5, 60. , 44.5, 53.5, 54. ])

    ndarray sınıfının median isimli bir metodu yoktur.

    Benzer biçimde Pandas kütüphanesinde de Series ve DataFrame sınıflarının median isimli metotları eksensel median işlemi 
    yapabilmektedir. Tabii DataFrame sınıfının median metodunun default ekseni yine 0'dır (yani sütunsal işlem yapılmaktadır).
    Örneğin:

    >>> a = np.random.randint(1, 100, (10, 10))
    >>> a
    array([[39, 73, 50, 26,  3, 87, 78, 49, 87, 39],
        [91, 39,  6, 99, 85,  1, 50, 59, 20, 19],
        [83, 91, 94, 54, 28, 84, 25, 41, 19, 83],
        [38, 46, 78, 56, 53,  3,  2,  8, 27, 42],
        [33, 32, 36, 61,  7,  6, 88, 72, 71, 88],
        [32, 71, 68, 46, 64, 70, 45, 92,  8, 64],
        [71, 42, 36, 82, 45, 17, 13, 63,  8, 60],
        [70, 56, 76, 59, 64, 28, 81, 62, 60, 72],
        [82, 66, 93, 44, 67, 53, 97, 92, 89, 44],
        [34,  2,  3, 75, 19, 17, 34, 89, 98, 14]])
    >>> df = pd.DataFrame(a)
    >>> df
        0   1   2   3   4   5   6   7   8   9
    0  39  73  50  26   3  87  78  49  87  39
    1  91  39   6  99  85   1  50  59  20  19
    2  83  91  94  54  28  84  25  41  19  83
    3  38  46  78  56  53   3   2   8  27  42
    4  33  32  36  61   7   6  88  72  71  88
    5  32  71  68  46  64  70  45  92   8  64
    6  71  42  36  82  45  17  13  63   8  60
    7  70  56  76  59  64  28  81  62  60  72
    8  82  66  93  44  67  53  97  92  89  44
    9  34   2   3  75  19  17  34  89  98  14
    >>> df.median()
    0    54.5
    1    51.0
    2    59.0
    3    57.5
    4    49.0
    5    22.5
    6    47.5
    7    62.5
    8    43.5
    9    52.0
    dtype: float64

    Tabii median işlemi de ancak sayısal verilere yani aralıklı ve oransal ölçeklere uygulanabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Merkezi eğilim ölçülerinin bir diğeri de "mod (mode)" denilen ölçüdür. Bir grup verideki en çok yinelenen değere "mod"
    denilmektedir. Mod özellikle kategorik ve sıralı ölçeklerde ortalamanın yerini tutan bir işlem olarak kullanılmaktadır. 
    Mod işlemi genel olarak O(N log N) karmaşıklıkta yapılabilmektedir. (Tipik mod algoritmasında değerler önce sıraya dizilir,
    sonra yan yana aynı değerlerden kaç tane olduğu tespit edilir.) Mod işlemi Python standart kütüphanesindeki statistics
    modülünde bulunan mode fonksiyonuyla yapılabilir. Örneğin:

    >>> a = [1, 3, 3, 4, 2, 2, 5, 2, 7, 9, 5, 3, 5, 7, 5]
    >>> statistics.mode(a)
    5

    Eğer en çok yinelenen değer birden fazla ise mode fonksiyonu dizilimde ilk karşılaşılan en çok yinelenen değere geri
    dönmektedir. 

    NumPy kütüphanesinde mod işlemini yapan bir fonksiyon bulunmamaktadır. Ancak SciPy kütüphanesinde mod işlemi için stats
    modülü içerisindeki mode fonksiyonu kullanılabilir. Bu fonksiyon yine eksensel işlemler yapabilmektedir. mmode fonksiyonu 
    tuple sınıfından türetilen ModeResult isimli bir sınıf türünden bir nesne verir.  Bu sınıfın mode ve count örnek öznitelikleri 
    en çok yinelenen değerleri ve onların sayılarını bize vermektedir. ModeResult sınıfı bir çeşit demet özelliği gösterdiği 
    için demet gibi de kullanılabilir. Örneğin:

    >>> import scipy.stats
    >>> import numpy as np
    >>> a = np.random.randint(1, 10, (20, 10))
    >>> a
    array([[2, 5, 1, 9, 9, 9, 1, 5, 1, 8],
        [3, 1, 1, 8, 2, 5, 3, 2, 5, 4],
        [7, 3, 7, 6, 1, 2, 4, 5, 3, 7],
        [8, 3, 9, 4, 9, 9, 4, 5, 1, 8],
        [6, 1, 6, 6, 3, 2, 2, 4, 3, 9],
        [1, 4, 5, 6, 4, 4, 6, 2, 7, 3],
        [6, 5, 7, 4, 5, 8, 5, 4, 4, 9],
        [5, 1, 4, 8, 8, 9, 6, 1, 8, 6],
        [8, 8, 5, 4, 2, 8, 6, 1, 1, 5],
        [3, 8, 4, 9, 7, 6, 6, 9, 6, 4],
        [5, 8, 1, 6, 7, 8, 7, 7, 6, 4],
        [7, 7, 1, 8, 8, 3, 1, 8, 3, 1],
        [5, 5, 5, 4, 9, 3, 8, 7, 9, 8],
        [8, 9, 9, 2, 7, 3, 3, 6, 2, 6],
        [1, 6, 6, 8, 4, 4, 3, 2, 2, 4],
        [1, 2, 4, 5, 8, 3, 1, 5, 1, 3],
        [5, 9, 3, 1, 1, 6, 9, 5, 4, 1],
        [3, 1, 4, 9, 1, 2, 5, 1, 3, 7],
        [8, 7, 3, 3, 7, 4, 1, 7, 6, 9],
        [9, 2, 1, 6, 5, 3, 7, 9, 5, 7]])
    >>> mr = scipy.stats.mode(a, axis=0)
    >>> mr.mode
    array([[5, 1, 1, 6, 7, 3, 1, 5, 1, 4]])
    >>> mr.count
    array([[4, 4, 5, 5, 4, 5, 4, 5, 4, 4]])

    Pandas kütüphanesinde Series ve DataFrame sınıflarının mode metotları da mod işlemi yapmaktadır. Örneğin:

    >>> a = np.random.randint(1, 10, (20, 10))
    >>> df = pd.DataFrame(a)
    >>> df
        0  1  2  3  4  5  6  7  8  9
    0   5  5  9  1  4  7  4  3  9  6
    1   1  3  2  1  6  8  4  4  3  3
    2   8  7  9  5  8  1  5  3  8  1
    3   8  7  1  7  1  7  5  8  7  6
    4   1  9  9  5  4  7  5  6  9  9
    5   7  9  1  7  9  6  5  4  7  8
    6   3  2  6  5  4  8  6  5  5  9
    7   4  9  6  9  5  4  9  4  4  7
    8   2  3  3  8  4  8  2  1  4  1
    9   7  6  6  1  7  3  5  1  6  9
    10  3  3  1  5  9  6  1  3  1  4
    11  4  6  2  1  1  1  6  3  2  1
    12  9  1  1  6  3  7  1  1  7  8
    13  8  3  3  5  9  1  1  8  2  4
    14  8  7  7  1  6  5  8  6  4  8
    15  4  8  4  9  2  6  7  9  2  1
    16  6  3  2  6  1  2  5  3  9  2
    17  3  9  3  6  9  1  9  7  9  4
    18  6  2  9  1  4  2  4  8  6  2
    19  3  2  7  2  5  9  1  7  9  2
    >>> df.mode()
    0    1  2    3    4  5    6    7    8    9
    0  3  3.0  1  1.0  4.0  1  5.0  3.0  9.0  1.0
    1  8  NaN  9  NaN  NaN  7  NaN  NaN  NaN  NaN

    DataFrame sınıfının mode metodu bize bir DataFrame nesnesi vermektedir. Uygulamacı genellikle bunun ilk satırı ile ilgilenir. 
    Diğer satırlar eşit miktarda tekrarlanan elemanlardan oluşmaktadır. Tabii belli bir sütunda eşit miktarda tekrarlanan 
    elemanların sayısı az ise artık geri döndürülen DataFrame'in o sütuna ilişkin satırlarında NaN değeri bulunacaktır.
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Değerlerin merkezine ilişkin bilgiler dağılım hakkında iyi bir fikir vermeyebilir. Örneğin iki ülkede kişi başına düşen 
    ortalama yıllık gelir (gayri safi milli hasıla) 15000 dolar olabilir. Ancak bu iki ülke arasında gelir dağılımında önemli 
    farklılıklar bulunuyor olabilir. O halde değerlerin ortalamasının yanı sıra onların ortalamaya göre nasıl yayıldıkları da 
    önemlidir. İstatistikte değerlerin ortalamaya göre yayılımı için "merkezi yayılım ölçüleri (measures of dispersion)" denilen 
    bazı ölçüler kullanılmaktadır. Merkezi yayılım ölçüleri aslında değerlerin ortalamadan ortalama uzaklığını belirlemeyi 
    hedeflemektedir. 

    Eğer biz değerleri ortalamadan çıkartıp onların ortalamasını alırsak 0 elde ederiz. Aşağıdaki programda değerlerin ortalamadan
    uzaklıklarının ortalaması bulunmuştur. (Bu tür durumlarda yuvarlama hatalarından dolayı sıfır yerine sıfıra çok yakın 
    değerler elde edilebilir.)
#----------------------------------------------------------------------------------------------------------------------------
   
import numpy as np

a = np.array([1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8])
mean = np.mean(a)
print(mean)

result = np.mean(a - mean)
print(result) 

#----------------------------------------------------------------------------------------------------------------------------
    Merkezi yayılım ölçüsü olarak "değerlerin ortalamadan ortalama mutlak uzaklığına" başvurulabilir. Burada mutlak değer 
    alınmasının nedeni uzaklıkları negatif olmaktan kurtarmak içindir. Bu durumda ortalama 0 çıkmaz. Ancak bu yöntem de aslında 
    çok iyi bir yöntem değildir. Aşağıda aynı dizilimin ortalamadan ortalama mutlak uzaklığı hesaplanmışır.
#----------------------------------------------------------------------------------------------------------------------------
   
import numpy as np

a = np.array([1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8])
mean = np.mean(a)
print(mean)

result = np.mean(np.abs(a - mean))
print(result)               # 2.5

#----------------------------------------------------------------------------------------------------------------------------
    Aslında ortalamadan ortalama uzaklık için "standart sapma (standard deviation)" denilen ölçü tercih edilmektedir. Standart 
    sapmada ortalamadan uzaklıkların mutlak değeri değil kareleri alınarak negatiflikten kurtulunmaktadır. Kare alma işlemi 
    değerleri daha fazla farklılaştırmaktadır. Aynı zamanda bu işlem bazı durumlarda başka faydalara da yol açmaktadır. 
    (Örneğin ileride bu kare alma işleminin optimizasyon problemleri için daha uygun bir işlem olduğunu göreceğiz.)

    Standart sapma, değerlerin ortalamadan farklarının karelerinin ortalamasının karekökü alınarak hesaplanmaktadır. Ancak 
    burada ortalama bulunurken değer sayısı n olmak üzere bölme n'e ya da (n - 1)'e yapılabilmektedir. Anakütle (population) 
    için n'e bölme yapılırken örneklemden hareketle anakütle standart sapmasının tahmin edileceği durumlarda (n - 1)'e bölme 
    yapılmaktadır. (n - 1)'e bölme işlemine "Bessel düzeltmesi (Bessel's correction)" denilmektedir. Genellikle standart sapma 
    hesaplayan fonksiyonlar kaça bölüneceğini "ddof (delta degrees of freedom)" parametresiyle programcıdan istemektedir. ddof 
    değeri "n eksi kaça bölüneceğini" belirtir. Örneğin ddof=0 ise n'e bölme ddof=1 ise (n - 1)'e bölme uygulanır. 

    Aşağıdaki örnekte NumPy kullanılarak bir standart sapma hesaplayan fonksiyon yazılmıştır. 
#----------------------------------------------------------------------------------------------------------------------------
   
import numpy as np

def sd(a, ddof = 0):
    return np.sqrt(np.sum((a - np.mean(a)) ** 2)  / (len(a)  - ddof))

a = [1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8]
result = sd(a)
print(result)           # 2.7688746209726918

#----------------------------------------------------------------------------------------------------------------------------
    Python Standart Kütüphanesinde statistics modülü içerisinde standart sapma hesabı yapan stdev ve pstdev fonksiyonları 
    bulunmaktadır. stdev fonksiyonu (n - 1)'e bölme yaparken pstdev (buradaki 'p' harfi "population" sözcüğünden gelmektedir) 
    fonksiyonu n'e bölme yapmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

import statistics

a = [1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8]

std = statistics.stdev(a)
print(std)                          # 2.891995221924885  

std = statistics.pstdev(a)
print(std)                          # 2.7688746209726918

#----------------------------------------------------------------------------------------------------------------------------
    NumPy kütüphanesinde std isimli fonksiyon eksensel standart sapma hesaplayabilmektedir. Fonksiyonun ddof parametresi 
    default durumda 0'dır. Yani default durumda fonksiyon n'e bölme yapmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

a = np.array([1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8])
result = np.std(a)
print(result)                       # 2.7688746209726918 

#----------------------------------------------------------------------------------------------------------------------------
    Pandas kütüphanesinde de Series ve DataFrame sınıflarının std isimli metotları eksensel standart sapma hesabı yapabilmektedir. 
    Ancak bu metotlarda ddof parametresi default 1 durumundadır. Yani bu metotlar default durumda (n - 1)'e bölme yapmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

s = pd.Series([1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8])
result = s.std()
print(result)                       # 2.891995221924885        

s = pd.Series([1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8])
result = s.std(ddof=0)
print(result)                       # 2.7688746209726918 

#----------------------------------------------------------------------------------------------------------------------------
    Standart sapmanın karesine "varyans (variance)" denilmektedir. Varyans işlemi Python standart kütüphanedeki statistics 
    modülünde bulunan variance ve pvariance fonksiyonlarıyla yapılmaktadır. NumPy kütüphanesinde varyans işlemi var fonksiyonuyla 
    ya da ndarray sınıfının var metoduyla, Pandas kütüphenesinin Series ve DataFrame sınıflarındaki var metoduyla yapılmaktadır.
    Yine NumPy'ın variance fonksiyonundaki ddof parametresinin default değeri 0, Pandas'ın variance fonksiyonundaki ddof
    parametresinin default değeri 1’dir.

    Peki neden standart sapma varken ayrıca onun karesi için varyans terimi uydurulmuştur? İşte istatistikte pek çok durumda
    aslında doğrudan ortalamadan farkların karesel ortalamaları (yani standart sapmanın karesi) kullanılmaktadır. Bu nedenle 
    bu hesaba ayrı bir isim verilerek anlatımlar kolaylaştırılmak istenmiştir. Aşağıda varyans işleminin nasıl yapıldığına 
    yönelik bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------
   
import statistics
import numpy as np
import pandas as pd

data = [1, 4, 6, 8, 4, 2, 1, 8, 9, 3, 6, 8]

result = statistics.pvariance(data)
print(result)               # 7.666666666666667

result = np.var(data)
print(result)               # 7.666666666666667

s = pd.Series(data)
result = s.var(ddof=0)
print(result)               # 7.666666666666667

#----------------------------------------------------------------------------------------------------------------------------
    Varyans hesaplamanın alternatif bir yolu daha vardır. Varyans değerlerin karelerinin ortalamasının değerlerin ortalamasının 
    karesinden çıkartılmasıyla da hesaplanabilmektedir:

    varyans = değerlerin karelerinin ortalaması - değerlerin ortalamasının karesi

    Varyansın bu formülü tek geçişle (single pass) varyans hesaplamakta çokça kullanılmaktadır. Klasik formülle varyans hesaplamak
    için çift geçiş gerekmektedir. Ancak bu yöntemde bir noktaya da dikkat etmek gerekir. Değerlerin kareleri çok büyümektedir.
    Bu durumda çok fazla büyük değerin bulunduğu veri kümelerinde karelerinm toplamı kullanılan veri türünün limitleri dışına 
    çıkabilir ya da yuvarlama hatalarından kaynaklanan sorunlar ortaya çıkabilir. 
    
    Aşağıda tek geçiş ile bu biçimde varyans hesabına bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------
   
total = 0
total_square = 0
count = 0

for x in a:
    total += x
    total_square += x ** 2
    count += 1

v = total_square / count - (total / count) ** 2
print(v)

#----------------------------------------------------------------------------------------------------------------------------
                                        13. Ders - 10/02/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir deney sonucunda oluşacak durum baştan tam olarak belirlenemiyorsa böyle deneylere "rassal deneyler (random experiments)"
    denilmektedir. Örneğin bir paranın atılması deneyinde para "yazı" ya da "tura" gelebilir. O halde "paranın atılması" 
    rassal bir deneydir. Benzer biçimde bir zarın atılması, bir desteden  bir kağıt çekilmesi birer rassal deneydir. Bir 
    deneyin sonucu önceden bilinebiliyorsa bu tür deneylere "deterministik deneyler" de denilmektedir. Bazı bilimlerdeki 
    süreçler deterministiktir. Ancak bazı bilimlerdeki süreçlerin sonucunda oluşacak durumlar önceden tam olarak kestirilememektedir. 
    Önceden sonucu tam olarak kestirilemeyen süreçlere "olasılıksal (probabilistic)" ya da "stokastik (stochastic)" süreçler 
    denilmektedir. 

    Deterministik süreçlerle olasılıksal süreçler aslında üzerinde çok düşünülmüş konulardandır. Kimilerine göre olasılıksal 
    süreç diye bir şey yoktur. Her şey deterministiktir. Bir sürecin olasılıksal olması sadece "bizim onun sonucunu 
    belirleyemememizden" kaynaklanmaktır. Örneğin paranın atılması sırasındaki tüm bilgilere sahip olsak artık bu deneyin sonucu 
    olasılıksal değil deterministik hale gelecektir. Tabii evrende kaotik süreçler söz konusudur. Yani bir olay başka bir 
    olayı etkilemekte ve küçük değişiklikler büyük sonuçlara yol açabilmektedir. Buna felsefede "kaos teorisi" halk arasında 
    da "kelebek etkisi" denilmektedir. Eğer evrende mutlak bir determinizm varsa evren reset konumuna alındığında her şey 
    yine bugüne aynı biçimde gelecektir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir rassal deney sonucunda oluşabilecek tüm olası durumların kümesine "örnek uzayı (sample space)" denilmektedir. Örneğin 
    bir paranın atılması deneyinde örnek uzayı S = {Yazı, Tura} biçimindedir. Biz zarın atılması deneyindeki örnek uzayı ise
    S = {1, 2, 3, 4, 5, 6} biçimindedir. İki zarın atılmasındaki örnek uzayı S = {(1, 1), (1, 2), (6, 5), (6, 6)} biçimindedir.

    Örnek uzayın her bir alt kümesine "olay (event)" denilmektedir. Örneğin bir zarın atılmasındaki bazı olaylar şunlar olabilir:

    E1 = {1, 3}
    E2 = {3, 4, 5}
    E3 = {6}

    Bir kümenin bütün alt kümelerine o kümenin "kuvvet kümesi (power set)" denilmektedir. Bir rassal deneydeki bütün olaylar
    kuvvet kümesi içerisindeki olaylardır.  (Bir kümenin toplam 2^n tane alt kümesi olduğunu anımsayınız.) 

    Örnek uzayın tek elemanlı olaylarına (yani alt kümelerine) "basit olay (simple events)" denilmektedir. Örneğin paranın 
    atılması deneyindeki basit olaylar {Yazı} ve {Tura} biçimindedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Olasılığın (probablity) değişik tanımları yapılabilmektedir. Olasılığın en yaygın tanımlarından birisi "göreli sıklık 
    (relative frequency)" tanımıdır. Bu tanıma göre bir rassal deney çok sayıda yinelendikçe elde edilen olasılık değerleri 
    belli bir değere yakınsamaya başlar. Örneğin bir paranın 100 kere atılmasında 50 kere yazı 50 tura gelmeyebilir. Ancak para 
    sonsuz sayıda atılırsa (ya da çok fazla sayıda atılırsa) tura gelme sayısının paranın atılma sayısına oranı 0.5'e yakınsayacaktır. 
    Buna istatistike "büyük sayılar yasası (law of large numbers)" da denilmektedir. 

    Aşağıdaki örnekte bir para değişik miktarlarda yazı tura biçiminde atılmıştır. Elde edilen oranlar gitgide 0.5'e yakınsayacaktır.
    Programın çalıştırılmasıyla şu değerler elde edilmiştir:

    head = 0.4, tail = 0.6
    head = 0.5, tail = 0.5
    head = 0.479, tail = 0.521
    head = 0.4977, tail = 0.5023
    head = 0.50005, tail = 0.49995
    head = 0.500075, tail = 0.499925
    head = 0.5002417, tail = 0.4997583
    head = 0.49997078, tail = 0.50002922
#----------------------------------------------------------------------------------------------------------------------------
   
import random

HEAD = 1

def head_tail(n):
    head = tail = 0

    for _ in range(n):
        val = random.randint(0, 1)
        if val == HEAD:
            head += 1
        else:
            tail += 1
    return head / n, tail / n
                    
head, tail = head_tail(10)
print(f'head = {head}, tail = {tail}')        

head, tail = head_tail(100)
print(f'head = {head}, tail = {tail}') 

head, tail = head_tail(1000)
print(f'head = {head}, tail = {tail}') 

head, tail = head_tail(10_000)
print(f'head = {head}, tail = {tail}') 

head, tail = head_tail(100_000)
print(f'head = {head}, tail = {tail}') 

head, tail = head_tail(1_000_000)
print(f'head = {head}, tail = {tail}') 

head, tail = head_tail(10_000_000)
print(f'head = {head}, tail = {tail}') 

head, tail = head_tail(100_000_000)
print(f'head = {head}, tail = {tail}') 

#----------------------------------------------------------------------------------------------------------------------------
    Olasılığın temel matematiksel teorisi Kolmogorov tarafından oluşturulmuştur. Kolmogorov üç aksiyom kabul edildiğinde bütün
    olasılık kurallarının teorem-ispat biçiminde açıklanabileceğini göstermiştir. Kolmogorov'un üç aksiyomu şöyledir:

    1) Örnek uzayının olasılığı 1'dir. Yani P(S) = 1 'dir. Buradan olasılığın en yüksek değerinin 1 olacağını ve tüm örnek 
    uzayının olma olasılığının 1 olduğunu anlamalıyız. Örneğin bir zarın atılmasındaki örnek uzayı {1, 2, 3, 4, 5, 6} biçimindedir. 
    P({1, 2, 3, 4, 5, 6}) = 1'dir. Buradaki P({1, 2, 3, 4, 5, 6}) bir zarın 1 ya da 2 ya da 3 ya da 4 ya da 5 ya da 6 gelme 
    olasılığı anlamındadır. 

    2) Herhangi bir olayın olasılığı 0 ya da 0'dan büyüktür. Yani P(E) >= 0'dır. Burada olasılığın en düşük değerinin 0 olduğu 
    belirtilmektedir. O halde olasılık değeri 0 ile 1 arasındadır. 

    3) Bir kümenin olasılığı demek rassal deney sonucunda o kümenin herhangi elemanının oluşma olasılığı demektir. Örneğin 
    zarın atılmasındaki P({3, 5}) olasılığı zarın üç ya da 5 gelme olasılığı anlamına gelir. İki küme ayrıksa (yani ortak 
    elemanları) yoksa bu iki kümenin birleşimlerinin olasılığı bu iki kümenin olasılık toplamlarına eşittir. Yani E1 ve E2 iki
    olay olmak üzere eğer bu iki olay ayrıksa (yani E1 ⋂ E2 = ∅ ise) P{E1 ∪ E2} = P{E1} + P{E2}'dir.

    Bu üç kural kabul edildiğinde kümeler teorisi, permütasyon, kombinasyon gibi işlemlerle tüm olasılık formülleri elde 
    edilebilmektedir.
#----------------------------------------------------------------------------------------------------------------------------
 
#----------------------------------------------------------------------------------------------------------------------------
    Olasılıkta ve istatistikte en çok kullanılan temel kavramlardan biri "rassal değişken (random variable)" denilen kavramdır. 
    Her ne kadar "rassal değişken" teriminde bir "değişken" sözcüğü geçiyorsa da aslında rassal değişken bir fonksiyon belirtmektedir. 
    Rassal değişken bir rassal deney ile ilgilidir. Bir rassal değişken örnek uzayın her bir elemanını (yani basit olayını)
    gerçek (reel) bir değere eşleyen bir fonksiyon belirtmektedir. Rassal değişkenler genellikle "sözel biçimde" ifade edilirler. 
    Ancak bir fonksiyon belirtirler. Rassal değişkenler matematiksel gösterimlerde genellikle büyük harflerle belirtilmektedir. 
    Örneğin:

    - Z rassal değişkeni "iki zar atıldığında zarların üzerindeki sayıların toplamını" belirtiyor olsun. Burada aslında Z
    bir fonksiyondur. Örnek uzayın her bir elemanını bir değere eşlemektedir. Matematiksel gösterimle Z rassal değişkeni
    şöyle belirtilebilir:

    Z: S -> R

    Burada Z fonksiyonunun S örnek uzayından R'ye bir fonksiyon belirttiği anlaşılmaıdır. Burada Z fonksiyonu aşağıdaki gibi 
    eşleme yapmaktadır:

    (1, 1) -> 2
    (1, 2) -> 3
    (1, 3) -> 4
    ...
    (6, 5) -> 11
    (6, 6) -> 12

    K rassal değişkeni "rastgele seçilen bir kişinin kilosunu belirtiyor" olsun. Bu durumda örnek uzayı aslında dünyaki 
    tüm insanlardır. Burada K fonksiyonu da her insanı onun kilosuna eşleyen bir fonksiyondur. 

    C rassal değişkeni "rastgele seçilen bir rengin RGB değerlerinin ortalamasını" belirtiyor olsun. Bu durumda her rengin
    bir RGB ortalaması vardır. Bu fonksiyon belli bir rengi alıp onun ortalamasını belirten bir sayıya eşlemektedir. 

    Rassal değişkenler kümeler üzerinde işlemler yapmak yerine gerçek sayılar üzerinde işlem yapmamızı sağlayan, anlatımlarda
    ve gösterimlerde kolaylık sağlayan bir kavramdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Her rassal değişkenin belli değerler almasının bir olasılığı vardır. Örneğin "iki zarın atılması deneyinde üste gelen sayılar
    toplamına ilişkin" R rassal değişkenini düşünelim. P(R = 5) demek S örnek uzayındaki 5 değerini veren kümenin olasılığı 
    demektir. Yani P(R = 5) ile aslında P({(1, 4), (4, 1) (2, 3), (3, 2)}) aynı anlamdadır. Buradaki P({(1, 4), (4, 1) (2, 3), 
    (3, 2)}) olasılığın "bu değerlerden herhangi birinin oluşmasına ilişkin" olasılık olduğunu anımsayınız. Bu olasılığın 
    değeri 1/9'dur.

    Bir rassal değişkenin olasılığı belirtilirken P harfi olasılığı anlatmaktadır. Ancak bu P harfinden sonra bazı kişiler 
    normal parantezleri bazı kişiler küme parantezlerini tercih ederler. Yani örneğin bazı kişiler P(R = 5) gibi bir gösterimi 
    tercih derken bazı kişiler P{R = 5} gösterimini tercih etmektedir. Küme parantezli gösterim aslında "R = 5" ifadesinin 
    bir küme belirttiğini ve bu kümenin olasılığının hesaplanmak istediğini" belirtmesi açısından daha doğal gibi gözükmektedir. 

    K rassal değişkeni rastgele seçilen bir insanın kilosunu belirtiyor olsun. Bu durumda K aslında tüm insanları tek tek 
    onların kilolarına eşleyen bir fonksiyondur. O halde P{K < 60} olasılığı aslında "rastgele seçilen bir kişinin kilosunun 
    60'tan küçük olma olasılığı" anlamına gelmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Rassal değişkenler tıpkı matematiksel diğer fonksiyonlarda olduğu gibi "kesikli (discrete)" ya da "sürekli (continuous)" 
    olabilmektedir. Eğer bir rassal değişken (yani fonksiyon) teorik olarak belli bir aralıkta tüm gerçek sayı değerlerini 
    alabiliyorsa böyle rassal değişkenlere "sürekli (continuous)", yalnızca belli gerçek sayı değerlerini alabiliyorsa böyle 
    rassal değişkenlere ise "kesikli (discrete)" rassal değişkenler denilmektedir. Örneğin "iki zarın atılmasında üste gelen 
    sayılar toplamını belirten Z rassal değişkeni" kesiklidir. Çünkü yalnızca belli değerleri alabilmektedir. Ancak "rastgele 
    seçilen bir kişinin kilosunu belirten" K rassal değişkeni süreklidir. Çünkü teorik olarak belli bir aralıkta tüm gerçek 
    değerleri alabilir. (Biz kişilerin kilolarını yuvarlayarak ifade etmekteyiz. Ancak aslında onların kiloları belli aralıktaki 
    tüm gerçek değerlerden biri olabilir.)

    Sürekli rassal değişkenlerin noktasal olasılıkları 0'dır. Örneğin "rastgele seçilen kişinin kilosunu" belirten K rassal
    değişkeni söz konusu olsun. P{K = 67} gibi bir olasılık aslında 0'dır. Çünkü gerçek sayı ekseninde sonsuz tane nokta vardır. 
    67 yalnızca bu sonsuz noktadan bir tanesini belirtir. Sayı / sonsuz da 0'dır. Ancak sürekli rassal değişkenlerin aralıksal
    olasılıkları 0 olmak zorunda değildir. Yani örneğin P{66 < K < 67} olasılığı 0 değildir. Burada {66 < K < 67} kümesinin de 
    elemanlarının sonsuz sayıda olduğuna dikkat ediniz. Artık bu hesap matematikte "limit, türev, integral" konularıyla 
    ilişkili hale gelmektedir. Sürekli rassal değişkenlerin aralıksal olasılıklarını belirtirken aralıklardaki '=' sembolünün 
    bir anlamının olmayacağına dikkat ediniz. Örneğin P{66 < K < 67} ile P{66 <= K <= 67} arasında aslında bir fark yoktur. 
    Bu konuda da kişiler farklı gösterimleri tercih edebilmektedir. Bazı kişiler bir tarafa '=' sembolünü koyup diğer tarafa
    koymamaktadır. Örneğin P{66 <= K < 67} gibi. Ancak bu '=' sembollerinin sürekli rassal değişkenlerde bir etkisi yoktur.
    (Sonsuz büyüklükte bir kümeye bir eleman daha eklesek bunun bir etkisi olabilir mi?)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesinde sürekli rassal değişkenler daha fazla karşımıza çıkmaktadır. Bu nedenle biz sürekli 
    rassal değişkenler üzerinde biraz daha duracağız. 

    Sürekli bir rassal değişkenin aralıksal olasılıkları "intergral" işlemi ile hesaplanmaktadır. Tabii integral işlemi için 
    bir fonksiyona gereksinim vardır. İşte sürekli rassal değişkenlerin aralıksal olasılıklarının hesaplanması için kullanılan 
    fonksiyonlara "olasılık yoğunluk fonksiyonları (probability density functions)" denilmektedir.  Birisi bize bir rassal 
    değişkenin belli bir aralıktaki olasılığını soruyorsa o kişinin bize o rassal değişkene ilişkin "olasılık yoğunluk fonksiyonunu" 
    vermiş olması gerekir. Biz de örneğin P{x0 < X < x1} olasılığını x0'dan x1'e f(x)'in integrali ile elde ederiz. 

    Bir fonksiyonun olasılık yoğunluk fonksiyonu olabilmesi için eksi sonsuzdan artı sonsuze integralinin (yani tüm eğri altında 
    kalan alanın) 1 olması gerekir. Bir rassal değişkenin olasılık yoğunluk fonksiyonuna "o rassal değişkenin dağılımı" da 
    denilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki mademki bizim bir rassal değişkenin aralıksal olasılığını elde etmek için bir olasılık yoğunluk fonksiyonuna ihtiyacımız 
    var o halde bu fonksiyonu nasıl elde edeceğiz? İşte pek çok rassal değişkenin olasılık yoğunluk fonksiyonunun bazı kalıplara 
    uyduğu görülmektedir. Örneğin doğadaki pek çok olgunun (boy gibi, kilo gibi, zeka gibi) olasılık yoğunluk fonksiyonu "normal 
    dağılım" denilen dağılıma uymaktadır. Normal dağılım eğrisine "Gauss eğrisi" ya da "çan eğrisi" de denilmektedir. Olasılık 
    yoğun fonksiyonları bazı parametrelere de sahip olabilmektedir.  Örneğin Gauss eğrisinin iki parametresi vardır: Orta noktasını 
    belirten "ortalama" ve zayıflığını şişmanlığını belirten "standart sapma".
    
    Bu durumda örneğin sürekli rassal değişkenlerle ilgili bir olasılık sorusu şöyle olabilir: "Kişilerin zeka puanları (IQ) 
    "ortalaması 100 standart sapması 15 olan normal dağılımla" temsil ediliyor olsun. Rastgele seçilen bir kişinin zeka puanının 
    120 ile 130 arasında olma olasılığı nedir?" Burada bize rassal değişkenin olasılık yoğunluk fonksiyonu parametreleriyle 
    verilmiştir. Bizim de bu olasılığı hesaplamak için tek yapacağımız şey Guass fonksiyonunun 120'den 130'a fonksiyonun 
    integralini hesaplamaktır. 

    Peki rassal değişkenimiz başkaları tarafından belirlenen herhangi bir kalıba uymuyorsa ne yapabiliriz? Bir rassal 
    değişkenin olasılık yoğunluk fonksiyonunun çıkarılması biraz zahmetli bir işlemdir. Ancak kümülatif olasılıklar yardımıyla
    olasılık yoğunluk fonksiyonları tatmin edici bir biçimde elde edilebilmektedir. Tabii yukarıda da belirttiğimiz gibi
    aslında pek çok olay zaten tespit edilmiş çeşitli kalıplara uymaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            14. Ders - 11/02/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Doğada en fazla karşılaşılan sürekli dağılım "normal dağılım (normal distribution)" denilen dağılımdır. Bu dağılımın 
    olasılık yoğunluk fonksiyonu çan eğrisine benzemektedir. Yukarıda da belirttiğimiz gibi normal dağılımın iki parametresi 
    vardır: "Ortalama" ve "standart sapma". Ortalama çan eğrisinin orta noktasını belirler. Standart sapma ise eğrinin zayıf 
    ya da şişman olması üzerinde etkili olur. Çan eğrisine teknik olarak "Gauss eğrisi", Gauss eğrisine ilişkin olasılık 
    yoğunluk fonksiyonuna da "Gauss fonksiyonu" denilmektedir. 

    Gauss fonksiyonu simetrik bir fonksiyondur. Bu fonksiyon ortalama etrafında büyük bir alan kaplar, iki uca gidildikçe 
    hızlı bir düşüş yaşanır. Ancak eğri hiçbir zaman X ekseni ile kesişmemektedir. Yani eğri iki taraftan y değeri olarak 
    sıfıra oldukça yaklaşır, ancak sıfır olmaz. 

    Gauss fonksiyonu bir olasılık yoğunluk fonksiyonu belirttiğine göre fonksiyonun toplam eğri altında kalan alanı 1'dir. 
    Fonksiyon simetrik olduğuna göre ortalamanın iki yanındaki eğri altında kalan alan 0.5'tir. 

    Yukarıda da belirttiğimiz gibi değişik ortalama ve standart sapmaya ilişkin sonsuz sayıda Gauss eğrisi çizilebilir. 
    Ortalaması 0, standart sapması 1 olan normal dağılıma "standart normal dağılım", standart normal dağılımdaki X değerlerine 
    de Z değerleri" denilmektedir. 

    Aşağıdaki örnekte Gauss eğrisi çizdirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------
   
import numpy as np
import matplotlib.pyplot as plt

def gauss(x, mu = 0, std = 1):
    return 1 / (std * np.sqrt(2 * np.pi)) * np.e ** (-0.5 * ((x - mu) / std) ** 2)
    
x = np.linspace(-5, 5, 1000)
y = gauss(x)

plt.title('Gauss Function')
plt.plot(x, y)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıdaki çizimde eksenleri kartezyen koordinat sistemindeki gibi de gösterebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

def gauss(x, mu = 0, std = 1):
    return 1 / (std * np.sqrt(2 * np.pi)) * np.e ** (-0.5 * ((x - mu) / std) ** 2)
  
def draw_gauss(mu = 0, std = 1):
    x = np.linspace(-5 * std + mu, 5 * std + mu, 1000)
    y = gauss(x, mu, std)
    
    mu_y = gauss(mu, mu, std)
    
    plt.figure(figsize=(10, 4))
    plt.title('Gauss Function', pad=10, fontweight='bold')
    axis = plt.gca()
    
    axis.set_ylim([-mu_y * 1.1, mu_y * 1.1])
    axis.set_xlim([-5 * std + mu, 5 * std + mu])
    axis.set_xticks(np.arange(-4 * std + mu, 5 * std + mu, std))
    # axis.set_yticks(np.round(np.arange(-mu_y, mu_y, mu_y / 10), 2))
    axis.spines['left'].set_position('center')
    axis.spines['top'].set_color(None)
    axis.spines['bottom'].set_position('center')
    axis.spines['right'].set_color(None)
    axis.plot(x, y)
    plt.show()

draw_gauss(100, 15)

#----------------------------------------------------------------------------------------------------------------------------
    Normal dağılımda eğri altında kalan toplam alanın 1 olduğunu belirtmiştik. Bu dağılımda toplaşmanın ortalama civarında 
    olduğu eğrinin şeklinden anlaşılmaktadır. Gerçekten de normal dağılımda ortalamadan bir standart sapma soldan ve sağdan 
    kaplanan alan yani P{mu - std < X < mu + std} olasılığı 0.6827, ortalamadan iki standart sapma soldan ve sağdan kaplanan 
    alan yani P{mu - std * 2 < X < mu + std * 2} olasılığı 0.9545, otalamadan üç standart sapma soldan ve sağdan kaplanan alan 
    yani P{mu - std * 3 < X < mu + std * 3} olasılığı ise 0.9956 biçimindedir.

    Matplotlib'te bir eğrinin altındaki alanı boyamak için fill_between isimli fonksiyon kullanılmaktadır. Bu fonksiyon axis 
    sınıfının bir metodu olarak da bulundurulmuştur. Aşağıdaki örnekte eğrinin altındaki belli bir alan fill_between metodu
    ile boyanmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

def gauss(x, mu = 0, std = 1):
    return 1 / (std * np.sqrt(2 * np.pi)) * np.e ** (-0.5 * ((x - mu) / std) ** 2)
      
def fill_gauss(mu = 0, std = 1, fstart= 0, fstop = 0):
    x = np.linspace(-5 * std + mu, 5 * std + mu, 1000)
    y = gauss(x, mu, std)
    
    mu_y = gauss(mu, mu, std)
    
    plt.figure(figsize=(10, 4))
    plt.title('Gauss Function', pad=10, fontweight='bold')
    axis = plt.gca()
    
    axis.set_ylim([-mu_y * 1.1, mu_y * 1.1])
    axis.set_xlim([-5 * std + mu, 5 * std + mu])
    axis.set_xticks(np.arange(-4 * std + mu, 5 * std + mu, std))
   # axis.set_yticks(np.round(np.arange(-mu_y, mu_y, mu_y / 10), 2))
    axis.spines['left'].set_position('center')
    axis.spines['top'].set_color(None)
    axis.spines['bottom'].set_position('center')
    axis.spines['right'].set_color(None)
    axis.plot(x, y)
    
    x = np.linspace(fstart, fstop, 1000)
    y = gauss(x, mu, std)
    axis.fill_between(x, y)
    plt.show()

fill_gauss(100, 15, 85, 115)

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi "kümülatif dağılım fonksiyonu (cummulative distribution function)" belli bir değere kadar 
    tüm birikimli olasılığı veren fonksiyondur. Genellikle F harfi gösterilmektedir. Örneğin F(x0) aslında P{X < x0} anlamına 
    gelmektedir. Normal dağılımda F(x0) değeri aslında X değerinin x0 olduğu noktanın solundaki tüm eğri altında kalan alanı 
    belirtmektedir. (Başka bir deyişle sürekli dağılımlarda F(x0) değeri "-sonsuzdan x0'a kadar olasılık yoğunluk fonksiyonunun 
    integraline eşittir.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi normal dağılım istatistikte çok önemli bir sürekli dağılımdır. Normal dağılımın önemi 
    "merkezi limit teoremi (central limit theorem)" ile daha iyi anlaşılabilir.  Çıkarımsal istatistiğin dayandığı temel merkezi 
    limit teoremi olduğu için normal dağılım da çok önemli olmaktadır. Merkezi limit teoremini izleyen paragraflarda 
    ele alacağız.
    
    Bizim de Python programcısı olarak normal dağılım üzerinde aşağıdaki dört işlemi yapabiliyor olmamız gerekir:
    
    1) Belli bir x değeri için eksi sonsuzdan o x değerine kadar eğri altında kalan alanı bulmak. Yukarıda da belirttiğimiz gibi 
    aslında bu alanı veren fonksiyona istatistikte "kümülatif dağılım fonksiyonu (cummulative distribution function)" denmektedir. 
    O halde aslında P{x0 < X < x1} olasılığı da F(x1) - F(x0) ile aynıdır. 
    
    2) Yukarıdaki işlemin tersi olan işlem. Yani bize kümülatif dağılım fonksiyonundan elde edilen değer verilmiş olabilir. Bizden 
    buna ilişkin x değeri istenebilir. 

    3) Belli bir x değeri için Gauss fonksiyon değerinin elde edilmesi. (Yani belli bir x değeri için olasılık yoğunluk fonksiyonunun
    değerinin elde edilmesi.)

    4) Normal dağılıma uygun rastgele sayıların üretilmesi. Aslında bunun için 0 ile 1 arasında rasgele sayı üretilip onu 2'inci 
    maddede belirtelen işleme sokarsak normal dağılmış rastgele sayı elde edebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Normal dağılımla ilgili işlemleri yapabilmek için Python standart kütüphanesinde statistics modülü içerisinde NormalDist 
    isimli bir sınıf bulundurulmuştur. Programcı bu sınıf türünden bir nesne yaratır. İşlemlerini bu sınıfın metotlarıyla yapar.
    NormalDist nesnesi yaratılırken ortalama ve standart sapma değerleri girilir. (Bu değerler girilmezse ortalama için 0, 
    standart sapma için 1 default değerleri kullanılmaktadır.) Örneğin:

    import statistics 

    nd = statistics.NormalDist(100, 15)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    NormalDist sınıfının cdf (cummulative distribution function) isimli metodu verilen x değeri için eğrinin solunda kalan 
    toplam alanı yani kümülatif olasılığı bize vermektedir. Örneğin standart normal dağılımda x = 0'ın solundaki alan 0.5'tir.
#----------------------------------------------------------------------------------------------------------------------------

import statistics

nd = statistics.NormalDist()

result = nd.cdf(0)
print(result)           # 0.5

#----------------------------------------------------------------------------------------------------------------------------
    Örneğin biz ortalaması 100, standart sapması 15 olan bir normal dağılımda P{130 < X < 140} olasılığını aşağıdaki gibi 
    elde edebiliriz:

    nd = statistics.NormalDist(100, 15)
    result = nd.cdf(140) - nd.cdf(130)

    Şöyle bir soru sorulduğunu düşünelim: "İnsanların zekaları ortalaması 100, standart sapması 15 olan normal dağılıma uyuyor olsun. 
    Bu durumda zeka puanı 140'ın yukarısında olanların toplumdaki yüzdesi nedir?". Bu soruda istenen şey aslında normal dağılımdaki
    P{X > 140} olasılığıdır. Yani x ekseninde belli bir noktanın sağındaki alan sorulmaktadır. Bu alanı veren doğrudan bir fonksiyon 
    olmadığı için bu işlem 1 - F(140) biçiminde ele alınarak sonuç elde edilebilir:

    nd = statistics.NormalDist(100, 15)
    result = 1 - nd.cdf(140)
    print(result)                   # 0.003830380567589775
#----------------------------------------------------------------------------------------------------------------------------

import statistics

nd = statistics.NormalDist(100, 15)

result = nd.cdf(140) - nd.cdf(130)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi bir normal dağılımda iki x arasındaki alanı yani P{x1 < X <= x2} olasılığını biz cdf 
    fonksiyonuyla F(x2) - F(x1) işlemi ile elde edebiliriz. Örneğin ortalaması 100 standart sapması 15 olan normal doğılımda 
    120 ile 130 arasındaki olasılık aaşağıdaki gibi hesaplanıp grafiği çizdirilebilir.  
#----------------------------------------------------------------------------------------------------------------------------

import statistics
import numpy as np

nd = statistics.NormalDist(100, 15)

result = nd.cdf(130) - nd.cdf(120)
print(result)

import matplotlib.pyplot as plt

plt.title('P{120 < x <= 130} Olasılığı ', pad=20, fontsize=14, fontweight='bold')
x = np.linspace(40, 160, 1000)
y = [nd.pdf(val) for val in x]

axis = plt.gca()
axis.set_ylim(-0.030, 0.030)
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_xticks(range(40, 170, 10))

plt.plot(x, y)

x = np.linspace(120, 130, 100)
y = [nd.pdf(val) for val in x]

axis.fill_between(x, y)
plt.text(120, -0.01, f'P{{120 < x <= 130}} = {result:.3f}', color='blue', fontsize=14)

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Eskiden bilgisayarların bu kadar yoğun kullanılmadığı zamanlarda standart normal dağılım için "Z Tabloları" düzenlenmekteydi.
    Bu tablolar belli bir Z değeri için (standart normal dağılımdaki X değerlerine Z değeri dendiğini anımsayınız) kümülatif 
    dağılım fonksiyonun değerini vermektedir. Örnek bir Z tablosu aşağıdakine benzemektedir:

            0.00	0.01	0.02	0.03	0.04	0.05	0.06	0.07	0.08	0.09
    0.0	    0.5000	0.5040	0.5080	0.5120	0.5160	0.5199	0.5239	0.5279	0.5319	0.5359
    0.1	    0.5398	0.5438	0.5478	0.5517	0.5557	0.5596	0.5636	0.5675	0.5714	0.5753
    0.2	    0.5793	0.5832	0.5871	0.5910	0.5948	0.5987	0.6026	0.6064	0.6103	0.6141
    0.3	    0.6179	0.6217	0.6255	0.6293	0.6331	0.6368	0.6406	0.6443	0.6480	0.6517
    0.4	    0.6554	0.6591	0.6628	0.6664	0.6700	0.6736	0.6772	0.6808	0.6844	0.6879
    0.5	    0.6915	0.6950	0.6985	0.7019	0.7054	0.7088	0.7123	0.7157	0.7190	0.7224
    0.6	    0.7257	0.7291	0.7324	0.7357	0.7389	0.7422	0.7454	0.7486	0.7517	0.7549
    0.7	    0.7580	0.7611	0.7642	0.7673	0.7704	0.7734	0.7764	0.7794	0.7823	0.7852
    0.8	    0.7881	0.7910	0.7939	0.7967	0.7995	0.8023	0.8051	0.8078	0.8106	0.8133
    0.9	    0.8159	0.8186	0.8212	0.8238	0.8264	0.8289	0.8315	0.8340	0.8365	0.8389
    1.0	    0.8413	0.8438	0.8461	0.8485	0.8508	0.8531	0.8554	0.8577	0.8599	0.8621
    1.1	    0.8643	0.8665	0.8686	0.8708	0.8729	0.8749	0.8770	0.8790	0.8810	0.8830
    1.2	    0.8849	0.8869	0.8888	0.8907	0.8925	0.8944	0.8962	0.8980	0.8997	0.9015
    ......................................................................................

    Bu tabloda ilgili hücredeki değer o Z değerinin kümülatif olasılığını vermektedir. Tabii artık bir bilgisayar programcısının
    böyle bir tablo kullanmasına gerek yoktur. Yukarıdaki gibi tablolar yalnızca belli kesikliği değerileri yuvarlayarak vermektedir.  

    Peki bu Z tabloları standart normal dağılıma göre hazırlandığına göre biz belli bir ortalama ve standart sapmaya ilişkin
    kümülatif olasılığı bu tablo yoluyla nasıl elde edebilmekteyiz? İşte aslında herhangi bir normal dağılım standart normal 
    dağılıma dönüştürülebilmektedir. Ortalaması mu standart sapması std olan normal dağılımdaki x değerinin standart normal 
    dağılımdaki Z değeri şöyle hesaplanmaktadır:

    Z = (x - mu) / std

    Örneğin ortalaması 100, standart sapması 15 olan normal dağılımdaki x = 136 değerinin standart normal dağılımdaki Z değeri 
    2.40'tür.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Belli bir kümülatif olasılık değeri için x değerinin bulunması işlemi de NormalDist sınıfının inv_cdf metoduyla yapılmaktadır. 
    Örneğin standart normal dağılımda 0.99 olan kümülatif olasılığın Z değeri aşağıdaki gibi bulunabilir:

    n = statistics.NormalDist()
    result = nd.inv_cdf(0.99)
#----------------------------------------------------------------------------------------------------------------------------

import statistics

nd = statistics.NormalDist()

result = nd.inv_cdf(0.99)
print(result)       # 2.32

#----------------------------------------------------------------------------------------------------------------------------
    Belli bir x değeri için Gauss fonksiyonunda ona karşı gelen y değeri sınıfın pdf metoduyla elde edilmektedir. Örneğin 
    x = 0 için standart normal dağılımda Gauss fonksiyonun değerini aşağıdaki gibi elde edebiliriz:

    nd = statistics.NormalDist()
    result = nd.pdf(0)
    print(result)
#----------------------------------------------------------------------------------------------------------------------------

import statistics

nd = statistics.NormalDist()

result = nd.pdf(0)
print(result)   # 0.3989422804014327

#----------------------------------------------------------------------------------------------------------------------------
    Normal dağılmış rastgele sayı üretmek için NormalDist sınıfının samples isimli metodu kullanılmaktadır. Bu metot kaç rassal
    sayının üretileceğini parametre olarak alıp üretilen rassal sayıları float elemanlardan oluşan bir liste biçiminde vermektedir.
    Örneğin:

    nd = statistics.NormalDist()
    result = nd.samples(10)

    Bu işlemden biz normal dağılmış 10 tane rasgele değerden oluşan bir liste elde ederiz. 
#----------------------------------------------------------------------------------------------------------------------------

import statistics

nd = statistics.NormalDist()

result = nd.samples(10)
print(result) 

#----------------------------------------------------------------------------------------------------------------------------
    Biz normal dağılmış rastgele sayılardan histogram çizersek histogramımızın Gauss eğrisine benzemesi gerekir.
#----------------------------------------------------------------------------------------------------------------------------

import statistics

nd = statistics.NormalDist()
result = nd.samples(10000)

import matplotlib.pyplot as plt

plt.hist(result, bins=20)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Peki normal dağılmış rastgele sayı üretme işlemini samples metodu nasıl yapmaktadır? Aslında klasik yöntem önce [0, 1]
    aralığında rastgele sayı üretim bunu kümülatif dağılım olarak kabul etmek ve bu kümülatif dağılımın x değerini hesaplamaktır.
#----------------------------------------------------------------------------------------------------------------------------

import random
import statistics

nd = statistics.NormalDist(100, 15)

result = [nd.inv_cdf(random.random()) for _ in range(10000)]

import matplotlib.pyplot as plt

plt.hist(result, bins=20)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Aslında normal dağılmış rastgele sayı üretmek için standartkütüphanede random modülü içerisinde gauss isimli bir fonksiyon 
    da bulundurulmuştur. gauss fonksiyonunun parametrik yapısı şöyledir:

    gauss(mu=0.0, sigma=1.0)

    Python 3.11’den önce bu parametreler default değer almıyordu. Fonksiyonu kullanırken bu durumu dikkate alınız. Aşağıda 
    gauss fonksiyonu kullanılarak üretilen normal dağılmış rassal sayıların histrogramı çizilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import random

result = [random.gauss(0, 1) for _ in range(10000)]

import matplotlib.pyplot as plt

plt.hist(result, bins=20)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Python'un statistics modülündeki NormalDist sınıfı vektörel işlemler yapamamaktadır. Maalesef NumPy ve Pandas kütüphanelerinde 
    normal dağılım üzerinde vektörel işlem yapan öğeler yoktur. Ancak SciPy kütüphanesi içerisinde pek çok dağılım üzerinde 
    vektörel işlemler yapan sınıflar bulunmaktadır. Bu nedenle pratikte Python kütüphanesi yerine bu tür işlemler için SciPy 
    kütüphanesi tercih edilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    scipy.stats modülü içerisindeki "norm" isimli singleton nesne normal dağılım üzerinde vektörel işlem yapan metotlara sahiptir. 
    norm bir sınıf nesnesidir ve zaten yaratılmış bir biçimde bulunmaktadır. Dolayısıyla programcı doğrudan bu nesne ile ilgili 
    sınıfın metotlarını çağırabilir. Genellikle programcılar bu tür nesneleri kullanmak için "from import" deyimini tercih 
    ederler:

    from scipy.stats import norm
     
    norm nesnesine ilişkin sınıfın cdf isimli metodu üç parametre almaktadır:

    cdf(x, loc=0, scale=1)

    Buradaki x bir NumPy dizisi ya da Python dolaşılabilir nesnesi olabilir. Bu durumda tüm x değerlerinin kümülatif olasılıkları 
    hesaplanıp bir NumPy dizisi olarak verilmektedir. Burada loc ortalamayı, scale ise standart sapmayı belirtmektedir. Örneğin:

    result = norm.cdf([100, 130, 140], 100, 15)
    print(result)
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm

result = norm.cdf([100, 130, 140], 100, 15)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    norm nesnesinin ilişkin olduğu sınıfın ppf (percentage point function) isimli metodu cdf işleminin tersini yapmaktadır. 
    Yani kümülatif olasılığı bilindiği durumda bize bu kümalatif olasılığa karşı gelen x değerini verir. (Yani ppf NormalDist
    sınıfındaki inv_cdf metoduna karşılık gelmektedir.):

    ppf(q, loc=0, scale=1)

    ppf (percentage point function) ismi size biraz tuhaf gelebilir. Bu isim birikimli dağılım fonksiyonunun tersini belirtmek
    için kullanılmaktadır. ppf aslında "medyan (median)" kavramının genel biçimidir. Anımsanacağı gibi medyan ortadan ikiye 
    bölen noktayı belirtiyordu. Örneğin standart normal dağılımda medyan 0'dır. Yani ortalamaya eşittir. İstatistikte tam 
    ortadan bölen değil de diğer noktalardan bölen değerler için "percentage point" de denilmektedir. Örneğin normal dağılımda 
    1/4 noktasından bölen değer aslında birikimli dağılm fonksiyonunun 0.25 için değeridir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm

result = norm.ppf([0.50, 0.68, 0.95], 100, 15)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    norm nesnesinin ilişkin olduğu sınıfın pdf (probability density function) isimli metodu yine x değerlerinin Gauss eğrisindeki 
    y değerlerini vermektedir. Metodun parametrik yapısı şöyledir:

    pdf(x, loc=0, scale=1) 

    Burada yine x hesaplanacak değeri, loc ortalamayı ve scale de standart sapmayı belirtmektedir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

x = np.linspace(40, 160, 1000)
y = norm.pdf(x, 100, 15)

plt.plot(x, y)

x = np.full(200, 100)       # 200 tane 100'lerden oluşan dizi
yend = norm.pdf(100, 100, 15)
y = np.linspace(0, yend, 200)
plt.plot(x, y, linestyle='--')

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    norm nesnesinin ilişkin olduğu sınıfın rvs metodu ise normal dağılıma ilişkin rassal sayı üretmek için kullanılmaktadır.
    Metodun parametrik yapısı şöyledir:

    rvs(loc=0, scale=1, size=1)
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm
import matplotlib.pyplot as plt

x = norm.rvs(100, 15, 10000)

plt.hist(x, bins=20)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Normal dağılımda ortalamadan birer standart sapma arasındaki bölgenin olasılığı, yani P{mu - sigma < X < mu + sigma} olasılığı 
    0.68 civarındadır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

result = norm.cdf(1) - norm.cdf(-1)
print(result)

x = np.linspace(-5, 5, 1000)
y = norm.pdf(x)

plt.title('Ortalamadan 1 Standart Sapma Arası Bölge', fontweight='bold')
axis = plt.gca()
axis.set_ylim(-0.5, 0.5)
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_xticks(range(-4, 5))
axis.text(2, 0.3, f'{result:.3f}', fontsize=14, fontweight='bold')

plt.plot(x, y)

x = np.linspace(-1, 1, 1000)
y = norm.pdf(x)
plt.fill_between(x, y)
axis.arrow(2.5, 0.25, -2, -0.1, width=0.0255)

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
     Normal dağılımda ortalamadan iki standart sapma arasındaki bölgenin olasılığı, yani P{mu - 2 * sigma < X < mu + 2 * sigma} 
     olasılığı 0.95 civarındadır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

result = norm.cdf(2) - norm.cdf(-2)
print(result)

x = np.linspace(-5, 5, 1000)
y = norm.pdf(x)

plt.title('Ortalamadan 2 Standart Sapma Arası Bölge', fontweight='bold')
axis = plt.gca()
axis = plt.gca()
axis.set_ylim(-0.5, 0.5)
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_xticks(range(-4, 5))
axis.text(2, 0.3, f'{result:.3f}', fontsize=14, fontweight='bold')

plt.plot(x, y)

x = np.linspace(-2, 2, 1000)
y = norm.pdf(x)
plt.fill_between(x, y)
axis.arrow(2.5, 0.25, -2, -0.1, width=0.0255)

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Normal dağılımda ortalamadan üç standart sapma arasındaki bölgenin olasılığı, yani P{mu - 3 * sigma < X < mu + 3 * sigma} 
    olasılığı 0.997 civarındadır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

result = norm.cdf(3) - norm.cdf(-3)
print(result)

x = np.linspace(-5, 5, 1000)
y = norm.pdf(x)

axis = plt.gca()
axis.set_ylim(-0.5, 0.5)
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_xticks(range(-4, 5))
axis.text(2, 0.3, f'{result:.3f}', fontsize=14, fontweight='bold')

plt.plot(x, y)

x = np.linspace(-3, 3, 1000)
y = norm.pdf(x)
plt.fill_between(x, y)
axis.arrow(2.5, 0.25, -2, -0.1, width=0.0255)

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok karşılaşılan sürekli dağılım "sürekli düzgün dağılım (continuos uniform distribution)" denilen dağılımdır. Burada 
    dağılımın a ve b biçiminde isimlendirebileceğimiz iki parametresi vardır. Sürekli düzgün dağılımın olasılık yoğunluk fonksiyonu 
    dikdörtgensel bir alandır. Dolayısıyla kümülatif dağılım fonksiyonu b - a değeriyle orantılı bir değer vermektedir. Sürekli 
    düzgün dağılımın olasılık yoğunluk fonksiyonu şöyle ifade edilebilir:

    f(x) = {
                1 / (b - a)     a < x < b
                0               diğer durumlarda    
           }

    Sürekli düzgün dağılım için Python'un standart kütüphanesinde bir sınıf bulunmamaktadır. NumPy'da da böyle bir sınıf yoktur. 
    Ancak SciPy içerisinde stats modülünde uniform isimli bir singleton nesne bulunmaktadır. Bu nesneye ilişkin sınıfın yine
    cdf, ppf, pdf ve rvs metotları vardır. Bu metotlar sırasıyla a değerini ve a'dan uzunluğu parametre olarak almaktadır. 
    Örneğin:

    result = uniform.pdf(15, 10, 10)

    Burada aslında a = 10, b = 20 olan bir sürekli düzgün dağılımdaki olasılık yoğunluk fonksiyon değeri elde edilmektedir. 
    Tabii aslında 10 ile 20 arasındaki tüm olasılık yoğunluk fonksiyon değerleri 1 / 10 olacaktır. Örneğin:

    result = uniform.cdf(15, 10, 10)

    Burada a = 10, b = 20 olan bir sürekli düzgün dağılımda 15'in solundaki alan elde edilecektir. 15 burada orta nokta olduğuna 
    göre elde edilecek bu değer 0.5'tir. uniform nesnesinin metotlarındaki ikinci parametrenin (loc) a değeri olduğuna ancak 
    üçüncü parametrenin a'dan uzaklık belirttiğine (scale) dikkat ediniz. Bu dağılımın parametrik bilgileri a ve b olsa da 
    SciPy stats modülünde tüm dağılımlar için ortak bir parametrik yapı tercih edildiğinden dolayı böyle bir tasarım kullanılmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import uniform
import matplotlib.pyplot as plt

A = 10
B = 20

x = np.linspace(A - 5, B + 5, 1000)
y = uniform.pdf(x, A, B - A)

plt.title('Continuous Uniform Distribution', fontweight='bold')
plt.plot(x, y)

x = np.linspace(10, 12.5, 1000)
y = uniform.pdf(x, A, B - A)
plt.fill_between(x, y)
plt.show()

result = uniform.cdf(12.5, A, B - A)
print(result)                               # 0.25

result = uniform.ppf(0.5, A, B - A)
print(result)                               # 15

#----------------------------------------------------------------------------------------------------------------------------
    Düzgün dağılmış rastgele sayı üretimi aslında bizim aşina olduğumuz klasik rastgele sayı üretimidir. Örneğin Python standart 
    kütüphanesindeki random modülünde bulnan random fonksiyonu 0 ile 1 arasında rastgele bir sayı veriyordu. Aslında bu fonksiyon 
    a = 0, b = 1 olan düzgün dağılımda rastegele sayı veren fonksiyonla tamamne aynıdır. Benzer biçimde NumPy'daki random 
    modülündeki random fonksiyonu 0 ile 1 arasında düzgün dağılmış rastgele sayı üretmektedir. Örneğin:

    result = uniform.rvs(10, 10, 10)

    Burada a = 10, b = 20 olan sürekli düzgün dağılımda 10 tane rastgele noktalı sayı elde edilecektir. 

    Örneğin 100 ile 200 arasında rastgele 10 tane gerçek sayı üretmek istesek bu işlemi şöyle yapmalıyız:

    uniform.rvs(100, 100, 10)
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import uniform
import numpy as np

x = np.random.random(10)
print(x)

x = uniform.rvs(0, 1, 10)       # yukarıdakiyle tamamen aynı biçimde rassal sayı üretir
print(x)

#----------------------------------------------------------------------------------------------------------------------------
    Özellikle güven aralıklarında (confidence intervals) ve hipotez testlerinde (hypothesis testing) kullanılan diğer önemli 
    bir sürekli dağılım da "t dağılımı (t-distribution)" denilen dağılımdır. t dağılımını bulan kişi (William Sealy Gosset) 
    makalesini "Student" takma adıyla yayınladığından dolayı bu dağılıma İngilizce "Student's t-distribution" da denilmektedir. 
    
    t dağılımı standart normal dağılıma oldukça benzemektedir. Bu dağılımın ortalaması 0'dır. Ancak standart sapması "serbestlik 
    derecesi (degrees of freedom)" denilen bir değere göre değişir. t dağılımının standart sapması sigma = karekök(df / (df - 2)) 
    biçimindedir. t dağılımın olasılık yoğunluk fonksiyonu biraz karmaşık bir görünümdedir. Ancak fonksiyon standart normal 
    dağılıma göre "daha az yüksek ve biraz daha şişman" gibi gözükmektedir. t dağılımının serbestlik derecesi artırıldığında 
    dağılım standart normal dağılıma çok benzer hale gelir. Serbestlik derecesi >= 30 durumunda standart normal dağılımla oldukça 
    örtüşmektedir. Yani serbestlik derecesi >= 30 durumunda artık t dağılımı kullanmakla standart normal dağılım kullanmak 
    arasında önemli bir farklılık kalmamaktadır.

    t dağılımı denildiğinde her zaman ortalaması 0 olan standart sapması karekök(df / (df - 2)) olan dağılım anlaşılmaktadır. 
    Tabii t dağılımı da eksende kaydırılabilir yani ortalaması değiştirilebilir. t Dağılımı standart normal dağılım için üretilmiştir.
    Dolayısıyla eğer söz konusu dağılım standart normal dağılım değilse t dağılımını kullanmak için önce söz konusu normal 
    dağılımı standart normal dağılıma dönüştürmek gerekir.

    t dağılımı teorik bir dağılımdır. Yukarıda da belirttiğimiz gibi özellikle "güven aralıklarının oluşturulması" ve "hipotez
    testlerinde" kullanım alanı bulmaktadır. Bu tür durumlarda anakütle standart sapması bilinmediği zaman örnek standart sapması
    anakütle standart sapması olarak kullanılmakta ve t dağılımından faydalanılmaktadır. Eskiden t dağılımı özellikle "anakütle
    standart sapmasının bilinmediği ve örneklem büyüklüğünün 30'dan küçük olduğu durumlarda" kullanılıyordu. (Çünkü örneklem 
    büyüklüğü >= 30 olduğu durumda zaten t dağılımı standart normal dağılıma çok yaklaşmaktadır.) Ancak artık bilgisayarların 
    yoğun kullanıldığı bu zamanlarda örnek büyüklüğü >= 30 olsa bile t dağılımını kullanmak toplamda çok küçük bir farklılık 
    oluştursa da tercih edilebilmektedir.

    t dağılımının "serbestlik derecesi (degrees of freedom)" denilen bir parametresi vardır. Serbestlik derecesi "örneklem 
    büyüklüğünden bir eksik olan" değerdir. Örneğin örneklem büyüklüğü 10 ise serbestlik derecesi 9'dur. Serbestlik derecesi
    (dolayısıyla örneklem büyüklüğü) artırıldıkça t dağılımı standart normal dağılıma benzer. Yukarıda da belirttiğimiz gibi 
    n >= 30 durumunda artık yavaş yavaş t dağılımının standart normal dağılımdan önemli bir farkı kalmamaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    t dağılımına ilişkin Python standart kütüphanesinde bir sınıf yoktur. NumPy kütüphanesinde de t dağılımına ilişkin bir öğe 
    bulunmamaktadır. Ancak SciPy kütüphanesindeki stats modülünde t isimli singleton nesne t dağılımı ile işlem yapmak için 
    kullanılmaktadır. t isimli singleton nesnenin metotları norm nesnesinin metotlarıyla aynıdır. Bu fonksiyonlar genel olarak 
    önce x değerini sonra serbestlik derecesini, sonra da ortalama değeri ve standart sapma değerlerini parametre olarak almaktadır. 
    Ortalama ve standart sapma değerleri kullanılarak dağılım önce standart normala dağılıma dönüştürülmekte ondan sonra t 
    dağılımı ile işlemler yapılmaktadır.

    Aşağıdaki programda standart normal dağılım ile 5 serbestlik derecesi ve 30 serbestlik derecesine ilişkin t dağılımlarının
    olasılık yoğunluk fonksiyonları çizdirilmiştir. Burada özellikle 30 serbestlik derecesine ilişkin t dağılımının grafiğinin 
    standart normal dağılım grafiği ile örtüşmeye başladığına dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import numpy as np
from scipy.stats import norm, t
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 10))
x = np.linspace(-5, 5, 1000)
y = norm.pdf(x)

axis = plt.gca()
axis.set_ylim(-0.5, 0.5)
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)
axis.set_xticks(range(-4, 5))
plt.plot(x, y)

y = t.pdf(x, 5)
plt.plot(x, y)

y = t.pdf(x, 30)
plt.plot(x, y, color='red')

plt.legend(['Standart Normal Dağılım', 't Dağılımı (Serbestlik Derecesi = 5)', 
            't dağılımı (Serbestlik Derecesi = 30)'])

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Tabii standart normal dağılımla t dağılımının olasılık yoğunluk fonksiyonları farklı olduğuna göre aynı değerlere ilişkin 
    kümülatif olasılık değerleri de farklı olacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm,  t

x = [-0.5, 0, 1, 1.25]s
result = norm.cdf(x)
print(result)               # [0.30853754 0.5        0.84134475 0.89435023]

x = [-0.5, 0, 1, 1.25]
result = t.cdf(x, 5)    
print(result)               # [0.31914944 0.5        0.81839127 0.86669189]

#----------------------------------------------------------------------------------------------------------------------------
    Biz bu konuda sürekli dağılım olarak yalnızca "normal dağılımı", "sürekli düzgün dağılımı" ve "t dağılımını" inceledik. 
    Aslında değişik olayları modellemek için başka dağılımlar da kullanılmaktadır. Bunlardan bazılarını ileride başka konular 
    içerisinde ele alacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda bazı sürekli dağılımları inceledik. Kursumuzda genellikle sürekli dağılımları kullanacağız. Ancak bu noktada
    kesikli (discrete) dağılımlar üzerinde de bazı bilgiler vermek istiyoruz. 

    Kesikli dağılımlarda X değerleri her gerçek değeri almamaktadır. Dolayısıyla bunların fonksiyonları çizildiğinde sürekli
    fonksiyonlar elde edilemeyecek kesikli noktalar elde edilecektir. Kesikli dağılımlarda X değerlerini onların olasılıklarına
    eşleyen fonksiyonlara "olasılık kütle fonksiyonu (probability mass function)" denilmektedir. Sürekli rassal değişkenlerin
    olasılık yoğunluk fonksiyonları integral hesap için kullanılırken kesikli rassal değişkenler için olasılık kütle fonksiyonları
    doğrudan rassal değişkeninin ilgili noktadaki olasılığını elde etmek için kullanılmaktadır. Olasılık kütle fonksiyonu 
    genellikle büyük P ya da küçük p harfi ile gösterilmektedir:

    px(x) = P{X = x}

    Bu ifade X'in x değerine eşit olma olsılığını belirtmektedir. Buradaki X rassal değişkeni her gerçek değeri alamaz yalnızca 
    bazı değerleri alabilir. Tabii olasılık kütle fonksiyonu px(x) olmak üzere X'in her x değeri için px(x) değerlerinin 
    toplamının yine 1 olması gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tıpkı sürekli rassal değişkenlerde olduğu gibi kesikli rassal değişkenlerde de çok sık karşılaşılan bazı olasılık dağılımları 
    vardır. En çok karşılaşılan kesikli dağılımlardan biri "poisson (genellikle "puason" biçiminde okunuyor)" dağılımıdır. Bu 
    kesikli dağılım adeta normal dağılımın kesikli biçimi gibidir. Poisson dağılımının olasılık kütle fonksiyonu şöyledir:

    P(X = x) = (e^-lamda * lamda^x) / x!

    Dağılımın olasılık kütle fonksiyonu size biraz karışık gelebilir. Buradaki x olasılığını hesaplamak istediğimiz kesikli 
    değeri belirtmektedir. Lamda ise ortalama olay sayısını belirtmektedir. Lamda değeri ortalama belirttiği için gerçek 
    bir değer olabilir. Ancak x değerleri 0, 1, 2, ... n biçiminde 0 ve pozitif tamsayılardan oluşmaktadır.

    Yukarıda da belirttiğimiz gibi poisson dağılımı adeta normal dağılımın kesikli hali gibidir. Dolayısıyla doğada da çok 
    karşılaşılan kesikli dağılımlardandır. Buradaki lamda değeri "ortalama olay sayısını" belirtmektedir. Örneğin lamda değeri
    "bir futbol maçındaki ortalama gol sayısını" ya da "İstanbul'da bir günde meydana gelen ortalama trafik kazası sayısını" 
    belirtiyor olabilir. Bir olgunun poisson dağılımı ile temsil edilmesi aslında "ortalama civarında yüksek olasıkların bulunduğu, 
    ortalamadan iki yandan uzaklaştıkça olasılıkların çan eğrisi gibi düştüğü" bir olasılık kütle fonksiyonunu belirtmektedir. 

    Poisson dağılımı için de Python standart kütüphanesinde ya da NumPy ve Pandas kütüphanelerinde özel fonksiyonlar ve sınıflar 
    bulunmamaktadır. Ancak SciPy kütüphanesinin stats modülü içerisinde poisson isimli bir singleton nesne ile bu dağılımla 
    ilgili işlemler kolaylıkla yapılabilmektedir. 

    SciPy'da kesikli dağılımlar üzerinde işlemler yapan singleton nesneler sürekli dağılımlarla işlemler yapan singleton nesnelere
    kullanım bakımından oldukça benzemektedir. Ancak kesikli dağılımlar için fonksiyonun ismi "pdf" değil "pmf" biçimindedir. 
    Buradaki "pmf" ismi "probability mass function" sözcüklerinden kısaltılmıştır. 

    SciPy'daki poisson nesnesinin fonksiyonları genel olarak bizden x değerini ve lambda değerini parametre olarak istemektedir. 
    Örneğin futbol maçlarındaki gol sayısının poisson dağılımına uyduğunu varsayalım. Maçlardaki ortalama gol sayısının 2 
    olduğunu kabul edelim. Bu durumda bir maçta 5 gol olma olasılığı aşağıdaki gibi elde edilebilir:

    from scipy.stats import poisson

    result = poisson.pmf(5, 2)
    print(result)           # 0.03608940886309672

    Yukarıdaki gibi poisson dağılımı sorularında genellikle soruyu soran kişi belli bir olayın ortalama gerçekleşme sayısını 
    verir. Sonra kişiden bazı değerleri bulmasını ister. Peki soruda "bir maçta ikiden fazla gol olma olasılığı" sorulmuş 
    olsaydı biz soruyu nasıl çözerdik? poisson nesnesi ile cdf fonksiyonunu çağırdığımızda bu cdf fonksiyonu bize x değerine 
    kadarki (x değeri de dahil olmak üzere) kümülatif olasılığı verecektir. Bu değeri de 1'den çıkartırsak istenen olasılığı 
    elde edebiliriz:

    result = 1 - poisson.cdf(2, 2)
    print(result)   # 0.3233235838169366
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda lamda değeri (ortalaması) 10 olan poisson dağılımı için saçılma grafiği çizdirilmiştir. Bu grafiğin normal dağılım
    grafiğini andırmakla birlikte sağdan çarpık olduğuna dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import poisson
import matplotlib.pyplot as plt

plt.title('Lamda = 10 İçin Poisson Dağılımı', fontweight='bold', pad=10)
x = range(0, 40)
y = poisson.pmf(x, 10)

plt.scatter(x, y)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Poisson dağılımında lamda değeri yüksek tutulduğunda saçılma grafiğinin Gauss eğrisine benzediğine dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import poisson
import matplotlib.pyplot as plt

plt.title('Lamda = 100 İçin Poisson Dağılımı', fontweight='bold', pad=10)
x = range(0, 200)
y = poisson.pmf(x, 100)

plt.scatter(x, y)
plt.show()

result = poisson.pmf(3, 4)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    Bernoulli dağılımında X değeri 0 ya da 1 olabilir. X = 0 durumu bir olayın olumsuz olma ya da gerçekleşmeme olasılığını, 
    X = 1 durumu ise bir olayın olumlu olma ya da gerçekleşme olasılığını belirtmektedir. Bu rassal değişkenin yalnızca iki 
    değer aldığına dikkat ediniz. Sonucu iki değerden biri olan rassal deneylere "Bernoulli deneyleri" de denilmektedir. Örneğin
    bir paranın atılması durumunda yazı ya da tura gelmesi iki durumlu bir deneydir. Dolayısıyla bir Bernoulli deneyidir. 
    Bernouli deneylerinde genellikle X = 1 durumundaki olasılık verilir. Biz de bu olasılığı 1'den çıkartarak X = 0 durumundaki
    olasılığı elde ederiz. Bernoulli dağılımının olasılık kütle fonksiyonu şöyle ifade edilebilir:

    P{X = x} =  {
                    p           X = 1 ise
                    1 - p       X = 0 ise

                }
    
    Ya da bu olasılık kütle fonksiyonunu aşağıdaki gibi de ifade edebiliriz:

    P{X = x} = p^x * (1 - p)^(1 - x)

    Burada X = 0 için 1 - p değerinin X = 1 için p değerinin elde edildiğine dikkat ediniz. 

    Bernoulli dağılımı için de SciPy kütüphanesinde stats modülü içerisinde bernoulli isimli bir singleton nesne bulundurulmuştur.
    Tabii bu dağılım çok basit olduğu için bu nesnenin kullanılması da genellikle gereksiz olmaktadır. bernoulli nesnesinin
    ilişkin olduğu sınıfın metotları bizden X değerini (0 ya da 1 olabilir) ve X = 1 için p değerini almaktadır. Örneğin:

    bernoulli.pmf(0, 0.7)

    buradan 0.3 değeri elde edilecektir. 
----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok karşılaşılan bir kesikli dağılım da "binom dağılımı" denilen dağılımdır. Binom dağılımında bir Bernoulli deneyi
    (yani iki sonucu olan bir deney) toplam n defa yinelenmektedir. Bu n defa yinelenmede olayın tam olarak kaç defa olumlu
    sonuçlanacağının olasılığı hesaplanmak istenmektedir. Dolayısıyla binom dağılımının olasılık kütle fonksiyonunda X değerleri
    0, 1, 2, ... gibi tamsayı değerler alır. Örneğin bir para 5 kez atılıyor olsun. Biz de bu 5 kez para atımında tam olarak 
    3 kez Tura gelme olasılığını hesaplamak isteyelim. Burada paranın atılması bir Bernoulli deneyidir. Bu deney 5 kez yinelenmiş
    olumlu kabul ettiğimiz Tura gelme durumunun toplamda 3 kez olmasının olasılığı elde edilmek istenmiştir. 

    Binom dağılımının olasılık kütle fonksiyonu kombinasyon hesabıyla basit bir biçimde oluşturulabilir. Yukarıda bahsettiğimiz 
    paranın 5 kez atılması durumunda 3 Tura gelmesi aşağıdakiler gibi olabilir:

    T Y T Y T
    T T T Y Y
    Y T T Y T
    ...

    Burada toplamda 3 kez Tura gelme durumu aslında C(5, 3) kadardır. Bu olaylar bir arada gerçekleşeceğine göre bu değerle
    p^3 değerini çarpmamız gerekir. Öte yandan Tura gelmeyen olasılıkların toplamı de 5 - 3 = 2 kadardır. O halde bunun da 
    (1 - p)^2 ile çarpılması gerekir. Binom dağılımının olasılık kütle fonksiyonu şöyledir:

    P{X = x} = C(n, x)p^x * (1 - p)^(n - x)

    Binom dağılımı çin SciPy kütüphanesindeki stats modülünde bulunan binom isimli singleton nesne kullanılabilir. Nesnenin
    kullanılması diğer singleton nesnelere benzemektedir. Örneğin ilgili sınıfın pmf fonksiyonu olasılık kütle fonksiyonunu 
    belirtmektedir. Bu fonksiyonlarda birinci parametre "olumlu gerçekleşme sayısını", ikinci parametre "toplam deney sayısını" 
    ve üçüncü parametre de "olumlu gerçekleşme olasılığını" belirtmektedir. Örneğin yukarıda belirttiğimiz paranın 5 kez 
    atılmasında tam olarak 3 kez tura gelme olsaılığı şöyle elde edilebilir:

    binom.pmf(3, 5, 0.5)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şüphesiz "sonuç çıkartıcı istatistiğin (inferential statistics)" en önemli teoremi "merkezi linmit teoremi (central limit 
    theorem)" denilen teoremdir. Bu teorem George Pólya tarafından 1920 yılında ilk kez resmi bir biçimde ifade edilmiştir. 
    Merkezi limit teoreminin biçimsel ifadesi biraz karmaşık bir görünümdedir. Biz burada teroremi basit biçimde açıklayacağız. 
    Bu teoreme göre bir anakütleden elde edilen örnek ortalamaları normal dağılma eğilimindedir. Eğer anakütle normal dağılmışsa 
    küçük örneklerin ortalamaları da normal dağılır. Ancak anakütle normal dağılmamışsa örnek ortalamalarının normal dağılması 
    için örneklerin belli bir büyüklükte (tipik olarak >= 30) olması gerekmektedir. Aksi takdirde örneklem dağılımı soldan ya 
    da sağdan çarpık olabilmektedir. Örneğin elimizde 1,000,000 elemanlı bir anakütle olsun. Bu anakütleden 50'lik tüm alt 
    kümeleri yani örnekleri elde edip bunların ortalamalarını hesaplarsak bu ortalamaların normal dağıldığı görülecektir. Bir 
    anakütleden alınan alt kümelere "örnek (sample)" bu işleme de genel olarak "örnekleme (sampling)" denilmektedir. 

    Aşağıdaki örnekte 0 ile 1,000,000 arasındaki sayılardan oluşan 1,000,000 elemanlık düzgün dağılmış anakütle içerisinden 
    100,000 tane 50'lik rastgele örnekler elde edilmiştir. Sonra da elde edilen bu örneklerin ortalamaları hesaplanmış ve bu
    ortalamaların histogramı çizdirilmiştir. Örnek ortalamalarının dağılımına ilişkin histogramın normal dağılıma benzediğine 
    dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

POPULATION_RANGE = 1_000_000
NSAMPLES = 100_000
SAMPLE_SIZE = 50

samples = np.random.randint(0, POPULATION_RANGE + 1, (NSAMPLES, SAMPLE_SIZE))
samples_means = np.mean(samples, axis=1)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
plt.title('Merkezi Limit Teoremi', fontweight='bold', pad=10)
plt.hist(samples_means, bins=50)

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Tabii yukarıdaki örneği hiç NumPy kullanmadan tamamen Python standart kütüphanesi ile de yapabilirdik.
#----------------------------------------------------------------------------------------------------------------------------

import random
import statistics

POPULATION_RANGE = 1_000_000
NSAMPLES = 100_000
SAMPLE_SIZE = 50

samples_means = [statistics.mean(random.sample(range(POPULATION_RANGE + 1), SAMPLE_SIZE)) for _ in range(NSAMPLES)]

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
plt.title('Merkezi Limit Teoremi', fontweight='bold', pad=10)
plt.hist(samples_means, bins=50)

plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Merkezi limit teoremine göre örnek ortalamalarına ilişkin normal dağılımın ortalaması anakütle ortalamasına eşittir. 
    (Yani bir anakütleden çekilen örnek ortalamalarının ortalaması anakütle ortalaması ile aynıdır.) İstatistiksel sembollerle
    bu durum aşağıdaki gibi ifade edilmektedir:

    Muxbar = Mu (Mu yerine mu sembolü xbar yer yerine de x üstü çizgi olduğunu varsayınız) 

    Aşağıdaki programda bu durum gösterilmiştir. Ancak aşağıdaki örnekte anakütle ortalaması ile örnek ortalamalarının 
    ortalaması arasında nispeten küçük bir fark oluşabilecektir. Bunun nedeni bizim tüm örnekleri değil yalnızca 10,000 
    örneği almamızdan kaynaklanmaktadır. 1,000,000 tane elemanın 50'li altküme sayıları Python'da math.comb fonksiyonu ile 
    elde edilebilir. Bu aşağıdaki gibi bir değerdir:

    328392407820232468120659145980537063071733850478231049855714233423603813357488386226050559849769116542462344220085901
    45569291064005813572729171162888456999821437660641232861427083961136203001102459836375149013720622748800690778924980000
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

POPULATION_RANGE = 1_000_000
NSAMPLES = 100_000
SAMPLE_SIZE = 50

population_mean = np.mean(range(POPULATION_RANGE + 1))
samples = np.random.randint(1, POPULATION_RANGE + 1, (NSAMPLES, SAMPLE_SIZE))
samples_means = np.mean(samples, axis=1)
samples_means_mean = np.mean(samples_means)

print(f'Anakütle ortalaması: {population_mean}')
print(f'Örnek ortalamalarının Ortalaması: {samples_means_mean}')

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda bir anakütleden alınan örnek ortalamalarının normal dağıldığını ve bu normal dağılımın ortalamasının da anakütle 
    ortalamasına eşit olduğunu söyledik. Peki örnek ortalamalarına ilişkin normal dağılımın standart sapması nasıldır? İşte 
    merkezi limit teoremine göre örnek ortalamalarının standart sapması "anakütle kütle standart sapması / karekök(n)" biçimindedir. 
    Yani örnek ortalamalarının standart sapması anakütle sapmasından oldukça küçüktür ve örnek büyüklüğüne bağlıdır. Örnek 
    ortalamalarının standart sapmasına "standart hata (standard error)" da denilmektedir. Görüldüğü gibi eğer örnek ortalamalarına 
    ilişkin normal dağılımın standart sapması düşürülmek isteniyorsa (başka bir deyişle standart hata azaltılmak isteniyorsa) 
    örnek büyüklüğü artırılmalıdır. 

    Aşağıdaki örnekte örnek ortalamalarına ilişkin dağılımın standart sapmasının merkezi limit teoreminde belirtilen durum ile 
    uygunluğu gösterilmektedir. Bu bu örnekte de aslında biz anakütle içerisinden az sayıda örnek aldığımız için olasmı gereken
    değerlerden bir sapma söz konusu olacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

POPULATION_RANGE = 1_000_000
NSAMPLES = 100_000
SAMPLE_SIZE = 50

population_mean = np.mean(range(POPULATION_RANGE + 1))
population_std = np.std(range(POPULATION_RANGE + 1))

samples = np.random.randint(1, POPULATION_RANGE + 1, (NSAMPLES, SAMPLE_SIZE))
samples_means = np.mean(samples, axis=1)
samples_means_mean = np.mean(samples_means)
sample_means_std = np.std(samples_means)

print(f'Anakütle ortalaması: {population_mean}')
print(f'Örnek ortalamalarının Ortalaması: {samples_means_mean}')
print(f'Fark: {np.abs(population_mean - samples_means_mean)}')

print(f'Merkezi limit teroreminden elde edilen örnek ortalamalarının standart sapması: {population_std / np.sqrt(50)}')
print(f'Örnek ortalamalarının standart sapması: {sample_means_std}')

#----------------------------------------------------------------------------------------------------------------------------
    Merkezi limit teoremine göre eğer anakütleden çekilen örnekler büyükse yani tipik olarak n örnek büyüklüğü, N ise anakütle 
    büyüklüğü olmak üzere n / N >= 0.05 ise bu durumda örneklem dağılımın standart sapması için "anakütle standart sapması / kök(n)" 
    değeri "düzeltme faktörü (correction factor)" denilen bir çarpanla çarpılmalıdır. Düzeltme faktörü karekök((N - n)/(N -1))
    biçimindedir. Bu konunun ayrıntıları için başka kaynaklara başvurabilirsiniz. Örneğin anakütle 100 elemandan oluşuyor olsun. 
    Biz 30 elemanlı örnekler çekersek bu düzeltme faktörünü kullanmalıyız. Tabii paratikte genellikle n / N değeri 0.05'ten 
    oldukça küçük olma eğilmindedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıdaki örneklerde anakütlenin normal dağılmadığına düzgün (uniform) dağıldığına dikkat ediniz. Çünkü biz anakütleyi 
    0'dan 1,000,000'a kadar sayılardan anakütleyi oluşturduk. Yukarıdaki örneklerde oluşturduğumuz anakütlenin histogramı
    aşağıda çizdirilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt

POPULATION_RANGE = 1_000_000

population = range(0, POPULATION_RANGE + 1)

plt.figure(figsize=(12, 8))
plt.title('Merkezi Limit Teoremi', fontweight='bold', pad=10)
plt.hist(population, bins=50)

#----------------------------------------------------------------------------------------------------------------------------
    Merkezi limit teoreminde anakütlenin normal dağılmış olması gerekmez. Nitekim yukarıdaki örneklerimizde bir anakütleyi 
    "düzgün dağılıma (uniform distribution)" ilişkin olacak biçimde oluşturduk. Ancak eğer anakütle normal dağılmamışsa 
    örneklem ortalamalarının dağılımının normal olması için örneklerin belli bir değerden büyük olması gerekmektedir. Bu değer 
    tipik olarak >= 30 biçimindedir. Tabii örnek büyüklüğü 30'dan küçükse de dağılım yine normal dağılıma benzemektedir. Ancak 
    anakütlenin normal dağılmadığı durumda örnek ortalamalarının normal dağılması için örnek büyüklüğü >= 30 biçiminde alınmalıdır. 
    Bu nedenle eğer anakütle "normal dağılmamışsa ve örnek büyüklüğü < 30" ise parametre tahmininde merkezi limit teoreminden 
    faydalanılması tatminkar sonuç elde edilmesini engelleyebilmektedir. Bu durumda "parametrik olmayan (non-paranmetric) yöntemler" 
    denilen yöntemler tercih edilmelidir. 

    Buradan çıkan sonuçlar özetle şöyledir:

    - Eğer anakütle normal dağılmışsa örnek büyüklüğü ne olursa olsun örneklem ortalamalarının dağılımı normaldir. 

    - Eğer anakütle normal dağılmamışsa örnek ortalamalarının dağılımının normal olması için örnek büyüklüğünün >= 30 
    olması gerekir. Tabii n < 30 durumunda yine örneklem dağılımı normale benzemektedir ancak kusurlar oluşmaktadır.

    Özetle anakütle normal dağılmamışsa örnek büyüklüklerinin artırılması gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Parametrik istatistiksel yöntemler genel olarak anakütle ve/veya örneklem dağılımının normal olduğu varsayımı ile yürütülmektedir. 
    Bu nedenle bazen anakütlenin normal dağılıp dağılmadığının örneğe dayalı olarak test edilmesi gerekebilmektedir. Bunun için 
    anakütleden bir örnek alınır. Sonra bu örneğe bakılarak anakütlenin normal dağılıp dağılmadığı belli bir "güven düzeyinde 
    (confidence level)" belirlenir. Buna "normallik testleri" denilmektedir. Aslında normallik testi gözle de üstünkörü 
    yapılabilmektedir. Anakütle içerisinden bir örnek çekip onun histogramını çizersek eğer bu histogram Gauss eğrisine benziyorsa 
    biz anakütlenin de normal dağılmış olduğunu varsayabiliriz. Ancak anakütlenin normal dağılıp dağılmadığının tespit edilmesi 
    için bazı "hipotez testlerinden (hypothesis testing)" faydalanılmaktadır. Normal dağılıma ilişkin iki önemli hipotez testi 
    vardır: "Kolmogorov-Smirnov" testi ve "Shapiro-Wilk" testi. Bu testlerin istatistiksel açıklaması biraz karmaşıktır ve ayrıntılar 
    içermektedir. Biz bu bölümde bu konuda ayrıntılı açıklamalarda bulunmayacağız. Hipotez testleri kursumuzun sonraki bölümleri 
    içerisinde ayrı bir başlık halinde ele alınmaktadır.
 
    Hipotez testlerinde bir hipotez öne sürülür ve bu hipotezin belirli güven düzeyi içerisinde doğrulanıp doğrulanmadığına bakılır. 
    Genel olarak bizim doğrulanmasını istediğimiz hipoteze H0 hipotezi (buna "null hipotez" de denilmektedir), bunun tersini belirten, 
    yani arzu edilmeyen durumu belirten hipoteze de H1 hipotezi denilmektedir. Örneğin normallik testindeki H0 ve H1 hipotezleri 
    şöyle oluşturulabilir:

    H0: Seçilen örnek normal bir anakütleden gelmektedir. 
    H1: Seçilen örnek normal dağılmış bir anakütleden gelmemektedir.

    Normallik testlerinden birisi Kolmogorov-Smirnov testidir (test ünlü Sovyet matematikçisi ve istatistikçisi Andrey Kolmogorov 
    ve Nikolai Smirnov tarafından geliştirildiği için bu isimle anılmaktadır). Bu test SciPy kütüphanesindeki stats modülü içerisinde 
    bulunan kstest fonksiyonuyla uygulanabilmektedir. Fonksiyonun parametrik yapısı şöyledir:

    kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto', *, axis=0, nan_policy='propagate', keepdims=False)

    Fonksiyonunun ilk iki parametresi zorunlu parametrelerdir. Birinci parametre anakütleden rastgele seçilen örneği, ikinci 
    parametre testi yapılacak dağılımın kümülatif dağılım fonksiyonunu almaktadır. Ancak bu parametre kolaylık olsun diye 
    yazısal biçimde de girilebilmektedir. (Kolmogorov-Simirnov testi aslında başka dağılımları test etmek amacıyla da 
    kullanılabilmektedir.) Normallik testi için bu parametre 'norm' ya da norm.cdf biçiminde girilebilir. Fonksiyonun diğer 
    parametrelerini SciPy dokümanlarından inceleyebilirsiniz. Biz burada bu fonksiyonu hedefe yönelik bir biçimde kullanacağız.
    
    kstest fonksiyonu çağrıldığktan sonra bize KstestResult türünden "isimli bir demet (named tuple)" verir. Demetin statistic 
    isimli ilk elemanı test istatistiğini, pvalue isimli ikinci elemanı p değerini belirtir. Bizim burada yapmamız gereken bu 
    p değerinin kendi seçtiğimiz belli bir kritik değerden büyük olup olmadığına bakmaktır. Bu kritik değer tipik olarak 0.05 
    olarak alınmaktadır. Ancak testi daha katı yapacaksanız bu değeri 0.01 gibi daha küçük tutabilirsiniz. Yukarıda da belirttiğimiz 
    gibi bu testte iki hipotez vardır:

    H0: Seçilen örnek normal dağılmış bir anakütleden gelmektedir.
    H1: Seçilen örnek normal dağılmış bir anakütleden gelmemektedir.

    Eğer test sonucunda elde ettiğimiz "p değeri belirlediğimiz kritik değereden (0.05) büyükse H0 hipotezi kabul edilir, 
    H1 hipotezi reddedilir. Eğer bu p değeri bu kritik değerden küçükse H0 hipotezi reddedilip, H1 hipotezi kabul edilmektedir. 
    Yani özetle bu p değeri 0.05 gibi bir kritik değerden büyükse örnek normal dağılmış bir anakütleden gelmektedir, 0.05 gibi 
    bir kritik değereden küçükse örnek normal dağılmamış bir anakütleden gelmektedir.

    Aşağıdaki örnekte normal dağılmış bir anakütleden ve düzgün dağılmış bir anakütleden rastgele örnekler çekilip kstest
    fonksiyonuna sokulmuştur. Burada birinci örnek için p değeri 1’e yakın, ikinci örnek için p değeri 0’a çok yakın çıkmıştır. 
    Böylece birinci örneğin alındığı anakütlenin normal dağılmış olduğu ikinci örneğin alındığı anakütlenin ise normal 
    dağılmamış olduğu söylenebilir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm, uniform, kstest

sample_norm = norm.rvs(size=1000)

result = kstest(sample_norm, 'norm')
print(result.pvalue)        # 1'e yakın bir değer             

sample_uniform = uniform.rvs(size=1000) 

result = kstest(sample_uniform, norm.cdf)
print(result.pvalue)        # 0'a çok yakın bir değer

#----------------------------------------------------------------------------------------------------------------------------
                                            17. Ders - 24/02/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    kstest fonksiyonunda ikinci parametreye 'norm' girildiğinde birinci parametredeki değerler ortalaması 0, standart sapması 
    1 olan standart normal dağılıma uygun değerler olmalıdır. Eğer ortalama ve standart sapması farklı bir normal dağılım için 
    test yapılacaksa dağılımın parametreleri de ayrıca args parametresiyle verilmelidir. Örneğin biz ortalaması 100 standart 
    sapması 15 olan bir dağılıma ilişkin test yapmak isteyelim. Bu durumda test aşağıdaki gibi yapılmalıdır. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm, uniform, kstest

sample_norm = norm.rvs(100, 15, size=1000)
result_norm = kstest(sample_norm, 'norm', args=(100, 15))  

sample_uniform = uniform.rvs(100, 100, size=1000) 
result_uniform = kstest(sample_uniform, 'norm', args=(100, 15))

import matplotlib.pyplot as plt 

plt.figure(figsize=(20, 8))
ax1 = plt.subplot(1, 2, 1)
ax1.set_title(f'p değeri: {result_norm.pvalue}', pad=15, fontweight='bold')

ax2 = plt.subplot(1, 2, 2)
ax2.set_title(f'p değeri: {result_uniform.pvalue}', pad=15, fontweight='bold')
ax1.hist(sample_norm, bins=20)

ax2.hist(sample_uniform, bins=20)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Shapiro-Wilk testi scipy.stats modülündeki shapiro fonksiyonuyla yapılmaktadır. Fonksiyonun parametrik yapısı şöyledir:

    shapiro(x, *, axis=None, nan_policy='propagate', keepdims=False)

    Bu fonksiyonun kullanılması daha kolaydır. Bu fonksiyonun tek bir zorunlu parametresi vardır. Bu parametre anakütleden 
    çekilen örneği belirtir. Buradaki normal dağılım herhangi bir ortalama ve standart sapmaya ilişkin olabilir. Yani bizim 
    dağılım değerlerini ortalaması 0, standart sapması 1 olacak biçimde ölçeklendirmemiz gerekmemektedir.

    Aşağıdaki örnekte ortalaması 100, standart sapması 15 olan normal dağılmış ve düzgün dağılmış bir anakütleden 1000'lik 
    bir örnek seçilip Shapiro-Wilk testine sokulmuştur. Buradan elde edine pvalue değerlerine dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import norm, uniform, shapiro

sample_norm = norm.rvs(100, 15, size=1000)

result_norm = shapiro(sample_norm)       
sample_uniform = uniform.rvs(100, 100, size=1000) 
result_uniform = shapiro(sample_uniform)

import matplotlib.pyplot as plt 

plt.figure(figsize=(20, 8))
ax1 = plt.subplot(1, 2, 1)
ax1.set_title(f'p değeri: {result_norm.pvalue}', pad=15, fontweight='bold')

ax2 = plt.subplot(1, 2, 2)
ax2.set_title(f'p değeri: {result_uniform.pvalue}', pad=15, fontweight='bold')
ax1.hist(sample_norm, bins=20)

ax2.hist(sample_uniform, bins=20)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Peki Kolmogorov-Simirnov testi ile Shapiro-Wilk testi arasında ne farklılık vardır? Aslında bu iki test arasında bazı 
    spesifik farklılıklar bulunmaktadır. Ancak bunların her ikisi de normalliğin test edilmesi için kullanılabilmektedir. 
    Shapiro-Wilk testinin kullanımı daha kolaydır. Ayrıca anakütleden çekilen örnekler küçükse (tipik olarak <= 50) Shapiro-Wilk
    testi Kolmogorov-Simirnov testine göre daha iyi bir sonucun elde edilmesine yol açmaktadır. Yani örneğiniz küçükse Shapiro-Wilk
    testini tercih edebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistikte örneğe dayalı olarak anakütlenin ortalamasını (ve/veya standart sapmasını) tahmin etme sürecine "parametre 
    tahmini (parameter estimation)" denilmektedir. Parametre tahmini "noktasal olarak (point estimate)" ya da "aralıksal 
    olarak (interval estimate)" yapılabilmektedir. Örnekten hareketle anakütle ortalamasının belli bir aralıkta ve belli bir 
    güven düzeyinde tahmin edilmesi önemli bir konudur. Böylece biz anakütlenin tamamını gözden geçirmeden, oradan aldığımız 
    bir örneğe bakarak anakütle ortalamasını belli bir güven düzeyinde (confidence level) aralıksal olarak tahmin edebiliriz. 
    Anakütle parametrelerinin aralıksal tahminine "güven aralıkları (confidence interval)" da denilmektedir. Güven aralıkları 
    tamamen merkezi limit teroremi kullanılarak oluşturulmaktadır. Yani güven aralıkları merkezi limit teoreminin en açık 
    uygulamalarından biridir.

    Güven aralıkları bir anakütleden çekilen örneğe bağlı olarak anakütle parametrelerinin belli bir güven düzeyi (confidence 
    level) içerisinde aralıksal olarak belirlenmesini hedeflemektedir. Merkezi limit teoremine göre bir anakütleden çekilen 
    örneklemlerin ortalamalarının normal dağıldığını görmüştük. O halde biz bir anakütleden rastgele bir örnek seçip onun 
    ortalamasına bakarak anakütle ortalamasını belli bir güven düzeyinde tahmin edebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bizim anakütle ortalamasını bilmediğimizi ancak anakütle standart sapmasını bildiğimizi varsayalım. (Genellikle aslında 
    anakütle standart sapmasını da bilmeyiz. Ancak burada bildiğimizi varsayıyoruz.) Bu anakütleden rastgele bir örnek seçtiğimizde 
    o örneğin ortalamasına bakarak anakütle ortalamasını belli bir aralıkta belli bir güven düzeyinde tahmin edebiliriz. Şöyle 
    ki: Örneğin seçtiğimiz güven düzeyi %95 olsun. Bu durumda bizim örneğimiz örnek ortalamalarının dağılımında en kötü olasılıkla 
    soldan 0.025 ve sağdan 0.975 kümülatif olasılığa karşı gelen x değerlerinden biri olabilir. O halde yapacağımız şey örnek 
    ortalamasının örneklem dağılımına göre seçtiğimiz örneğin ortalamasının %47.5 soluna ve %47.5 sağına ilişkin değerlerin elde 
    edilmesidir. Bu durumda anakütle ortalaması %95 güven düzeyi içerisinde bu aralıkta olacaktır. Tabii aslında bu işlemi daha 
    basit olarak "rastgele elde ettiğimiz örneğin ortalamasını normal dağılımın merkezine alarak soldan 0.025 ve sağdan 0.975 
    kümülatif olasılık değerlerine karşı gelen noktaları elde ederek de" yapabiliriz.  

    Örneğin standart sapması 15 olan bir anakütleden rastgele 60 elemanlık bir örnek elde etmiş olalım. Bu örneğin ortalamasının 
    109 olduğunu varsayalım. Bu durumda %95 güven düzeyi içerisinde anakütle ortalamasına ilişkin güven aralıkları aşağıdaki gibi
    elde edilebilir:

    import numpy as np
    from scipy.stats import norm

    sample_size = 60
    population_std = 15
    sample_mean = 109
    sampling_mean_std = population_std / np.sqrt(sample_size)

    lower_bound = norm.ppf(0.025, sample_mean, sampling_mean_std)
    upper_bound = norm.ppf(0.975,  sample_mean, sampling_mean_std)

    print(f'[{lower_bound}, {upper_bound}]')              # [105.20454606435501, 112.79545393564499]

    Burada biz anakütlenin standart sapmasını bildiğimiz için örnek ortalamalarına ilişkin normal dağılımın standart sapmasını
    hesaplayabildik. Buradan elde ettiğimiz güven aralığı şöyle olmaktadır:

    [105.20454606435501, 112.79545393564499]

    Güven düzeyini yükseltirsek güven aralığının genişleyeceği açıktır. Örneğin bu problem için güven düzeyini %99 olarak 
    belirlemiş olalım:

    import numpy as np
    from scipy.stats import norm

    sample_size = 60
    population_std = 15
    sample_mean = 109
    sampling_mean_std = population_std / np.sqrt(sample_size)

    lower_bound = norm.ppf(0.005, sample_mean, sampling_mean_std)
    upper_bound = norm.ppf(0.995,  sample_mean, sampling_mean_std)

    print(f'[{lower_bound}, {upper_bound}]')              [104.01192800234102, 113.98807199765896]

    Burada güven aralığının aşağıdaki gibi olduğunu göreceksiniz:

    [104.01192800234102, 113.98807199765896]

    Gördüğünüz gibi aralık büyümüştür.

    Aşağıdaki programda %95 güven düzeyi için güven aralığı hesaplanmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm

sample_size = 60
population_std = 15
sample_mean = 109
sampling_mean_std = population_std / np.sqrt(sample_size)

lower_bound = norm.ppf(0.025, sample_mean, sampling_mean_std)
upper_bound = norm.ppf(0.975,  sample_mean, sampling_mean_std)

print(f'[{lower_bound}, {upper_bound}]')      # [105.20454606435501, 112.79545393564499]

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki programda ise %99 güven düzeyi için güven aralığı elde edilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm

sample_size = 60
population_std = 15
sample_mean = 109
sampling_mean_std = population_std / np.sqrt(sample_size)

lower_bound = norm.ppf(0.005, sample_mean, sampling_mean_std)
upper_bound = norm.ppf(0.995,  sample_mean, sampling_mean_std)

print(f'{lower_bound}, {upper_bound}')      # 104.01192800234102, 113.98807199765896

#----------------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi örnek ortalamalarına ilişkin dağılımın standart sapmasına "standart hata (standard error)" deniliyordu. 
    Örnek ortalamalarına ilişkin dağılımın standart sapması azaltılırsa (yani standart hata düşürülürse) değerler ortalamaya 
    yaklaşacağına göre güven aralıkları da daralacaktır. O halde anakütle ortalamasını tahmin ederken büyük örnek seçmemiz 
    güven aralıklarını daraltacaktır. Aşağıdaki örnekte yuklarıdaki problemin 30'dan 100'e kadar beşer artırımla örnek 
    büyüklükleri için %99 güven düzeyinde güven aralıkları elde edilmiştir. Elde edilen aralıklar şöyledir:

    sample size: 30: [103.63241756884852, 114.36758243115148]
    sample size: 35: [104.03058429805395, 113.96941570194605]
    sample size: 40: [104.35153725771579, 113.64846274228421]
    sample size: 45: [104.61738729711709, 113.38261270288291]
    sample size: 50: [104.84228852695097, 113.15771147304903]
    sample size: 55: [105.03577765357056, 112.96422234642944]
    sample size: 60: [105.20454606435501, 112.79545393564499]
    sample size: 65: [105.3534458105975, 112.6465541894025]
    sample size: 70: [105.48609245861904, 112.51390754138096]
    sample size: 75: [105.60524279777148, 112.39475720222852]
    sample size: 80: [105.71304047283782, 112.28695952716218]
    sample size: 85: [105.81118086644236, 112.18881913355763]
    sample size: 90: [105.9010248384772, 112.0989751615228]
    sample size: 95: [105.98367907149301, 112.01632092850699]
    sample size: 100: [106.06005402318992, 111.93994597681008]

    Buradan da gördüğünüz gibi örneği büyüttüğümüzde güven aralıkları daralmakta ve anakütle ortalaması daha iyi tahmin 
    edilmektedir. Örnek büyüklüğünün artırılması belli bir noktaya kadar aralığı iyi bir biçimde daraltıyorsa da belli bir
    noktadan sonra bu daralma azalmaya başlamaktadır. Örneklerin elde edilmesinin belli bir çaba gerektirdiği durumda örnek
    büyüklüğünün makul seçilmesi önemli olmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm

population_std = 15
sample_mean = 109

for sample_size in range(30, 105, 5):
    sampling_mean_std = population_std / np.sqrt(sample_size)

    lower_bound = norm.ppf(0.025, sample_mean, sampling_mean_std)
    upper_bound = norm.ppf(0.975,  sample_mean, sampling_mean_std)

    print(f'sample size: {sample_size}: [{lower_bound}, {upper_bound}]')     

#----------------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi anakütle normal dağılmamışsa merkezi limit teoreminin yeterli bir biçimde uygulanabilmesi için örneklerin
    büyük olması (tipik olarak >= 30) gerekiyordu. O halde güven aralıklarını oluştururken eğer anakütle normal dağılmamışsa
    bizim örnekleri >= 30 olacak biçimde seçmemiz uygun olur.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Daha önceden de belirttiğimiz gibi eskiden bilgisayarların yoğun olarak kullanılmadığı zamanlarda hesaplamaları pratik 
    hale getirmek için ortalaması 0, standart sapması 1 olan "standart normal dağılım" tabloları kullanılıyrodu. Bunlara Z
    tablosu dendiğini, bu Z tablolarında belli bir Z değeri için (standart normal dağılımdaki X değerlerine de Z değerleri 
    dendiğini anımsayınız) kümülatif olasılıklar bulundurulmaktaydı. Tabii artık bilgisayarların bu kadar yoğun kullanıldığı 
    günümüzde okul sınavları dışında Z tablolarının kullanımı ortadan kalkmıştır. İşte bu eski devirlerde güven aralıkları 
    da bu Z tablolarına bakılarak oluşturulordu. Herhangi bir standart sapma ve ortalamaya ilişkin x değerinin Z değerine 
    aşağıdaki gibi dönüştürüldüğünü anımsayınız:

    Z = (x - mu) / sigma

    O halde buradaki X örneğin ortalaması, sigma ise örnek ortalamalarına ilişkin örneklem dağılımının standart sapması olmak 
    üzere Z tablosuna dayalı olarak güven aralıkları aşağıdaki gibi oluşturulabilir:

    xbar ⩲ Z * sigma_xbar

    Aşağıda bu yöntemle Z değerleri kullanılarak güven aralığının oluşturulmasına bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm

sample_size = 60
population_std = 15
sample_mean = 109
sampling_mean_std = population_std / np.sqrt(sample_size)

upper_bound = sample_mean + norm.ppf(0.975) * sampling_mean_std
lower_bound = sample_mean - norm.ppf(0.975) * sampling_mean_std

print(f'[{lower_bound}, {upper_bound}]')      # [105.20454606435501, 112.79545393564499]

#----------------------------------------------------------------------------------------------------------------------------
    Örnekten hareketle anakütle ortalamaları aslında tek hamlede norm nesnesinin ilişkin olduğu sınıfın interval metoduyla da
    elde edilebilmektedir. interval metodunun parametrik yapısı şöyledir:
    
    interval(confidence, loc=0, scale=1)

    Metodun birinci parametresi olan confidence güven düzeyini belirtmektedir. Örneğin %95 güven düzeyi için bu parametre 0.95 
    girilmelidir. Metodun ikinci ve üçüncü parametreleri (loc ve scale parametreleri) örneğin ortalamasını ve örneklem dağılımının 
    standart sapmasını belirtir. Yani biz ikinci parametreye örnek ortalamasını, üçüncü parametreye de anakütle standart sapmasından 
    hareketle elde ettiğimiz örneklem standart sapmasını (sigma / kök(n)) girmeliyiz. Metot güven aralığını belirten bir demetle 
    geri döner. Demetin ilk elemanı alt sınır, ikinci elemanı üst sınır değerlerini vermektedir.
    
    Bu durumda yukarıdaki problemi interval metoduyla aşağıdaki gibi de çözebiliriz:

    import numpy as np
    from scipy.stats import norm

    sample_size = 60
    population_std = 15
    sample_mean = 109
    sampling_mean_std = population_std / np.sqrt(sample_size)

    lower_bound, upper_bound = norm.interval(0.95, sample_mean, sampling_mean_std)
    print(f'{lower_bound}, {upper_bound}')     
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm

sample_size = 60
population_std = 15
sample_mean = 109

sampling_mean_std = population_std / np.sqrt(sample_size)

lower_bound = norm.ppf(0.025, sample_mean, sampling_mean_std)
upper_bound = norm.ppf(0.975,  sample_mean, sampling_mean_std)

print(f'{lower_bound}, {upper_bound}')     

lower_bound, upper_bound = norm.interval(0.95, sample_mean, sampling_mean_std)

print(f'{lower_bound}, {upper_bound}')     

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte ortalaması 100, standart sapması 15 olan normal dağılıma uygun rastgeele 1,000,000 değer üretilmiştir.
    Bu değerlerin anakütleyi oluşturduğu varsayılmıştır. Sonra bu anakütle içerisinden rastgele 60 elemanlık bir örnek elde
    edilmştir. Bu örneğe dayanılarak anakütle ortalaması norm nesnesinin interval metoduyla 0.95 güven düzeyiyle elde edilip 
    ekrana yazdırılmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import norm

POPULATION_SIZE = 1_000_000
SAMPLE_SIZE = 60

population = norm.rvs(100, 15, POPULATION_SIZE)

population_mean = np.mean(population)
population_std = np.std(population)

print(f'population mean: {population_mean}')
print(f'population std: {population_std}')

sample = np.random.choice(population, SAMPLE_SIZE)
sample_mean = np.mean(sample)
sampling_mean_std = population_std / np.sqrt(SAMPLE_SIZE)

print(f'sample mean: {sample_mean}')

lower_bound, upper_bound = norm.interval(0.95, sample_mean, sampling_mean_std)
print(f'[{lower_bound}, {upper_bound}]')

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki örneklerde güven aralıklarını oluştururken anakütle standart sapmasının bilindiğini varsaydık. Halbuki genellikle
    anakütle ortalamasının bilinmediği durumda anakütle standart sapması da bilinmemektedir. Peki bu durumda örnekten hareketle 
    anakütle ortalamasının aralık tahmini nasıl yapılacaktır? İşte bu durumda çektiğimiz örneğin standart sapması sanki anakütlenin 
    standart sapmasymış gibi işleme sokulmaktadır. Ancak dağılım olarak normal dağılım değil t dağılımı kullanılmaktadır. Zaten 
    William Gosset t dağılımını tamamen böyle bir problem üzerinde çalışırken geliştirmiştir. Yani t dağılımı zaten "anakütle 
    standart sapmasının bilinmediği durumda örneğin standart sapmasının anakütle standart sapması olarak alınmasıyla" elde edilen 
    bir dağılımdır. 

    t dağılımının serbestlik derecesi denilen bir değere sahip olduğunu anımsayınız. Serbestlik derecesi örnek büyüklüğünün bir 
    eksik değeridir. Ayrıca 30 serbestlik derecesinden sonra zaten t dağılımının normal dağılıma çok benzediğini de belirtmiştik. 
    Çektiğimiz örneğin standart sapmasını anakütle standart sapması olarak kullanırken örneğin standart sapması N'e değil (N - 1)'e 
    bölünerek hesaplanmalıdır. Burada bölmenin neden (N - 1)'e yapıldığının açıklaması biraz karmaşıktır. Biz kursumuzda bu konu 
    üzerinde durmayacağız. Ancak Internet'te bu konuyu açıklayan pek çok kaynak bulunmaktadır. İstatistikte çekilen örneklerin 
    standart sapmaları genellikle sigma sembolü ile değil s harfiyle belirtilmektedir.

    Anımsanacağı gibi pek çok kütüphanede standart sapma ya da varyans hesaplanırken kaça bölme yapılacağına "ddof (delta degrees 
    of freedom)" deniyordu. Standart sapma ya da varyans hesabı yapan fonksiyonların ddof parametreleri vardı. (NumPy'da bu 
    ddof parametreleri default 0 iken Pandas'da 1'dir.) Yani bu ddof parametresi (N - değer)'deki değeri belirtmektedir. Örneğin 
    ddof = 0 ise bölme N'e ddof = 1 ise bölme (N - 1)'e yapılmaktadır. 
   
    Örneğin anakütleden aşağıdaki gibi 35'lik bir örnek çekmiş olalım:

    sample = np.array([101.93386212, 106.66664836, 127.72179427,  67.18904948, 87.1273706 ,  76.37932669,  87.99167058,  95.16206704,
       101.78211828,  80.71674993, 126.3793041 , 105.07860807, 98.4475209 , 124.47749601,  82.79645255,  82.65166373, 92.17531189, 
       117.31491413, 105.75232982,  94.46720598, 100.3795159 ,  94.34234528,  86.78805744,  97.79039692, 81.77519378, 117.61282039, 
       109.08162784, 119.30896688, 98.3008706 ,  96.21075454, 100.52072909, 127.48794967, 100.96706301, 104.24326515, 101.49111644])

    Anakütlenin standart sapmasının da bilinmediğini varsayalım. Bu değerlerden hareketle %95 güven düzeyinde güven aralığını 
    şöyle oluşturabiliriz:

    import numpy as np
    from scipy.stats import t

    sample = np.array([101.93386212, 106.66664836, 127.72179427,  67.18904948, 87.1273706 ,  76.37932669,  
                    87.99167058,  95.16206704, 101.78211828,  80.71674993, 126.3793041 , 105.07860807, 
                    98.4475209 , 124.47749601,  82.79645255,  82.65166373, 92.17531189, 117.31491413, 
                    105.75232982,  94.46720598, 100.3795159 ,  94.34234528,  86.78805744,  97.79039692, 
                    81.77519378, 117.61282039, 109.08162784, 119.30896688, 98.3008706 ,  96.21075454, 
                    100.52072909, 127.48794967, 100.96706301, 104.24326515, 101.49111644])

    sample_mean = np.mean(sample)
    sample_std = np.std(sample, ddof=1)
    sampling_mean_std = sample_std / np.sqrt(len(sample))

    lower_bound = t.ppf(0.025, len(sample) - 1, sample_mean, sampling_mean_std)
    upper_bound = t.ppf(0.975, len(sample) - 1, sample_mean, sampling_mean_std)

    print(f'[{lower_bound}, {upper_bound}]')    # [94.81902512213665, 105.0959541612919]
  
    Burada örneğin standart sapmasını hesaplarken ddof=1 kullandığımıza dikkat ediniz. Güven aralıkları normal dağılım kullanılarak 
    değil t dağılımı kullanılarak elde edilmiştir. t dağılımındaki serbestlik derecesinin (ppf fonksiyonun ikinci parametresi) 
    örnek büyüklüğünün bir eksik değeri olarak alındığına dikkat edniz.

    Örneklem dağılımının standart sapmasına standart hata denildiğini anımsayınız. Aslında scipy.stats modülünde bu değeri hesaplayan 
    sem isimli bir fonksiyon da bulunmaktadır. Yukarıdaki örneğimizde bu standart hata değerini iki satırda bulmuştuk:

    sample_std = np.std(sample, ddof=1)
    sampling_mean_std = sample_std / np.sqrt(len(sample))

    Bu işlem tek hamlede sem fonksiyonuyla da yapılabilmektir:

    sampling_mean_std = sem(sample)

    30 serbestlik derecesinden sonra artık t dağılımının normal dağılımla örtüşmeye başladığını anımsayınız. Buradaki örneğimizde
    örnek büyüklüğü 35'tir. Örnek büyüklüğü >= 30 durumunda t dağılı ile normal dağılım birbirine çok benzediği için aslında
    bu örnekte t dağılımı yerine normal dağılım kullanılsaydı da önemli bir fark oluşmayacaktı. Eskiden bilgisayarların yoğun 
    kullanılmadığı zamanlarda  t dağılımı için de "t tabloları" kullanılıyordu. t tablolarında her bir serbestlik derecesi için 
    ayrı girişler bulunduruluyordu. Genellikle t tabloları çok büyük olmasın diye 30'a kadar serbestlik derecesini içeriyordu. 
    Dolayısıyla eskiden N >= 30 durumunda t tablosu yerine Z tablolarının kullanılması çok yaygındı. Ancak günümüzde bu tür 
    işlemleri bilgisayarlarla yaptığımız için N >= 30 durumunda da t dağılımını kullanmak daha uygun olmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import t

sample = np.array([101.93386212, 106.66664836, 127.72179427,  67.18904948, 87.1273706 ,  76.37932669,  
                   87.99167058,  95.16206704, 101.78211828,  80.71674993, 126.3793041 , 105.07860807, 
                   98.4475209 , 124.47749601,  82.79645255,  82.65166373, 92.17531189, 117.31491413, 
                   105.75232982,  94.46720598, 100.3795159 ,  94.34234528,  86.78805744,  97.79039692, 
                   81.77519378, 117.61282039, 109.08162784, 119.30896688, 98.3008706 ,  96.21075454, 
                   100.52072909, 127.48794967, 100.96706301, 104.24326515, 101.49111644])

sample_mean = np.mean(sample)
sample_std = np.std(sample, ddof=1)
sampling_mean_std = sample_std / np.sqrt(len(sample))

lower_bound = t.ppf(0.025, len(sample) - 1, sample_mean, sampling_mean_std)
upper_bound = t.ppf(0.975, len(sample) - 1, sample_mean, sampling_mean_std)

print(f'[{lower_bound}, {upper_bound}]')

#----------------------------------------------------------------------------------------------------------------------------
                                                18. Ders - 25/02/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tıpkı scipy.stats modülündeki norm nesnesinde olduğu gibi t nesnesinin de ilişkin olduğu snıfın interval isimli bir metodu 
    bulunmaktadır. Bu metot zaten doğrudan t dağılımını kullanarak güven aralıklarını hesaplamaktadır. interval metodunun 
    parametrik yapısı şöyledir:

    interval(confidence, df, loc=0, scale=1)

    Buradaki confidence parametresi yine "güven düzeyini (confidence level)" belirtmektedir. df parametresi serbestlik derecesini
    belirtir. loc ve scale parametreleri de sırasıyla ortalama ve standart sapma değerlerini belirtmektedir. Burada loc parametresine 
    biz örneğimizin ortalamasını, scale parametresine de örneklem dağılımının standart sapmasını girmeliyiz. Tabii örneklem 
    dağılımının standart sapması yine örnekten hareketle elde edilecektir. Metot güven aralığının alt ve üst sıbır değerlerini 
    bir demet biçiminde geri döndürmektedir. Örneğin:

    sample_mean = np.mean(sample)
    sample_std = np.std(sample, ddof=1)
    sampling_mean_std = sample_std / np.sqrt(len(sample))
    
    lower_bound, upper_bound = t.interval(0.95, len(sample) - 1, sample_mean, sampling_mean_std)
    print(f'[{lower_bound}, {upper_bound}]')

    Burada sample örneğine dayanılarak %95 güven düzeyinde güven aralıkları oluşturulmuştur.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import t

sample = np.array([101.93386212, 106.66664836, 127.72179427,  67.18904948, 87.1273706 ,  76.37932669,  
                   87.99167058,  95.16206704, 101.78211828,  80.71674993, 126.3793041 , 105.07860807, 
                   98.4475209 , 124.47749601,  82.79645255,  82.65166373, 92.17531189, 117.31491413, 
                   105.75232982,  94.46720598, 100.3795159 ,  94.34234528,  86.78805744,  97.79039692, 
                   81.77519378, 117.61282039, 109.08162784, 119.30896688, 98.3008706 ,  96.21075454, 
                   100.52072909, 127.48794967, 100.96706301, 104.24326515, 101.49111644])

sample_mean = np.mean(sample)
sample_std = np.std(sample, ddof=1)
sampling_mean_std = sample_std / np.sqrt(len(sample))

lower_bound = t.ppf(0.025, len(sample) - 1, sample_mean, sampling_mean_std)
upper_bound = t.ppf(0.975, len(sample) - 1, sample_mean, sampling_mean_std)

print(f'[{lower_bound}, {upper_bound}]')

lower_bound, upper_bound = t.interval(0.95, len(sample) - 1, sample_mean, sampling_mean_std)
print(f'[{lower_bound}, {upper_bound}]')

#----------------------------------------------------------------------------------------------------------------------------
    Anakütle standart sapmasının bilinmediği durumda yine eğer örnek yeteri büyük değilse (tipik olarak < 30 biçimindeyse)
    anakütlenin normal dağılmış olması gerekmektedir. Aksi takdirde yapılan hesaplarda hatalar ortaya çıkabilmektedir. Eğer 
    örnek yeteri kadar büyükse (tipik olarak >= 30) anakütlenin hesaplamalarda normal dağılmış olması gerekmemektedir. Tabii
    t dağılımın asıl kullanılma nedeni örneğin küçük olduğu (tipik olarak < 30) durumlardır. Zaten örnek büyüdükçe (tipik olarak
    >= 30) t dağılımı normal çok yaklaşmaktadır. 

    Peki örneğimiz küçükse (tipik olarak < 30) ve anakütle normal dağılmamışsa güven aralıklarını oluşturamaz mıyız? İşte 
    bu tür durumlarda güven aralıklarının oluşturulması ve bazı hipotez testleri için "parametrik olmayan (nonparametric) 
    yöntemler kullanılmaktadır. Ancak genel olarak parametrik olmayan yöntemler parametrik yöntemlere göre daha daha az güvenilir
    sonuçlar vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesi ve veri bilimi uygulamalarında öncelikle verilerin elde edilmesi ve kullanıma hazır hale getirilmesi 
    gerekmektedir. Kursumuzun bu bölümünde biz "verilerin kullanıma hazır hale getirilmesi (data preparation)" süreci üzerinde
    duracağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Her türlü veri analizi, veri bilimi ve makine öğrenmesi uygulamasında ilk aşama "verilerin toplanması ya da toplanmış olan 
    verilerin elde edilmesi" aşamasıdır. Verilerin toplanması çeşitli yöntemlerle yapılabilmektedir. Örneğin "anket (survey)" 
    yoluyla veriler toplanabilir. Günümüzde sensör teknolojilerinin de gelişmesiyle verilerin otomatik olarak toplanmasıyla 
    da sık karşılaşılmaktadır. Veriler birtakım faaliyet sonucunda kendiliğin de oluşabilmektedir. Örneğin sosyal medyadaki 
    yazışmalarda oluşan veriler zaten sürecin doğal akışı içerisinde elde edilmektedir. Bazen veriler birtakım kurumlar ya da 
    topluluklar tarafından zaten oluşturulmuş durumdadır. Uygulamacının bu verileri ilgili yerden elde etmesi gerekir. Tabii 
    başka kurumlar ve topluluklar tarafından oluşturulmuş olan bu verileri kullanabilmeniz için bu kurumların ya da toplulukların 
    web sitelerine üye olmanız gerekebilir. Örneğin bunlardan en ünlüsü "kaggle.com" isimli sitedir. Bu siteye üye olmanızı 
    tavsiye ederiz. Kaggle 2010 yılında kurulmuştur ve 2017 yılında Google tarafından satın alınmıştır.
    
    Veriler toplandığında onların bir biçimde saklanması gerekir. Veriler veritabanlarında ya da dosyalarda saklanabilirler. 
    Verilerin dosyalarda saklanmsı için çeşitli formatlar kullanılabilmektedir. Fakat önceden de belirttiğimiz gibi makine 
    öğrenmesi ve veri bilimi uygulamalarında en çok kullanılan formatlardan biri "CSV (Comma Separated Values)" formatıdır. 
    Eğer veriler veritabanlarının içerisindeyse uygulamacının ilgili veri tabanı yötetim sistemine bağlanıp verileri oradan 
    çekmesi gerekebilir. Bazı metinsel verilerin doğrudan web sitelerinden elde edilmesi de gerekebilmektedir. Biz kursumuzda 
    genellikle "CSV" ve "HDF (Hierarchical Data Format)" formatlarını kullanacağız. Ancak bazı uygulamalarında gerektiğinde 
    verileri veritabanlarından da çekeceğiz. 

    İstatistik ve veri biliminde organize edilmiş veri topluluklarına "veri kümeleri (data sets)" denilmektedir. Veri kümeleri 
    tipik olarak bir tablo gibi sütunlardan ve satırlardan oluşur. Veri kümesindeki sütunlara "sütun (column)" ya da "özellik
    (feature)", satırlara ise "satır (row)" ya da "kayıt (record)" denilmektedir. Bu bakımdan veri kümelerinin ilişkisel 
    veritabanlarındaki tablolara benzediğine dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Veriler toplandıktan ya da elde edildiktan sonra hemen işleme sokulamayabilir. Veriler üzerinde çeşitli ön işlemlerin yapılması 
    gerekebilmektedir. Bu ön işlemlere "verilerin kullanıma hazır hale getirilmesi (data preparation)" denilmektedir. Biz bu 
    bölümde verilerin kullanıma hazır hale getirilmesi için gerekli olabilecek bazı temel ön işlemler üzerinde duracağız. Ancak 
    bazı ön işlem etkinliklerini gerçekleştirebilmek için başka konuların bilinmesi gerekmektedir. Bu nedenle daha karmaşık ve 
    alana özel ön işlemler ilgili konuların açıklandığı bölümde ele alınacaktır. 
#----------------------------------------------------------------------------------------------------------------------------
    
#----------------------------------------------------------------------------------------------------------------------------
    Veriler tipik olarak aşağıdaki süreçlerden geçilerek kullanıma hazır hale getirilmektedir:

    1) Verilerin Temizlenmesi (Data Cleaning): Veriler eksik, geçersiz ya da aşırı uç değerler (outliers) içerebilir. Aşırı uç 
    değerler genellikle "şüpheli" değerler olarak ele alınmaktadırç. Verilerden eksiki, geçersiz ve aşırı uç değerlerin atılmasına 
    verilerin temizlenmesi denilmektedir. 

    2) Özellik seçimi (Feature Selection): Veri kümelerindeki tüm sütunlar bizim için anlamlı ve gerekli olmayabilir. Gereksiz 
    sütunların atılıp gereklilerin seçilmesi stkinliğine "özellik seçimi" denilmektedir. Örneğin bir veri tablosundaki kişinin 
    "Adı Soyadı" sütunu, ya da verinin sıra numarasını belirten "Indeks" sütunu veri analizi açısından genellikle (ama her zaman 
    değil) bir fayda sağlamamaktadır. Bu durumda bu sütunların atılması gerekir. 

    3) Verilerin Dönüştürülmesi (Data Transformation): Kategorik veriler, tarih ve zaman belirten veriler, resimsel ya da yazısal
    veriler doğrudan işleme sokulamazlar. Bunların sayısal biçime dönüştürülmesi gerekir. Bazen veri kümesindeki sütunlarda önemli 
    skala farklılıkları olabilmektedir. Bu skala farklılıkları algoritmaları olumsuz yönde etkileme eğilimindedi. Sütunların 
    skalalarını birbirine benzer hale getirme sürecine "özellik ölçeklemesi (feature scaling)" denilmektedir. 

    4) Özellik Mühendisliği (Feature Engineering): Veri kümesindeki var olan sütunlardan olmayan başka sütunların oluşturulması 
    sürecine "özellik mühendisliği" denilmektedir. Yani özellik mühendisliği var olan bilgilerden hareketle önemli başka bilgilerin 
    elde edilmesi sürecidir. Örneğinin kişinin boy ve kilosu biliniyorsa biz vücut kitle endeksini veri kümesine yeni bir sütun 
    larak ekleyebiliriz. 

    5) Boyutsal Özellik İndirgemesi (Dimensionality Feature Reduction): Veri kümesinde çok fazla sütun olmasının pek çeşitli 
    dezavantajı olabilmektedir. Örneğin bu tür durumlarda işlem yükü artabilir. Gereksiz sütunlar kestirim sürecini olumsuz 
    biçimde etkileyebilir. Fazla sayıda sütun kursumuzun ilerleyen zamanalarında sıkça karşılaşacağımız "overfitting" denilen 
    yanlış öğrenmelere yol açabilir. İşte bazı durumlarda  sütunların sayısının azaltılması gerekebilir. İşte n tane sütunun 
    k < n olmak üzere k tane sütun haline getirilmesi sürecine "boyutsal özellik indirgemesi" denilmektedir. Bu konu kursumuzda 
    ileride ayrı bir bölümde ele alınacaktır.

    6) Verilerin Çoğaltılması (Data Augmentation): Elimizdeki veriler (veri kümesindeki satırlar) ilgili makine öğrenmesi 
    yöntemini uygulayabilmek için sayı bakımından ya da nitelik bakımından yetersiz olabilir. Eldeki verilerden (satırları 
    kastediyoruz) hareketle yeni verilerin oluşturulması (yeni satırların oluşturulması) sürecine "verilerin çoğaltılması
    (data augmentation)" denilmektedir. Örneğin bir resim döndürülerek ondan pek çok resim elde edilebilir. Benzer biçimde 
    örneğin bir resmin çeşitli kısımları kırpılarak yeni resimler oluşturulabilir. Özellik mühendisliğinin "sütun eklemeye 
    yönelik", verilerin çoğaltılmasının ise "satır eklemeye yönelik" bir süreç olduğuna dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eksik verilerin ortaya çıkma nedenleri çeşitli olabilir. Ancak veri bilimcisini bu noktada ilgilendiren en önemli durum 
    eksik verilerin bir kalıp izleyip izlemediğidir. Genellikle teorik kaynaklar eksik verilerin ortaya çıkma biçimlerini üç 
    grupta ele almaktadır. 

    1) Tamamen Rastgele Oluşan Eksik Veriler (Missing Completely At Random - MCAR): Burada eksik veriler rastgele satırların
    rastgele sütunlarındadır. Bu nedenle eksik veri içeren satır ya da sütunların atılması geri kalan veri kümesini yanlı 
    (biased) hale getirmez. 

    2) Rastgele Oluşan Ekisik Veriler (Missing At Random - (MAR): Burada eksik veriler bir kalıp oluşturmaktadır. Bu kalıp 
    tablonun başka sütunları ile ilgilidir. Örneğin tablonun "Yaş" sütunundaki eksik verilerin çoğunun Cinsiyet sütunundaki 
    Kadın'lara ilişkin olması bu türden bir eksik veridir. Böylesi eksik verilerde eksik verinin bulunduğu satırın tamamen 
    atılması geri kalan veriyi yanlı hale getirebilmektedir. 

    3) Rastgele Oluşmayan Eksik Veriler (Missing Not At Random - MNAR): Burada eksik verilerde bir kalıp vardır ancak bu kalıp 
    tablodaki diğer sütunlarla açıklanamamaktadır. Eğer eksik veriler "tamamen rastgele" ve "rastgele" oluşmuyorsa bu kategoride 
    değerlendirilebilir. Örneğin bir sağlık çalışmasında, kontrol randevusundaki değerler veri kümesinin bir sütununu oluşturuyor 
    olsun. Hastalar tedaviden memnun kalmadıkları için takip randevularına gitmiyorsa bu durumda eksik veriler diğer sütunlarla 
    ilgili değildir. Yine bu türden eksik verilerin bulunduğu satırlar veri kümesinden atılırsa veri kümesinde bir yanlılık 
    oluşabilecektir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eksik veriler CSV dosyalarında genellikle "boş bir eleman biçiminde ya da NaN" , "nan" "NULL", "null" biçiminde ya da "NA" 
    biçiminde karşımıza çıkmaktadır.  
   
    Eksik veri içeren CSV dosyaları Pandas kütüphanesinin read_csv fonksiyonu ile okunduğunda bunlara karşı gelen DataFrame 
    elemanları NaN (Not a Number) olarak elde edilmektedir. (NaN değerinin IEEE 754 kayan noktalı formatlarda geçerli bir sayı 
    belirtmediğini anımsayınız.) Örneğin "person.csv" isimli dosya şu içeriği sahip olsun:

    AdıSoyadı,Kilo,Boy,Yaş,Cinsiyet
    Sacit Bulut,78,172,34,Erkek
    Ayşe Er,67,168,45,Kadın
    Ahmet San,,182,32,Erkek
    Macit Şen,98,156,65,Erkek
    Talat Demir,85,,49,Erkek

    Bu dosyayı şöyle okuyalım:

    import pandas as pd

    df = pd.read_csv('person.csv')

    Şöyle bir çıktı elde ederiz:

        AdıSoyadı  Kilo    Boy  Yaş Cinsiyet
    0  Sacit Bulut  78.0  172.0   34    Erkek
    1      Ayşe Er  67.0  168.0   45    Kadın
    2    Ahmet San   NaN  182.0   32    Erkek
    3    Macit Şen  98.0  156.0   65    Erkek
    4  Talat Demir  85.0    NaN   49    Erkek

    Bir CSV dosyasında özel bazı sözcükler de eksik veri anlamına gelebilmektedir. Ancak hangi özel sözcüklerin eksik veri
    anlamına geldiği CSV okuyucları arasında farklılıklar gösterebilmektedir. Örneğin Pandas'ın read_csv fonksiyonu şu 
    özel sözcükleri "eksik veri" gibi ele almaktadır: NaN, '', '#N/A', '#N/A' 'N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', 
    '-nan', '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None', 'n/a', 'nan', ‘null’. CSV dosyalarında en çok
    karşımıza çıkan eksik veri gösterimlri şunlardır: '', 'NaN', 'nan', 'NA', 'null'. read_csv fonksiyonu ayrıca na_values 
    isimli parametresi yoluyla programcınn istediği yazıları da eksik veri olarak ele alabilmektedir. Bu parametreye yazılardan 
    oluşan dolaşılabilir bir nesne girilmeldir. Örneğin:

    df = pd.read_csv('person.csv', na_values=['NE'])

    Burada read_csv yukarıdakilere ek olarak 'NE' yazısını da eksik veri olarak ele alacaktır. read_csv fonksiyonunun 
    keep_default_na parametresi False olarak girilirse (bu parametrenin default değeri True biçimdedir) bu durumda yukarıda 
    belirttiğimiz eksik veri kabul edilen yazılar artık eksik veri olarak kabul edilmeyecek onlara normal yazı muamalesi 
    yapılacaktır. read_csv fonksiyonu bir süredir yazısal olan sütunların dtype özelliğini "object" olarak tutmaktadır. 
    Yani bu tür sütunların her elemanı farklı türlerden olabilir. Bu tür sütunlarda NaN gibi eksik veriler söz konusu 
    olduğunda read_csv fonksiyonu yine onu np.float64 NaN değeri olarak ele almaktadır.
    
    Eksik veri içeren CSV dosyaları NumPy kütüphanesi ile loadtxt fonksiyonu kullanılarak da okunabilir.  Ancak loadtxt 
    fonksiyonunun bu amaçla kullamılması çok zahmetlidir. Bu nedenle biz kursumuzda CSV dosyalarını genellikle Pandas'ın 
    read_csv fonksiyonu 
    ile okuyacağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eksik verilerle çalışırken ilk yapılması gereken şey eksikliğin analiz edilmesidir. Yani kaç tane eksik veri vardır? Kaç 
    satırda ve hangi sütunlarda eksik veriler bulunmaktadır? Eksik veriler toplam verilerin yüzde kaçını oluşturmaktadır gibi 
    sorulara yanıtlar aranmalıdır.

    Eksik verilerin ele alınmasında iki temel strateji vardır:

    1) Eksik verilerin bulunduğu satırı (nadiren de sütunu) tamamen atmek
    2) Eksik verilerin yerine başka değerler yerleştirmek (imputation). 

    Eksik verilerin bulunduğu satırın atılması yönteminde şunlara dikkat edilmelidir:

    a) Eksik verili satırlar atıldığında elde kalan veri kümesi çok küçülecek midir?
    b) Eksik verili satırlar atıldığında elde kalan veri kümesi yanlı (biased) hale gelecek midir?

    Eğer bu soruların yanıtı "hayır" ise eksik verilerin bulunduğu satırlar tamamen atılabilir. 

    Eksik veriler başka değerlerle doldurulması işlemine İngilizce "imputation" denilmektedir. Eğer eksik verilerin bulunduğu 
    satırın (nadiren de sütunun) atılması uygun görülmüyorsa "imputation" uygulanmalıdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Pandas'ta DataFrame nesnesi df olmak üzere, eksik veri analizinde şu kalıpları kullanabilirsiniz:

    1) Sütunlardaki eksik verilerin miktarları şöyle bulunabilir:

    df.isna().sum() ya da pd.isna(df).sum()

    Pandas'taki isna fonksiyonu aynı zamanda DataFrame ve Series sınıflarında bir metot biçiminde de bulunmaktadır. isna bize 
    bool türden bir DataFrame ya da Series nesnesi vermektedir. bool üzerinde sum işlemi yapıldığında False değerler 0 olarak, 
    True değerler 1 olarak işleme girer. Dolayısıyla yularıdaki işlemlerde biz sütunlardaki eksik veri sayılarını elde etmiş 
    oluruz. isna fonksiyonunun diğer bir ismi isnull biçimindedir. isna ve isnull aynı fonksiyonu belirtmektedir. 

    2) Eksik verilerin toplam sayısı şöyle bulunabilir:

    df.isna().sum().sum() ya da pd.isna(df).sum().sum() 

    isna fonksiyonu (ya da metodu) sütunsal temelde eksik verilerin sayılarını verdiğine göre onların toplamı da toplam eksik 
    verileri verecektir.
    
    3) Eksik verilerin bulunduğu satır sayısı şöyle elde edilebilir:

    pd.isna(df).any(axis=1).sum() ya da df.any(axis=1).sum()

    any fonksiyonu ya da metodu bir eksen parametresi alarak satırsal ya da sütunsal işlem yapabilmektedir. any "en az bir 
    True değer varsa True değerini veren hiç True değer yoksa False değerini veren" bir fonksiyondur. Yukarıdaki ifadede 
    biz önce isna fonksiyonu ile eksik verileri matrisel bir biçimde DataFrame olarak elds ettik. Sonra da onun satırlarına 
    any işlemi uyguladık. Dolayısıyla "en az bir eksik veri olan satırların" sayısını elde etmiş olduk.

    4) Eksik verilerin bulunduğu satır indeksleri şöyle elde edilebilir:

    df.index[pd.isna(df).any(axis=1)] ya da df.loc[df.isna().any(axis=1)].index

    5) Eksik verilerin bulunduğu sütun isimleri şöyle elde edilebilir:

    missing_columns = [name for name in df.columns if df[name].isna().any()]

    Burada liste içlemi kullandık. Önce sütun isimlerini elde edip o sütun bilgilerini doğrudan indesklemeyle elde ettik. Sonra
    o sütunda en az bir eksik veri varsa o sütunun ismini listeye ekledik.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            19. Ders - 02/03/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    "Melbourne Housing Snapshot (MHS)" veri kümesinin eksik veri bakımından incelenmesi ve eksik verilerin rapor edilmesi 
    aşağıdaki gibi yapılabilir. Veri kümesini aşağıdaki sayfadan indirebilirsiniz:

    https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot?resource=download
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('melb_data.csv')

missing_columns = [colname for colname in df.columns if df[colname].isna().any()]
print(f'Eksik verilen bulunduğu sütunlar: {missing_columns}', end='\n\n')

missing_column_dist = df.isna().sum()
print('Eksik verilerin sütunlara göre dağılımı:')
print(missing_column_dist, end='\n\n')

missing_total = df.isna().sum().sum()
print(f'Eksik verilen toplam sayısı: {missing_total}')

missing_ratio = missing_total / df.size
print(f'Eksik verilen oranı: {missing_ratio}')

missing_rows = df.isna().any(axis=1).sum()
print(f'Eksik veri bulunan satırların sayısı: {missing_rows}')

missing_rows_ratio = missing_rows / len(df)
print(f'Eksik veri bulunan satırların oranı: {missing_rows_ratio}')

""" 
Elde Edilen Çıktı

Eksik verilen bulunduğu sütunlar: ['Car', 'BuildingArea', 'YearBuilt', 'CouncilArea']

Eksik verilerin sütunlara göre dağılımı:
Suburb              0
Address             0
Rooms               0
Type                0
Price               0
Method              0
SellerG             0
Date                0
Distance            0
Postcode            0
Bedroom2            0
Bathroom            0
Car                62
Landsize            0
BuildingArea     6450
YearBuilt        5375
CouncilArea      1369
Lattitude           0
Longtitude          0
Regionname          0
Propertycount       0
dtype: int64

Eksik verilen toplam sayısı: 13256
Eksik verilen oranı: 0.04648292306613367
Eksik veri bulunan satırların sayısı: 7384
Eksik veri bulunan satırların oranı: 0.543740795287187
"""

#----------------------------------------------------------------------------------------------------------------------------
    Eksik verileri DataFrame nesnesinden silmek için DataFrame sınıfının dropna metodu kullanılabilir. Bu metotta default axis 
    parameresinin default değeri 0'dır. Yani default durumda satırlar atılmaktadır. Ancak axis=1 argümanıyla sütunların da 
    atılmasını sağlayabiliriz. Metot bize eksik verilerin atıldığı yeni bir DataFrame nesnesi vermektedir. Ancak metodun inplace 
    parametresi True yapılırsa nesne üzerinde atım yapılmaktadır.

    Veri kümesinde çok fazla eksik veri içeren satır bulunuyorsa eksik verilerin bulunduğu satırların atılması önemli bir veri 
    kaybına yol açabilmektedir. Eksik verilerin bulunduğu satırların atılması bazen veri kümesini yanlı hale de getirebilir. 
    (Örneğin veri kümesi anket yöntemiyle elde ediliyor olsun. Kadınların yaşlarını söylememesi Türk kültüründe yaygın bir 
    davranıştır. Böyle bir veri kümesinde eksik verilerin bulunduğu satırlar atılırsa geri kalan satırların çoğu erkeklere 
    ilişkin olabilir.) İşte bu tür durumlarda eksik verilerin bulunduğu satırları atmak yerine eksik değerleri başka değerlerle 
    doldurmaya çalışmak (imputation) daha uygun olabilmektedir. 

    Aşağıda "Melbourne Housing Snapshot (MHS)" veri kümesinde eksik verilerin bulunduğu satır ve sütunların atılmasına bir 
    örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('melb_data.csv')
print(f'Veri kümesinin boyutu: {df.shape}')

df_deleted_rows = df.dropna(axis=0)
print(f'Satır atma sonucundaki yeni boyut: {df_deleted_rows.shape}')

df_deleted_cols = df.dropna(axis=1)
print(f'Sütun atma sonucundaki yeni boyut: {df_deleted_cols.shape}')

#----------------------------------------------------------------------------------------------------------------------------
    Eksik verilerin yerine başka değerlerin yerleştirilmesi işlemine İngilizce "imputation" denildiğini söylemiştik. Biz 
    İngilizce "imputation" sözüğü için Türkçe "doldurma" sözcüğünü de kullanacağız. Kullanılan tipik imputation stratejileri 
    şunlardır::

    - Sütun sayısal ise eksik verileri sütun ortalaması ile doldurulması
    - Sütun kaegorik ya da sırasal ise eksik verilerin mod değeri ile doldurması
    - Sütunlarda uç değeler varsa eksik değerlerin medyan değeri ile doldurulması 
    - Eksik verilerin yerine belli aralıkta ya da dağılımda rastgele değer yerleştirme yöntemi
    - Eksik verilerin zaman serileri biçimindeki veri kğmelerinde önceki ya da sonraki sütun değerleriyle doldurulması
    - Eksik değerlerin regresyonla tahmin tahmin edilerek doldurulması
    - Eksik değerlerin k-NN (K-Nearest Neighbours) yöntemi ile doldurulması

    En çok uygulanan yöntem basitliği nedeniyle sütun ortalaması, sütun modu ya da sütun medyanı ile doldurma yöntemidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi imputation işlemine MHS veri kümesi üzerinde örnek verelim. Bu veri kümesinde eksik veriler şu sütunlarda bulunmaktaydı:
    Car, BuildingArea, YearBuilt, CouncilArea. Imputation işlemi için bu sütunların incelenmesi gerekir. Car sütunu ev için 
    ayrılan otopark alanının kaç arabayı içerdiğini belirtmektedir. Bu sütunda ayrık küçük tamsayı değerler vardır. Burada 
    imputation için sütun oralaması kullanılabilir. Ancak bunların yuvarlanması daha uygun olabilir. Bu işlem şöyle yapılabilir:
    
    impute_val = df['Car'].mean().round()
    df['Car'] = df['Car'].fillna(impute_val)    # eşdeğeri df['Car'].fillna(impute_val, inplace=True)

    DataFrame ve Series sınıflarının fillna metotları eksik verileri belli bir değerle doldurmak için kullanılmaktadır. 
    Bu metotta inplace parametresi True yapılırsa zaten doldurma doğrudan DataFrame ya da Series nesnesi güncellenecek biçimde 
    yapılmaktadır.

    Eksik veri içeren diğer bir sütun da BuildingArea sütunudur. Bu sütun evin metrekare cinsindende büyüklüğünü belirtmektedir. 
    Eksik değerleri ortalamanın yuvarlanmış haliyle doldurabiliriz:

    impute_val = df['BuildingArea'].mean().round()
    df['BuildingArea'] = df['BuildingArea'].fillna(impute_val)    # eşdeğeri df['Car'].fillna(impute_val, inplace=True)

    Veri kümesinin YearBuilt sütunu binanın yapım yılını belirtmektedir. Bu tür tarih bilgileri ya da yıl bilgileri hangi 
    ölçeğe sahiptir? Aslında bu tür bilgilerin ölçeklerini belirleme amaca da bağlıdır. Yıl bilgisi "kategorik", "sıralı" 
    ya da aralıklı ölçek olarak değerlendirilebilir. Ancak genellikle bu tür yıl bilgilerinin "sıralı (ordinal)" değerlendirilmesi 
    daha uygun olabilmektedir. O halde biz bu sütunun ortalama değeri olarak medyan işlemi uygulayabiliriz:
    
    impute_val = df['YearBuilt'].median()
    df['YearBuilt'] = df['YearBuilt'].fillna(impute_val)    # eşdeğeri df['YearBuilt'].fillna(impute_val, inplace=True)

    Eksik veri içeren CouncilArea binanın içinde bulunduğu bölgeyi belirtmektedir. Dolayısıyla kategorik bir sütundur. O halde 
    mod işlemi ile "imputation" uygulayabiliriz:

    impute_val = df['CouncilArea'].mode()
    df['CouncilArea'] = df['CouncilArea'].fillna(impute_val[0])    # eşdeğeri df['CouncilArea'].fillna(impute_val, inplace=True)

    Pandas'taki DataFrame ve Series sınıflarının mode metotları sonucu Series nesnesi biçiminde vermektedir. (Aynı miktarda 
    yinelenen birden fazla değer olabilecğei için bu değerlerin hepsinin verilmesi tercih edilmiştir.) Dolayısıyla biz bu 
    Series nesnesinin ilk elemanını alarak mod değeri elde ettik.

    Aşağıda MHS veri kümesinde uygulanan imputation işlemini bir bütün olarak veriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('melb_data.csv')

impute_val = df['Car'].mean().round()
df['Car'] = df['Car'].fillna(impute_val)    # eşdeğeri df['Car'].fillna(impute_val, inplace=True)

impute_val = df['BuildingArea'].mean().round()
df['BuildingArea'] = df['BuildingArea'].fillna(impute_val)    # eşdeğeri df['Car'].fillna(impute_val, inplace=True)

impute_val = df['YearBuilt'].median()
df['YearBuilt'] = df['YearBuilt'].fillna(impute_val)    # eşdeğeri df['YearBuilt'].fillna(impute_val, inplace=True)

impute_val = df['CouncilArea'].mode()
df['CouncilArea'] = df['CouncilArea'].fillna(impute_val[0])    # eşdeğeri df['CouncilArea'].fillna(impute_val, inplace=True)

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda ortalama, medyan ve mod değerleriyle imputation uyguladık. Gerçekten de en çok uygulanan imputation yöntemleri
    bunlardır. Ancak bu yöntemler bazen yetersiz kalabilmektedir. Bu durumda daha karmaşık yöntemlerin uygulanması gerebilmektedir. 
    Örneğin MHS veri kümesinde eksik veri olan evin metrekaresini ortalama yoluyla doldurmak uygun olmayabilir. Çünkü bazı bölgeler
    pahalı olduğu için oradaki evler büyük ya da küçük olabilmektedir. Bazı bölgelerin imar mevzuatları farklı olabilmektedir. 
    Yani evin metrekaresi başka özelliklere göre (sütun bilgilerine göre) değişebilmektedir. Peki bu tür durumlarda ne yapmak 
    gerekir?  İşte sütun değerleri satırdaki diğer değerlerden hareketle regresyon modelleriyle tahmin edilebilir. Ancak 
    uygulamada genellikle bu tür durumlar göz ardı edilip temel imputation yöntemleri uygulanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesi uygulamalarında en çok tercih edilen kütüphanelerden biri "scikit-learn" isimli kütüphanedir. NumPy ve 
    Pandas sayısal işlemler için kullanılan genel amaçlı ve temel kütüphanelerdir. SciPy ise matematik ve lineer cebir konularına 
    odaklanmış bir sayısal analiz kütüphanesidir. Oysa scikit-learn makine öğrenmesi uygulamaları için tasarlanmıştır. 
    
    scikit-learn kütüphanesinin temelleri 2007 yılında atıldı. Önceleri SciPy kütüphanesine bir eklenti oluşturma amacıyla 
    geliştirilmeye başlandı. İlk sürümü 2010 yılında yayınlandı (0.1 sürümü). 2012'den sonra hızla yaygınlaştı. 2015 ve sonrasında 
    pek çok özelliği bünyesine katarak ve sürekli geliştirilerek bugünkü durumuna geldi. Kütüphanenin yüklenmesi şöyle yapılabilir:

    pip install scikit-learn

    Ancak scikit-learn kütüphanesi yapay sinir ağları ve derin öğrenme ağlarına yönelik tasarlanmamıştır. scikit-learn içerisindeki 
    fonksiyonlar ve sınıflar matematiksel ve istatistiksel ağırlıklı öğrenme yöntemlerine ilişkindir. scikit-learn kütüphanesinin 
    import ismi sklearn biçimindedir. Örneğin:

    import sklearn

    Ancak genellikle uygulamacılar kütüphaneyi bir bütün olarak import etmek yerine kullanacakları öğeleri from import deyimi 
    ile import etmeyi tercih ederler. Örneğin:

    from sklearn.impute import SimpleImputer
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    scikit-learn genel olarak "nesne yönelimli" biçimde oluşturulmuştur. Yani kütüphane daha çok fonksiyonlar yoluyla değil 
    sınıflar yoluyla kullanılmaktadır. Kütüphanenin belli bir kullanım biçimi vardır. Öğrenme kolay olsun diye ve bazı nesne 
    yönelimli tasarım kalıpları uygulansın diye bu biçim değişik sınıflarda uygulanmıştır. Kütüphanenin tipik kullanımı biçimi 
    şöyledir:

    1) Önce ilgili sınıf türünden nesne yaratılır. Örneğin sınıf SimpleImputer isimli sınıf olsun:

    from sklearn.impute import SimpleImputer

    si = SimpleImputer(...)    

    2) Nesne yaratıldıktan sonra onun bir veri kümesi ile eğitilmesi gerekir. (Burada “eğitme” demekle denetimli bir öğrenmeyi 
    kastetmiyoruz. Bunu genel bir terim olarak kullanıyoruz.) Bu işlem sınıfların fit metotlarıyla yapılmaktadır. fit işlemi 
    sonrasında metot birtakım değerler elde edip onu nesnenin özniteliklerinde saklar. Yani fit metotları söz konusu veri 
    kümesini ele alarak oradan gerekli faydalı bilgileri elde etmektedir. Örneğin:

    si.fit(dataset)

    fit işlemi genel olarak transform için kullanılacak bilgilerin elde edilmesi işlemini yapmaktadır. DOlayısıyla fit işleminden 
    sonra artık nesnenin özniteliklerini kullanabiliriz. 

    3) fit işleminden sonra fit işlemiyle elde edilen bilgilerin bir veri kümesine uygulanması gerekir. Bu işlem de sınıfların 
    transform metotlarıyla yapılmaktadır. Bir veri kümesi üzerinde fit işlemi uyguladıktan sonra burada oluşan bilgilerle birden 
    fazla veri kümesini transform edebiliriz. Örneğin:

    si.fit(dataset)
    result1 = si.transform(dataset_foo)
    result2 = si.transform(dataset_bar)
    ...

    fit ve transform metotları bizden bilgiyi NumPy dizisi olarak, Pandas'ın Series ya da DataFrame nesnesi olarak ya da Python
    listesi olarak alabilmektedir. fit metotları nesnenin kendisine, transform metotları da transform edilmiş NumPy dizilerine 
    geri dönmektedir.

    4) Eğer fit edilecek veri kümesi ile transform edilecek veri kümesi aynı ise bu durumda önce fit sonra transform işlemini 
    ayrı ayrı yapmak yerine bu iki işlem sınıfların fit_transform metotlarıyla tek hamlede de yapılabilir. Örneğin:

    result = si.fit_transform(dataset)

    5) Ayrıca sınıfların fit işlemi sonucunda oluşan bilgileri almak için kullanılan birtakım örnek öznitelikleri ve metotları 
    da olabilmektedir.

    6) Bazı sınıfların inverse_transform metotları da bulunmaktadır. inverse_transform metotları transform işleminin tersini 
    yapmaktadır. Yani transform edilmiş bilgileri alıp onları transform edilmemiş hale getirmektedir. 

    fit, transform ve fit_transform metotları iki boyutlu bir veri kümelerini kabul etmektedir. Bu nedenle örneğin biz bu 
    metotlara Pandas'ın DataFrame nesnelerini verebiliriz, Series nesnelerini veremeyiz.
#----------------------------------------------------------------------------------------------------------------------------
 
#----------------------------------------------------------------------------------------------------------------------------
                                            20. Ders - 03/03/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    scikit-learn sınıflarının genel kullanımını açıkladıktan sonra şimdi eksik verilerin doldurulması işleminde kolaylık sağlayan 
    scikit-learn SimpleImputer sınıfını görebiliriz. SimpleImputer sınıfının kullanılması yukarıda açıkladığımız genel kalıba 
    uygundur. Önce nesne yaratılır, sonra fit ve transform işlemleri yapılır. Sınıfın __init__ metodunun parametrik yapısı 
    şöyledir:
    
    class sklearn.impute.SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, copy=True, 
            add_indicator=False, keep_empty_features=False)
    
    Görüldüğü gibi metodun bütün parametreleri default değer almış durumdadır. strategy parametresi doldurmanın nasıl yapılacağını 
    belirtmektedir. Bu parametrenin default değerinin "mean" biçiminde olduğunu görüyorsunuz. Yani default durumda eksik veriler 
    sütun ortalamaları ile doldurulmaktadır. "mean" dışında şu stratejiler bulunmaktadır:

    "median"
    "most_frequent"
    "constant"

    "constant" stratejisi doldurmanın belli bir değerle yapılacağı anlamına gelmektedir. Eğer bu strateji seçilirse doldurulacak 
    değerin fill_value parametresiyle belirtilmesi gerekir. Diğer parametreler için sınıfın dokümantasyonuna başvurabilirsiniz. 

    SimpleImputer sınıfının fit, transform ve fit_transform metotları iki boyutlu bir dizi almaktadır. Yani bu metotlara bir 
    NumPy dizisi geçirecekseniz onun iki boyutlu olması gerekir. Pandas'ın Series nesnelerinin tek boyutlu bir dizi belirttiğini 
    anımsayınız. Bu durumda biz bu metotlara Series nesnesi geçemeyiz. Ancak DataFrame nesneleri iki boyutlu dizi belirttiği 
    için DataFrame nesnelerini geçebiliriz. Eğer elimizde tek boyutlu bir dizi varsa onu bir sütundan n satırdan oluşan iki
    boyutlu bir diziye dönüştürmeliyiz. Bunun için NumPy ndarray sınıfının reshape metodunu kullanabilirsiniz. 

    SimpleImputer nesnesi yaratılırken doldurma stratejisi nesnenin yaratımı sırasında verilmektedir. Yani nesne başta belirtilen
    stratejiyi uygulamaktadır. Ancak veri kümelerinin değişik sütunları değişik stratejilerle doldurulmak istenebilir. Bunun 
    için birden fazla SimpleImputer nesnesi yaratılabilir. Örneğin:

    si1 = SimpleImputer(strategy='mean')
    si2 = SimpleImputer(strategy='median')
    ...

    Ancak bunun yerine SİmpleImputer sınıfının set_params metodu da kullanılabilir. Bu metot önceden belirlenmiş parametreleri 
    değiştirmekte kullanılmaktadır. Örneğin:

    si = SimpleImputer(strategy='mean')
    ...
    si.set_params(strategy='median')
    ...

    SimpleImputer sınıfının __init__ metodunun missing_values parametresi eksik değerlerin özel değerler olarak değerlendirilmesini 
    sağlamak için kullanılabilmektedir. Yani örneğin eksik değerler NaN değerleir olmayabilir 0 değerleri olabilir. Bu durumda 
    bizim bunu bu parametreyle belirtmemiz gerekir. Örneğin:

    si = SimpleImputer(strategy='mean')

    SimpleImputer sınıfında yukarıda belirttiğimiz gibi doldurma işlemini fit metodu yapmaz, fit metodu doldurma işlemi için 
    gereken bilgileri elde eder. Yani Örneğin:

    a = np.array([1, 1, None, 4, None]).reshape(-1, 1)

    si = SimpleImputer(strategy='mean')
    si.fit(a)

    Burada fit metodu aslında yalnızca a dizisindeki sütunların ortalamalarını elde etmektedir. (Örneğimizde tek bir sütun
    var). Biz fit yaptığımız bilgiyi transform etmek zorunda değiliz. Örneğin:

    b = np.array([1, 1, None, 4, None]).reshape(-1, 1)
    result = si.transform(b)

    Biz şimdi burada a'dan elde ettiğimiz ortalama 3 değeri ile bu b dizisini doldurmuş oluruz. Tabii genellikle bu tür durumlarda
    fit yapılan dizi ile transform yapılan dizi aynı dizi olur. Örneğin:

    a = np.array([1, 1, None, 4, None]).reshape(-1, 1)

    si = SimpleImputer(strategy='mean')
    si.fit(a)
    result = si.transform(a)

    İşte bu tür durumlarda fit ve transform işlemleri bir arada fit_transform metoduyla yapılabilmektedir. Örneğin:

    a = np.array([1, 1, None, 4, None]).reshape(-1, 1)

    si = SimpleImputer(strategy='mean')
    result = si.fit_transform(a)

    scikit-learn kütüphanesindeki pek çok sınıf aynı anda birden fazla sütun üzerinde işlem yapabilmektedir. Bu nedenle bu 
    sınıfların fit ve transform metotları bizden iki boyutlu dizi istemektedir. tranform metotları da bize iki iki boyutlu
    dizi geri döndürmektedir. Örneğin SimpleImputer sınıfına biz fit işleminde iki bıyutlu bir dizi veriririz. Bu durumda 
    fit metodu her sütunu diğerinden ayrı bir biçimde ele alır ve o sütunlara ilişkin bilgileri oluşturur. Örneğin biz fit 
    metoduna aşağıdaki gibi iki boyutlu bir dizi vermiş olalım:

    1       4
    None    7
    5       None
    3       8
    9       2

    Stratejinin "mean" olduğunu varsayalım. Bu durumda fit metodu her iki sütunun da ortalamasını alıp nesnenin içerisinde
    saklayacaktır. Biz artık transform metoduna iki boyutlu iki sütundan oluşan bir dizi verebiliriz. Bu transform metodu
    bizim verdiğimiz dizinin ilk sütunununu fit ettiğimiz dizinin ilk sütunundan elde ettiği bilgiyle, ikinci sütununu da 
    fit ettiğimiz dizinin ikinci sütunundan elde ettiği bilgiyle dolduracakır. İşte sckit-learn sınıflarının fit ve transform 
    metotlarına biz iki boyutlu diziler veririz. O da bize iki boyutlu diziler geri döndürür. Eğer elimizde tek boyutlu 
    bir dizi varsa biz onu reshape metoduyla iki boyutlu hale getirerek fit ve transform metotlarına vermeliyiz. Örneğin:

    >>> a = np.array([1, 2, 3, None, 5])
    >>> si.fit_transform(a.reshape(-1, 1))
    array([[1.  ],
        [2.  ],
        [3.  ],
        [2.75],
        [5.  ]])

    Aşağıda MHS veri kümesi üzerinde eksik verilerin bulunduğu sütunlar sckit-learn SimpleImputer sınıfı kullanılarak 
    doldurulmuştur.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('melb_data.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean')
df[['Car', 'BuildingArea']] = np.round(si.fit_transform(df[['Car', 'BuildingArea']]))

si.set_params(strategy='median')
df[['YearBuilt']] = np.round(si.fit_transform(df[['YearBuilt']]))

si.set_params(strategy='most_frequent')
df[['CouncilArea']] = si.fit_transform(df[['CouncilArea']])

#----------------------------------------------------------------------------------------------------------------------------
    scikit-learn kütüphanesinde sklearn.impute modülünde aşağıdaki imputer sınıfları bulunmaktadır:
    
    SimpleImputer
    IterativeImputer
    MissingIndicator
    KNNImputer

    Buradaki KNNImputer sınıfı "en yakın komşuluk" yöntemini kullanmaktadır. Bu konu ileride ele alınacaktır. IterativeImputer
    sınıfı ise regresyon yaparak doldurulacak değerleri oluşturmaktadır. Örneğin biz bu sınıfa fit metoduna 5 sütunlu bir dizi 
    vermiş olalım. Bu sınıf bu sütunların birini çıktı olarak dördünü girdi olarak ele alıp girdilerden çıktıyı tahmin edecek
    doğrusal bir model oluşturmaktadır. Yani bu sınıfta doldurulacak değerler yalnızca doldurmanın yapılacağı sütunlar dikkate 
    alınarak değil diğer sütunlar da dikkate alınarak belirlenmektedir. Örneğin MHS veri kümesinde evin metrakaresi bilinmediğinde
    bu eksik veriyi ortalama metrakareyle doldurmak yerine "bölgeyi", "binanın yaşını" da dikkate alarak doldurmak isteyebiliriz. 
    Ancak yukarıda da belirttiğimiz gibi genellikle bu tür karmaşık imputation işlemleri seyrek kullanılmaktadır.

    Aşağıda MHS veri kümesi için üç sütuna dayalı olarak tahminleme yöntemiyle doldurmaya örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('melb_data.csv')

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

si = IterativeImputer()
df[['Car', 'BuildingArea', 'YearBuilt']] = si.fit_transform(df[['Car', 'BuildingArea', 'YearBuilt']])

#----------------------------------------------------------------------------------------------------------------------------
    Verilerin kullanıma hazır hale getirilmesi sürecinin en önemli işlemlerinden biri de "kategorik (nominal)" sütunların 
    sayısal biçime dönüştürülmesidir. Çünkü makine öğrenmesi algoritmaları veriler üzerinde "toplama", "çarpma" gibi işlemler 
    yaparlar. Dolayısıyla kategorik veriler böylesi işlemlere sokulamazlar. Bunun için önce onların sayısal biçime dönüştürülmeleri 
    gerekir. Genellikle bu dönüştürme "eksik verilerin ele alınması" işleminden sonra yapılmaktadır. Ancak bazen kategorik verilerin
    dönüştürülmesi işlemi eksik verilerin ele alınması işleminde önce de yapılabilmektedir. 
    
    Kategorik verilerin sayısal biçime dönüştürülmesi için kullanılan en temel yöntemlerden biri “etiket kodlaması (laben encoding)” 
    denilen yöntemdir. Bu yöntemde kodlama  her kategori (sınıf) için 0’dan başlanarak artan bir tamsayı karşı düşürülerek yapılmaktadır. 
    Örneğin bir veri kümesinde kişilerin renk tercihlerine ilişkin “Kırmızı”, “Yeşil” ve “Mavi” seçeneklerinden oluşan aşağıdaki 
    gibi bir sütun bulunuyor olsun:

    Kırmızı
    Mavi
    Kırmızı
    Yeşil
    Mavi
    Yeşil
    ...

    Biz şimdi burada bu kategorik sütunu Kırmızı = 0, Mavi = 1, Yeşil = 2 biçiminde sayısallaştırabiliriz. Bu durumda bu sütun 
    şu hale gelecektir:

    0
    1
    0
    2
    1
    2
    ...

    Biz yukarıdaki işlemi yapan bir fonksiyon da yazabiliriz. Fonksiyonun birinci parametresi bir DataFrame nesesini, ikinci 
    parametresi ise hangi sütunun sayısallaştırılacağına ilişkin sütun isimlerinden oluşan belirten bir nesneyi alabilir.

    def category_encoder(df, colnames):
        for colname in colnames:
            labels = df[colname].unique()
            for index, label in enumerate(labels):
                df.loc[df[colname] == label, colname] = index
    
    Burada biz önce sütun isimlerini tek tek elde etmek için dış bir döngü kullandık. Sonra ilgili sütundaki "tek olan (unique)" 
    etiketleri (labels) elde ettik. SOnra bu etiketleri iç bir döngüde dolaşarak sütunda ilgili etiketin bulunduğu satırlara
    onları belirten sayıları yerleştirdik. Test işlemi için aşağıdaki gibi "test.csv" isimli bir CSV dosyasını kullanabiliriz:

        AdıSoyadı,Kilo,Boy,Yaş,Cinsiyet,RenkTercihi
        Sacit Bulut,78,172,34,Erkek,Kırmızı
        Ayşe Er,67,168,45,Kadın,Yeşil
        Ahmet San,85,182,32,Erkek,Kırmızı
        Macit Şen,98,192,65,Erkek,Mavi
        Talat Demir,85,181,49,Erkek,Yeşil
        Sibel Ünlü,72,172,34,Kadın,Mavi
        Ali Serçe,75,165,21,Erkek,Yeşil

    Test kodu da şöyle olabilir:

    import pandas as pd

    def label_encode(df, colnames):
        for colname in colnames:
            labels = df[colname].unique()
            for index, label in enumerate(labels):
                df.loc[df[colname] == label, colname] = index
         
    label_encode(df, ['RenkTercihi', 'Cinsiyet'])
    print(df)

    Şöyle bir çıktı elde edilmiştir:

            AdıSoyadı  Kilo  Boy  Yaş Cinsiyet RenkTercihi
    0  Sacit Bulut    78  172   34        0            0
    1      Ayşe Er    67  168   45        1            1
    2    Ahmet San    85  182   32        0            0
    3    Macit Şen    98  192   65        0            2
    4  Talat Demir    85  181   49        0            1
    5   Sibel Ünlü    72  172   34        1            2
    6    Ali Serçe    75  165   21        0            1

    Aşağıda da MHS veri kümesi üzerinde aynı işlem yapılmıştır.
#---------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('melb_data.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean')
df[['Car', 'BuildingArea']] = np.round(si.fit_transform(df[['Car', 'BuildingArea']]))

si.set_params(strategy='median')
df[['YearBuilt']] = np.round(si.fit_transform(df[['YearBuilt']]))

si.set_params(strategy='most_frequent')
df[['CouncilArea']] = si.fit_transform(df[['CouncilArea']])

def category_encoder(df, colnames):
    for colname in colnames:
        labels = df[colname].unique()
        for index, label in enumerate(labels):
            df.loc[df[colname] == label, colname] = index
    
category_encoder(df, ['Suburb', 'SellerG', 'Method', 'CouncilArea', 'Regionname'])
print(df)

#----------------------------------------------------------------------------------------------------------------------------
    Aslında yukarıdaki işlem scikit-learn kütüphanesindeki preprocessing modülünde bulunan LabelEncoder sınıfıyla yapılabilmektedir. 
    LabelEncoder sınıfının genel çalışma biçimi scikit-learn kütüphanesinin diğer sınıflarındaki gibidir. Ancak bu sınıfın fit 
    ve transform metotları tek boyutlu bir NumPy dizisini ya da Series nesnesini parametre olarak almaktadır. (Halbuki yukarıda 
    da belirttiğimiz gibi genel olarak fit ve transform metotlarının çoğu iki boyutlu dizileri ya da DataFrame nesnelerini 
    parametre olarak almaktadır.) fit metodu yukarıda bizim yaptığımız gibi tek olan (unique) elemanları tespit edip bunu nesne 
    içerisinde saklamaktadır. Asıl dönüştürme işlemi transform metoduyla yapılmaktadır. Tabii eğer fit ve transform metotlarında 
    aynı veriler kullanılacaksa bu işlemler tek hamlede fit_transform metoduyla da yapılabilir. LabelEncoder sınıfı yukarıda bizim 
    yaptığımızdan farklı olarak teke düşürdüğü etiketleri aynı zamanda "yazısal biçimde (lexicographic)" de sıralamaktadır. Bu nedenle
    LabelEncoder ile sayısallaştırma sözlükteki sıraya göre yapılmaktadır. 

    Örneğin yukarıdaki "test.csv" veri kümesindeki "Cinsiyet" ve "RenkTercihi" sütunlarını kategorik olmaktan çıkartıp sayısal 
    biçime şöyle dönüştürebiliriz:

    import pandas as pd
    from sklearn.preprocessing import LabelEncoder

    df = pd.read_csv('test.csv')

    le = LabelEncoder()

    transformed_data = le.fit_transform(df['RenkTercihi'])
    df['RenkTercihi'] = transformed_data

    transformed_data = le.fit_transform(df['Cinsiyet'])
    df['Cinsiyet'] = transformed_data
    print(df)

    LabelEncoder sınıfının classes_ isimli örnek özniteliği fit metodu tarafından oluşturulmaktadır. Bu öznitelik "tek olan
    (unique)" etiketleri bize NumPy dizisi olarak vermektedir. 

    Aşağıda MHS veri kümesindeki gerekli sütunlar LabelEncoder sınıfı ile sayısal biçime dönüştürülmüştür.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('melb_data.csv')
print(df, end='\n\n')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean')
df[['Car', 'BuildingArea']] = np.round(si.fit_transform(df[['Car', 'BuildingArea']]))

si.set_params(strategy='median')
df[['YearBuilt']] = np.round(si.fit_transform(df[['YearBuilt']]))

si.set_params(strategy='most_frequent')
df[['CouncilArea']] = si.fit_transform(df[['CouncilArea']])
    
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for colname in ['Suburb', 'SellerG', 'Method', 'CouncilArea', 'Regionname']:
    df[colname] = le.fit_transform(df[colname])

print(df)

#----------------------------------------------------------------------------------------------------------------------------
    LabelEncoder sınıfının inverse_transform metodu ters işlemi yapmaktadır. Yani bir kez fit işlemi yapıldıktan sonra nesne 
    zaten hangi etiketlerin hangi sayısal değerlere karşı geldiğini kendi içerisinde tutmaktadır. Böylece biz sayısal değer 
    verdiğimizde onun yazısal karşılığını inverse_transform ile elde edebiliriz. Örneğin:
    
    some_label_numbers = [0, 1, 1, 2, 2, 1]
    label_names = le.inverse_transform(some_label_numbers)

    Aşağıda "test.csv" dosyasının "RenkTercihi" ve "Cinsiyet" sütunları üzerinde önce transform sonra inverse_transform işlemi
    örneği verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------
    import pandas as pd
    from sklearn.preprocessing import LabelEncoder

    df = pd.read_csv('test.csv')
    print(df, end='\n\n')

    le = LabelEncoder()

    transformed_data = le.fit_transform(df['RenkTercihi'])
    df['RenkTercihi'] = transformed_data

    some_label_numbers = [0, 1, 1, 2, 2, 1]
    label_names = le.inverse_transform(some_label_numbers)
    print(label_names, end='\n\n')

    transformed_data = le.fit_transform(df['Cinsiyet'])
    df['Cinsiyet'] = transformed_data
    print(df)

    some_label_numbers = [0, 1, 1, 1, 0, 1]
    label_names = le.inverse_transform(some_label_numbers)
    print(label_names)

#----------------------------------------------------------------------------------------------------------------------------
   Aslında kategorik verilerin 0'dan itibaren birer tamsayı ile numaralandırılması çoğu kez iyi bir teknik değildir. Kategorik 
   verilerin sayısallaştırılması için "one-hot-encoding" denilen yöntem tercih edilmektedir. Biz "one-hot-encoding" dönüştürmesini 
   izleyen paragraflarda ele alacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sıralı (ordinal) verilerin sayısal biçime dönüştürülmesi tipik olarak 0’dan başlanarak düşük sıranın düşük numarayla ifade 
    edilmesi biçiminde olabilir. Örneğin "EğitimDurumu", "ilkokul", "ortaokul", "lise", "üniversite" biçiminde dört kategoride 
    sıralı bir bilgi olabilir. Biz de bu bilgilere sırasıyla birer numara vermek isteyebiliriz. Örneğin:

    İlkokul     --> 0
    Ortaokul    --> 1
    Lise        --> 2
    Üniversite  --> 3

    scikit-learn içerisindeki LabelEncoder sınıfı bu amaçla kullanılamamaktadır. Çünkü LabelEncoder etiketlere bizim istediğimiz 
    gibi numara vermemektedir. scikit-learn içerisinde bu işlemi pratik bir biçimde yapan hazır bir sınıf bulunmamaktadır. 
    Gerçi scikit-learn içerisinde OrdinalEncoder isimli bir sınıf vardır ama o sınıf bu tür amaçları gerçekleştirmek için 
    tasarlanmamıştır. scikit-learn içerisindeki OrdinalEncoder sınıfı LabelEncoder sınıfının yaptığına benzer bir işlem 
    yapmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            21. Ders - 09/03/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    OrdinalEncoder sınıfında da önce fit sonra transform işlemi yapılır. Eğer fit ve transform metodunda aynı veri kümesi 
    kullanılacaksa fit_transform metodu ile bu iki işlem bir arada da yapılabilir. OrdinalEncoder sınıfının categories_ örnek 
    özniteliği oluşturulan kategorileri NumPy dizisi olarak vermektedir.  fit işlemine sokulan sütunların sayısı ise sınıfın 
    n_features_in_ elemanından elde edilmektedir. 

    OrdinalEncoder sınıfı kodlanacak sütunları eğer onlar yazısal biçimdeyse tıpkı LabelEncoder sınıfı gibi önce onları 
    "lexicographic" olarak sıralayıp  numaralandırmaktadır. (Yani sözlükte ilk gördüğü kategoriye düşük numara vermektedir. 
    Tabii bu işlem UNICODE tabloya göre yapılmaktadır.) Bu sınıf kategorilere bizim istediğimiz numaraları vermemektedir. 
    Ayrıca bu sınıfın fit ve transform metotları iki boyutlu nesneleri kabul etmektedir. Bu bağlamda OrdinalEncoder sınıfının 
    LabelEncoder sınıfından farkı OrdinalEncoder sınıfının birden fazla sütunu (özelliği) kabul etmesidir. Halbuki LabelEncoder 
    sınıfı tek bir sütunu (özelliği) sayısallaştırmaktadır. Örneğin "test.csv" veri kümemiz şöyle olsun:

    AdıSoyadı,Kilo,Boy,Yaş,Cinsiyet,RenkTercihi,EğitimDurumu
    Sacit Bulut,78,172,34,Erkek,Kırmızı,İlkokul
    Ayşe Er,67,168,45,Kadın,Yeşil,Ortaokul
    Ahmet San,85,182,32,Erkek,Kırmızı,İlkokul
    Macit Şen,98,192,65,Erkek,Mavi,Lise
    Talat Demir,85,181,49,Erkek,Yeşil,Üniversite
    Sibel Ünlü,72,172,34,Kadın,Mavi,Ortaokul
    Ali Serçe,75,165,21,Erkek,Yeşil,İlkokul

    Burada "Cinsiyet" ve "RenkTercihi" kategorik (nominal) ölçekte sütunlardır. "EğitimDurumu" sütunu kategorik ya da sıralı
    olarak ele alınabilir. Eğer biz İlkokul = 0, Ortaokul = 1, Lise = 2, Üniversite = 3 biçiminde sıralı ölçeğe ilişkin bir 
    kodlama yapmak istersek bunu LabelEncoder ya da OrdinalEncoder ile sağlayamayız. Örneğin:

    df = pd.read_csv('test.csv')
    print(df, end='\n\n')

    oe = OrdinalEncoder()
    transformed_data = oe.fit_transform(df[['Cinsiyet', 'RenkTercihi', 'EğitimDurumu']])
    df[['Cinsiyet', 'RenkTercihi', 'EğitimDurumu']] = transformed_data

    Buradan şöyle bir DataFrame elde edilecektir:

            AdıSoyadı  Kilo  Boy  Yaş  Cinsiyet  RenkTercihi  EğitimDurumu
    0  Sacit Bulut    78  172   34       0.0           0.0            3.0
    1      Ayşe Er    67  168   45       1.0           2.0            1.0
    2    Ahmet San    85  182   32       0.0           0.0            3.0
    3    Macit Şen    98  192   65       0.0           1.0            0.0
    4  Talat Demir    85  181   49       0.0           2.0            2.0
    5   Sibel Ünlü    72  172   34       1.0           1.0            1.0
    6    Ali Serçe    75  165   21       0.0           2.0            3.0
                                    
    Bunu OrdinalEncoder sınıfı ile kodlamaya çalışırsak muhtemelen tam istediğimiz gibi bir kodlama yapamayız. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder

df = pd.read_csv('test.csv')
print(df, end='\n\n')

oe = OrdinalEncoder()
transformed_data = oe.fit_transform(df[['Cinsiyet', 'RenkTercihi', 'EğitimDurumu']])

df[['Cinsiyet', 'RenkTercihi', 'EğitimDurumu']] = transformed_data
                                    
print(df)

#----------------------------------------------------------------------------------------------------------------------------
    Eğer kategorilere istediğiniz gibi değer vermek istiyorsanız bunu manuel bir biçimde yapabilirsiniz. Örneğin veri kümesini 
    read_csv fonksiyonuyla okunurken converters parametresi yoluyla hemen dönüştürme yapılabilirsiniz. read_csv ve load_txt 
    fonksiyonlarında converters parametresi bir sözlük nesnesi almaktadır. Bu sözlük nesnesi hangi sütun değerleri okunurken
    hangi dönüştürmenin yapılacağını belirtmektedir. Buradaki sözlüğün her elemanının anahtarı bir sütun isminden ya da sütun 
    indeksinden, değeri ise o sütunun dönüştürülmesinde kullanılacak fonksiyondan oluşmaktadır. Tabii bu fonksiyon lambda 
    ifadesi olarak da girilebilir. Örneğin:

    df = pd.read_csv('test.csv', converters={'EğitimDurumu': lambda s: {'İlkokul': 0, 'Ortaokul': 1, 'Lise': 2, 'Üniversite': 3}[s]})

    Elde edilecek DataFrame nesnesi şöyle olacaktır:

            AdıSoyadı  Kilo  Boy  Yaş Cinsiyet RenkTercihi  EğitimDurumu
    0  Sacit Bulut    78  172   34    Erkek      Kırmızı              0
    1      Ayşe Er    67  168   45    Kadın        Yeşil              1
    2    Ahmet San    85  182   32    Erkek      Kırmızı              0
    3    Macit Şen    98  192   65    Erkek         Mavi              2
    4  Talat Demir    85  181   49    Erkek        Yeşil              3
    5   Sibel Ünlü    72  172   34    Kadın         Mavi              1
    6    Ali Serçe    75  165   21    Erkek        Yeşil              0
    
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('test.csv', converters={'EğitimDurumu': lambda s: {'İlkokul': 0, 'Ortaokul': 1, 'Lise': 2, 'Üniversite': 3}[s]})
print(df, end='\n\n')

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında makine öğrenmesi algoritmalarının bazılarında kategorik verilerin 0'dan itibaren LabelEncoder ya da OrdinalEncoder 
    sınıfı ile sayısallaştırılması olumsuz etkilere yol açabilmektedir. Çünkü bu durumda bazı algoritmalar bu kategorik verileri 
    sanki "sıralı (ordinal)" bir veriymiş gibi ele almaktadır. Örneğin veri kümesinin bir sütununda "RenkTercihi" olsun. Bu 
    “RenkTercihi” de "Kırmızı", "Mavi" ve "Yeşil" renklerinden oluşuyor olsun. Biz bu sütunu LabelEncoder ile sayısal hale 
    getirdiğimizde örneğin Kırmızı = 0, Mavi = 1, Yeşil = 2 biçiminde renklere numara verilecektir. İşte burada makine öğrenmesi 
    algoritmaları sanki "Mavi" > "Kırmızı", "Yeşil" > "Mavi" biçiminde bir ilişki varmış gibi sonuçlara yol açabilecektir. Hatta 
    örneğin bu kodlamadan "Kırmızı" + "Mavi" = "Mavi" gibi, gerçekte var olmayan özellikler de ortaya çıkabilecektir. İşte bu 
    nedenle kategorik verilerin tek bir sütunla değil birden fazla sütunla ifade edilmesi yoluna gidilmektedir. Kategorik verilerin 
    birden fazla sütunla ifade edilmesinde en yaygın kullanılan yöntem "one-hot-encoding" denilen yöntemdir. 

    One-hot-encoding yönteminde önce veri kümesine sütundaki kategorilerin sayısı kadar sütun eklenir. ("One-hot" terimi "bir 
    grup bitten hepsinin sıfır yalnızca bir tanesinin 1 olma durumunu" anlatmaktadır.)  Sonra her kategori yalnızca tek bir 
    sütunda 1 diğer sütunlarda 0 olacak biçimde kodlanır. Örneğin:
    
    RenkTercihi
    -----------
    Kırmızı
    Kırmızı
    Mavi
    Kırmızı
    Yeşil
    Mavi
    Yeşil
    ...

    Burada 3 renk olduğunu düşünelim. Bunun "one-hot-encoding" dönüştürmesi şöyle olacaktır:

    Kırmızı     Mavi        Yeşil
    1           0           0
    1           0           0
    0           1           0
    1           0           0
    0           0           1
    0           1           0
    0           0           1
    ...

    Eğer sütundaki kategori sayısı 2 tane ise böyle sütunlar üzerinde "one-hot-encoding" uygulamanın bir faydası yoktur. Bu tür 
    ikili sütunlar 0 ve 1 biçiminde kodlanabilir. (Yani bu işlem LabelEncoder sınıfyla yapılabilir). Biz de kursumuzda yalnızca
    iki kategori içeren sütunları "one-hot-encoding" ile kodlamayacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Programlama yoluyla "one-hot-encoding" uygulamak için birkaç yöntem kullanılabilmektedir. Örneğin sctkit-learn içerisindeki 
    preprocessing modülünde bulunan OneHotEncoder sınıfı bu işlem için kullanılabilir. Sınıfın genel kullanımı diğer sckit-learn 
    sınıflarında olduğu gibidir. Yani önce fit işlemi sonra transform işlemi yapılır. fit işleminden sonra sütunlardaki "tek 
    olan (unique)" elemanlar sınıfın categories_ örnek özniteliğine NumPy dizilerinden oluşan bir liste biçiminde kaydedilmektedir. 
    Örneğimizde kullanacağımız "test.csv" dosyası şöyle olsun:

    AdıSoyadı,Kilo,Boy,Yaş,RenkTercihi
    Sacit Bulut,78,172,34,Kırmızı
    Ayşe Er,67,168,45,Yeşil
    Ahmet San,85,182,32,Kırmızı
    Macit Şen,98,192,65,Mavi
    Talat Demir,85,181,49,Yeşil
    Sibel Ünlü,72,172,34,Mavi
    Ali Serçe,75,165,21,Yeşil

    Pandas'ın DataFrame nesnesi üzerinde "one-hot-encoding" yapılırken DataFrame içerisindeki eski sütun silinmeli onun yerine 
    elde edilen yeni sütunlar eklenmelidir. DataFrame sütununa bir isim vererek atama yapılırsa nesne o sütunu zaten otomatik 
    eklemektedir. "One-hot-encoding" ile oluşturulan sütunların isimleri önemli değildir. Ancak OneHotEncoder nesnesi önce 
    sütunu np.unique fonksiyonuna sokmakta ondan sonra o sırada encoding işlemi yapmaktadır. NumPy'ın unique fonksiyonu aynı 
    zamanda sıraya dizme işlemini de zaten yapmaktadır. Dolayısıyla OneHotEncoder aslında kategorik değerleri alfabetik sıraya 
    göre sütunsal olarak dönüştürmektedir. Yukarıda da belirttiğimiz gibi OneHotEncoder nesnesi fit işlemi yapıldığında zaten 
    kategorileri categories_ isimli örnek özniteliği yoluyla bir NumPy dizi listesi olarak bize vermektedir. Biz DataFrame eklemesi 
    yaparken doğrudan bu isimleri kullanabiliriz. categories_ örnek özniteliğinin bir liste olduğuna dikkat ediniz. Çünkü birden 
    fazla sütun tek hamlede "one-hot-encoding" yapılabilmektedir. Dolayısıyla programcı bu listenin ilgili elemanından kategorileri 
    elde etmelidir. 

    Yukarıda da belirttiğimiz gibi OneHotEncoder sınıfının fit ve transform metotları çok boyutlu dizileri kabul etmektedir. 
    Bu durumda biz bu metotlara Pandas'ın Series nesnesini değil DataFrame nesnesini vermeliyiz.

    OneHotEncoder nesnesini yaratırken "sparse_output" parametresini False biçimde vermeyi unutmayınız. (Bu parametrenin eski 
    ismi yalnızca "sparse" biçimindeydi). Çünkü bu sınıf default olarak transform edilmiş nesneyi "seyrek matris (sparse matrix)" 
    olarak vermektedir. Elemanlarının büyük çoğunluğu 0 olan matrislere "seyrek matris (sparse matrix)" denilmektedir. Bu tür 
    matrisler "fazla yer kaplamasın diye" sıkıştırılmış bir biçimde ifade edilebilmektedir. Seyrek matrisler üzerinde işlemler 
    kursumuzun sonraki bölümlerinde ele alınacaktır.
        
    Yine OneHotEncoder nesnesi yaratılırken parametre olarak dtype türünü belirtebiliriz. Default dtype türü np.float64 alınmaktadır. 
    Matris seyrek formda elde edilmeyecekse bu dtype türünü "uint8" gibi en küçük türde tutabilirsiniz. max_categories parametresi 
    kategori sayısını belli bir değerde tutmak için kullanılmaktadır. Bu durumda diğer tüm kategoriler başka bir kategori oluşturmaktadır.
    
    Örneğin:

    df = pd.read_csv('test.csv')
    ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
    transformed_data = ohe.fit_transform(df[['RenkTercihi']])
    df.drop(['RenkTercihi'], axis=1, inplace=True)
    df[ohe.categories_[0]] = transformed_data

    Burada önce veri kümesi okunmuş sonra OneHotEncoder sınıfıyla fit_transform işlemi yapılmıştır. Daha sonra ise DataFrame 
    içerisindeki "RenkTercihi" sütunu silinip onun yerine categories_ örnek özniteliğindeki kategori isimleri nesneye eklenmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('test.csv')

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
transformed_data = ohe.fit_transform(df[['RenkTercihi']])
df.drop(['RenkTercihi'], axis=1, inplace=True)
df[ohe.categories_[0]] = transformed_data

print(df)

#----------------------------------------------------------------------------------------------------------------------------
    DataFrame nesnesine yukarıdaki gibi birden fazla sütun eklerken dikkat etmek gerekir. Çünkü tesadüfen bu kategori isimlerine
    ilişkin sütunlardan biri zaten varsa o sütun yok edilip yerine bu kategori sütunu oluşturulacaktır. Bunu engellemek için 
    oluşturacağınız kategori sütunlarını önek vererek isimlendirebilirsiniz. Önek verirken orijinal sütun ismini kullanırsanız
    bu durumda çakışma olmayacağı garanti edilebilir. Yani örneğin RenkTercihi sütunu için "Kırmızı", "Mavi" "Yeşil" isimleri 
    yerine "RenkTercihi_Kırmızı", "RenkTercihi_Mavi" ve "RenkTercihi_Yeşil" isimlerini kullanabilirsiniz. Bu biçimde isim elde
    etmek "liste işlemiyle" oldukça kolaydır. Örneğin:

    category_names = ['RenkTercihi_' + category for category in ohe.categories_[0]]

    Aşağıda buna örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('test.csv')

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')

transformed_data = ohe.fit_transform(df[['RenkTercihi']])

df.drop(['RenkTercihi'], axis=1, inplace=True)

category_names = ['RenkTercihi_' + category for category in ohe.categories_[0]]
df[category_names] = transformed_data

print(df)

#----------------------------------------------------------------------------------------------------------------------------
    Veri kümesinde birden fazla sütunda kategorik bilgiler olabilir. Bunlar OneHotEncoder sınıfıyla tek tek kodlanabilirler. 
    Ancak aslında yukarıda da belirttiğimiz gibi OneHotEncoder sınıfı bir grup sütunu tek hamlede de kodlayabilmektedir. 
    Örneğin "test.csv" veri kümemiz şöyle olsun:

    AdıSoyadı,Kilo,Boy,Yaş,RenkTercihi,Meslek
    Sacit Bulut,78,172,34,Kırmızı,Mühendis
    Ayşe Er,67,168,45,Yeşil,Mühendis
    Ahmet San,85,182,32,Kırmızı,Avukat
    Macit Şen,98,192,65,Mavi,Doktor
    Talat Demir,85,181,49,Yeşil,Avukat
    Sibel Ünlü,72,172,34,Mavi,Doktor
    Ali Serçe,75,165,21,Yeşil,Mühendis

    Burada "RenkTercihi"nin yanı sıra "Meslek" de kategorik bir sütundur. Bunun her ikisini birden tek hamlede "one-hot-encoding" 
    işlemine sokabiliriz:

    ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
    transformed_data = ohe.fit_transform(df[['RenkTercihi', 'Meslek']])
    df.drop(['RenkTercihi', 'Meslek'], axis=1, inplace=True)
    categories1 = ['RenkTercihi_' + category for category in ohe.categories_[0]]
    categories2 = ['Meslek_' + category for category in ohe.categories_[1]]
    df[categories1 + categories2] = transformed_data
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('test.csv')

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')

transformed_data = ohe.fit_transform(df[['RenkTercihi', 'Meslek']])

df.drop(['RenkTercihi', 'Meslek'], axis=1, inplace=True)

categories1 = ['RenkTercihi_' + category for category in ohe.categories_[0]]
categories2 = ['Meslek_' + category for category in ohe.categories_[1]]

df[categories1 + categories2] = transformed_data

print(df)

#----------------------------------------------------------------------------------------------------------------------------
    Aslında makine öğrenmesi uygulamalarında veri kümesinin sütun isimlerinin hiçbir önemi yoktur. Dolayısıyla biz "one-hot-encoding" 
    işleminden önce NumPy dizisine geçip hiç sütun isimlerini oluşturmadan geri kalan kısmı NumPy üzerinde  de yapabiliriz. 
    Örneğin:

    df = pd.read_csv('test.csv')

    ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
    transformed_data = ohe.fit_transform(df[['RenkTercihi', 'Meslek']])

    df.drop(['AdıSoyadı', 'RenkTercihi', 'Meslek'], axis=1, inplace=True)
    dataset = df.to_numpy()
    dataset = np.concatenate((dataset, transformed_data), axis=1)

    Aşağıda buna bir örnek verilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('test.csv')

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
transformed_data = ohe.fit_transform(df[['RenkTercihi', 'Meslek']])

df.drop(['AdıSoyadı', 'RenkTercihi', 'Meslek'], axis=1, inplace=True)
dataset = df.to_numpy()
dataset = np.concatenate((dataset, transformed_data), axis=1)

print(dataset)

#----------------------------------------------------------------------------------------------------------------------------
    one-hot-encoding uygulamanın diğer bir yolu Pandas kütüphanesindeki get_dummies fonksiyonunu kullanmaktadır. get_dummies 
    fonksiyonu bizden bir DataFrame, Series ya da dolaşılabilir herhangi bir nesneyi alır. Eğer biz get_dummies fonksiyonuna 
    bütün bir DataFrame geçirirsek fonksiyon oldukça akıllı davranmaktadır. Bu durumda fonksiyon DataFrame nesnesi içerisindeki 
    yazısal sütunları tespit eder. Yalnızca yazısal sütunları "one-hot-encoding" işlemine sokar ve bize yazısal sütunları 
    dönüştürülmüş yeni bir DataFrame nesnesi verir. Pandas ile çalışırken bu fonksiyon çok kolaylık sağlamaktadır. yine "test.csv" 
    veri kümemiz şöyle olsun:

    AdıSoyadı,Kilo,Boy,Yaş,RenkTercihi,Meslek
    Sacit Bulut,78,172,34,Kırmızı,Mühendis
    Ayşe Er,67,168,45,Yeşil,Mühendis
    Ahmet San,85,182,32,Kırmızı,Avukat
    Macit Şen,98,192,65,Mavi,Doktor
    Talat Demir,85,181,49,Yeşil,Avukat
    Sibel Ünlü,72,172,34,Mavi,Doktor
    Ali Serçe,75,165,21,Yeşil,Mühendis

    Biz aslında get_dummies fonksiyonu yoluyla yapmış olduğumuz işlemleri tek hamlede yapabiliriz:

    df = pd.read_csv('test.csv')
    transformed_df = pd.get_dummies(df, dtype='uint8')

    Burada biz tek hamlede istediğimiz dönüştürmeyi yapabildik. Bu dönüştürmede yine sütun isimleri orijinal sütun isimleri 
    ve kategori simleriyle öneklendirilmiştir. Eğer isterse programcı "prefix" parametresi ile bu öneki değiştirebilir, "prefix_sep"
    parametresiyle de '_' karakteri yerine başka birleştirme karakterlerini kullanabilir. get_dummies fonksiyonu default durumda 
    sparse olmayan bool türden bir DataFrame nesnesi vermektedir. Ancak get_dummies fonksiyonunda "dtype" parametresi belirtilerek 
    "uint8" gibi bir türden çıktı oluşturulması sağlanabilmektedir. 
    
    Biz bir DataFrame nesnesinin tüm yazısal sütunlarını değil bazı yazısal sütunlarını da "one-hot-encoding" işlemine sokmak 
    isteyebiliriz. Bu durumda fonksiyon DataFrame nesnesinin diğer sütunlarına hiç dokunmamaktadır. Örneğin:
    
    transformed_df = pd.get_dummies(df, columns=['Meslek'], dtype='uint8')

    Biz burada yalnızca DataFrame nesnesinin "Meslek" sütununu "one-hot-encoding" yapmış olduk. get_dummies fonksiyonun zaten 
    "one-hot-encoding" yapılan sütunu sildiğine dikkat ediniz. Bu bizim genellikle istediğimiz bir şeydir. Yukarıdaki örnekte 
    "test.csv" dosyasında "AdıSoyadı" sütunu yazısal bir sütundur. Dolayısıyla default durumda bu sütun da "one-hot-encoding" 
    işlemine sokulacaktır. Bunu engellemek için "columns" parametresinden faydalanabiliriz ya da baştan o sütunu atabiliriz. 
    Örneğin:

   transformed_df = pd.get_dummies(df.iloc[:, 1:], dtype='uint8')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                22. Ders - 10/03/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('test.csv')

print(df, end='\n\n')

ohe_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'])

print(ohe_df)

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi get_dummies fonksiyonu default durumda oluşturduğu sütunlara kategorik sütun isimlerini ve 
    kategoroileri '_' ile birleştirerek vermektedir. Ancak bir istersek prefix parametresi ile bu önekleri değiştirebiliriz. 
    Örneğin:

    df = pd.read_csv('test.csv')

    transformed_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'], dtype='uint8', prefix=['R', 'M'])

    Burada oluşturulacak sütunlar için önekler sırasıyla 'R' ve 'M' biçiminde verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('test.csv')

transformed_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'], dtype='uint8', prefix=['R', 'M'])
print(transformed_df)

#----------------------------------------------------------------------------------------------------------------------------
   Yine yukarıda belirttiğimiz gibi sütun isimlerinin birleştirilmesi için kullanılan karakter prefix_sep isimli parametreyle 
   değiştirilebilmektedir. Örneğin:

   df = pd.read_csv('test.csv')

    transformed_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'], dtype='uint8', prefix=['R', 'M'], prefix_sep='-')

    Burada artık önekler 'R-' ve 'M-' haline getirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('test.csv')

transformed_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'], dtype='uint8', prefix=['R', 'M'], prefix_sep='-')
print(transformed_df)

#----------------------------------------------------------------------------------------------------------------------------
    One-hot-encoding" dönüştürmesi tensorflow.keras kütüphanesindeki to_categorical fonksiyonuyla da yapılabilmektedir. Bazen 
    zaten Keras ile çalışıyorsak bu fonksiyonu tercih edebilmekteyiz. to_categorical fonksiyonunu kullanmadan önce kategorik 
    sütunun sayısal biçime dönüştürülmüş olması gerekir. Yani biz önce sütun üzerinde eğer sütun yazısal ise LabelEncoder 
    işlemini uygulamalıyız. to_categorical fonksiyonu aynı anda birden fazla sütunu "one-hot-encoding" yapamamaktadır. Bu nedenle 
    diğer seçeneklere göre kullanımı daha zor bir fonksiyondur. to_categorical fonksiyonu Keras kütüphanesindeki utils isimli 
    modülde bulunaktadır. Fonksiyonu kullanmak için aşağıdaki gibi import işlemi yapabilirsiniz:

    from tensorflow.keras.utils import to_categorical

    Tabii bu fonksiyonu kullanabilmeniz için tensorflow kütüphanesinin de yüklü olması gerekir. Biz zaten izleyen konularda 
    bu kütüphaneyi kullanacağız. Kütüphane şöyle yüklenebilir:

    pip install tensorflow
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('test.csv')

from tensorflow.keras.utils import to_categorical

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

transformed_color = le.fit_transform(df['RenkTercihi'])
transformed_occupation = le.fit_transform(df['Meslek'])

ohe_color = to_categorical(transformed_color)
ohe_occupation = to_categorical(transformed_occupation)

color_categories = ['RenkTercihi_' + color for color in df['RenkTercihi'].unique()]
occupation_categories = ['Meslek_' + occupation for occupation in df['Meslek'].unique()]

df.drop(['RenkTercihi', 'Meslek'], axis=1, inplace=True)

df[color_categories] = ohe_color
df[occupation_categories] = ohe_occupation

print(df)

#----------------------------------------------------------------------------------------------------------------------------
    "one-hot-encoding" dönüştürmesi manuel yöntemle de yapılabilir. Bu işlemi manuel yapmanın çeşitli yöntemleri vardır. Ancak 
    en basit yöntemlerden biri NumPy'ın eye fonksiyonundan faydalanmaktır. Bu fonksiyon bize birim matris verir. Bir NumPy 
    dizisi bir listeyle indekslenebildiğine göre bu birim matris LabelEncoder ile sayısal biçime dönüştürülmüş bir dizi ile 
    indekslenirse istenilen dönüştürme yapılmış olur. Örneğin dönüştürülecek sütun bilgisi şunlardan oluşsun:

    RenkTercihi
    -----------
    Kırmızı
    Mavi
    Yeşil
    Yeşil
    Mavi
    Kırmızı 
    Mavi

    Burada sütunda üç farklı kategori vardır: Kırmızı, Mavi, ve Yeşil. Bunlar LabelEncoder yapılırsa sütun şu biçime dönüştürülmüştür:

    RenkTercihi
    -----------
    0
    1
    2
    2
    1
    0
    1

    3 farklı kategori olduğuna göre 3X3'lük aşağıdaki gibi bir birim matris oluşturulabilir:

    1 0 0
    0 1 0 
    0 0 1

    Sonrada bu birim matris bir dizi ile indekslenirse "one-hot-encoding" işlemi gerçekleştirilir. 

    Aşağıda buna ilişkin bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('test.csv')

print(df, end='\n\n')

import numpy as np

color_cats = np.unique(df['RenkTercihi'].to_numpy())
occupation_cats = np.unique(df['Meslek'].to_numpy())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['RenkTercihi'] = le.fit_transform(df['RenkTercihi'])
df['Meslek'] = le.fit_transform(df['Meslek'])

print(df, end='\n\n')

color_um = np.eye(len(color_cats))
occupation_um = np.eye(len(occupation_cats))

ohe_color = color_um[df['RenkTercihi'].to_numpy()]
ohe_occupation = occupation_um[df['Meslek'].to_numpy()]

df.drop(['RenkTercihi', 'Meslek'], axis=1, inplace=True)
df[color_cats] = ohe_color
df[occupation_cats] = ohe_occupation

print(df, end='\n\n')

#----------------------------------------------------------------------------------------------------------------------------
    "one-hot-encoding" işleminin bir versiyonuna da "dummy variable encoding" denilmektedir. Şöyle ki: "one-hot-encoding" 
    işleminde n tane kategori için n tane sütun oluşturuluyordu. Halbuki "dummy variable encoding" işleminde n tane kategori 
    için n - 1 tane sütun oluşturulmaktadır. Çünkü bu yöntemde bir kategori tüm sütunlardaki sayının 0 olması ile ifade edilmektedir. 
    Örneğin Kırmızı, Yeşil, Mavi kategorilerinin bulunduğu bir sütun şöyle "dummy variable encoding" biçiminde dönüştürülebilir:

    Mavi Yeşil
    0       0       (Kırmızı)
    1       0       (Mavi)
    0       1       (Yeşil)

    Görüldüğü gibi kategorilerden biri (burada "Kırmızı") tüm elemanı 0 olan satırla temsil edilmiştir. Böylece sütun sayısı 
    bir eksiltilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    "Dummy variable encoding" işlemi için farklı sınıflar ya da fonksiyonlar kullanılmamaktadır. Bu işlem "one-hot-encoding"
    yapan sınıflar ve fonksiyonlarda özel bir parametreyle gerçekleştirilmektedir. Örneğin scikit-learn kütüphanesindeki 
    OneHotEncoder sınıfının drop parametresi 'first' olarak geçilirse bu durumda transform işlemi "dummy variable encoding" 
    biçiminde yapılmaktadır. Örneğin "test.csv" dosyası aşağıdaki gibi olsun:

    AdıSoyadı,Kilo,Boy,Yaş,RenkTercihi,Meslek
    Sacit Bulut,78,172,34,Kırmızı,Mühendis
    Ayşe Er,67,168,45,Yeşil,Mühendis
    Ahmet San,85,182,32,Kırmızı,Avukat
    Macit Şen,98,192,65,Mavi,Doktor
    Talat Demir,85,181,49,Yeşil,Avukat
    Sibel Ünlü,72,172,34,Mavi,Doktor
    Ali Serçe,75,165,21,Yeşil,Mühendis

    Biz de "RenkTercihi" sütununu "dummy variable encoding" ile dönüştürmek isteyelim. Bu işlemi şöyle yapabiliriz:

    df = pd.read_csv('test.csv')

    ohe = OneHotEncoder(sparse_output=False, drop='first')
    transformed_data = ohe.fit_transform(df[['RenkTercihi']])

    print(df['RenkTercihi'])
    print(ohe.categories_)
    print(transformed_data)

    Buradan şu çıktılar elde edilmiştir:

    0    Kırmızı
    1      Yeşil
    2    Kırmızı
    3       Mavi
    4      Yeşil
    5       Mavi
    6      Yeşil
    Name: RenkTercihi, dtype: object
    [array(['Kırmızı', 'Mavi', 'Yeşil'], dtype=object)]
    [[0. 0.]
    [0. 1.]
    [0. 0.]
    [1. 0.]
    [0. 1.]
    [1. 0.]
    [0. 1.]]

    Görüldüğü gibi burada "Kırmızı" kategorisi [0, 0] biçiminde kodlanmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
  
df = pd.read_csv('test.csv')

ohe = OneHotEncoder(sparse_output=False, drop='first')
transformed_data = ohe.fit_transform(df[['RenkTercihi']])

print(df['RenkTercihi'])
print(ohe.categories_)
print(transformed_data)

#----------------------------------------------------------------------------------------------------------------------------
    Pandas'ın get_dummies fonksiyonunda drop_first parametresi True geçilirse "dummy variable encoding" uygulanmaktadır. 
    Örneğin:

    df = pd.read_csv('test.csv')

    transformed_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'], dtype='uint8', drop_first=True)

    Burada veri kümesinin "RenkTercihi" ve "Meslek" sütunları "dummy variable encoding" olarak kodlanmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
  
df = pd.read_csv('test.csv')

transformed_df = pd.get_dummies(df, columns=['RenkTercihi', 'Meslek'], dtype='uint8', drop_first=True)
print(transformed_df)

#----------------------------------------------------------------------------------------------------------------------------
    Scikit-learn içerisinde OneHotEncoder sınıfına benzer işlem yapan LabelBinarizer isimli bir sınıf da bulunmaktadır. Bu 
    sınıf özellikle veri kümesindeki kategorik X sütunları için değil kategorik Y sütunları için kullanılmaktadır. Bu sınıfla
    fit ve transform işlemi yapılırken bu metotlara çok boyutlu değil tek boyutlu bir liste, demet, NumPy dizisi ya da Series 
    nesnesi verilir. LabelBinarizer sınıfı da OneHotEncoder sınıfı gibi tüm sütunları 0 olan yalnızca bir tane sütunu 1 olan 
    değerler üretir. Örneğin:

    from sklearn.preprocessing import LabelBinarizer

    y = ['elma', 'armut', 'elma', 'kayısı', 'elma', 'kayısı', 'armut']

    lb = LabelBinarizer()
    transformed_labels = lb.fit_transform(y)
    print(transformed_labels)

    Burada "elma", "armut", "kayısı" olmak üzere toplam üç farklı sınıf vardır. Bu nedenle kodlama sonucunda üç sütun oluşacaktır. 
    Yukarıdaki kodlama sonucunda elde edilen transformed_labels dizisi şöyle olacaktır:
    
    [[0 1 0]
    [1 0 0]
    [0 1 0]
    [0 0 1]
    [0 1 0]
    [0 0 1]
    [1 0 0]]

    Görüldüğü gibi tıpkı one-hot-encoding kodlamasında olduğu gibi satırda yalnızca bir tane 1 vardır. Sınıflar yine önce 
    sıraya dizilerek numaralandırılmaktadır:

    armut  elma   kayısı
    
     0      1       0   
     1      0       0
     0      1       0
     0      0       1
     0      1       0
     0      0       1
     1      0       0

    Peki LabelBinarizer sınıfı ile OneHotEncoder sınıfı arasında ne fark vardır? Aslında bu iki sınıf birbirine oldukça 
    benzemektedir. Aralarındaki farkları ve benzerlikleri şöyle açıklayabiliriz:

    - Her iki sınıf da tüm bitleri 0 olan yalnızca tek biti 1 olan satırlar vermektedir.

    - LabelBinarizer sınıfı giridiyi tek boyutlu bir dizi (liste, demet, Series gibi) biçiminde alırken OneHotEncoder sınıfı 
    çok boyutlu bir dizi olarak (liste listesi, NumPy dizi listesi, DataFrame nesnesi gibi) almaktadır.  

    - LabelBinarizer sınıfı çıktıyı int64 türünden bir NumPy dizisi olarak verirken OneHotEncoder sınıfı default olarak float64 
    türünden vermektedir. (OneHotEncoder sınıfında çıktının dtype türü değiştirilebilmektedir.)
#----------------------------------------------------------------------------------------------------------------------------

from sklearn.preprocessing import LabelBinarizer

y = ['elma', 'armut', 'elma', 'kayısı', 'elma', 'kayısı', 'armut']

lb = LabelBinarizer()
transformed_labels = lb.fit_transform(y)
print(transformed_labels)

#----------------------------------------------------------------------------------------------------------------------------
    Sütundaki kategori sayısı çok fazla ise "one-hot-encoding" dönüştürmesi çok fazla sütunun veri tablosuna eklenmesine yol 
    açmaktadır. Aslında pek çok durumda bunun önemli bir sakıncası yoktur. Ancak sütun sayısının fazlalaşması veri kümesinin
    bellekte çok yer kaplamasına ve bunların işlenmesi için gereken sürenin uzamasına yol açmaktadır. Peki kategori çok 
    fazla ise ve biz çok fazla sütunun veri kümesine eklenmesini istemiyorsak bu durumda ne yapabiliriz? Yöntemlerden biri 
    çeşitli kategorileri başka üst kategoriler içerisinde toplamak olabilir. Böylece aslında bir grup kategori sınıflandırılarak 
    bunlardan daha az kategori elde edilebilir. Yöntemlerden diğeri "one-hot-encoding" yerine alternatif başka kodlamaların 
    kullanılmasıdır. Burada da akla "binary encoding" yöntemi gelmektedir. Tabii kategorik veriler için en iyi yöntem aslında 
    "one-hot-encoding" yöntemidir. Uygulamacı mümkün olduğunca bu yöntemi kullanmaya çalışmalıdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Binary encoding yönteminde her kategori "ikilik sistemde bir sayıymış" gibi ifade edilmektedir. Örneğin sütunda 256 tane 
    kategori olsun. Bu kategoriler 0'dan 255'e kadar numaralandırılabilir. 0 ile 255 arasındaki sayılar 2'lik sistemde 8 bit 
    ile ifade edilebilir. Örneğin bir kategorinin sayısal değeri (LabelEncoder yapıldığını düşünelim) 14 olsun. Biz bu kategoriyi 
    aşağıdaki gibi 8 bit'lik 2'lik sistemde bir sayı biçiminde kodlayabiliriz:

    0 0 0 0 1 1 1 0 

    Tabii kategori sayısı tam 2'nin kuvveti kadar olmak zorunda değildir. Bu durumda kategori sayısı N olmak üzere gerkeli olan 
    bit sayısı (yani sütun sayısı) ceil(log2(N)) hesabı ile elde edilebilir. 

    Binary encoding işlemi manuel bir biçimde yapılabilir. Bunun için maalesef Pandas'ta ya da NumPy'da bir fonksiyon bulundurulmamıştır. 
    scikit-learn kütüphanesinin ana modülleri içerisinde de böyle bir sınıf yoktur. Ancak scikit-learn kütüphanesinin contribute 
    girişimlerinden birinde bu işlemi yapan bir BinaryEncoder isminde bir sınıf bulunmaktadır. Bu sınıf category_encoders isimli 
    bir paket içerisindedir ve bu paket ayrıca yüklenmelidir. Yükleme şöyle yapılabilir:

    pip install category_encoders

    BinaryEncoder sınıfının genel kullanımı diğer scikit-learn sınıflarında olduğu gibidir. Yani önce fit, sonra tra Yine 
    "test.csv" dosyasımız aşağıdaki gibi olsun:
    
    AdıSoyadı,Kilo,Boy,Yaş,RenkTercihi,Meslek
    Sacit Bulut,78,172,34,Kırmızı,Mühendis
    Ayşe Er,67,168,45,Yeşil,Mühendis
    Ahmet San,85,182,32,Kırmızı,Avukat
    Macit Şen,98,192,65,Mavi,Doktor
    Talat Demir,85,181,49,Yeşil,Avukat
    Sibel Ünlü,72,172,34,Mavi,Doktor
    Ali Serçe,75,165,21,Yeşil,Mühendis

    BinaryEncoder sınıfı category_encoders paketinin Binary modülünün içerisinde bulunmaktadır. Dolayısıyla sınıfı kullanabilmek
    için aşağıdaki gibi import işlemi yapabiliriz:

    from category_encoders.binary import BinaryEncoder

    BinaryEncoder sınıfının transform fonksiyonu default durumda Pandas DataFrame nesnesi vermektedir. Ancak nesne yaratılırken 
    return_df parametresi False geçilirse bu durumda transform fonksiyonları NumPy dizisi geri döndürmektedir. Örneğin:

    df = pd.read_csv('test.csv')

    be = BinaryEncoder()
    transformed_data = be.fit_transform(df['Meslek'])
    print(transformed_data) 

    Burada "Meslek" sütunu "binary encoding" biçiminde kodlanmıştır. BinaryEncode kodlamada değerleri 1'den başlatılmaktadır. 
    Yukarıdaki işlemden aşağıdaki gibi bir çıktı elde edilmiştir:

       Meslek_0  Meslek_1
    0         0         1
    1         0         1
    2         1         0
    3         1         1
    4         1         0
    5         1         1
    6         0         1

    BinaryEncoder sınıfı ile biz tek boyutlu ya da çok boyutlu (yani tek sütunu ya da birden fazla sütunu) kodlayabiliriz. 
    Örneğin:

    be = BinaryEncoder()
    transformed_data = be.fit_transform(df[['RenkTercihi', 'Meslek']])
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
  
df = pd.read_csv('test.csv')

from category_encoders.binary import BinaryEncoder

be = BinaryEncoder()
transformed_data = be.fit_transform(df[['RenkTercihi', 'Meslek']])
print(transformed_data)

df.drop(['RenkTercihi', 'Meslek'], axis=1, inplace=True)
df[transformed_data.columns] = transformed_data    # pd.concat((df, transformed_data), axis=1)
print(df)

#----------------------------------------------------------------------------------------------------------------------------
    Tıpkı get_dummies fonksiyonunda olduğu gibi aslında bir DataFrame bütünsel olarak da verilebilir. Yine default durumda 
    tüm yazısal sütunlar "binary encoding" dönüştürmesine sokulmaktadır. Ancak biz BinaryEncoding sınıfının __init__ metodunda
    cols parametresi ile hangi sütunların dönüştürüleceğini belirleyebiliriz. Örneğin:

    df = pd.read_csv('test.csv')

    from category_encoders.binary import BinaryEncoder

    be = BinaryEncoder(cols=['RenkTercihi', 'Meslek'])
    transformed_df = be.fit_transform(df)
#----------------------------------------------------------------------------------------------------------------------------
   
    import pandas as pd
    
    df = pd.read_csv('test.csv')

    from category_encoders.binary import BinaryEncoder

    be = BinaryEncoder(cols=['RenkTercihi', 'Meslek'])
    transformed_df = be.fit_transform(df)
    print(transformed_df)

#----------------------------------------------------------------------------------------------------------------------------
    Bir sütunun satırlarına sıfır tane ya da daha fazla etiket iliştirilebiliyorsa böyle sütunlara "çok etiketli (multi-label)
    sütunlar denilmektedir. Bu tür sütunlar genellikle "çok etiketli sınıflandırma (multi-label classification)" denilen 
    sınıflandırma problemlerinde karşımıza çıkmaktadır. Çok etiketli sınıflandırma problemlerinde hedef değişken birden fazla 
    etiketten oluşabilen kategorik bir sütun görünümündedir. Bu tür problemlerde satırlar için en uygun etiketlerin atanması 
    istenir. Örneğin kişilerin yazdıkları yazılardan hareketle psikiyatrik bağlamda klinik odaklarının  belirlenmesi istensin.  
    Bu veri kümesinde satırlar yazılardan hedef değişken de bu yazılara atanan etiketlerdne oluşmaktadır. Aşağıda böyle bir
    veri kümesi için bir örnek görüyorsunuz:

                            Yazılar                                                           Etiketler

    "Son zamanlarda hiçbir şeyden keyif almıyorum ve sürekli mutsuzum.",                    "depresyon"
    "Topluluk önünde konuşmam gerektiğinde panik oluyorum, nefes alamıyorum.",              "anksiyete,fobi"
    "Duygularım çok hızlı değişiyor, bir anda sinirlenip sonra ağlamaya başlıyorum.",       "borderline"
    "Kafamda sürekli endişe düşünceleri var, gece bile rahat uyuyamıyorum.",                "anksiyete"
    "İnsanların bana zarar vereceğinden korkuyorum, yalnız kalmak istiyorum.",              "fobi,anksiyete"
    "Hayatın anlamını sorguluyorum, çoğu zaman yaşamak istemiyorum.",                       "depresyon,borderline"

    Bu biçimdeki çok etiketli veri kümelerinde her satırın etiket sayısı aynı olmadığı için bu durum pek çok yöntemi uygularken 
    bir sorun haline gelmektedir. İşte bu tür çok etiketli veri kümelerinde sayısal hale getirme işlemi genellikle şöyle 
    yapılmaktadır:
    
    - Önce tüm etiketler elde edilir.
    - Etiketler için etiket sayısı kadar sütun oluşturulur.
    - Sonra her satırdaki etiketler tüm etiketler dikkate alınarak sütunsal biçimde 1 ya da 0 olarak kodlanır. (İlgili etiket
    satırda varsa 0, yoksa 1 olarak kodlama yapılmaktadır.)

    Şimdi bir örnek verelim. Yukarıdaki veri kümesindeki etiketler şunlardır: "depresyon", "anksiyete", "fobi", "borderline". 
    Bu durumda bu etiketler toplan 4 sütun ile aşağıdaki gibi ifade edilecektir:

                            Yazılar                                                 depresyon anksiyete fobi borderline

    "Son zamanlarda hiçbir şeyden keyif almıyorum ve sürekli mutsuzum."                      1 0 0 0
    "Topluluk önünde konuşmam gerektiğinde panik oluyorum, nefes alamıyorum."                0 1 1 0
    "Duygularım çok hızlı değişiyor, bir anda sinirlenip sonra ağlamaya başlıyorum."         0 0 0 1 
    "Kafamda sürekli endişe düşünceleri var, gece bile rahat uyuyamıyorum."                  0 1 0 0 
    "İnsanların bana zarar vereceğinden korkuyorum, yalnız kalmak istiyorum."                0 1 1 0 
    "Hayatın anlamını sorguluyorum, çoğu zaman yaşamak istemiyorum."                         1 0 0 1

    Çok etiketli veri kümelerini yularıdaki gibi sayısal hale getirmek için scikit-learn içerisinde MultiLabelBinarizer 
    isimli bir sınıf bulundurulmuştur. Sınıfın genel kullanımı diğer scikit-learn sınıflarında olduğu gibibir. Önce 
    MultiLabelBinarizer sınıfı türünden bir nesne yaratılır. Sonra bu nesne ile fit ve transform işlemleri yapılır. Ancak 
    sınıfın fit metoduna liste, demet, NumPy dizisi gibi nesnelerden oluşan liste, demet, Series gibi nesneleri vermeliyiz. 
    Örneğin biz fit metoduna her bir elemanı bir listeden oluşan bir liste (yani liste listesi) verebileceğimiz gibi her 
    bir elemanı listeden oluşan bir Series nesnesi de verebiliriz. Sınıfın transform metodu fit metoduyla aynı biçimde organize 
    edilmiş değerleri parametre olarak alır, onları sayısal hale getirir ve bir NumPy dizisine geri döner. Tabii fit_transform 
    işlemi birlikte de yapılabilmektedir. Sınıfın diğer bazı ayrıntıları için sckit-leran dokümanlarına başvurabilirsiniz.

    Şimdi MultiLableBinarizer sınıfının kullanımına bir örnek verelim. Örnek veri kümemiz "dataset.csv" isimli bir dosyada 
    aşağıdaki içeriğe sahip biçimde bulunuyor olsun:

    text,labels
    "Son zamanlarda hiçbir şeyden keyif almıyorum ve sürekli mutsuzum.","depresyon"
    "Topluluk önünde konuşmam gerektiğinde panik oluyorum, nefes alamıyorum.","anksiyete,fobi"
    "Duygularım çok hızlı değişiyor, bir anda sinirlenip sonra ağlamaya başlıyorum.","borderline"
    "Kafamda sürekli endişe düşünceleri var, gece bile rahat uyuyabirmıyorum.","anksiyete"
    "İnsanların bana zarar vereceğinden korkuyorum, yalnız kalmak istiyorum.","fobi,anksiyete"
    "Hayatın anlamını sorguluyorum, çoğu zaman yaşamak istemiyorum.","depresyon,borderline"

    Biz bu CSD dosyasını Pandas'ın read_csv fonksiyonu ile okuduğumuzda her iki sütunu da yazılardan oluşan bir DataFrame 
    nesnesi elde ederiz:

    df = pd.read_csv('dataset.csv')

    DataFrame içerisindeki "labels" sütununu Series nesnesi olarak elde edip ona apply metodunu uygulayarak DataFrame sütununu
    litelerden oluşan bir sütun haline getirebiliriz.

    df['labels'] = df['labels'].apply(lambda s: s.split(','))

    apply metodu her satırı bizim veridğimiz fonksiyona sakup o satırı veridğimiz fonksiyonun geri dönüş değeri ile değiştirmektedir.
    Bu işlem soncunda df['labels'] sütunu şu hale gelecektir:

    0                [depresyon]
    1          [anksiyete, fobi]
    2               [borderline]
    3                [anksiyete]
    4          [fobi, anksiyete]
    5    [depresyon, borderline]
    Name: labels, dtype: object     

    Artık MultiLableBinarizer nesnesini yaratıp fit ve transform işlemlerini yapabiliriz:

    mlb = MultiLabelBinarizer()
    transformed_labels = mlb.fit_transform(df['labels'])
    print(transformed_labels)

    Bu işlemlerden şöyle bir NumPy dizisi elde edilecektir:

    [
    [0 0 1 0]
    [1 0 0 1]
    [0 1 0 0]
    [1 0 0 0]
    [1 0 0 1]
    [0 1 1 0]
    ]

    Tabii aslında biz yukarıdaki fit_ranmsform işlemini doğrudan aşağıdaki gibi de yapabilirdik:

    mlb = MultiLabelBinarizer()
    transformed_labels = mlb.fit_transform(df['labels'].apply(lambda s: s.split(',')))
    print(transformed_labels)
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer

df = pd.read_csv('dataset.csv')

mlb = MultiLabelBinarizer()
transformed_labels = mlb.fit_transform(df['labels'].apply(lambda s: s.split(',')))
print(transformed_labels)

#----------------------------------------------------------------------------------------------------------------------------
                                            23. Ders - 16/03/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesi alanının en önemli yöntemlerinin başında "yapay sinir ağları (artificial neural networks)" 
    ve "derin öğrenme (deep learning)" denilen yöntemler gelmektedir. Biz de bu bölümde belli bir derinlikte bu konuları ele 
    alacağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay Sinir ağlarının teorisi ilk zamanlar sinir bilimle (neuroscience), psikolojiyle ve matematikle uğraşan bilim insanları 
    tarafından geliştirilmiştir. Yapay sinir ağları ilk kez Warren McCulloch ve Walter Pitts isimli kişiler tarafından 1943 
    yılında ortaya atılmıştır. 1940'lı yılların sonlarına doğru Donald Hebb isimli psikolog da "Hebbian Learning" kavramıyla, 
    1950'li yıllarda da Frank Rosenblatt isimli araştırmacı da "perceptron" kavramıyla alana önemli katkılarda bulunmuştur. 
    Bu yıllar henüz elektronik bilgisayarların çok yeni olduğu yıllardı. Halbuki yapay sinir ağlarına yönelik algoritmalar 
    için önemli bir CPU gücü gerekmekteydi. Bu nedenle özellikle 1960 yıllarda bu konuda bir motivasyon eksikliği oluşmuştur. 
    Yapay sinir ağları sonraki dönemlerde yeniden popüler olmaya başlamıştır. Derin öğrenme konusunun gündeme gelmesiyle de 
    popülaritesi hepten artmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay Sinir ağlarının pek çok farklı alanda uygulaması vardır. Örneğin:

    - Metinlerin sınıflandırılması ve kategorize edilmesi (text classification and categorization)
    - Ses tanıma (speech recognition)
    - Karakter tanıma (character recognition) 
    - İsimlendirilmiş Varlıkların Tanınması (mamed entitiy recognition)
    - Sözcük gruplarının aynı anlama gelip gelmediğinin belirlenmesi (paraphrase detection and identification)
    - Metin üretimi (text generation)
    - Makine çevirisi (machine translation)
    - Örüntü tanıma (pattern recognition)
    - Yüz tanıma (face recognition)
    - Finansal uygulamalar (portföy yönetimi, kredi değerlendirmesi, sahtecilik, gayrimenkul değerlemesi, döviz fiyatlarının 
    tahmini vs.)
    - Endüstriyel problemlerin çözümü
    - Biyomedikal mühendisliğine ilişkin bazı uygulamalar (Örneğin medikal görüntü analizi, hastalığa tanı koyma ve tedavi 
    planı oluşturma)
    - Optimizasyon problemlerinin çözümü
    - Pazarlama süreçlerinde karşılaşılan problemlerin çözümü
    - Ulaştırma problemlerinin çözümü
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay Sinir Ağları (Artificial Neural Networks) insanın sinir sisteminden esinlenerek geliştirilmiş olan yöntemler grubudur. 
    Her ne kadar artık yapay sinir ağlarının insanın sinir sistemiyle yakın bir kalmadıysa da biz yine de klasik biçimde önce 
    bu yöntemler grubunun ilham kaynağı olan insanın sinir sistemi üzerinde bazı açıklamalarla konuya başlayacağız. 
    
    Sinir sisteminin temel yapı taşı "nöron (neuron)" denilen hücrelerdir. Bir nöron bir çekirdeğe sahiptir. Nöronon başka 
    nöröndan gelen iletileri alan "dendrit (dendrite)" denilen bir kısmı vardır. Pek çok nöronda bir dal biçiminde uzanan aksonlar 
    da bulunmaktadır. Aksonların uçlarında küçük "düğmecikler (terminal buttons)" vardır. Bir nöron ateşlendiğinde bu düğmeciklerden 
    "nörotransmiter (neurotransmitter)" denilen kimyasallar zerk edilir. Bunlar diğer nöronun reseptörleri tarafından alınmaktadır. 
    Ateşleme nörondaki "aksiyon potansiyeli (action potantial)" belli  bir düzeye geldiğinde  gerçekleşmektedir. Bir nöron duruma 
    göre yüzlerce nörona bağlı olabilmektedir. Bir nöronun akson ucu ile diğer nönronun dendrit reseptörleri arasındaki bölgeye 
    "sinaps" denilmektedir. 
    
    Çeşitli nörotransmiterler vardır. Her nörotransmiter anahtarın kilide uyması gibi farklı reseptörler tarafından alınmaktadır. 
    Bir nörotransmiteri sinapslarda artıran maddelere "agonist", azaltan maddelere ise "antagonist" denilmektedir. Agonist etki 
    çeşitli biçimlerde sağlanabilmektedir. Örneğin bunun için presinaptik nöronda (nörotransmiterleri zerk eden nöron) nörotransmiter 
    miktarı ya da örneğin post sinaptik nörondaki (nörotransmiterleri alan nöron) reseptör duyarlılığı artırılabilir. Bir nörotransmiter 
    zerk edildikten sonra akson uçları tarafından geri alınmaktadır. Buna "geri alım (reuptake)" işlemi denir. Reuptake mekanizmasının 
    inhibe edilmesiyle de sinapstaki nörotransmiter etkinliği artırılabilmektedir. Örneğin SSRI (Selective Serotonin Reuptake 
    Inhibitors) denilen antideprasan ilaçlar bu mekanizmayla sinapslardaki serotonin miktarını artırma iddiasındadır. 

    Nöronların bir kısmına "duyusal nöronlar (sensory neurons)" denilmektedir. Bunlar dış fiziksel uyaranları alıp onu kimyasal 
    düzeye dönüştürmekte işlev görürler. İletiler nihai olarak beyindeki bazı bölümlerde işlenmektedir. Örneğin beynin arka kısmına 
    "oksipital lob" denilmektedir. Görsel iletiler buradaki nöron ağı tarafından işlenir. Bazı nöronlara ise "motor nöronlar 
    (motor neurons)" denilmektedir. Motor nöronlar kaslara bağlıdır ve kasların kasılmalarını sağlarlar. Örneğin elimizi hareket 
    ettirmek istediğimizde bu süreç beynin emir vermesiyle başlayan nöral iletinin ele ulaşıp oradaki kasları hareket ettirmesiyle 
    sağlanmaktadır. Dolayısıyla bu nöral ileti bozulursa felç durumu ortaya çıkmaktadır. 
    
    Nöronlarda "miyelin kılıfı (myline sheet)" denilen özel bir kılıf bulunabilmektedir. Bu kılıf nöral iletiyi hızlandırmaktadır. 
    Beyinde bu kılıfın olduğu nöronlar beyaz renkte gözüktüğü için bu bölgelere "beyaz madde (white matter)", miyelinsiz nöronlar 
    da gri bir biçimde gözüktüğü için bu nöronların bulunduğu bölgelere de "gri madde (gray matter)" denilmektedir. 

    Makine öğrenmesindeki yapay sinir ağları yukarıda da belirttiğimiz gibi insanın sinir sisteminden ilham alınarak tasarlanmıştır.
    Ancak artık bu ilham noktası önemini kaybetmiş gibidir. Bu nedenle pek çok uzman artık bu konuyu "yapay sinir ağları" 
    yerine yalnızca "sinir ağları" biçiminde ifade etmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağları yapay nöronların birbirlerine bağlanmasıyla oluşturulmaktadır. Bir nöronun çeşitli girdileri olabilir
    fakat yalnızca bir tane çıktısı vardır. Nöronun girdileri aslında veri kümesindeki satırları temsil etmektedir. Yani veri 
    kümesindeki satırlar nöronun girdileri olarak kullanılmaktadır. Nöronun girdilerini xi ile temsil edersek her girdi "ağırlık 
    (weight) değeri" denilen bir değerle çarpılır ve bu çarpımların toplamları elde edilir. Ağırlık değerlerini wi ile gösterirsek 
    bu xi değerleri onlara karşı gelen wi değerleriyle çarpılıp toplanmaktadır. Örneğin nöronun 5 tane girdisi olsun, bu 5 girdi 
    aşağıdaki gibi 5 ayrı ağırlık değerleriyle çarpılıp toplanacaktır:

    total = x1w1 + x2w2 + x3w3 + x4w4 + x5w5
  
    İki vektörün karşılıklı elemanlarının çarpımlarının toplamına İngilizce "dot product" denilmektedir. (Dot product işlemi 
    np.dot fonksiyonuyla yapılabilmektedir.) Elde edilen dot product değeri "bias" denilen bir değerle toplanır. Biz bias dieğerini 
    b ile temsil edeceğiz. Bu durumda nöronda elde edilen toplam şöyle olacaktır:

    total = x1w1 + x2w2 + x3w3 + x4w4 + x5w5 + b
    
    İşte elde edilen bu toplam da "aktivasyon fonsiyonu (activation function)" ya da "transfer fonksiyonu (transfer function)" 
    denilen bir fonksiyona sokulmaktadır. Böylece o nöronun çıktısı edilmektedir. Bu işlemleri aşağıdaki gibi ifade edebiliriz.

    out = activation(x1w1 + x2w2 + x3w3 + x4w4 + x5w5 + b)

    Dot product işlemini matrisel olarak XW biçiminde ifade edersek yukarıdaki eşitliği daha sade biçimde aşağıdaki gibi de 
    ifade edebiliriz:

    out = activation(XW + b)

    Dot product işlemini tek boyutlu iki matrisin çarpımı biçiminde de düşünebilirsiniz. Bu durumda örneğiX bir satır vektörü 
    W ise bir sütun vektörü olacaktır.

    Bir nöronun çıktısı başka nöronlara girdi yapılabilmektedir. Böylece nöronlar birbirine bağlanarak tüm ağdan nihai bir çıktı 
    elde edilir. 
 #----------------------------------------------------------------------------------------------------------------------------

 #----------------------------------------------------------------------------------------------------------------------------
    Şimdi ağımızda tek bir nöron değil K tane farklı nöronun bulunduğunu düşünelim. N tane girdinin de (yani Xi'lerin) bu nöronların 
    hepsine bağlandığını varsayalım. Her nöronun ağırlık değerleri ve bias değeri diğerlerinden farklıdır. Dolayısıyla K tane 
    nöronun çıktısı aşağıdaki gibi bir matrisle gösterilebilir:

    outs = activation(XW + b)

    Artık burada X 1XN boyutunda, W matrisi ise NxK boyutunda ve b matrisi de 1XK boyutundadır. Sonuç olarak buradan K tane çıktı 
    değeri elde edilecektir. Buradaki XW işleminin artık dot product belirtmediğine bir matris çarpımı belirttiğine dikkat ediniz. 
    Gösterimimizdeki X matrisi bir satır vektörü durumundadır:

    X = [x1, x2, x3, ..., xn]

    Bu matris Nx1 boyutundadır. W matrisi ise aşağıdaki görünümdedir:

    w11  w21 w31 ... wk1
    w12  w22 w31 ... wk2
    w13  w23 w33 ... wk3
    ...  ... ... ... ...
    w1n  w2n w3n ... wnk

    Bu matris de NxK boyutundadır. Buradaki X matrisi ile W matrisi matrisi matris çarpımına sokuldupunda 1XK boyutunda bir matris 
    elde edilecektir. Gösterimimizdeki b matrisi şöyle temsil edilebilir:

    b = [b1, b2, b2, ...., bk]
       
    Bu matrisin de 1XK boyutunda olduğuna dikkat ediniz. Böylece XW + b işleminden 1XK boyutunda bir matris elde edilecektir. Tabii 
    biz bu matris çarpımını WX + b biçiminde ters de oluşturabilirdik. Bu durumda W matrisi şöyle olacaktır:

    w11  w12 w13 ... w1n
    w21  w22 w23 ... w2n
    w31  w32 w33 ... w3n
    ...  ... ... ... ...
    wk1  wk2 wk3 ... wkn

    Söz konusu X matrisi de şöyle olacaktır:

    x1
    x2
    x3
    ...
    xk

    Tabii bu durumda b matrisi de şöyle olacaktır:

    b1 b2 b3 ... bk
   
    Artık burada W matrisinin artık KxN boyutunda olduğuna, X ve b matrislerinin de Kx1 boyutunda olduğuna dikkat ediniz. 
    Genellikle XW + b gösterimi yerine WX + b gösterimi tercih edilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir sinir ağının amacı "kestirimde" bulunmaktır. Yani biz ağa girdi olarak xi değerlerini veririz. Ağdan hedeflediğimiz
    çıktıyı elde etmeye çalışırız. Ancak bunu yapabilmemiz için nöronlardaki w değerlerinin ve b değerlerinin biliniyor olması 
    gerekir. Peki bu değerleri nasıl elde edebiliriz?
    
    Sinir ağları "denetimli (supervised)" bir öğrenme modeli sunmaktadır. Daha önceden de belirttiğimiz gibi denetimli öğrenmede 
    önce ağın mevcut verilerle eğitilmesi gerekmektedir. İşte bir sinir ağının eğitilmesi aslında nöronlardaki w değerlerinin 
    ve b değerlerinin uygun biçimde belirlenmesi anlamına gelmektedir. Başka bir deyişle önce biz ağımızı mevcut verilerle eğitip 
    bu w ve b değerlerinin uygun biçimde oluşturulmasını sağlarız. Ondan sonra kestirim yaparız. Tabii ağ ne kadar iyi eğitilirse 
    ağın yapacağı kestirim de o kadar isabetli olacaktır. 

    Örneğin biz bir dairenin fiyatını tahmin etmeye çalışalım. Bunun için  öncelikle veri toplamamız gerekir. Çeşitli daireler 
    için şu verilerin toplandığını varsayalım:

    - Dairenin büyüklüğü
    - Dairenin kaçıncı katta olduğu
    - Dairenin içinde bulunduğu binanın yaşı
    - Dairenin yaşam alanına uzaklığı
    - Dairenin ne kadar yakında metro durağı olduğu
    - Dairenin içinde bulunduğu binanın otopark miktarı
    - Apartman aidatı

    İşte elimizde veri kümesinin sütunlarını (özelliklerini) bu bilgiler oluşturmaktadır. Bizim çeşitli dairelerin bu bilgilerini 
    elde etmiş olmamız ve onların satış fiyatlarını da biliyor olmamız gerekir. Yani bizim işin başında sinir ağına girdi 
    yapacağımız bilgilerle ağın vermesi gereken gerçek çıktılardan oluşan bir veri kümesine gereksinimimiz vardır. İşte ağın 
    eğitimi için bu veri kümesini kullanırız. Ağımızı bu gerçek verilerle eğittikten sonra artık nöronlardaki w ve b değerleri 
    konumlandırılmış olacaktır. Biz de kestirim yapmak istediğimizde ağımıza girdi olarak yukarıdaki bilgilere sahip bir 
    dairenin özelliklerini veririz. Ağımız da bize çıktı olarak o dairenin olması gereken fiyatını verir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda bir sinir ağının temel çalışma mekanizmasını açıkladık. Ancak bir sinir ağını oluşturabilmek için bu ağın 
    bileşenleri hakkında daha fazla bilgi sahibi olmamız gerekir. Ağın bileşenleri hakkında aklımıza gelen tipik sorular şunlar
    olnmaktadır:
        
    - Ağın nöron katmanlarının sayısı ne olmalıdır?
    - Katmalardaki nöronların sayıları ve katman bağlantıları nasıl olmalıdır?
    - Nöronlarda kullanılan aktivasyon fonksiyonları nasıl olmalıdır?
    - Ağdaki w ve b değerlerini oluşturmak için kullanılan optimizasyon algoritmaları nelerdir ve nasıl çalışmaktadır?
        
    Biz de bir süreç içerisinde bu sorulara yanıtlar vereceğiz. Ancak bir noktaya dikatinizi çekmek istiyoruz: Sinir ağlarında
    önemli bir işlem yükü vardır. Buradaki işlemlerin programcılar tarafından manuel bir biçimde her defasında yeniden (add-hoc)
    yapılması çok zahmetlidir. İşte zamanla bu sinir ağı işlemlerini kendi içlerinde yapan kütüphaneler geliştirilmiştir. 
    Bugün artık uygulamacılar bu işlemleri programlar yazarak değil zaten bu konuda çözüm üreten hazır kütüphaneleri kullanarak
    yapmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistikte girdi değerlerinden hareketle çıktı değerinin belirlenmesine (tahmin edilmesine) yönelik süreçlere "regresyon
    (regression)" denilmektedir. Regresyon modelleri çok çeşitli biçimlerde sınıflandırılabilmektedir. Çıktıya göre regresyon 
    modelleri tipik olarak ikiye ayrılmaktadır:

    1) Sınıflandırma Amacıyla Oluşturulan Regresyon Modelleri
    2) Gerçek Bir Değer Etmek Amacıyla Oluşturulan Regresyon Modelleri

    Aslında istatistikte "regresyon" denildiğinde zaten "gerçek değer elde etmek amacıyla oluşturulan regresyon modelleri" 
    anlaşılmaktadır. Bu tür regresyonlarda girdilerden hareketle kategorik bir değer değil sayısal bir değer elde edilmektedir. 
    Örneğin bir dairenin yukarıda belirttiğimiz özellikleri girdiler olabilir, dairenin fiyatı da çıktı olabilir. Burada çıktı 
    sürekli sayısal bir değerdir. Çıktının bir sınıf biçiminde elde edildiği resgresyon modellerinde çıktı gerçek bir değer 
    değil sınıf ya da kategori belirten ayrık bir değerdir. Örneğin girdiler bir resmin pixel'leri olabilir. Çıktı da bu resmin 
    elma mı, armut mu, kayısı mı olduğuna yönelik kategorik bir bilgi olabilir. Bu tür regresyonlara istatistikte "lojistik 
    regresyonlar" ya da "logit regresyonları" denilmektedir. (Lojistik regresyondaki "lojistik" sözcüğünün günlük hayatta çokça 
    karşılaştığımız "lojistik hizmetlerdeki" "lojistik" sözcüğü ile bir ilgisi yoktur. Buradaki "lojistik" sözcüğü matematikteki 
    "logaritma" sözcüğünün kısaltmasından gelmektedir.) Makine öğrenmesinde lojistik regresyon terimi yerine genellikle "sınıflandırma
    (classification)" terimi kullanılmaktadır. Biz kursumuzda makine öğrenmesi bağlamında "sınıflandırma (classification)" terimini 
    kullanacağız. Kursumuzda sınıfsal çıktı vermeyen, sayısal çıktı veren regresyonlar için ise yalnızca "regresyon" teriminini 
    kullanacağız. 

    Regresyon modeli ister sınıflandırma amacıyla ister sayısal bir değer elde etme amacıyla oluşturulsun regresyon işlemlerinin 
    hepsi aslında girdiyi çıktıya dönüştüren bir f foonksiyonunun elde edilmesi sürecidir. Örneğin:

    y = f(x1, x2, ..., xn)

    İşte makine öğrenmesinde bu f fonksiyonunun elde edilmesinin çeşitli yöntemleri vardır. Yapay sinir ağlarında da aslında
    bu biçimde bir f fonksiyonu bulunmaya çalışılmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sınıflandırma sürecindeki çoktının olabileceği değerlere "sınıf (class)" denilmektedir.  Sınıflandırma problemlerinde eğer 
    çıktı ancak iki değerden biri olabiliyorsa bu tür sınıflandırma problemlerine "iki sınıflı sınıflandırma (binary classification)" 
    problemleri denilmektedir. Örneğin bir film hakkında yazılan yorum yazısının "olumlu" ya da "olumsuz" biçiminde iki değerden 
    oluştuğunu düşünelim. Buradaki sınıflandırma işlemi ikili sınıflandırma işlemidir. Benzer biçimde bir biyomedikal görüntüdeki 
    kitlenin "iyi huylu (benign)" mu "kötü huylu (malign)" mu olduğuna yönelik sınıflandırma da ikili sınıflandırmaya bir örnektir. 
    Eğer sınıflandırmada çıktı sınıflarının sayısı ikiden fazla ise böyle sınıflandırma problemlerine "çok sınıflı (multiclass)" 
    sınıflandırma problemleri denilmektedir. Örneğin bir resmin hangi meyveye ilişkin olduğunun tespit edilmesi için kullanılan 
    sınıflandırma modeli "çok sınıflı" bir modeldir. 

    Yukarıda da belirttiğimiz gibi istatistikte "lojistik regresyon" denildiğinde aslında default olarak "iki sınıflı (binary)" 
    lojistik regresyon anlaşılmaktadır. Çok sınıflı lojistik regresyonlara istatistikte genellikle İngilizce "multiclass logistic 
    regression" ya da "multinomial logistic regression" denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                24. Ders - 17/03/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    En basit yapay sinir ağı mimarisi tek bir nörondan oluşan mimaridir. Buna "perceptron" denilmektedir. Perceptron'da bir
    tane nöron vardır. N tane girdi bu nörondaki w değerleriylye dot product yapılıp bias değeri ile toplanarak aktivasyon 
    fonksiyonuna sokulur. Bu durumu şekilsel olarak şöyle gösterebiliriz:

            x1       x2      x3      ...       xn
            │        │       │                  │
            ▼        ▼       ▼                  ▼
         [×1w1]   [×2w2]   [×3w3]    ...     [×nwn]      ← Ağırlıklarla çarpma
             \      |       |        ...       /
              \     |       |     .  ..      /
               ▼    ▼       ▼       ...     ▼
                    [ TOPLAM ]                            ← z = x1w1 + x2w2 + ... + xn*wn 
                        │
                      +[b]                               ← Bias eklenir
                        │
                        ▼
                  [ f(z) = ŷ ]                           ← Aktivasyon fonksiyonu (örnek: adım, sigmoid, vs.)
                        │
                        ▼
                        ŷ                                ← Preceptron'unb ürettiği değer

    Yalnızca tek nörondan oluşan perceptron ile "doğrusal olarak ayrıştırılabilen (linearly separable)" ikili sınıflandırma 
    problemleri ya da yalın çoklu regresyon problemleri çözülebilmektedir. Her ne kadar tek bir nöron bile bazı problemleri 
    çözebiliyorsa da problemler karmaşıklaştıkça ağdaki nöron sayılarının ve katmanların artırılması gerekmektedir.  

    Aşağıda bir nöronun bir sınıfla temsil edilmesine ilişkin bir örnek veriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

class Neuron:
    def __init__(self, w, b):
        self.w= w
        self.b = b
        
    def output(self, x, activation):
        return activation(np.dot(x, self.w) + self.b)

def sigmoid(x):
    return np.e ** x / (1 + np.e ** x)
        
w = np.array([1, 2, 3])
b = 1.2

n = Neuron(w, b)

x = np.array([1, 3, 4])
result = n.output(x, sigmoid)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    Bir yapay sinir ağı modelin aslında tek bir nörondan değil çok sayıda nöronun bulunduğu katmanlardan (layers) oluşmaktadır. 
    Katman aynı düzeydeki nöron grubu için kullanılan bir terimdir. Yapay sinir ağlarında katmanlar tipik olarak üçe ayırmaktadır:

    1) Girdi Katmanı (Input Layer)
    2) Saklı Katmanlar (Hidden Layers)
    3) Çıktı Katmanı (Output Layer)

    Girdi katmanı veri kümesindeki satırları temsil eden yani ağa uygulanacak verileri belirten katmandır. Aslında girdi katmanı 
    gerçek anlamda nöronlardan oluşmaz. Ancak anlatımları kolaylaştırmak için bu katmanın da nöronlardan oluştuğu varsayılmaktadır. 
    Başka bir deyişle girdi katmanının tek bir nöron girişi ve tek bir çıktısı vardır. Bunların w değerleri 1'dir, aktivasyon 
    fonksiyonları ise f(x) = x biçimindedir. Yani girdi katmanı bir şey yapmaz, girdiyi değiştirmeden çıktıya verir. Girdi 
    katmanındaki nöron sayısı veri kümesindeki sütunların (yani özelliklerin) sayısı kadar olmalıdır. Örneğin 5 tane girdiye 
    (özelliğe) sahip olan bir veri kümesine ilişkin sinir ağının girdi katmanı şöyle edilebilir:

    x1 ---> O --->
    x2 ---> O --->
    x3 ---> O --->
    x4 ---> O --->
    x5 ---> O --->

    Buradaki O sembolleri girdi katmanındaki nöronları temsil etmektedir. Girdi katmanındaki nöronların 1 tane girdisinin 
    1 tane  de çıktısının olduğuna dikkat ediniz. Buradaki nöronlar girdiyi değiştirmediğine göre bunların w değerleri 1, 
    b değerleri 0, aktivasyon fonksiyonu da f(x) = x biçiminde olmalıdır. 
    
    Girdiler (yani girdi katmanının çıktıları) saklı katman denilen katmanlardaki nöronlara bağlanırlar. Modelde sıfır tane, 
    bir tane ya da birden fazla saklı katman bulunabilir. Saklı katmanların sayısı ve saklı katmanlardaki nöronların sayısı 
    ve bağlantı biçimleri problemin niteliğine göre değişebilmektedir. Yani saklı katmanlardcaki nöronların girdi katmanıyla
     aynı sayıda olması gerekmez. Her saklı katmandaki nöron sayıları aynı olmak zorunda da değildir. 

    Çıktı katmanı bizim sonucu alacağımız katmandır. Çıktı katmanındaki nöron sayısı bizim kestirmeye çalıştığımız olgularla 
    ilgilidir. Örneğin biz bir evin fiyatını kestirmeye çalışıyorsak çıktı katmanında tek bir nöron bulunur. Yine örneğin biz 
    ikili sınıflandırma problemi üzerinde çalışıyorsak çıktı katmanı yine tek bir nörondan oluşabilir. Ancak biz evin fiyatının 
    yanı sıra evin sağlamlığını da kestirmek istiyorsak bu durumda çıktı katmanında iki nöron olacaktır. Benzer biçimde çok 
    sınıflı sınıflandırma problemlerinde genel olarak çıktı katmanlarında sınıf sayısı kadar nöron bulunur.
   
    Bir yapay sinir ağındaki katmanların sayısı belirtilirken bazıları girdi katmanını bu sayıya dahil ederken, bazıları 
    etmemektedir. Bu nedenle katman sayılarını konuşurken yalnızca saklı katmanları belirtmek bu bakımdan iki anlamlılığı 
    giderebilmektedir. Örneğin biz "modelimizde 5 katman var" dediğimizde birisi bu 5 katmanın içerisinde girdi katmanı dahil 
    mi diye tereddütte kalabilir. O halde iki anlamlılığı ortadan kaldırmak için "modelimizde 3 saklı katman var" gibi bir 
    ifade daha uygun olacaktır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir yapay sinir ağı modelinde katman sayısının artırılması daha iyi bir sonucun elde edileceği anlamına gelmez. Benzer 
    biçimde katmanlardaki nöron sayılarının artırılması da daha iyi bir sonucun elde edileceği anlamına gelmemektedir. Katmanların 
    sayısından ziyade onların işlevleri daha önemli olmaktadır. Ağa gereksiz katman eklemek, katmanlardaki nöronları artırmak 
    tam ters bir biçimde ağın kestirim başarısının düşmesine de yol açabilmektedir. Yani gerekmediği halde ağa saklı katman 
    eklemek, katmanlardaki nöron sayılarını artırmak bir fayda sağlamamakta tersine kestirim başarısını düşürebilmektedir. Ancak 
    görüntü tanıma gibi yazı anlamlandırma gibi özel ve zor problemlerde saklı katman sayılarının artırılması gerekebilmektedir. 

    Peki bir sinir ağı modelinde kaç tane saklı katman olmalıdır? Pratik olarak şunları söyleyebiliriz:

    - Sıfır tane saklı katmana sahip tek bir nörondan oluşan en basit modele "perceptron" dendiğini belirtmiştir. Bu perceptron 
    "doğrusal olarak ayrıştırılabilen (linearly separable)" sınıflandırma problemlerini ve yalın doğrusal regresyon problemlerini 
    çözebilmektedir. Doğrusal olarak ayrıştırılabilirlik kursumuzda başka bir bölümde ele alınmaktadır.

    - Tek saklı katmanlı modeller aslında pek çok sınıflandırma problemini ve regresyon problemini belli bir yeterlilikte 
    çözebilmektedir. Ancak tek saklı katman yine de bu tarz bazı problemler için yetersiz kalabilmektedir. 

    - İki saklı katman pek çok karmaşık olmayan sınıflandırma problemi için ve regresyon problemi için iyi bir modeldir. Bu 
    nedenle karmaşık olmayan problemler için ilk akla gelecek model iki saklı katmanlı modeldir. 

    - İkiden fazla saklı katmana sahip olan modeller karmaşık ve özel problemleri çözmek için kullanılmaktadır. İki saklı 
    katmandan fazla katmana sahip olan sinir ağlarına genel olarak "derin öğrenme ağları (deep learning networks)" denilmektedir. 

    Yukarıda da belirttiğimiz gibi "derin öğrenme (deep learning)" farklı bir yöntemi belirtmemektedir. Derin öğrenme özel ve
    karmaşık problemleri çözebilmek için ikiden fazla saklı katman içeren sinir ağı modellerini belirtmek için kullanılan bir 
    terimdir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağlarında çalışmak için "alçak seviyeli" ve "yüksek seviyeli" kütüpaheneler ve framework'ler bulunmaktadır. 
    Aşağı seviyeli üç önemli framework şunlardır: TensorFlow, PyTotch ve Theno. Yüksek seviyeli kütüphane olarak en fazla Keras 
    tercih edilmektedir. Keras eskiden bağımsız bir kütüphaneydi ve kendi içinde TensorFlow, Theno, Microsoft Coginitive Toolkit, 
    PlaidML gibi  kütüphaneleri "backend" olarak kullanbiliyordu. Ancak daha sonraları Keras TensorFlow bünyesine katılmıştır. 
    Dolayısıyla artık 2.3 ile birlikte Keras TensorFlow kütüphanesinin bir parçası haline gelmiştir. PyTorch kütüpahnesinin de 
    yüksek seviyeli bir katmanı bulunmaktadır. iz kursumuzda önce Keras üzerinde ilerleyeceğiz. Ancak sonra diğer aşağı seviyeli 
    kütüphaneleri de gözden geçireceğiz. Keras'la çalışmak için TensorFlow kütüphanesinin kurulması gerekmektedir. Kurulum 
    şöyle yapılabilir:

    pip install tensorflow

    TensorFlow çok thread'li ve paralel işlem yapan bir kütüphanedir. Paralel işlemler sırasında grafik kartınının olanaklarını 
    da kullanmaktadır. Eğer Windows sistemlerinde NVidia kartlarını kullanıyorsanız TensorFlow kütüphanesinin performansını 
    artırabilmek için Cuda isimli kütüphaneyi ayrıca yüklemelisiniz. Yükleme aşağıdaki bağlantıdan yapılabilir:

    https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_local
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir veri kümesini CSV dosyasından okuduktan sonra onu Keras'ın kullanımına hazırlamak için bazı işlemlerin yapılması gerekir. 
    Yapılması gereken ilk işlem veri kümedinin dataset_x ve dataset_y biçiminde iki parçaya ayrılmasıdır. Çünkü ağın eğitilmesi
    sırasında girdilerle çıktıların ayrıştırılması gerekmektedir. Burada dataset_x girdileri dataset_y ise kestirilecek çıktıları 
    belirtmektedir. 

    Eğitim bittikten sonra genellikle ağın kestirimine hangi ölçüde güvenileceğini belirleyebilmek için bir test işlemi yapılır. 
    Test işleminin eğitimde kullanılmamış olan bir test veri kümesi ile yapılması gerekir. (Örneğin bir sınavda yalnızca sınıfta 
    çözülen sorular sorulursa test işleminin gücü düşecektir.) O halde bizim ana veri kümesini "eğitim veri kümesi" ve "test veri 
    kümesi" biçiminde iki kısma ayırmamız gerekir. Oranlar çeşitli koşullara göre değişebilirse de tipik olarak %80'lik verinin 
    eğitim için %20'lik verinin test için kullanılması tercih edilmektedir. 

    Eğitim ve test veri kümesini manuel olarak ayırabiliriz. Ancak ayırma işleminden önce veri kümesini satırsal bakımdan 
    karıştırmak uygun olur. Çünkü bazı veri kümeleri CSV dosyasına karışık bir biçimde değil yanlı bir biçimde kodlanmış 
    olabilmektedir. Örneğin bazı veri kümeleri bazı alanlara göre sıraya dizilmiş bir biçimde bulunabilmektedir. Biz onun
    baştaki belli kısmını eğitim, sondaki belli kısmını test veri kümesi olarak kullanırsak eğitim ve test veri kümeleri 
    yanlı hale gelebilmektedir.

    Biz programlarımızda genellikle bir veri kümesinin tamamı için "dataset" ismini, onun girdi kısmı için "dataset_x" ismini, 
    çıktı kısmı için "dataset_y" ismini kullanacağız. Veri kümesini eğitim ve test olarak ayırdığımızda da isimleri şu biçimde 
    vereceğiz: "training_dataset_x", "training_dataset_y", "test_dataset_x", "test_dataset_y".

    Aşağıda bir kişinin çeşitli biyomedikal bilgilerinden hareketle onun şeker hastası olup olmadığını anlamaya yönelik bir 
    veri kümesinin manuel ayrıştırılması örneği verilmiştir. Veri kümesini aşağıdaki bağlantıdan indirebilirsiniz:

    https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database

    Veri kümesinin görünümü şöyledir:

    Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome
    6,148,72,35,0,33.6,0.627,50,1
    1,85,66,29,0,26.6,0.351,31,0
    8,183,64,0,0,23.3,0.672,32,1
    1,89,66,23,94,28.1,0.167,21,0
    0,137,40,35,168,43.1,2.288,33,1
    5,116,74,0,0,25.6,0.201,30,0
        
    Bu kümesinde bazı sütunlar eksik veri içermektedir. Bu eksik verile NaN biçiminde değil 0 biçiminde kodlanmıştır. 
    Biz bu eksik verileri ortalama değerle doldurabiliriz. Eksik veri içeren sütunlar şunlardır:

    Glucose
    BloodPressure
    SkinThickness
    Insulin
    BMI
#----------------------------------------------------------------------------------------------------------------------------

TRAINING_RATIO = 0.80

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

import numpy as np

np.random.shuffle(dataset)

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

training_len = int(np.round(len(dataset_x) * TRAINING_RATIO))

training_dataset_x = dataset_x[:training_len]
test_dataset_x = dataset_x[training_len:]

training_dataset_y = dataset_y[:training_len]
test_dataset_y = dataset_y[training_len:]

#----------------------------------------------------------------------------------------------------------------------------
                                            25. Ders - 23/03/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Veri kümesini eğitim ve test olarak ayırma işlemi için sklearn.model_selection modülündeki train_test_split isimli fonksiyon 
    sıkça kullanılmaktadır. Biz de programlarımızda çoğu zaman bu fonksiyonu kullanacağız. Fonksiyon NumPy dizilerini ya da 
    Pandas DataFrame ve Series nesnelerini ayırabilmektedir. Fonksiyon bizden dataset_x ve dataset_y değerlerini ayrı ayrı ister. 
    test_size ya da train_size parametreleri 0 ile 1 arasında test ya da eğitim verilerinin oranını belirlemek için kullanılmaktadır. 
    train_test_split fonksiyonu bize 4'lü bir liste vermektedir. Listenin elemanları sırasıyla şunlardır: training_dataset_x, 
    test_dataset_x, training_dataset_y, test_dataset_y. Örneğin:

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)
    
    Aslında fonksiyonun train_size ve test_size parametreleri float yerine int olarak da girilebilir. Bu parametreler için int
    türden değer verilirse bu int değer eğitim ve test veri kümesinin sayısını belirtmektedir. Ancak genellikle uygulamacılar 
    bu parametreler için oransal değerler vermeyi tercih etmektedir. 

    train_test_split fonksiyonu veri kümesini bölmeden önce karıştırma işlemini de yapmaktadır. Fonksiyonun shuffle parametresi 
    False yapılırsa fonksiyon karıştırma yapmadan bölme işlemini yapar.

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

    Burada fonksiyona dataset_x ve dataset_y girdi olarak verilmiştir. Fonksiyon bunları bölerek dörtlü bir listeye geri dönmüştür.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

#----------------------------------------------------------------------------------------------------------------------------
    train_test_split fonksiyonu ile biz doğrudan Pandas DataFrame ve Series nesneleri üzerinde de bölme işlemini yapabiliriz. 
    Bu durumda fonksiyon bize dörtlü listeyi DataFrame ve Series nesneleri biçiminde verecektir. Aşağıda bu biçimde bölmeye 
    bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

dataset_x = df.iloc[:, :-1]
dataset_y = df.iloc[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında train_test_split fonksiyonuna parametre olarak bir ya da birden fazla veri kümesi girilebilmektedir. Fonksiyon 
    bunların hepsini karşılıklı oalrak aynı satırlardan eğitim ve test veri kümesi biçiminde ikiye ayırmaktadır. Biz fonksiyona 
    k tane parametre girdiğimize fonksiyon bize k * 2 elemanlık bir liste vermektedir. Örneğin:

    splitted_datasets = train_test_split(dataset_x, dataset_y, dataset_z, test_size=0.2)

    Burada splitted_datasets listesi toplam  6 elemana sahip olacaktır. Bu 6 elemanın ilk ikisi dataset_x'in bölünmüş halini,
    sonraki ikisi dataset_y'nin bölünmüş halini, sonraki ikisi ise dataset_z'nin bölünmüş halini içerecektir. Biz train_test_split
    fonksiyonuna tek bir veri kümesini de verebiliriz. Bu durumda fonksiyon bunu da eğitim ve test olmak üzere iki parçaya ayıracaktır.
    Örneğin:

    training_df, test_df = train_test_split(df, test_size=0.2)

    Burada fonksiyon iki elemanlı bir listeye geri dönmüştür. Biz de onu açarak iki değişkene atamış atadık.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Keras'ta bir sinir ağı oluşturmanın çeşitli adımları vardır. Burada sırasıyla bu adımlardan ve adımlarla ilgili bazı 
    olgulardan bahsedeceğiz.

    1) Öncelikle bir model nesnesi oluşturulmalıdır. tensorflow.keras modülü içerisinde çeşitli model sınıfları bulunmaktadır. 
    En çok kullanılan model sınıfı Sequential isimli sınıftır. Tüm model sınıfları Model isimli sınıftan türetilmiştir. Sequential 
    modelde ağa her eklenen katman sona eklenir. Böylece ağ katmanların sırasıyla eklenmesiyle oluşturulur. Sequential nesnesi 
    yaratılırken name parametresiyle modele bir isim de verilebilir. Örneğin:

    from tensorflow.keras import Sequential

    model = Sequential(name='Sample')

    Aslında Sequential nesnesi yaratılırken katmanlar belirlendiyse layers parametresiyle bu katmanlar da verilebilmektedir. 
    Ancak sınıfın tipik kullanımında katmanlar daha sonra izleyen maddelerde ele alınacağı gibi sırasıyla eklenmektedir.

    2) Model nesnesinin yaratılmasından sonra katman nesnelerinin oluşturulup model nesnesine eklenmesi gerekir. Keras'ta farklı 
    gereksinimler için farklı katman sınıfları bulundurulmuştur. En çok kullanılan katman sınıfı tensorflow.keras.layers modülündeki 
    Dense sınıfıdır. Dense bir katman modele eklendiğinde önceki katmandaki tüm nöronların çıktıları eklenen katmandaki nöronların 
    hepsine girdi yapılmaktadır. Bu durumda örneğin önceki katmanda k tane nöron varsa biz de modele n tane nörondan oluşan bir 
    Dense katman ekliyorsak bu durumda modele k * n + n tane yeni parametre (yani tahmin edilmesi gereken parametre) eklemiş oluruz. 
    Burada k * n tane ayarlanması gereken w (ağırlık) değerleri ve n tane de ayarlanması gereken bias değerleri söz konusudur. 
    Bir nörondaki w (ağırlık) değerlerinin o nörona giren nöron sayısı kadar olduğuna ve bias değerlerinin her nöron için bir 
    tane olduğuna dikkat ediniz.

    Dense sınıfının __init__ metodunun ilk parametresi eklenecek katmandaki nöron sayısını belirtir. İkinci parametre olan 
    activation parametresi o katmandaki tüm nöronların aktivasyon fonksiyonlarının ne olacağını belirtmektedir. Bu parametreye 
    aktivasyon fonksiyonları birer yazı biçiminde isimsel olarak girilebilir. Ya da tensorflow.keras.activations modülündeki 
    fonksiyonlar olarak girilebilir. Örneğin:

    from tensorflow.keras.layers import Dense

    layer = Dense(100, activation='relu')

    ya da örneğin:

    from tensorflow.keras.activations import relu

    layer = Dense(100, activation=relu)

    Dense fonksiyonun use_bias parametresi default durumda True biçimdedir. Bu parametre katmandaki nöronlarda "bias" değerinin 
    kullanılıp kullanılmayacağını belirtmektedir. Metodun kernel_initializer parametresi katmandaki nöronlarda kullanılan w 
    parametrelerinin ilkdeğerlerinin rastgele biçimde hangi algoritmayla oluşturulacağını belirtmektedir. Bu parametrenin default
    değeri "glorot_unfiorm" biçimindedir. Metodun bias_initializer parametresi ise katmandaki nöronların "bias" değerlerinin 
    başlangıçta nasıl alınacağını belirtmektedir. Bu parametrenin default değeri de "zero" biçimdedir. Yani bias değerleri 
    başlangıçta 0 durumundadır. Dense sınıfının __init__ metodunun diğer parametreleri hakkında bu aşamada bilgi vermeyeceğiz. 

    Keras'ta Sequential modelde girdi katmanı programcı tarafından yaratılmaz. İlk saklı katman yaratılırken girdi katmanındaki 
    nöron sayısı input_dim parametresiyle ya da input_shape parametresiyle belirtilmektedir. input_dim tek boyutlu girdiler 
    için input_shape ise çok boyutlu girdiler için kullanılmaktadır. Örneğin:

    layer = Dense(100, activation='relu', input_dim=8)

    Tabii input_dim ya da input_shape parametrelerini yalnızca ilk saklı katmanda kullanabiliriz. Genel olarak ağın girdi katmanında
    dataset_x'tekü sütun sayısı kadar nöron olacağına göre ilk katmandaki input_dim parametresini aşağıdaki gibi de girebiliriz:

    layer = Dense(100, activation='relu', input_dim=training_dataset_x.shape[1])
        
    Aslında Keras'ta girdi katmanı için tensorflow.keras.layers modülünde Input isminde bir katman da kullanılmaktadır. Tenseoflow'un
    yeni versiyonlarında girdi katmanının Input katmanı ile oluşturulması istenmektedir. Aksi takdirde bu yeni versiyonlar uyarı 
    vermektedir. Girdi katmanını Input isimli katman sınıfıyla oluştururken bu Input sınıfının __init__ metodunun birinci parametresi
    bir demet biçiminde (yani sahpe olarak) girilmelidir. Örneğin:

    input = Input((8, ))

    Burada 8 nöronşuk bir girdi katmanı oluşturulmuştur. Yukarıda da belirttiğimiz gibi eskiden ilk saklı katmanda girdi katmanı 
    belirtiliyordu. Ancak TensorFlow kütüphanesinin yeni verisyonlarında ilk saklı katmanda girdi katmanının belirtilmesi artık 
    uyarıya (warning) yol açmaktadır. Önceki kurslarda biz girdi katmanını ilk saklı katmanda belirtiyorduk. Ancak artık bu 
    kursumuzda girdi katmanını ayrı bir Input nesnesi ile oluşturacağız. 

    Her katmana istersek name parametresi ile bir isim de verebiliriz. Bu isimler model özeti alınırken ya da katmanlara erişilirken 
    kullanılabilmektedir. Örneğin:

    layer = Dense(100, activation='relu',  name='Hidden-1')

    Oluşturulan katman nesnesinin model nesnesine eklenmesi için Sequential sınıfının add metodu kullanılmaktadır. Örneğin:

    input = Input((8, ))
    model.add(input)
    layer = Dense(100, activation='relu',  name='Hidden-1')
    model.add(layer)

    Programcılar genellikle katman nesnesinin yaratılması ve eklenmesini tek satırda aşağıdaki gibi yaparlar:

    model.add(Dense(100, activation='relu', input_dim=9, name='Hidden-1'))

    3) Modele katmanlar eklendikten sonra bir özet bilgi yazdırılabilir. Bu işlem Sequential sınıfının summary isimli metoduyla 
    yapılmaktadır.

    Yukarıda da belirttiğimiz gibi bir katmandaki "eğitilebilir (trainable)" parametrelerin sayısı şöyle hesaplanmaktadır: 
    Önceki katmanın çıktısındaki nöron sayısı, bu katmana dense biçimde bağlandığına göre toplamda "önceki katmandaki nöron 
    sayısı * bu katmandaki nöron sayısı" kadar w parametresi ayarlanacaktır. Öte yandan bu katmandaki nöronların her birinde 
    bir "bias" değeri olduğuna göre bu değerler de bu sayıya toplanmalıdır. O zaman önceki katmandaki nöron sayısı k, bu 
    katmandaki nöron  sayısı n ise eğitilebilir parametrelerin sayısı k * n + n tane olur. Aşağıdaki modeli inceleyiniz:

    model = Sequential(name='Diabetes')

    model.add(Input((training_dataset_x.shape[1],)))
    model.add(Dense(16, activation='relu', name='Hidden-1'))
    model.add(Dense(16, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    B modelde bir girdi katmanı, iki saklı katman (biz bunlara ara katman da diyeceğiz) bir de çıktı katmanı vardır. 
    summary metodundan elde edilen çıktı şöyledir.

    Model: "Diabetes"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 16)             │           144 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 16)             │           272 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │            17 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 433 (1.69 KB)
    Trainable params: 433 (1.69 KB)
    Non-trainable params: 0 (0.00 B)
    Trainable params: 433 (1.69 KB)
    Non-trainable params: 0 (0.00 B)

    Burada ağımızdaki girdi katmanında 8 nöron olduğuna göre ve ilk saklı katmanda da 16 nöron olduğuna göre ilk saklı katmana
    8 * 16 nöron girmektedir. Öte yandan her nöronun bir tane bias değeri de olduğuna göre ilk katmandaki tahmin ayarlanması 
    gereken parametrelerin (trainable parameters) sayısı 8 * 16 + 16 = 144 tanedir. İkinci saklı katmana 16 nöron dense biçimde
    bağlanmıştır. O halde ikinci saklı katmandaki ayarlanması gereken parametreler toplamda 16 * 16 + 16 = 272 tanedir. Modelimizin
    çıktı katmanında 1 nöron vardır. Önceki katmanın 16 çıkışı olduğuna göre bu çıktı katmanında 16 * 1 + 1 = 17 tane ayarlanması
    gereken parametre vardır. 

    Ağın saklı katmanlarında en çok kullanılan aktivasyon fonksiyonu "relu" isimli fonksiyondur. İkili sınıflandırma problemlerinde 
    çıktı katmanı tek nörondan oluşur ve bu katmandaki aktivasyon fonksiyonu "sigmoid" fonksiyonu olur. Sigmoid fonksiyonu 0 
    ile 1 arasında bir değer vermektedir. Biz aktivasyon fonksiyonlarını izleyen paragraflarda ele alacağız.

    Aşağıdaki örnekte "dibates" veri kümesi üzerinde ikili sınıflandırma problemi için bir sinir ağı oluşturulmuştur. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

#----------------------------------------------------------------------------------------------------------------------------
                                            26. Ders - 24/03/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    4) Model oluşturulduktan sonra modelin derlenmesi (compile edilmesi) gerekir. Buradaki "derleme" makine diline dönüştürme 
    anlamında bir terim değildir. Eğitim için bazı belirlemelerin yapılması anlamına gelmektedir. Bu işlem Sequential sınıfının 
    compile isimli metoduyla yapılmaktadır. Modelin compile metoduyla derlenmesi sırasında en önemli iki parametre "loss fonksiyonu" 
    ve "optimizasyon algoritması"dır. Eğitim sırasında ağın ürettiği değerlerin gerçek değerlere yaklaştırılması için w ve bias 
    değerlerinin nasıl güncelleneceğine ilişkin algoritmalara "optimizasyon algoritmaları" denilmektedir. Matematiksel optimizasyon 
    işlemlerinde belli bir fonksiyonun minimize edilmesi istenir. İşte minimize edilecek bu fonksiyona da "loss fonksiyonu" 
    denilmektedir. Başka bir deyişle optimizasyon algoritması loss fonksiyonun değerini minimize edecek biçimde işlem yapan 
    algoritmadır. Yani optimizasyon algoritması loss fonksiyonunu minimize etmek için yapılan işlemleri temsil etmektedir. 
    
    Loss fonksiyonları ağın ürettiği değerlerle gerçek değerler arasındaki farklılığı temsil eden fonksiyonlardır. Loss fonksiyonları 
    genel olarak iki girdi alıp bir çıktı vermektedir. Loss fonksiyonunun girdileri gerçek değerler ile ağın ürettiği değerlerdir. 
    Çıktı değeri ise aradaki farklığı belirten bir değerdir. Eğitim sırasında gitgide loss fonksiyonun değerinin düşmesini bekleriz. 
    Tabii loss değerinin düşmesi aslında ağın gerçek değerlere daha yakın değerler üretmesi anlamına gelmektedir. loss fonksiyonları 
    çıktının biçimine yani problemin türüne bağlı olarak seçilmektedir. Örneğin ikili sınıflandırma problemleri için "binary 
    cross-entropy", çoklu sınıflandırma problemleri için "categorical cross-entropy", regresyon problemleri için "mean squared 
    error" isimli loss fonksiyonları tercih edilmektedir. 
    
    Optimizasyon algoritmaları aslında genel yapı olarak birbirlerine benzemektedir. Pek çok problemde bu algoritmaların çoğu 
    benzer performans göstermektedir. En çok kullanılan optimizasyon algoritmaları "rmsprop", "adam" ve "sgd" algoritmalarıdır. 
    Bu algoritmalar "gradient descent" denilen genel optimizasyon yöntemini farklı biçimlerde uygulamaktadır. Optimizasyon 
    algoritmaları kursumuzun ilerleyen bölümlerinde ele alınacaktır.

    compile metodunda optimizasyon algoritması bir yazı olarak ya da tensorflow.keras.optimizers modülündeki sınıflar türünden 
    bir sınıf nesnesi olarak girilebilmektedir. Örneğin:

    model.compile(optimizer='rmsprop', ...)

    Örneğin:

    from tensorflow.keras.optimizers import RMSprop

    rmsprop = RMSprop()

    model.compile(optimizer=rmsprop, ...)

    Tabii optimizer parametresinin bir sınıf nesnesi olarak girilmesi daha ayrıntılı belirlemelerin yapılmasına olanak sağlamaktadır.
    Optimizasyon işlemlerinde bazı parametrik değerler vardır. Bunlara makine öğrenmesinde "üst düzey parametreler (hyper parameters)"
    denilmektedir. İşte optimizasyon algoritması bir sınıf nesnesi biçiminde verilirse bu sınıfın __init__ metodunda biz bu üst 
    düzey parametreleri istediğimiz gibi belirleyebiliriz. Eğer optimizasyon algoritması yazısal biçimde verilirse bu üst düzey 
    parametreler default değerlerle kullanılmaktadır. optimzer parametresinin default değeri "rmsprop" biçimindedir. Yani biz bu 
    parametre için değer girmezsek default optimazyon algoritması "rmsprop" olarak alınacaktır. 

    loss fonksiyonu compile metoduna yine isimsel olarak ya da tensorflow.keras.losses modülündeki sınıflar türünden sınıf nesleri 
    biçiminde ya da doğrudan fonksiyon olarak girilebilmektedir. loss fonksiyonları kısa ya da uzun isim olarak yazısal biçimde 
    kullanılabilmektedir. Tipik loss fonksiyon isimleri şunlardır:

    'mean_squared_error' ya da 'mse'
    'mean_absolute_error' ya da 'mae'
    'mean_absolute_percentage_error' ya da 'mape'
    'mean_squared_logarithmic_error' ya da 'msle'
    'categorical_crossentropy'
    'binary_crossentropy'

    tensorflow.keras.losses modülündeki fonksiyonlar da kısa ve uzun isimli olarak bulunabilmektedir:

    tensorflow.keras.losses.mean_squared_error ya da tensorflow.keras.losses.mse
    tensorflow.keras.losses.mean_absolute_error ya da tensorflow.keras.losses.mae
    tensorflow.keras.losses.mean_absolute_percentage_error ya da tensorflow.keras.losses.mape
    tensorflow.keras.losses.mean_squared_logarithmic_error ya da tensorflow.keras.losses.msle
    tensorflow.keras.losses.categorical_crossentropy
    tensorflow.keras.losses.binary_crossentropy

    Bu durumda compile metodu örnek bir biçimde şöyle çağrılabilir:

    model.compile(optimizer='rmsprop', loss='binary_crossentropy')

    compile metodunun üçüncü önemli parametresi "metrics" isimli parametredir. metrics parametresi bir liste ya da demet olarak 
    girilir. metrics parametresi daha sonra açıklanacak olan "sınama (validation)" işlemi için kullanılacak fonksiyonları 
    belirtmektedir. Sınamada her zaman zaten bizim loss fonksiyonu olarak belirttiğimiz fonksiyon kullanılmaktadır. metrics 
    parametresinde ilave fonksiyonlar da girilebilmektedir. Örneğin ikili sınıflandırma problemleri için tipik olarak "binary_accuracy"
    denilen metrik fonksiyon, çoklu sınıflandırma için "categorical_accuracy" denilen metrik fonksiyon ve regresyon problemleri 
    için de "mean_absolute_error" isimli metrik fonksiyon sıklıkla kullanılmaktadır. Örneğin ikili sınıflandırma problemi için 
    biz eğitim sırasında "loss" değerinin yanı sıra "binary_accuracy" değerini de elde etmek isteyelim. Bu durumda compile metodunu
     şöyle çağırmalıyız:

    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

    5) Model derlenip çeşitli belirlemeler yapıldıktan sonra artık gerçekten eğitim aşamasına geçilir. Eğitim süreci Sequential 
    sınıfının fit metoduyle yapılmaktadır. fit metodunun en önemli parametresi ilk iki parametre olan x ve y veri kümeleridir. 
    Biz burada training_dataset_x ve training_dataset_y verilerini fit metodunun ilk iki paramtresine geçirmeliyiz. 
    
    fit metodunun önemli bir parametresi batch_size isimli parametredir. Eğitim işlemi aslında satır satır değil batch batch 
    yapılmaktadır. batch bir grup satıra denilmektedir. Yani ağa bir grup satır girdi olarak verilir. Ağdan bir grup çıktı elde 
    edilir. Bu bir grup çıktı ile bu çıktıların gerçek değerleri loss fonksiyonuna sokulur ve optimizasyon algoritması çalıştırılarak 
    w ve bias değerleri güncellenir. Yani optimizasyon algoritması her batch işlemden sonra devreye sokulmaktadır. Batch büyüklüğü 
    fit metodunda batch_size parametresiyle belirtilmektedir. Bu değer girilmezse batch_size 32 olarak alınmaktadır. 32 değeri 
    pek çok uygulama için uygun bir değerdir. Optimizasyon işleminin satır satır yapılması yerine batch batch yapılmasının iki 
    önemli nedeni vardır: Birincisi işlem miktarının azaltılması, dolayısıyla eğitim süresinin kısaltılmasıdır. İkincisi ise 
    "overfitting" denilen olumsuz durum için bir önlem oluşturmasıdır. Overfitting hakkında ileride açıklama yapılacaktır. 

    fit metodunun diğer önemli parametresi de "epochs" isimli parametredir. eğitim veri kümesinin eğitim sırasında yeniden 
    eğitimde kullanılmasına "epoch" işlemi denilmektedir. Örneğin elimizde 1000 satırlık bir eğitim veri kümesi olsun. batch_size 
    parametresinin de 20 olduğunu varsayalım. Bu durumda bu eğitim veri kümesi 1000 / 20 = 50 batch işleminde bitecektir. Yani 
    model parametreleri 50 kere ayarlanacaktır. Pek çok durumda eğitim veri kümesinin bir kez işleme sokulması model parametrelerinin
    iyi bir biçimde konumlandırılması için yetersiz kalmaktadır. İşte eğitim veri kümesinin birden fazla kez yani fit metodundaki 
    epochs sayısı kadar yeniden eğitimde kullanılması yoluna gidilmektedir. Peki epochs değeri ne olmalıdır? Aslında bunu 
    uygulamacı belirler. Az sayıda epoch model parametrelerini yeterince iyi konumlandıramayabilir. Çok fazla sayıda epoch 
    "overfitting" denilen olumsuz duruma zemin hazırlayabilir. Ayrıca çok fazla epoch eğitim zamanını da uzatmaktadır. Uygulamacı 
    epoch'lar sırasında modelin davranışına bakabilir ve uygun epoch sayısında işlemi kesebilir. Eğitim sırasında Keras bizim 
    belirlediğimiz fonksiyonları çağırabilmektedir. Buna Keras'ın "callback" mekanizması denilmektedir. Uygulamacı bu yolla model
    belli bir duruma geldiğinde eğitim işlemini kesebilir. Ya da uygulamacı eğer eğitim çok uzamayacaksa yüksek bir epoch ile 
    eğitimini yapabilir. İşlemler bitince epoch'lardaki performansa bakabilir. Olması gerekn epoch değerini kestirebilir. Sonra 
    modeli yeniden bu sayıda epoch ile eğitir. fit metodunun shuffle parametresi her epoch'tan sonra eğitim veri kümesinin 
    karıştırılıp karıştırılmayacağını belirtmektedir. Bu parametre default olarak True biçimdedir. Yani eğitim sırasında her 
    epoch'ta eğitim veri kümesi karıştırılmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            27. Ders - 30/03/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Modelin fit metodu ile eğitilmesi sırasında "sınama (validation)" denilen önemli bir kavram daha devreye girmektedir. Sınama
    işlemi test işlemine benzemektedir. Ancak test işlemi tüm model eğitildikten sonra yapılırken sınama işlemi her epoch'tan 
    sonra modelin eğitim sürecinde yapılmaktadır. Başka bir deyişle sınama işlemi model eğitilirken yapılan test işlemidir. 
    Epoch'lar sırasında modelin performansı hakkında bilgi edinebilmek için sınama işlemi yapılmaktadır. Sınamanın yapılması 
    için fit metodunun validation_split parametresinin 0 ile 1 arasında oransal bir değer olarak girilmesi gerekir. Bu oransal 
    değer eğitim veri kümesinin yüzde kaçının sınama için kullanılacağını belirtmektedir. Örneğin validation_split=0.2 eğitim 
    veri kümesinin %20'sinin sınama için kullanılacağını belirtmektedir. fit metodu işin başında eğitim veri kümesini eğitimde 
    kullanılacak kısım ile sınamada kullanılacak kısım biçiminde ikiye ayırmaktadır. Sonra her epoch'ta yalnızca eğitimde kullanılacak 
    kümeyi karıştırmaktadır. Sınama işlemi aynı kümeyle her epoch sonrasında karıştırılmadan yapılmaktadır. fit metodunda ayrıca 
    bir de validation_data isimli bir parametre vardır. Bu parametre sınama verilerini girmek için kullanılmaktadır. Bazen programcı 
    sınama verilerinin eğitim veri kümesinden çekilip alınmasını istemez. Onu ayrıca fit metoduna vermek isteyebilir. Tabii 
    validation_data parametresi girildiyse artık validation_split parametresinin bir anlamı yoktur. Bu parametre girilse bile 
    artık fonksiyon tarafından dikkate alınmaz. Örneğin:

    model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)

    validation_split parametresinin default değerinin 0 olduğuna dikkat ediniz. validation_split değerinin 0 olması epoch'lar 
    sonrasında sınama işleminin yapılmayacağı anlamına gelmektedir. 

    Peki modelin fit metodunda her epoch'tan sonra sınama işleminde hangi ölçümler ekrana yazdırılacaktır? İşte compile metodunda 
    belirtilen ve ismine metrik fonksiyonlar denilen fonksiyonlar her epoch işlemi sonucunda ekrana yazdırılmaktadır. Her epoch 
    sonucunda fit metodu şu değerleri yazdırmaktadır:

    - Epoch sonrasında elde edilen loss değerlerinin ortalaması
    - Epoch sonrasında eğitim veri kümesinin kendisi için elde edilen metrik değerlerin ortalaması
    - Epoch sonrasında sınama için kullanılan sınama verilerinden elde edilen loss değerlerinin ortalaması
    - Epoch sonrasında sınama için kullanılan sınama verilerinden elde edilen metrik değerlerin ortalaması

    Bir epoch işleminin batch batch yapıldığını anımsayınız. Budurumda epoch sonrasında fit tarafından ekrana yazdırılan değerler
    bu batch işlemlerden elde edilen ortalama değerlerdir. Yani çrneğin her batch işleminden bir loss değeri elde edilir. Sonra 
    bu loss değerlerinin ortalaması hesap edilerek yazdırılır. Eğitim veri kümesindeki değerler ile sınama veri kümesinden elde 
    edilen değerler birbirine karışmasın diye fit metodu sınama verilerinden elde edilen değerlerin başına "val_" öneki getirmektedir. 
    Örneğin biz ikili sınıflandırma problemi üzerinde çalışıyor olalım ve metrik fonksion olarak "binary_accuracy" kullanmış 
    olalım. fit metodu her epoch sonrasında şu değerleri ekrana yazdıracaktır:

    loss (eğitim veri kümesinden elde edilen ortalama loss değeri)
    binary_accuracy (eğitim veri kümesinden elde edilen ortalama metrik değer)
    val_loss (sınama veri kümesinden elde edilen ortalama loss değeri)
    val_binary_accuracy (sınama veri kümesinden elde edilen ortalama metrik değer)

    Tabii compile metodunda birden fazla metirk değer de belirtilmiş olabilir. Bu durumda fit tüm bu metrik değerlerin ortalamasını
    ekrana yazdıracaktır. fit tarafından ekrana yazdırılan örnek bir çıktı şöyle olabilir:

    ....
    Epoch 91/100
    16/16 [==============================] - 0s 3ms/step - loss: 0.5536 - binary_accuracy: 0.7230 - val_loss: 0.5520 - 
    val_binary_accuracy: 0.7480
    Epoch 92/100
    16/16 [==============================] - 0s 3ms/step - loss: 0.5392 - binary_accuracy: 0.7251 - val_loss: 0.5588 - 
    val_binary_accuracy: 0.7805
    Epoch 93/100
    16/16 [==============================] - 0s 3ms/step - loss: 0.5539 - binary_accuracy: 0.7088 - val_loss: 0.5666 - 
    val_binary_accuracy: 0.8049
    ...

    Burada loss değeri epoch sonrasında eğitim verilerinden elde edilen ortalama loss değerini, val_loss değeri epoch sonrasında 
    sınama verilerinden elde edilen ortalama loss değerini, binary_accuracy epoch sonrasında eğitim verilerinden elde edilen
    ortalama isabet yüzdesini ve val_binary_accuracy ise epoch sonrasında sınama verilerinden elde edilen ortalama isabet 
    yüzdesini belirtmektedir.

    Eğitim sırasında eğitim veri kümesindeki başarının sınama veri kümesinde görülmemesi eğitimin kötü bir yöne gittiğine işaret 
    etmektedir. Örneğin ikili sınıflandırma probleminde epoch sonucunda eğitim veri kümesindeki binary_accuracy değerinin %99 
    olduğunu ancak val_binary_accuracy değerinin %65 olduğunu düşünelim. Bunun anlamı ne olabilir? Bu durum aslında epoch'lar 
    sırasında modelin bir şeyler öğrendiği ama bizim istediğimiz şeyleri öğrenemediği anlamına gelmektedir. Çünkü eğitim veri 
    kümesini epoch'larla sürekli bir biçimde gözden geçiren model artık onu ezberlemiştir. Ancak o başarıyı eğitimden bağımsız 
    bir veri kümesinde gösterememektedir. İşte bu olguya "overfitting" denilmektedir. Yanlış bir şeyin öğrenilmesi bir şeyin 
    öğrenilememesi kadar kötü bir durumdur. Overfitting oluşumunun çeşitli nedenleri vardır. Ancak overfitting epoch'lar 
    dolayısıyla oluşuyorsa epoch'ları uygun bir noktada kesmek gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    6) fit işleminden sonra artık model eğitilmiştir. Onun test veri kümesiyle test edilmesi gerekir. Bu işlem Sequential sınıfının 
    evaluate isimli metodu ile yapılmaktadır. evaluate metodunun ilk iki parametresi test_dataset_x ve test_dataset_y değerlerini 
    almaktadır. Diğer bir parametresi yine batch_size parametresidir. Buradaki bacth_size eğitim işlemi yapılırken fit metodunda 
    kullanılan batch_size ile benzer anlamdadır ancak işlevleri farklıdır. Model test edilirken test işlemi de birer birer değil 
    batch batch yapılabilmektedir. Ancak bu batch'ler arasında herhangi bir şey yapılmamaktadır. (Eğitim sırasındaki batch işlemleri 
    sonrasında ağ parametrelerinin ayarlandığını anımsayınız. Test işlemi sırasında böyle bir ayarlama yapılmamaktadır.) Buradaki 
    batch değeri yalnızca işlemlerin hızlı yapılması üzerinde etkili olmaktadır. Yine batch_size parametresi girilmezse default 
    32 alınmaktadır. evaluate metodu bir liste geri döndürmektedir. Listenin ilk elemanı test veri kümesinden elde edilen loss 
    fonksiyonunun değeridir. Diğer elemanları da sırasıyla metrik olarak verilen fonksiyonların değerleridir. Örneğin:

    eval_result = model.evaluate(test_dataset_x, test_dataset_y)

    Aslında eval_result listesinin elemanlarının hangi anlamlara geldiğini daha iyi ifade edebilmek için Sequential sınıfında 
    metrics_names isimli bir örnek özniteliği (instance attribute) bulundurulmuştur. Bu metrics_names listesindeki isimler bire 
    bir evalute metodunun geri döndürdüğü liste elemanları ile örtüşmektedir. Bu durumda evaluate metodunun geri döndürdüğü 
    listeyi aşağıdaki gibi de yazdırabiliriz:

    for i in range(len(eval_result)):
        print(f'{model.metrics_names[i]}: {eval_result[i]}')

    Aynı şeyi built-in zip fonksiyonuyla da şöyle yapabilirdik:

    for name, value in zip(model.metrics_names, eval_result):
        print(f'{name}: {value}')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        28. Ders - 06/04/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    7) Artık model test de edilmiştir. Şimdi sıra "kestirim (prediction)" yapmaya gelmiştir. Kestirim işlemi için Sequential
    sınıfının predict metodu kullanılır. Biz bu metoda girdi katmanına uygulanacak özelliklerin (yani satırların) değerlerini 
    veriririz. predict metodu da bize çıktı katmanındaki nöronların değerlerini verir. predict metoduna biz her zaman iki boyutlu 
    bir NumPy dizisi vermeliyiz. Çünkü predict metodu tek hamlede birden çok satır için kestirim yapabilmektedir. Biz predict 
    metoduna bir satır verecek olsak bile onu iki boyutlu bir matris biçiminde vermeliyiz. Örneğin:

    import numpy as np
 
    predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

    predict_result = model.predict(predict_data)
    print(predict_result)

    predict metodu bize tahmin edilen değerleri iki boyutlu bir NumPy dizisi biçiminde vermektedir. Bunun nedeni aslında ağın 
    birden fazla çıktısının olabilmesidir. Örneğin ağın bir çıktısı varsa bu durumda predict metodu bize "n tane satırdan 1 
    tane sütundan" oluşan bir matris, ağın iki çıktısı varsa "n tane satırdan 2 iki tane sütundan oluşan bir matris verecektir. 
    O halde örneğin çıktı olarak tek nöronun bulunduğu bir ağda ("diabetes" örneğindeki gibi) biz kestirim değerlerini şöyle 
    yazdırabiliriz:

    for i in range(len(predict_result)):
        print(predict_result[i, 0])

    Ya da şöyle yazdırabiliriz:

    for result in predict_result[:, 0]:
        print(result)

    Tabii iki boyutlu diziyi NumPy'ın flatten metoduyla ya da ravel metoduyla tek boyutlu hale getirerek de yazırma işlemini 
    yapabilirdik:

    for val in predict_result.flatten():
        print(val)

    predict metodu bize ağın çıktı değerini vermektedir. Yukarıdaki "diabetes.csv" örneğimizde ağın çıktı katmanındaki aktivasyon 
    fonksiyonunun "sigmoid" olduğunu anımsayınız. Sigmoid fonksiyonu 0 ile 1 arasında bir değer vermektedir. O halde biz ağın 
    çıktısındaki değer 0.5'ten büyükse ilgili kişinin şeker hastası olduğu (çünkü 1'e daha yakındır), 0.5'ten küçükse o kişinin 
    şeker hastası olmadığı (çünkü 0'a daha yakındır) sonucunu çıkartabiliriz. Tabii değer ne kadar 1'e yakınsa kişininin şeker 
    hastası olma olasılığı, değer ne kadar 0'a yakınsa kişinin şeker hastası olmama olasılığı o kadar yüksek olacaktır. O halde 
    sigmoid fonksiyonun çıktısının bir olasılık belirttiğini söyleyebiliriz. Bu durumda kişinin şeker hastası olup olmadığı 
    ağın çıktı değerinin 0.5'ten büyük olup olmamasıyla kesitirilebilir:
    
    for result in predict_result[:, 0]:
        print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte "diabetes.csv" dosyası üzerinde yukarıda belirtilen tüm adımlar uygulanmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import numpy as np

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

predict_result = model.predict(predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi yukarıdaki adımların bazı detayları üzerinde duralım. Daha önceden de belirttiğimiz gibi pek çok problem iki saklı 
    katmanla ve Dense bir bağlantı ile tatminkar biçimde çözülebilmektedir. Dolayısıyla bizim ilk aklımıza gelen model iki saklı 
    katmanlı klasik modeldir. Ancak özel problemler (şekil tanıma, yazıdan anlam çıkartma, görüntü hakkında çıkarım yapma gibi) 
    iki saklı katmanla tatminkar biçimde çözülememketedir. Bu durumda ikiden fazla saklı katman kullanılır. Bu modellere "derin 
    öğrenme (deep learning)" modelleri de denilmektedir. 
    
    Girdi katmanındaki nöron sayısı zaten problemdeki sütun sayısı kadar (özellik sayısı kadar) olmalıdır. Tabii kategorik sütunlar 
    "one-hot-encoding" işlemine sokulmalıdır. Çıktı katmanındaki nöron sayıları ise yine probleme bağlıdır. İkili sınıflandırma 
    (binary classification) problemlerinde çıktı katmanı tek nörondan, çoklu sınıflandırma problemlerinde (multiclass classification) 
    çıktı katmanı sınıf sayısı kadar nörondan oluşur. Regresyon problemlerinde ise çıktı katmanındaki nöron 
    sayıları genellikle bir tane olmaktadır.

    Saklı katmanlardaki nöron sayıları için çok pratik şeyler söylemek zordur. Çünkü saklı katmanlardaki nöron sayıları bazı 
    faktörlere de bağlı olarak ayarlanabilmektedir. Örneğin eğitimde kullanılacak veri miktarı, problemin karmaşıklığı, hyper 
    parametrelerin durumları saklı katmanlardaki nöron sayıları üzerinde etkili olabilmektedir. Saklı katmanlardaki nöron sayıları 
    için şunlar söylenebilir:

    - Problem karmaşıklaştıkça saklı katmanlardaki nöron sayılarını artırmak uygun olabilmektedir. 
    - Saklı katmanlarda çok az nöron bulundurmak "underfitting" yani yetersiz öğrenmeye yol açabilmektedir. 
    - Saklı katmanlarda gereksiz biçimde fazla sayıda nöron bulundurmak eğitim süresini uzatabileceği gibi "overfitting" durumuna 
    da yol açabilir. Aynı zamanda modelin diskte saklanması için gereken disk alanını da artırabilmektedir.
    - Eğitim veri kümesi azsa saklı katmanlardaki nöron sayıları düşürülebilir. 
    - Pek çok problemde saklı katmanlardaki nöron sayıları çok fazla ya da çok az olmadıktan sonra önemli olmayabilir. 
    - Saklı katmanlardaki nöron sayısı girdi katmanındaki nöron sayısından az olmamlıdır. 

    Çeşitli kaynaklar saklı katmanlardaki nöronların sayıları için üstünkörü şu pratik tavsiyelerde bulunmaktadır:

    - Saklı katmanlardaki nöronların sayıları girdi katmanındaki nöronların sayılarının 2/3'ü ile çıktı katmanındaki nöronların 
    sayısının toplamı kadar olabilir. Örneğin girdi katmanındaki nöron sayısı 5, çıktı katmanındaki 1 olsun. Bu durumda saklı 
    katmandaki nöron sayısı 4 ya da 5 olabilir. 

    - Saklı katmandaki nöronların sayısı girdi katmanındaki nöron sayısının iki katından fazla olmamalıdır.

    Biz genellikle genel problemlerde iki saklı katman ve katmalarda da 16, 32, 64, 100, 128 gibi sayılarda nöron kullanacağız. 
    Ancak ileride özel mimarilerde bu durumu farklılaştıracağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağı her farklı eğitimde farklı "w" ve "bias" değerlerini oluşturabilir. Bu nedenle ağın peformans değerleri de 
    eğitimden eğitime değişebilir. Her eğitimde ağın farklı değerlere konumlanırılmasının nedenleri şunlardır:

    1) train_test_split fonksiyonu her çalıştırıldığında aslında fonksiyon training_dataset ve test_dataset veri kümelerini 
    karıştırarak elde etmektedir. 
    2) Katmanlardaki "w" değerleri (ve istersek "bias" değerleri) programın her çalıştırılmasında rastgele başlangıç değerleriyle 
    set edilmektedir.
    3) fit işleminde her epoch sonrasında veri kümesi yeniden karıştırılmaktadır. 

    Bir rastgele sayı üretiminde üretim aynı "tohum değerden (seed)" başlatılırsa hep aynı değerler elde edilir. Bu duruma rassal 
    sayı üretiminin "reproducible olması" denmektedir. Eğer tohum değer belirtilmezse NumPy ve TensorFlow gibi kütüphanelerde bu 
    tohum değeri programın her çalıştırılmasında rastgele biçimde bilgisayarın saatinden hareketle oluşturmaktadır. 

    O halde eğitimden hep aynı sonucun elde edilmesi için (yani eğitimin "reproducible" hale getirilmesi için) yukarıdaki unsurların 
    dikkate alınması gerekir. Tipik yapılması gereken birkaç şeyi şöyle belirtebiliriz:
    
    1) scikit-learn ve diğer bazı makine öğrenmesi kütüphanelerinde aşağı seviyeli kütüphane olarak NumPy kullanıldığı için NumPy'ın 
    rassal sayı üreticisinin tohum değeri belli bir değerle set edilebilir. Örneğin:

    import numpy as np
    
    np.random.seed(12345)

    2) TensorFlow kütüphanesi bazı işlemlerde kendi rassal sayı üreticisini kullanmaktadır. Onun tohum değeri de belli bir değerle 
    set edilebilir. Örneğin:

    from tensorflow.keras.utils import set_random_seed

    set_random_seed(78901)

    Tabii yukarıdaki işlemler yapılsa bile rassal sayı üretimi "reproducible" hale getirilemeyebilir. Çünkü bu durum bu kütüphanelerin
    rassal sayı üretiminin hangi kütüpaheneler kullanılarak yapıldığı ile ilgilidir. Yukarıdaki iki madde sezgisel bir çıkarımı
    ifade etmekltedir.

    Peki neden ağın her eğitilmesinde aynı sonuçların elde edilmesini (yani ağın "reproducible" sonuçlar vermesini) isteyebiliriz?
    İşte bazen modellerimizde ve algoritmalarımızda yaptığımız değişiklikleri birbirleriyle kıyaslamak isteyebiliriz. Bu durumda
    kıyaslamanın herp aynı biçimde yapılmasını sağlayabilmek için rassal bir biçimde alınan değerlerin her çalıştırmada aynı
    değerler olmasını sağlamamız gerekir. Tabii aslında algoritmaları karşılaştırmak için bu biçimde "reproducible" rassal sayı 
    üretimi yapmak yerine algoritmaları çokça çalıştırıp bir ortalama değere de bakılabilir. Bu yöntem genellikle daha iyi bir 
    karşılaştırma olanağı sunmaktadır.

    O halde biz yukarıda belirttiğimiz iki ayarlamayı yaparak "diabetes" modelimizi çalıştırırsak bu durumda her eğitimde ve 
    test işleminde aynı sonucu elde edebiliriz. Aşağıda buna bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np
from tensorflow.keras.utils import set_random_seed

np.random.seed(1234567)
set_random_seed(678901)

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

predict_result = model.predict(predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de dikkatimizi katmanlardaki aktivasyon fonksiyonlarına yöneltelim. Katmanlardaki aktivasyon fonksiyonları ne olmalıdır? 
    Girdi katmanı gerçek bir katman olmadığına göre orada bir aktivasyon fonksiyonu yoktur. Saklı katmanlardaki aktivasyon 
    fonksiyonları için çeşitli seçenekler bulunmaktadır. Biz de bu bölümde belli başlı aktivasyon fonksiyonlarını daha ayrıntılı 
    olarak ele alacağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Özellikle son yıllarda saklı katmanlarda en fazla tercih edilen aktivasyon fonksiyonu "relu (rectified linear unit)" denilen 
    aktivasyon fonksiyonudur. Bu fonksiyona İngilizce "rectifier" da denilmektedir.  Relu fonksiyonu şöyledir:

    x >= 0  ise y = x
    x < 0   ise y = 0

    Yani relu fonksiyonu x değeri 0'dan büyük ise (ya da eşit ise) aynı değeri veren, x değeri 0'dan küçük ise 0 değerini veren 
    fonksiyondur. relu fonksiyonunu basit bir biçimde aşağıdaki gibi yazabiliriz:

    def relu(x):
      return np.maximum(x, 0)  

    NumPy kütüphanesinin maximum fonksiyonunun birinci parametresi bir NumPy dizisi ya da Python listesi biçiminde girilirse
    fonksiyon bu dizinin ya da listenin her elemanı ile maximum işlemi yapmaktadır. Örneğin:

    >>> import numpy as np
    >>> x = [10, -4, 5, 8, -2]
    >>> y = np.maximum(x, 3)
    >>> y
    array([10,  3,  5,  8,  3])

    Relu fonksiyonun grafiği aşağıdaki gibi çizilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

def relu(x):
      return np.maximum(x, 0)  

x = np.linspace(-10, 10, 1000)
y = relu(x)

plt.title('Relu Function', fontsize=14, fontweight='bold', pad=20)
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)
axis.set_ylim(-10, 10)
plt.plot(x, y, color='red')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Aktivasyon fonksiyonları katman nesnelerine isimsel olarak girilebileceği gibi tensorflow.keras.activations modülündeki 
    fonksiyonlar biçiminde de girilebilmektedir. Örneğin:

    from tensorflow.keras.activations import relu
    ...
    model.add(Dense(16, activation=relu, name='Hidden'))

    Bu modüldeki fonksiyonlar keras TensorFlow kullanılarak yazıldığı için çıktı olarak Tensor nesneleri vermektedir. Biz relu
    grafik çizdirirken fonksiyonunu kendimiz yazmak yerine tensorflow içerisindeki fonksiyonu doğrudan da kullanabiliriz. Tensor
    nesnelerinin NumPy dizilerine dönüştürülmesi için Tensor sınıfının numpy metodu kullanılabilir. Örneğin:

    from tensorflow.keras.activations import relu

    x = np.linspace(-10, 10, 1000)
    y = relu(x).numpy()

    Aşağıdakiş örnekte relu fonksiyonun grafiği TensorFlow'daki relu fonksiyonu yardımıyla çizdirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

def relu(x):
      return np.maximum(x, 0)  

x = np.linspace(-10, 10, 1000)

# y = relu(x)

from tensorflow.keras.activations import relu
y = relu(x).numpy()

plt.title('Relu Function', fontsize=14, fontweight='bold', pad=20)
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)
axis.set_ylim(-10, 10)
plt.plot(x, y, color='red')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    İkili sınıflandırma problemlerinde çıktı katmanında en fazla kullanılan aktivasyon fonksiyonu "sigmoid" denilen fonksiyondur. 
    Yukarıdaki "diabetes" örneğinde biz çıktı katmanında sigmoid fonksiyonunu kullanmıştık. Gerçekten de ikili sınıflandırma 
    problemlerinde ağın çıktı katmanında tek bir nöron bulunur ve bu nörounun da aktivasyon fonksiyonu "sigmoid" olur. 

    Peki sigmoid nasıl bir fonksiyondur? Bu fonksiyona "lojistik (logistic)" fonksiyonu da denilmektedir. Fonksiyonun matematiksel 
    ifadesi şöyledir:

    y = 1 / (1 + e ** -x)
    
    Burada e değeri 2.71828... biçiminde irrasyonel bir değerdir. Yukarıdaki kesrin pay ve paydası e ** x ile çarpılırsa fonksiyon 
    aşağıdaki gibi de ifade edilebilir:

    y = e ** x / (1 + e ** x)

    Fonksiyona "sigmoid" isminin verilmesinin nedeni S şekline benzemesinden dolayıdır. Sigmoid eğrisi x = 0 için 0.5 değerini 
    veren x pozitif yönde arttıkça 1 değerine hızla yaklaşan, x negatif yönde arttıkça 0 değerine hızla yaklaşan S şeklinde bir 
    eğridir. Sigmoid fonksiyonunun (0, 1) arasında bir değer verdiğine dikkat ediniz. x değeri artıkça eğri 1'e yaklaşır ancak 
    hiçbir zaman 1 olmaz. Benzer biçimde x değeri azaldıkça eğri 0'a yaklaşır ancak hiçbir zaman 0 olmaz. 
    
    Sigmoid fonksiyonu makine öğrenmesinde ve istatistikte belli bir gerçek değeri 0 ile 1 arasına hapsetmek için sıkça 
    kullanılmaktadır. Sigmoid çıktısı aslında bir bakımdan kestirimin 1 olma olasılığını vermektedir. Tabii biz kestirimde 
    bulunurken kesin bir yargı belirteceğimiz için eğrinin orta noktası olan 0.5 değerini referans alırız. Ağın ürettiği değer 
    0.5'ten büyükse bunu 1 gibi, 0.5'ten küçükse 0 gibi değerlendiririz.  
    
    Sigmoid eğrisi aşağıdaki gibi çizilebilir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 1000)
y = np.e ** x / (1 + np.e ** x)

plt.title('Sigmoid (Logistic) Function', fontsize=14, pad=20, fontweight='bold')
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_ylim(-1, 1)

plt.plot(x, y)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Yine benzer biçimde tensorflow.keras.activations modülü içerisinde sigmoid fonksiyonu zaten hazır biçimde bulunmaktadır. 
    Tabii bu fonksiyon da bize TensorFlow'daki bir Tensor nesnesini vermektedir. Aşağıda sigmoid eğrisini bu hazır fonksiyonu 
    kullanarak çizdiriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.activations import sigmoid

x = np.linspace(-10, 10, 1000)
y = sigmoid(x).numpy()

plt.title('Sigmoid (Logistic) Function', fontsize=14, fontweight='bold', pad=20)
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)
axis.set_ylim(-1, 1)
plt.plot(x, y, color='red')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Sigmoid fonksiyonu nasıl elde edilmiştir? Aslında bu fonksiyonun elde edilmesinin bazı mantıksal gerekçeleri vardır. 
    Sigmoid fonksiyonunun birinci türevi Gauss eğrisine benzemektedir. Aşağıdaki örnekte Sigmoid fonksiyonunun birinci türevi 
    alınıp eğrisi çizdirilmiştir. Ancak bu örnekte henüz görmediğimiz SymPy kütüphanesini kullandık. Sgmoid fonksiyonun birinci 
    türevi şöyledir:

    sigmoid'(x) = exp(x)/(exp(x) + 1) - exp(2 * x)/(exp(x) + 1) ** 2
#----------------------------------------------------------------------------------------------------------------------------

import sympy
from sympy import init_printing

init_printing()

x = sympy.Symbol('x')
fx = sympy.E ** x / (1 + sympy.E ** x)
dx = sympy.diff(fx, x)

print(dx)

import numpy as np

np.linspace(-10, 10, 1000)
pdx = sympy.lambdify(x, dx)

x = np.linspace(-10, 10, 1000)
y = pdx(x)

import matplotlib.pyplot as plt

plt.title('First Derivative of Sigmoid Function', fontsize=14, pad=20, fontweight='bold')
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_ylim(-0.4, 0.4)

plt.plot(x, y)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok kullanılan bir aktivasyon fonksiyonu da "hiperbolik tanjant" fonksiyonudur. Bu fonksiyona kısaca "tanh" fonksiyonu
    da denilmektedir. Fonksiyonun matematiksel ifadesi şöyledir:

    f(x) = (e ** (2 * x) - 1) / (e ** (2 * x) + 1)

    Fonksiyonun sigmoid fonksiyonuna benzediğine ancak üstel ifadenin x yerine 2 * x olduğuna dikkat ediniz. Tanh fonksiyonu adeta 
    sigmoid fonksiyonunun (-1, +1) arası değer veren biçimi gibidir. Fonksiyon yine S şekli biçimindedir. Ancak noktası x = 0'dadır.

    Tanh fonksiyonu saklı katmanlarda da bazen çıktı katmanlarında da kullanılabilmektedir. Eskiden bu fonksiyon saklı katmanlara 
    çok yoğun kullanılıyordu. Ancak artık saklı katmanlarda daha çok relu fonksiyonu tercih edilmektedir. Fakat tanh fonksiyonunun 
    daha iyi sonuç verdiği modeller de söz konusu olmaktadır. 

    tanh fonksiyonu Keras'ta tensorflow.keras.activations modülünde tanh ismiyle de bulunmaktadır.

    Fonksiyonun grafiğini aşağıdaki gibi çizdirebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 1000)
y = (np.e ** (2 * x) - 1) / (np.e ** (2 * x) + 1)

plt.title('Hiperbolik Tanjant (tanh) Fonksiyonunun Grafiği', fontsize=14, pad=20, fontweight='bold')
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)

axis.set_ylim(-1, 1)

plt.plot(x, y)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok karşılaşılan bir aktivasyon fonksiyonu da "softmax" isimli fonksiyondur. Softmax fonksiyonu çok sınıflı sınıflandırma 
    problemlerinde çıktı katmanlarında kullanılmaktadır. Bu aktivasyon fonksiyonu Keras'ta "softmax" ismiyle bulunmaktadır. 
    Örneğin bir resmin "elma", "armut", "kayısı", "şeftali", "karpuz" resimlerinden hangisi olduğunu anlamak için kullanılan 
    sınıflandırma modeli çok sınıflı bir sınıflandırma modelidir. Buna istatistikte "çok sınıflı lojistik regresyon (multinomial 
    logistic regression)" da denilmektedir. Bu tür problemlerde sinir ağında sınıf sayısı kadar nöron bulundurulur. Örneğin 
    yukarıdaki "elma", "armut", "kayısı", "şeftali", "karpuz" resim sınıflandırma probleminde ağın çıktısında 5 nöron bulunacaktır. 

    Ağın çıktı katmanındaki tüm nöronların aktivasyon fonksiyonları "softmax" yapılırsa tüm çıktı katmanındaki nöronların çıktı 
    değerlerinin toplamı her zaman 1 olur. Böylece çıktı katmanındaki nöronların çıktı değerleri ilgili sınıfın olasılığını belirtir 
    hale gelir. Biz de toplamı 1 olan çıktıların en yüksek değerine bakarız ve sınıflandırmanın o sınıfı kestirdiğini kabul ederiz. 
    Örneğin yukarıdaki "elma", "armut", "kayısı", "şeftali", "karpuz" sınıflandırma probleminde ağın çıktı katmanındaki nöronların 
    çıktı değerlerinin şöyle olduğunu varsayalım: 

    Elma Nöronunun Çıktısı ---> 0.2
    Armut Nöronunun Çıktısı ---> 0.2
    Kayısı Nöronunun Çıktısı ---> 0.3
    Şeftali Nöronunun Çıktısı ---> 0.2
    karpuz Nöronunun Çıktısı ---> 0.1

    Burada en büyük çıktı 0.3 olan kayısı nöronuna ilişkindir. O halde biz bu kestirimin "kayısı" olduğuna karar veririz. 
    Softmax fonksiyonu bir grup değer için o grup değerlere bağlı olarak şöyle hesaplanmaktadır: 

    softmax(x) = np.e ** x / np.sum(np.e ** x)

    Burada gruptaki değerler x vektörüyle temsil edilmektedir.  Fonksiyonda değerlerinin e tabanına göre kuvvetleri x değerlerinin 
    e tabanına göre kuvvetlerinin toplamına bölünmüştür. Bu işlemden yine gruptaki eleman sayısı kadar değer elde edilecektir. 
    Tabii bu değerlerin toplamı da 1 olacaktır. Örneğin elimizde aşağıdaki gibi x değerleri olsun:
    
    x = np.array([3, 6, 4, 1, 7])

    Şimdi bu x değerlerinin softmax değerlerini elde edelim:  

    >>> import numpy as np
    >>> x = np.array([3, 6, 4, 1, 7])
    >>> x
    array([3, 6, 4, 1, 7])
    >> sm = np.e ** x / np.sum(np.e ** x)
    >>> sm
    array([0.0127328 , 0.25574518, 0.03461135, 0.0017232 , 0.69518747])
    >>> np.sum(sm)
    1.0

    softmax fonksiyonu Keras'ta tensorflow.keras.activations modülünde softmax ismiyle de bulunmaktadır. Ancak bu fonksiyonu 
    kullanırken girdinin TensorFlow'daki bir Tensor nesnesi biçiminde ve iki boyutlu olarak verilmiş olması gerekmektedir. 
    TensorFlow kütüphanesindeki aktivasyon fonksiyonları dışarıdan değil TensorFlow içerisinden kullanılsın diye tasarlanmıştır.
    Bu nedenle softmax gibi bazı fonksiyonlarda biz NumPy dizisi verememekteyiz. Ayrıca TensorFlow'daki bu aktivasyon fonksiyonları
    birden fazla değer üzerinde de bir Tensor olarak işlem yapabilmektedir. (softmax fonksiyonunda aslında bir değer bir grup 
    değerden oluştuğu için girdi olarak da bizden iki boyutlu bir Tensor istenmektedir.) Örneğin:

    >>> import numpy as np
    >>> import tensorflow as tf
    >>> from tensorflow.keras.activations import softmax
    >>> x = np.array([[1, 2, 3, 4, 5]], dtype=np.float64)
    >>> t = tf.convert_to_tensor(x)
    >>> result = softmax(t)
    >>> result
    <tf.Tensor: shape=(1, 5), dtype=float64, numpy=array([[0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]])>
    
    Keras aslında çıktı katmanlarındaki tüm softmax aktivasyon fonksiyonlarının bir grup oluşturduğunu varsayar. Sonra girdilerle 
    ağırlık değelerini çarpıp bias değeriyle toplayarak yukarıdaki gibi bir x vektörü elde eder. Sonra da yukarıdaki işlemi 
    uygular. Başka bir deyişle çıktı katmanındaki softmax aktivasyon fonksiyonuna sahip olan nöronlar bir grup olarak 
    değerlendirilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok kullanılan aktivasyon fonksiyonlarından biri de "linear" aktivasyon fonksiyonudur. Aslında bu fonksiyon y = x 
    ya da f(x) = x fonksiyonudur. Yani "linear" fonksiyonu girdi ile aynı değeri üretmektedir. Başka bir deyişle bir şey yapmayan 
    bir fonksiyondur. Peki böyle bir aktivasyon fonksiyonunun ne anlamı olabilir? Bu aktivasyon fonksiyonu "regresyon problemlerinin 
    çıktı katmanında kullanılmaktadır. Regresyon problemleri çıktı olarak bir sınıf bilgisi değil gerçek bir değer bulmaya çalışan 
    problemlerdir. Örneğin bir evin fiyatının kestirilmesi, bir otomobilin mil başına yaktığı yakıt miktarının kestirilemsi gibi 
    problemler regresyon problemleridir.
    
    linear aktivasyon fonksiyonu Keras'ta "linear" ismiyle kullanılmaktadır. Her ne kadar bir şey yapmıyorsa da bu aktivasyon 
    fonksiyonu aynı zamanda tensorflow.keras.activations modülünde linear isimli bir fonksiyon biçiminde de bulunmaktadır. 
    Örneğin:

    >>> from tensorflow.keras.activations import linear
    >>> x = [1, 2, 3, 4, 5]
    >>> x = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> result = linear(x)
    >>> result
    array([1., 2., 3., 4., 5.])

    linear fonksiyonunun grafiğini -bariz olmasına karşın- aşağıda veriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

def linear(x):
      return x

x = np.linspace(-10, 10, 1000)
y = linear(x)

"""
from tensorflow.keras.activations import linear

y = linear(x).numpy()
"""

plt.title('Linear Function', fontsize=14, fontweight='bold', pad=20)
axis = plt.gca()
axis.spines['left'].set_position('center')
axis.spines['bottom'].set_position('center')
axis.spines['top'].set_color(None)
axis.spines['right'].set_color(None)
axis.set_ylim(-10, 10)
plt.plot(x, y, color='red')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda çok karşılaşılan temel aktivasyon fonksiyonlarını ele aldık. Aslında yukarıda ele aldığımızdan daha fazla 
    aktivasyon fonksiyonu vardır. Şimdi de aoptimizasyon algoritmasının minimize etmeye çalıştığı "loss" fonksiyonları 
    üzerinde duracağız. 

    loss fonksiyonları gerçek değerlerle ağın tahmin ettiği değerleri girdi olarak alıp bu farklılığı bir sayısal sayısal değerle 
    ifade eden fonksiyonlardır. Optimizasyon algoritmaları bu loss fonksiyonlarının değerini düşürmeye çalışmaktadır. Gerçek
    değerlerle ağın ürettiği değerlerin arasındaki farkın minimize edilmesi aslında ağın gerçek değerlere yakın değerler üretmesi
    anlamına gelmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi "w" ve "bias" değerleri "optimizer" denilen algoritma tarafından her batch işleminde güncellenmekteydi. 
    Yukarıda da belirttiğimiz gibi optimizer algoritması aslında "loss" denilen bir fonksiyonu minimize etmeye çalışmaktadır. 
    Başka bir deyişle "loss" fonksiyonu aslında "w" ve "bias" değerlerinin güncellenmesi için bir amaç fonksiyonu görevini 
    görmektedir. Değişik problemler için değişik loss fonksiyonları bulunmaktadır. Programcı model sınıfının compile metodunda 
    loss parametresiyle loss fonksiyonu isimsel biçimde girebilir. Ya da isterse tensorflow.keras.losses modülündeki sınıflar 
    ve fonksiyonlar yoluyla girebilir. (Loss fonksiyonları için TensorFlow hem callable sınıflar hem de fonksiyonlar bulundurmuştur.)

    Eğitim batch batch yapıldığı için loss fonksiyonları tek bir satırın çıktısından değil n tane satırın çıktısından hesaplanmaktadır. 
    Yani bir batch işleminin gerçek sonucu ile ağdan o batch için elde edilecek kestirim sonuçlarına dayanılarak loss değerleri 
    hesaplanmaktadır. Örneğin batch_size = 32 olduğu durumda aslında Keras ağa 32'lik bir giriş uygulayıp 32'lik bir çıktı elde 
    eder. Bu 32 çıktı değeri gerçek 32 değerle loss fonksiyonuna sokulur.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Regresyon problemleri için en yaygın kullanılan loss fonksiyonu "Mean Squared Error (MSE)" denilen fonksiyondur.  Bu fonksiyona 
    Türkçe "Ortalama Karesel Hata" diyebiliriz.  MSE fonksiyonu çıktı olarak gerçek değerlerden kestirilen değerlerin farkının 
    karelerinin ortalamasını vermektedr. Fonksiyonun sembolik gösterimi şöyledir:

    mse = np.mean((y - y_hat) ** 2)

    Burada y gerçek değerleri, y_hat ise kestirilen değerleri belirtmektedir. Örneğin:

    >>> y = np.array([1, 2, 3, 4, 5])
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> np.mean((y - y_hat) ** 2)
    0.020080000000000032

    Aynı işlemi tensorflow.keras.losses modülündeki mse (ya da mean_squared_error) fonksiyonuyla da aşağıdaki gibi yapabilirdik:

    >>> from tensorflow.keras.losses import mse
    >>> mse(y, y_hat)
    <tf.Tensor: shape=(), dtype=float64, numpy=0.020080000000000032>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Regresyon problemleri için kullanılabilen diğer bir loss fonksiyonu da "Mean Absolute Error (MAE)" isimli fonksiyondur. 
    Bu fonksiyona da Türkçe "Ortalama Mutlak Hata" diyebiliriz. Ortalama mutlak hata gerçek değerlerden kestirilen değerlerin 
    farklarının mutlak değerlerinin ortalaması biçiminde hesaplanmaktadır. Sembolik gösterimi şöyledir:

    mae = np.mean(np.abs(y - y_hat))

    Burada y gerçek değerleri y_hat ise ağın kestirdiği değerleri belirtmektedir. Regresyon problemleri için loss fonksiyonu 
    olarak çoğu kez MSE tercih edilmektedir. Çünkü kare alma işlemi algoritmalar için daha uygun bir işlemdir. Aynı zamanda d
    eğerleri daha fazla farklılaştırmaktadır. MAE loss fonksiyonundan ziyade metrik değer olarak "insan algısına 
    
    >>> y = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> mae = np.mean(np.abs(y - y_hat))
    >>> mae
    0.12400000000000003

    Ortalama karesel hata bir metrik değer olarak bizim için iyi bir çağrışım yapmamaktadır. Halbuki ortalama mutlak hata
    bizim için anlamlı bir çağrışım yapmaktadır. Örneğin ağımızın ortalama mutlak hatası 0.124 ise gerçek değer ağımızın bulduğu 
    değerden 0.124 solda ya da sağda olabilir. Yine ortalama mutlak hata tensorflow.keras.losses modülü içerisindeki "mae" ya da 
    "mean_absolute_error" isimli fonksiyonla da hesaplanabilmektedir. Örneğin:

    >>> from tensorflow.keras.losses import mae
    >>> y = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> result = mae(y, y_hat)
    >>> result
    <tf.Tensor: shape=(), dtype=float64, numpy=0.12400000000000003>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Regresyon problemleri için diğer bir loss fonksiyonu da "Mean Absolute Percentage Error (MAPE)" isimli fonksiyondur. Fonkisyonun 
    sembolik ifadesi şöyledir:

    mape = 100 * np.mean(np.abs(y - y_hat) / y)

    Burada y gerçek değerleri y_hat ise ağın kestirdiği değerleri belirtmektedir. Örneğin:

    >>> y = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> mape = 100 * np.mean(np.abs(y - y_hat) / y)
    >>> mape
    5.413333333333335

    Tabii aynı işlemi yine tensorflow.keras.losses modülündeki "mape" fonksiyonuyla da yapabiliriz:

    >>> from tensorflow.keras.losses import mape
    >>> y = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> result = mape(y, y_hat)
    >>> result
    <tf.Tensor: shape=(), dtype=float64, numpy=5.413333333333335>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Regresyon problemleri için diğer bir loss fonksiyonu da "Mean Squared Logarithmic Error (MSLE)" isimli fonksiyondur.  Bu 
    fonksiyon gerçek değerlerle kestirilen değerlerin logaritmalarının farklarının karelerinin ortalaması biçiminde hesaplanır. 
    Sembolik ifadesi şöyledir:

    msle = np.mean((np.log(y) - np.log(y_hat)) ** 2)

    Bazen bu fonksiyon gerçek ve kestirilen değerlere 1 toplanarak da oluşturulabilmektedir (TensorFlow "msle" fonksiyonunu 
    bu biçimde kullanmaktadır):

    msle = np.mean((np.log(y + 1) - np.log(y_hat + 1)) ** 2)

    Örneğin:

    >>> y = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> msle = np.mean((np.log(y + 1) - np.log(y_hat + 1)) ** 2)
    >>> msle
    0.0015175569737783628

    Aynı işlemi tensorflow.keras.losses modülündeki "msle" fonksiyonuyla da yapabiliriz:

    >>> from tensorflow.keras.losses import msle
    >>> y = np.array([1, 2, 3, 4, 5], dtype=np.float64)
    >>> y_hat = np.array([1.1, 1.9, 3.2, 3.8, 5.02])
    >>> result = msle(y, y_hat)
    >>> result
    <tf.Tensor: shape=(), dtype=float64, numpy=0.0015175569737783555>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İkili sınıflandırma problemleri için en yaygın kullanılan loss fonksiyonu "Binary Cross-Entropy (BCE)" denilen fonksiyondur. 
    Bu fonksiyonun sembolik gösterimi şöyledir:

    bce = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))

    Burada bir toplam teriminin olduğunu görüyorsunuz. Gerçek değerler 0 ya da 1 olduğuna göre bu toplam teriminin ya sol tarafı 
    ya da sağ tarafı 0 olacaktır.. Burada yapılmak istenen şey aslında isabet olasılığının logaritmalarının ortalamasının alınmasıdır. 
    Örneğin gerçek y değeri 0 olsun ve ağ da sigmoid çıktısından 0.1 elde etmiş olsun. Bu durumda toplam fadesinin sol tarafı 0,
    sağ tarafı ise log(0.9) olacaktır. Şimdi gerçek değerin 1 ancak ağın sigmoid çıktısından elde edilen değerim 0.9 olduğunu 
    düşününelim. Bu kez toplamın sağ tarafı 0, sol tarafı ise log(0.9) olacaktır. İşte fonksiyonda bu biçimde isabet olasılıklarının 
    logaritmalarının ortalaması bulunmaktadır. Örneğin:

    >>> y = np.array([1, 0, 1, 1, 0])
    >>> y_hat = np.array([0.9, 0.05, 0.095, 0.89, 0.111])
    >>> -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))
    0.5489448114302314

    Aynı işlemi tensorflow.keras.losses modülündeki binary_crossentropy isimli fonksiyonla da yapabiliriz:

    >>> y_hat = np.array([0.9, 0.05, 0.095, 0.89, 0.111])
    >>> result = binary_crossentropy(y, y_hat)
    >>> result
    <tf.Tensor: shape=(), dtype=float64, numpy=0.5489445126600796>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Cok sınıflı sınıflandırma problemleri için en yaygın kullanılan loss fonksiyonu ise "Categorical Cross-Entropy (CCE)" isimli 
    fonksiyondur. CCE fonksiyonu aslında BCE fonksiyonun çoklu biçimidir. Tabii CCE değerini hesaplayabilmek için ağın kategori 
    sayısı kadar çıktıya sahip olması ve bu çıktıların toplamının da 1 olması gerekmektedir. Başka bir deyişle ağın çıktı katmanındaki 
    nöronların aktivasyon fonksiyonları "softmax" olmalıdır. Ayrıca CCE çok sınıflı bir entropy hesabı yaptığına göre gerçek 
    değerlerin one-hot-encoding biçiminde kodlanmış olması gerekir. (Yani örneğin ileride göreceğimiz gibi K sınıflı bir sınıflandırma 
    problemi için biz ağa çıktı olarak "one-hot-encoding" kodlanmış K tane y değerini vermeliyiz.) K tane sınıf belirtem bir 
    satırın CCE değeri şöyle hesaplanır (tabii burada K tane "one-hot-encoding" edilmiş gerçek değer ile M tane softmax çıktı 
    değeri söz konusu olmalıdır):

    cce = -np.sum(yk * log(yk_hat))

    Burada yk one-hot-encoding yapılmış gerçek değerleri yk_hat ise softmax biçiminde elde edilmiş ağın çıktı katmanındaki 
    değerleri temsil etmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            30. Ders - 20/04/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi "metrik fonksiyonlar" her epoch'tan sonra sınama verlerine uygulanan ve eğitimin gidişatı hakkında bilgi 
    almak için kullanılan performans fonksiyonlar idi. Problemin türüne göre çeşitli metrik fonksiyonlar hazır biçimde 
    bulunmaktadır. Birden fazla metrik fonksiyon kullanılabileceği için metrik fonksiyonlar compile metodunda "metrics" parametresine 
    bir liste biçiminde girilmektedir. Metrik fonksiyonlar yazısal biçimde girilebilceği gibi tensorflow.keras.metrics modülündeki 
    fonksiyonlar ve sınıflar biçiminde de girilebilmektedir. 

    Metrik fonksiyonlar da tıpkı loss fonksiyonları gibi gerçek çıktı değerleriyle ağın ürettiği çıktı değerlerini parametre
    olarak almaktadır. Aslında loss fonksiyonları da bir çeşit metrik fonksiyonlardır. Ancak loss fonksiyonları optimizasyon
    algoritmaları tarafından minimize edilmek için kullanılmaktadır. Halbuki metrikm fonksiyonlar bizim durumu daha iyi anlamamız
    için bizim tarafımızdan kullanılmaktadır.

    Aslında loss fonksiyonları da bir çeşit metrik fonksiyonlar olarak kullanılabilir. Ancak Keras zaten bize loss fonksiyonlarının 
    değerlerini her epoch'ta eğitim ve sınama verileri için vermektedir. Dolayısıyla örneğin ikili sınıflandırma problemi için 
    eğer biz loss fonksiyonu olarak "binary_crossentropy" girmişsek ayrıca bu fonksiyonu metrik olarak girmenin bir anlamı yoktur. 
    Özetle er loss fonksiyonu bir metrik fonksiyon gibi de kullanılabilir. Ancak her metrik fonksiyon bir loss fonksiyonu olarak 
    kullanılmaz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    "binary_accuracy" isimli metrik fonksiyon ikili sınıflandırma problemleri için en yaygın kullanılan metrik fonksiyondur. 
    Bu fonksiyon kabaca kaç gözlemin değerinin kestirilen değerle aynı olduğunun yüzdesini vermektedir. Örneğin "diabetes.csv" 
    veri kümesinde "binary_accuracy" değerinin 0.70 olması demek her yüz ölçümün 70 tanesinin doğru biçimde kestirilmesi demektir. 
    "binary_accuracy" metrik değeri Keras'ta isimsel olarak girilebileceği gibi tensorflow.keras.metrics modülündeki fonksiyon
    ismi olarak da girilebilir. 

    Aslında Keras'ta programcı kendi loss fonksiyonlarını ve metrik fonksiyonlarını da yazabilir. Ancak tensorflow bu konuda 
    yeterli dokümantasyona sahip değildir. TensorFlow kütüphanesinin çeşitli versiyonlarında çeşitli farklılıklar bulunabilmektedir. 
    Bu nedenle bu fonksiyonların programcı tarafından yazılması için bu konuya dikkat etmek gerekir. 

    Örneğin biz tensorflow.keras.metrics modülündeki binary_accuracy fonksiyonunu aşağıdaki gibi kullanabiliriz.
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.metrics import binary_accuracy
import numpy as np

y = np.array([1, 0, 1, 1, 0])
y_hat = np.array([0.90, 0.7, 0.6, 0.9, 0.6])

result = binary_accuracy(y, y_hat)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    Çok sınıflı sınıflandırma problemlerinde tipik okullanılan metrik fonksiyon "categorical_accuracy" isimli fonksiyondur. 
    Bu fonksiyon da yine gözlemlerin yüzde kaçının tam olarak isabet ettirildiğini belirtmektedir. Örneğin ikili sınıflandırmada 
    0.50 değeri iyi bir değer değildir. Çünkü zaten rastgele seçim yapılsa bile ortalama 0.50 başarı elde edilmektedir. Ancak 
    100 sınıflı bir problemde 0.50 başarı düşük bir başarı olmayabilir. Yine biz Keras'ta "categorical_accuracy" metirk fonksiyonunu
    isimsel biçimde ya da tensorflow.keras.metrics modülündeki fonksiyon ismiyle kullanabiliriz. 

    tensorflow.keras.metrics modülündeki categorical_accuracy fonksiyonu aslında toplam isabetlere ilişkin bir Tensor nesnesi 
    vermektedir, ortalama vermemektedir. Aşağıda bu fonksiyonun kullanımına bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.metrics import categorical_accuracy
import numpy as np

# elma, armut, kayısı

y = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])
y_hat = np.array([[0.2, 0.7, 0.1], [0.2, 0.1, 0.7], [0.8, 0.1, 0.1], [0.7, 0.2, 0.1], [0.6, 0.2, 0.2]])

result = categorical_accuracy(y, y_hat) 
result_ratio = np.sum(result) / len(result)
print(result_ratio)

#----------------------------------------------------------------------------------------------------------------------------
    Regresyon problemleri için loss fonksiyonlarının çoğu aslında metrik fonksiyon olarak da kullanılabilmektedir. Örneğin biz 
    böyle bir problemde loss fonksiyonu olarak "mean_squared_error" seçmişsek metrik fonksiyon olarak "mean_absolute_error" 
    seçebiliriz. mean_absolute_error fonksiyonu loss fonksiyonu olarak o kadar iyi olmasa da metrik anlamda kişilerin kolay 
    anlayabileceği bir fonksiyondur. Benzer biçimde regresyon problemlerinde "mean_asbolute_percentage_error", 
    "mean_squared_logarithmic_error" fonksiyonları da metrik olarak kullanılabilmektedir. 

    Loss fonksiyonların metrik fonksiyonlar olarak kullanılabileceğini belirtmiştik. Aslında örneğin mean_absolute_error 
    loss fonksiyonu ile mean_absolute_error metrik fonksiyonu aynı işlemi yapmaktadır. Ancak Keras'ta bu fonksiyonlar 
    tensorflow.keras.losses ve tensorflow.keras.metrics modüllerinde ayrı ayrı bulundurulmuştur. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Metrik fonksiyonlar yazısal biçimde girilecekse onlar için uzun ya da kısa isimler kullanılabilmektedir. Örneğin:

    'binary_accuracy' (kısa ismi yok)
    'categorical_accuracy' (kısa ismi yok)
    'mean_absolute_error' (kısa ismi 'mae')
    'mean_absolute_percentage_error' (kısa ismi 'mape')
    'mean_squared_error' kısa ismi ('mse')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir sinir ağı eğitildikten sonra onun diskte saklanması gerekebilir. Çünkü eğitim uzun sürebilir ve her bilgisayarı açtığımızda
    ağı yeniden eğitmemiz verimsiz bir çalışma biçimidir. Ayrıca eğitim tek seferde de yapılmayabilir. Yani örneğin bir kısım 
    verilerle eğitim yapılıp sonuçlar saklanabilir. Sonra yeni veriler elde edildikçe eğitime devam edilebilir. 

    Keras'ta yapılan eğitimlerin saklanması demekle neyi kastediyoruz? Tabii öncelikle tüm nöronlardaki "w" ve "bias" değerleri 
    kastedilmektedir. Ayrıca Keras bize tüm modeli saklama imkanı da vermektedir. Bu durumda modelin yeniden kurulmasına da 
    gerek kalmaz. Tüm model katmanlarıyla "w" ve "bias" değerleriyle saklanıp geri yüklenebilmektedir. 

    Sinir ağı modelini saklamak için hangi dosya formatı uygundur? Çok fazla veri söz konusu olduğu için buna uygun tasarımı 
    olan dosya formatları tercih edilmelidir. Örneğin bu işlem için "CSV" dosyaları hiç uygun değildir. İşte bu tür amaçlar için 
    ilk akla gelen format "HDF (Hieararchical Data Format)" denilen formattır. Bu formatın beşinci versiyonu HDF5 ya da H5 formatı 
    olarak bilinmektedir. 

    Modeli bir bütün olarak saklamak için Sequential sınıfının save isimli metodu kullanılır. save metodunun birinci parametresi 
    dosyanın yol ifadesini almaktadır. save_format parametresi saklanacak dosyanın formatını belirtir. Bu parametre girilmezse 
    dosya TensorFlow kütüphanesinin kullandığı "tf" formatı ile saklanmaktadır. Biz HDF5 formatı için bu parametreye 'h5' 
    girmeliyiz. Örneğin:

    model.save('diabetes.h5', save_format='h5')

    Aşağıdaki örnekte "diabetes" örneği eğitildikten sonra save metodu ile saklanmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
model.save('diabetes.h5', save_format='h5')

#----------------------------------------------------------------------------------------------------------------------------
    HDF5 formatıyla sakladığımız model bir bütün olarak tensorflow.keras.models modülündeki load_model fonksiyonu ile geri 
    yüklenebilir. load_model fonksiyonu bizden yüklenecek dosyanın yol ifadesini alır. Fonksiyon geri dönüş değeri olarak model 
    nesnesini mektedir. Örneğin:

    from tensorflow.keras.models import load_model

    model = load_model('diabetes.h5')

    Artık biz modeli fit ettiğimiz biçimiyle tamamen geri almış durumdayız. Doğurdan predict metoduyla kestirim yapabiliriz. 

    Aşağıdaki örnekte yukarıda save ettiğimiz model yüklenerek kestirim işlemi yapılmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.models import load_model
import numpy as np

model = load_model('diabetes.h5')

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

predict_result = model.predict(predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Aslında modelin tamamını değil yalnızca "w" ve "bias" değerlerini save etmek de mümkündür. Bunun için Sequential sınıfının 
    save_weights metodu kullanılmaktadır. Örneğin:

    model.save_weights('diabetes-weights', save_format='h5')

    Aşağıdaki örnekte "diabetes" örneğindeki yalnızca modelin "w" ve "bias" değerleri saklanmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')
model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
model.save_weights('diabetes-weights.h5', save_format='h5')

#----------------------------------------------------------------------------------------------------------------------------
    save_weights metodu yalnızca "w" ve "bias" değerlerini save etmektedir. Bizim bunu geri yükleyebilmemiz için modeli yeniden 
    aynı biçimde oluşturmamız ve compile işlemini yapmamız gerekir. "w" ve "bias" değerlerinin geri yüklenmesi için Sequential 
    sınıfının load_weights metodu kullanılmaktadır. Örneğin:

    model.load_weights('diabetes-weights.h5')

    Aşağıdaki örnekte yukarıdaki örnekteki modelin "w" ve "bias" değerleri geri yüklenmiştir. Tabii yukarıda da belirttiğimiz 
    gibi save_weights model bilgilerini saklamadığı için modelin aynı biçimde yeniden oluşturulması gerekmektedir. Modelin 
    "w" ve "bias" değerlerini load_weights metodu ile geri yüklerken veri kümesini oluşturmamız gerekmemektedir. save_weights"
    metodu model yalnızca "w" ve "bias" değerlerini save ettiği için model programcı tarafından orijinal haliyle oluşturulmalıdır.
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((8,)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

model.load_weights('diabetes-weights.h5')

import numpy as np  

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

predict_result = model.predict(predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Biz katman nesnelerini (Dense nesnelerini) model sınıfının (Sequential sınıfının) add metotlarıyla modelimize ekledik. 
    Bu katman nesnelerini ileride kullanmak istediğimizd enasıl geri alabiliriz? Tabii ilk akla gelen yöntem katman nesnelerini 
    yaratırken aynı zamanda saklamak olabilir. Örneğin:

    model = Sequential(name='Diabetes')

    model.add(Input((8,)))
    layer1 = Dense(16, activation='relu', name='Hidden-1')
    model.add(layer1)
    layer2 = Dense(16, activation='relu', name='Hidden-2')
    model.add(layer2)
    layer3 = Dense(1, activation='sigmoid', name='Output')
    model.add(layer3)
    model.summary()

    Aslında böyle saklama işlemine gerek yoktur. Zaten model nesnesinin (Sequential sınıfının) layers isimli özniteliği bir 
    indeks eşliğinde bizim eklediğimiz katman nesnelerini bize vermektedir. layers örnek özniteliği bir Python listesidir.
    Örneğin:

    layer2 = model.layers[2]

    Tabii layers özniteliği bize Input katmanını gereksiz olduğundan dolayı vermemktedir. Buradaki 0'ıncı indeks ilk saklı 
    katmanı belirtmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        31. Ders - 21/04/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda tüm modeli, modelin "w" ve "bias" değerlerini saklayıp geri yükledik. Peki yalnızca tek bir katmandaki 
    ağırlık değerlerini alıp geri yükleyebilir miyiz? İşte bu işlem katman sınıfının (yani Dense sınıfının) get_weights  ve
    set_weights isimli metotları ile yapılmaktadır. Biz bu metotlar sayesinde bir katman nesnesindeki "w" ve "bias" değerlerini 
    NumPy dizisi olarak elde edip geri yükleyebiliriz. 

    Dense sınıfının get_weights metodu iki elemanlı bir listeye geri dönmektedir. Bu listenin her iki elemanı da NumPy dizisidir. 
    Listenin ilk elemanı (0'ıncı indeksli elemanı) o katmandaki nöronların "w" değerlerini, ikinci elemanı ise "bias" değerlerini 
    belirtmektedir. Katmandaki nöronların "w" değerleri iki boyutlu bir NumPy dizisi biçimindedir. Burada k'ıncı sütun önceki 
    katmanın nöronlarının sonraki katmanın k'ıncı nöronuna bağlanmasındaki ağırlık değerlerini belirtmektedir. Benzer biçimde 
    i'inci satır ise önceki katmanın i'inci nöronunun ilgili katmanın nöron bağlantısındaki ağırlık değerlerini belirtmektedir.
    Bu durumda örneğin [i, k] indeksindeki eleman önceki katmanın i'inci nörounun ilgili katmanın k'ıncı nöronu ile bağlantısındaki
    ağırlığı belirtmektedir. 

    get_weights metodunun verdiği listenin ikinci elemanı (1'inci indeksli elemanı) nöronların "bias" değerlerini vermektedir. 
    Bias değerlerinin ilgili katmandaki nöron sayısı kadar olması gerektiğine dikkat ediniz. Çünkü her nöron için bir tane bias
    değeri vardır. 

    Örneğin yukarıdaki "diabetes" örneğinde ilk saklı katmandaki ağırlıkları şöyle elde edebiliriz:

    layer = model.layers[0]
    weights, bias = layer.get_weights()

    Burada weights dizisinin shape'i yazdırıldığında (8, 16) görülecektir. bias dizisinin shape'i yazdırıldığında ise (16,)
    görülecektir.

    Aşağıdaki "diabetes" örneğindeki ilk saklı katmanın "w" ve "bias" değerleri görüntülenmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
hidden1 = model.layers[0]
weights, bias = hidden1.get_weights()

print('Weights')
print(weights)

print('Bias')
print(bias)

print("5'inci girdi nöronunun ilk katmanın 9'uncu nöronununa bağlantısındaki w değeri")
w = weights[5, 9]
print(w)

#----------------------------------------------------------------------------------------------------------------------------
    Bir katmandaki "w" ve "bias" değerlerini Dense sınıfının set_weights metodu ile geri yükleyebiliriz. Örneğin:
    
    hidden1 = model.layers[0]
    weights, bias = hidden1.get_weights()

    weights = weights + 0.1
    hidden1.set_weights([weights, bias])

    Burada birinci saklı katmandaki ağırlık değerlerine 0.1 eklenerek ağırlık değerleri geri yüklenmiştir. set_weights 
    metodunun iki elemanı bir liste aldığına dikkat ediniz. Nasıl get_weights metodu hem "w" hem de "bias" değerlerini 
    veriyorsa set_weights metodu da hem "w" hem de "bias" değerlerini istemektedir. 

    Aşağıdaki örnekte ilk saklı katmandaki "w" değerlerine 0.1 toplanarak değerler geri yüknemiştir. (Bu işlemde bir 
    anlam aramayınız).
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
hidden1 = model.layers[0]
weights, bias = hidden1.get_weights()

weights = weights + 0.1
hidden1.set_weights([weights, bias])

#----------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağları ile kestirim yapabilmek için bazı aşamalardan geçmek gerekir. Bu aşamaları şöyle özetleyebiliriz:

    1) Hedefin Belirlenmesi: Öncelikle uygulamacının ne yapmak istediğine karar vermesi gerekir. Yani uygulamacının çözmek 
    istediği problem nedir? Bir sınıflandırma problemi midir? Regresyon problemi midir? Şekil tanıma problemi midir? Doğal 
    dili anlama problemi midir? gibi...

    2) Kestirimle İlgili Olacak Özellerin (Sütunların) Belirlenmesi: Tespit edilen problemin kestirimi için hangi bilgilere 
    gereksinim duyulmaktadır? Kestirim ile ilgili olabilecek özellikler nelerdir? Örneğin bir eczanenin cirosunu tahmin etmek 
    isteyelim. Burada ilk gelecek özellikler şunlar olabilir:

    - Eczanenin konumu
    - Ecnanenin önünden geçen günlük insan sayısı
    - Eczanenin destek ürünleri satıp satmadığı
    - Eczanenin kozmetik ürünler satıp satmadığı
    - Eczanenin anlaşmalı olduğu kurumlar
    - Eczanenin büyüklüğü
    - Eczanenin yakınındaki, hastenelerin ve sağlık ocaklarının sayısı
    - Eczacının tanıdığı doktor sayısı
    
    3) Eğitim İçin Verilerin Toplanması: Eğitim için verilerin toplanması en zor süreçlerden biridir. Veri toplama için şu 
    yöntemler söz konusu olabilir:

    - Anketler (Surveys)
    - Daha önce elde edilmiş veriler
    - Çeşitli kurumlar tarafından zaten elde edilmiş olan veriler
    - Sensörler yoluyla elde edilen veriler
    - Sosyal ağlardan elde edilen veriler
    - Birtakım doğal süreç içerisinde oluşan veriler (Örneğin her müşteri için bir fiş kesildiğine göre bunlar kullanılabilir)
    
    4) Verilerin Kullanıma Hazır Hale Getirilmesi: Veriler toplandıktan sonra bunların üzerinde bazı önişlemlerin yapılması 
    gerekmektedir. Örneğin gereksiz sütunlar atılmalıdır. Eksik veriler varsa bunlar bir biçimde ele alınmalıdır. Kategorik 
    veriler sayısallaştırılmalıdır. Text ve görüntü verileri kullanıma hazır hale getirilmelidir. Özellik "ölçeklemeleri 
    (feature scaling)" ve "özellik mühendisliği (feature engineering)" işlemleri yapılmalıdır. 

    5) Yapay Sinir Ağı Modelinin Oluşturulması: Probleme uygun bir yapay sinir ağı modeli oluşturulmalıdır. 

    6) Modelin Eğitilmesi ve Test Edilmesi: Oluşturulan model eldeki veri kümesiyle eğitilmeli ve test edilmelidir. 

    7) Kestirim İşleminin Yapılması: Nihayet eğtilmiş model artık kestirim amacıyla kullanılmalıdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Callback sözcüğü programlamada belli bir olay devam ederken programcının verdiği bir fonksiyonun (genel olarak callable 
    bir nesnenin) çağrılmasına" ilişkin mekanizmayı anlatmak için kullanılmaktadır. Keras'ın da bir callback mekanizması vardır. 
    Bu sayede biz çeşitli olaylar devam ederken bu olayları sağlayan metotların bizim callable nesnelerimizi çağırmasını 
    sağlayabiliriz. Böylece birtakım işlemler devam ederken arka planda o işlemleri programlama yoluyla izleyebilir duruma göre 
    gerekli işlemleri yapabiliriz. 

    Sequential sınıfının fit, evalaute ve predict metotları "callbacks" isimli bir parametre almaktadır. İşte biz bu parametreye 
    callback fonksiyonlarımızı ve sınıf nesnelerimizi verebiliriz. Bu metotlar da ilgili olaylar sırasında bizim verdiğimiz 
    bu callable nesneleri çağırır. 

    Aslında Keras'ta hazır bazı callback sınıflar zaten vardır. Dolayısıyla her ne kadar programcı kendi callback sınıflarını 
    yazabilirse de aslında buna fazlaca gereksinim duyulmamaktadır. Keras'ın sağladığı hazır callback sınıfları genellikle 
    gereksinimi karşılamaktadır. Keras'ın hazır callback sınıfları tensorflow.keras.callbacks modülü içerisinde bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    En sık kullanılan callback sınıfı History isimli callback sınıftır. Aslında programcı bu callback sınıfını genellikle kendisi 
    kullanmaz. Sequential sınıfının fit metodu zaten bu sınıf türünden bir nesneye geri dönmektedir. Örneğin:

    hist = model.fit(....)

    History callback sınıfı aslında işlemler sırasında devreye girmek için değil (bu da sağlanabilir) epcoh'lar sırasındaki 
    değerlerin kaydedilmesi için kullanılmaktadır. Yani programcı fit işlemi bittikten sonra bu callback nesnenin içerisinden
    fit işlemi sırasında elde edilen epoch değerlerini alabilmektedir. Dolayısıyla fit metodunun bize verdiği History nesnesi 
    eğitim sırasında her epoch'tan sonra elde edilen loss ve metrik değerleri barındırmaktadır. Anımsanacağı gibi fit metodu 
    zaten eğitim sırasında her epoch'tan sonra birtakım değerleri ekrana yazıyordu. İşte fit metodunun geri döndürdüğü bu history 
    nesnesi aslında bu metodun ekrana yazdığı bilgileri barındırmaktadır. History nesnesinin epoch özniteliği uygulanan epoch 
    numaralarını bize verir. Ancak nesnenin en önemli elemanı history isimli isimli özniteliğidir. Nesnenin history isimli özniteliği 
    bir sözlük türündendir. Sözlüğün anahtarları yazısal olarak loss ve metrik değer isimlerini barındırır. Bunlara karşı gelen 
    değerler ise her epoch'taki ilgili değerleri belirten list türünden nesnelerdir. History nesnesinin history sözlüğü her zaman 
    "loss" ve "val_loss" anahtarlarını barındırır. Bunun dışında bizim belirlediğimiz metriklere ilişkin eğitim ve sınama sonuçlarını 
    da barındırmaktadır. Örneğin biz metrik olarak "binary_accuracy" girmiş olalım. history sözlüğü bu durumda "binary_accuracy" ve 
    "val_binary_accuracy" isimli iki anahtara da sahip olacaktır. Burada "val_xxx" biçiminde "val" ile başlayan anahtarlar sınama 
    verisinden elde edilen değerleri "val" ile başlamayan anahtarlar ise eğitim veri kümesinden elde edilen değerleri belirtir. 
    "loss" değeri ve diğer metrik değerler epoch'un tamamı için elde edilen değerlerdir. Her epoch sonucunda bu değerler sıfırlanmaktadır. 
    (Yani bu değerler kümülatif bir ortalama değil, her epoch'taki ortalamalara ilişkindir.)

    Epoch'lar sonrasında History nesnesi yoluyla elde edilen bilgilerin grafiği çizdirilebilir. Uygulamacılar eğitimin gidişatı 
    hakkında fikir edinebilmek için genellikle epoch grafiğini çizdirirler. Epoch sayıları arttıkça başarının artacağını söyleyemeyiz. 
    Hatta tam tersine belli bir epoch'tan sonra "overfitting" denilen olgu kendini gösterebilmekte model gitgide yanlış şeyleri 
    öğrenir duruma gelebilmektedir. İşte uygulamacı gelellikle "loss", "val_loss", "binary_accuracy", "val_binary_accuracy" gibi 
    grafikleri epoch sayılarına göre çizerek bunların uyumlu gidip gitmediğine bakabilir. Eğitimdeki verilerle sınama verilerinin
    birbirbirlerinden kopması genel olarak kötü bir gidişata işaret etmektedir. Uygulamacı bu grafiklere bakarak uygulaması 
    gereken epoch sayısına karar verebilir. Örneğin:

    import matplotlib.pyplot as plt

    plt.figure(figsize=(14, 6))
    plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
    plt.xticks(range(0, 300, 10))
    plt.plot(hist.epoch, hist.history['loss'])
    plt.plot(hist.epoch, hist.history['val_loss'])
    plt.legend(['Loss', 'Validation Loss'])
    plt.show()

    plt.figure(figsize=(14, 6))
    plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
    plt.xticks(range(0, 300, 10))
    plt.plot(hist.epoch, hist.history['binary_accuracy'])
    plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
    plt.legend(['Accuracy', 'Validation Accuracy'])
    plt.show()

    Aşağıda "diabetes" örneği için fit metodunun geri döndürdüğü History callback nesnesi kullanılarak epoch grafikleri 
    çizdirilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
#----------------------------------------------------------------------------------------------------------------------------
    Aslında History callback nesnesi fit metodunun callbacks parametresi yoluyla da elde edilebilir. Örneğin:

    from tensorflow.keras.callbacks import History
    hist = History()

    model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, validation_split=0.2, callbacks=[hist])

    Tabii buna hiç gerek yoktur. Zaten bu History callback nesnesi fit metodu tarafından metodun içerisinde oluşturulup geri
    dönüş değeri yoluyla bize verilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    CSVLogger isimli callback sınıfı epoch işlemlerinden elde edilen değerleri bir CSV dosyasının içerisine yazmaktadır. 
    CSVLogger nesnesi yaratılırken __init__ metodunda CSV dosyasının yol ifadesi verilir. Eğitim bittiğinde bu dosaynın içi 
    doldurulmuş olacaktır. Örneğin:

    from tensorflow.keras.callbacks import CSVLogger

    hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2, 
            callbacks=[CSVLogger('diabtes-epoch.csv')])

    Aşağıdaki "diabetes" örneği için CSVLogger callback sınıfı yoluyla bir CSV dosyası yaratılmıştır.
#----------------------------------------------------------------------------------------------------------------------------

iimport pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import CSVLogger
csv_logger = CSVLogger('diabetes-epoch-logs.csv')

hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, validation_split=0.2, callbacks=[csv_logger])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
                                                32. Ders - 27/04/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Daha önce de belirttiğimiz gibi tüm callback sınıfları tensorflow.keras.callbacks modülündeki Callback isimli sınıftan 
    türetilmiştir. Biz de bu sınıftan türetme yaparak kendi callback sınıflarımızı yazabiliriz. fit, evaluate ve predict metotları
    callbacks parametresine girilen callback nesnelerinin çeşitli metotlarını çeşitli olaylar sırasında çağırmaktadır. Programcı 
    da kendi callback sınıflarını yazarken aslında bu taban Callback sınıfındaki metotları override eder. Örneğin her epoch 
    bittiğinde Callback sınıfının on_epoch_end isimli metodu çağrılmaktadır. Callback sınıfının bu metodunun içi boştur. Ancak 
    biz türemiş sınıfta bu metodu overide edersek (yani aynı isimle yeniden yazarsak) bizim override ettiğimiz metot devreye 
    girecektir. on_epoch_end metodunun parametrik yapısı şöyle olmalıdır:

    def on_epoch_end(epoch, logs):
        pass

    Buradaki birinci parametre epoch numarasını (yani kaçıncı epoch olduğunu) ikinci parametre ise epoch sonrasındaki eğitim 
    ve sınama işlemlerinden elde edilen loss ve metrik değerleri veren bir sözlük biçimindedir. Bu sözlüğün anahtarları ilgili 
    değerleri belirten yazılardan değerleri de o anahtara ilişkin değerlerinden oluşmaktadır. 

    Örneğin her epoch sonrasında biz bir fonksiyonumuzun çağrılmasını isteyelim. Bu fonksiyon içerisinde de "loss" değerini ve
    "val_loss" değerini ekrana yazdırmak isteyelim. Bu işlemi şöyle yapabiliriz:

    class MyCallback(Callback):
        def on_epoch_end(self, epoch, logs):
            loss = logs['loss']
            val_loss = logs['val_loss']
            print(f'epoch: {epoch}, loss: {loss}, val_loss: {val_loss}')
 
    mycallback = MyCallback()
    hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, 
                    validation_split=0.2, callbacks=[mycallback], verbose=0)

    Burada fit metodunun verbose parametresine 0 değerini geçtik. fit metodu (evaluate ve predict metotlarında da aynı 
    durum söz konusu) çalışırken zaten "loss" ve "metik değerleri" ekrana yazdırmaktadır. verbose parametre için 0 girildiğinde 
    artık fit metodu ekrana bir şey yazmamaktadır. Dolayısıyşa yalnızca bizim callback fonksiyonda ekrana yazdırdıklarımız 
    ekranda görünecektir. 

    fit, evaluate ve predict tarafından çağrılan Callback sınıfının metotlarının en önemli olanları şunlardır:
    
    on_epoch_begin
    on_epoch_end
    on_batch_begin
    on_batch_end
    on_train_begin
    on_train_end

    Tabii bir sınıf söz konusu olduğuna göre bu metotların birinci parametreleri self olacaktır. Bu metotların diğer parametreleri 
    aşağıdaki gibidir:

    Metot                    Parametreler

    on_epoch_begin	            self, epoch ve logs
    on_epoch_end	            self, epoch ve logs
    on_batch_begin	            self, batch ve logs
    on_batch_end	            self, batch ve logs
    on_train_begin 	            self, logs
    on_train_end	            self, logs

    Aşağıdaki örnekte Callback sınıfından MyCallback isimli bir sınıf türetilmiştir. Bu sınıfta on_epch_end metodu override 
    edilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras.callbacks import Callback

class MyCallback(Callback):
    def on_epoch_end(self, epoch, logs):
        loss = logs['loss']
        val_loss = logs['val_loss']
        print(f'epoch: {epoch}, loss: {loss}, val_loss: {val_loss}')

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

mycallback = MyCallback()

hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, validation_split=0.2, callbacks=[mycallback], verbose=0)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
    LambdaCallback isimli callback sınıfı bizden __init__ metodu yoluyla çeşitli fonksiyonlar alır ve bu fonksiyonları belli 
    noktalarda çağırır. Metottaki parametreler belli olaylar gerçekleştiğinde çağrılacak fonksiyonları belirtmektedir. Parametrelerin 
    anlamları şöyledir:

    on_train_begin: Eğitim başladığında çağrılacak fonksiyonu belirtir. 
    on_train_end: Eğitim bittiğinde çağrılacak fonksiyonu belirtir. 
    on_epoch_begin: Her epoch başladığında çağrılacak fonksiyonu belirtir.
    on_epoch_end: Her epoch bittiğinde çağrılacak fonksiyonu belirtir.
    on_batch_begin: Her batch işleminin başında çağrılacak fonksiyonu belirtir.
    on_batch_end: Her batch işlemi bittiğinde çağrılacak fonksiyonu belirtir. 

    Bu fonksiyonların parametreleri şöyle olmalıdır:

      Fonksiyon                 Parametreler

    on_epoch_begin	            epoch ve logs
    on_epoch_end	            epoch ve logs
    on_batch_begin	            batch ve logs
    on_batch_end	            batch ve logs
    on_train_begin 	            logs
    on_train_end	            logs
   
    Burada epoch parametresi epoch numarasını, batch parametresi batch numarasını belirtir. los parametreleri ise birer sözlük 
    belirtmektedir. Bu sözlüğün içerisinde loss değeri gibi metrik değerler gibi önemli bilgiler vardır. epoch'lar için logs 
    parametresi History nesnesindeki anahtarları içermektedir. Ancak batch'ler için logs parametresi "val" önekli değerleri 
    içermeyecektir. Örneğin biz her epoch bittiğinde, her batch başladığında ve bittiğinde bir fonksiyonumuzun çağrılmasını 
    isteyelim. Bunu şöyle gerçekleştirebiliriz:

    def on_epoch_end_proc(epoch, logs):
       pass
    
    def on_batch_begin_proc(batch, logs):
       pass

    def on_batch_end_proc(batch, logs):
        pass

    from tensorflow.keras.callbacks import LambdaCallback

    lambda_callback = LambdaCallback(on_epoch_end=on_epoch_end_proc, on_batch_begin=on_batch_begin_proc, on_batch_end=on_batch_end_proc)

    hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, 
                validation_split=0.2, callbacks=[lambda_callback], verbose=0)

    Aşağıdaki örnekte her epoch sonrasında "loss" ve "val_loss" değerleri callback fonksiyonda yazdırılmıştır. Ayrıca her 
    epoch içerisindeki batch işlemlerinin sonucunda elde edilen "loss" değerleri de ekrana yazdırılmıştır. Aynı zamanda bu 
    örnekte batch'lerdeki loss değerleri bir listede saklanmış ve bu loss değerlerinin ortalaması da ekrana yazdırılmıştır. 
    fit metodu tarafından çağrılan on_epoch_end metodundaki "loss" değeri son batch'teki "loss" değeri olarak verilmektedir. 
    Ancak HistoryCallback (ve fit metodunun ekrana yazdırdığı) epoch'a ilişkin "loss" değeri epoch içerisindeki batch'lerdeki 
    ortalama "loss" değeridir. Maalesef Keras'taki bu değerlerin nasıl elde edildiği dokümanlarda ayrıntılı biçimde açıklanmamıştır. 
    Dokümanlarda açıklanmayan özelliklerin zaman içerisinde değiştirilebileceğine dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

import numpy as np

batch_losses= []

def on_epoch_begin_proc(epoch, logs):
    global batch_losses
    batch_losses = []
    print(f'eopch: {epoch}')    

def on_epoch_end_proc(epoch, logs):
    loss = logs['loss']
    val_loss = logs['val_loss']
    print(f'\nepoch: {epoch}, loss: {loss}, val_loss: {val_loss}')
    print(f'batch mean: {np.mean(batch_losses)}')
    print('-' * 30)
  
def on_batch_end_proc(batch, logs):
    global total
    loss = logs['loss']
    batch_losses.append(loss)
    print(f'\t\tbatch: {batch}, loss: {loss}')
    
from tensorflow.keras.callbacks import LambdaCallback

lambda_callback = LambdaCallback(on_epoch_begin=on_epoch_begin_proc, on_epoch_end=on_epoch_end_proc,  on_batch_end=on_batch_end_proc)

hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, 
        validation_split=0.2, callbacks=[lambda_callback], verbose='auto')

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
    LambdaCallback sınıfını aslında basit bir biçimde biz de yazabiliriz. Burada yapacağımız şey sınıfın __init__ metodunda 
    bize verilen fonksiyonları nesnenin özniteliklerinde saklamak ve override ettiğimiz metotlarda bunları çağırmaktır. Aşağıda
    LambdaCallback sınıfının nasıl yazıldığına ilişkin bir örnek verilmiştir. Bu örnekteki MyLambdaCallback sınıfı orijinal 
    LambdaCallback sınıfının aynısı değildir. Bu örnekte yalnızca bu sınıfın nasıl yazıldığına ilişkin bir fikir vermeye 
    çalışıyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import Callback

class MyLambdaCallback(Callback):
    def __init__(self, on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None,  on_batch_end=None):
        self._on_epoch_begin = on_epoch_begin
        self._on_epoch_end = on_epoch_end
        self._on_batch_begin = on_batch_begin
        self._on_batch_end = on_batch_end
        
    def on_epoch_begin(self, epoch, logs):
        if self._on_epoch_begin:
            self._on_epoch_begin(epoch, logs)
    
    def on_epoch_end(self, epoch, logs):
        if self._on_epoch_end:
            self._on_epoch_end(epoch, logs)
    
    def on_batch_begin(self, batch, logs):
        if self._on_batch_begin:
            self._on_batch_begin(batch, logs)
                
    def on_batch_end(self, batch, logs):
        if self._on_batch_end:
            self._on_batch_end(batch, logs)

import numpy as np

batch_losses= []

def on_epoch_begin_proc(epoch, logs):
    global batch_losses
    batch_losses = []
    print(f'eopch: {epoch}')    

def on_epoch_end_proc(epoch, logs):
    loss = logs['loss']
    val_loss = logs['val_loss']
    print(f'\nepoch: {epoch}, loss: {loss}, val_loss: {val_loss}')
    print(f'batch mean: {np.mean(batch_losses)}')
    print('-' * 30)
  
def on_batch_end_proc(batch, logs):
    global total
    loss = logs['loss']
    batch_losses.append(loss)
    print(f'\t\tbatch: {batch}, loss: {loss}')

mylambda_callback = MyLambdaCallback(on_epoch_begin=on_epoch_begin_proc, on_epoch_end=on_epoch_end_proc, on_batch_end=on_batch_end_proc)

hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2, callbacks=[mylambda_callback], verbose=0)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
    Bir nörona giren değerlerin "w" değerleriyle çarpılıp toplandığını (dot-product) ve sonuca bias değerinin toplanarak aktivasyon 
    fonksiyonuna sokulduğunu biliyoruz. Örneğin modelde girdi katmanında x1, x2 ve x3 olmak üzere üç "sütun (feature)" bulunuyor 
    olsun. Bu girdiler ilk saklı katmana sokulduğunda w1x1 + w2x2 + w3x3 + bias biçiminde bir toplam elde edilecektir. Bu 
    toplam da aktivasyon fonksiyonuna sokulacaktır. İşte bu "dot product" işleminde x1, x2, x3 sütunlarının mertebeleri brbirlerinden 
    çok farklıysa mertebesi yüksek olan sütunun "dot product" etkisi yüksek olacaktır. Bu durum da sanki o sütunun daha önemli 
    olarak değerlendirilmesine yol açacaktır. Bu biçimdeki bir sinir ağı "geç yakınsar" ve gücü kestirim bakımından zayıflar. 
    İşte bu nedenden dolayı işin başında sütunların (yani özelliklerin) mertebelerininin birbirlerine yaklaştırılması gerekmektedir. 
    Bu işlemlere "özellik ölçeklemesi (feature scaling)" denilmektedir. Yapay sinir ağlarında sütunlar arasında mertebe farklılıkları 
    varsa mutlaka özellik ölçeklemesi yapılmalıdır. 
    
    Özellik ölçeklemesi makine öğrenmesinde başka konularda da gerekebilmektedir. Ancak bazı konularda ise gerekmemektedir. Biz 
    kursumuzda her konuda özellik ölçeklemesinin gerekli olup olmadığını açıklayacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Çeşitli özellik ölçeklemesi yöntemleri vardır. Veri kümelerinin dağılımına ve kullanılan yöntemlere göre değişik özellik
    ölçeklendirmeleri diğerlerine göre avantaj sağlayabilmektedir. En çok kullanılan iki özellik ölçeklendirmesi yöntemi 
   " standart ölçekleme (standard scaling)" ve "minmax ölçeklemesi (minmax scaling)" dir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Özellik ölçeklemesi konusunda aşağıdaki sorular sıkça sorulmaktadır:

    Soru: Yapay sinir ağlarında özellik ölçeklemesi her zaman gerekir mi? 
    Cavap: Eğer sütunlar metrebe olarak birbirlerine zaten yakınsa özellik ölçeklemesi yapılmayabilir. 

    Soru: Gerektiği halde özellik ölçeklemesini yapmazsak ne olur?
    Cevap: Modelin kestirim gücü azalır. Yani performans düşer.

    Soru: Özellik ölçeklemesi gerekmediği halde özellik ölçeklemesi yaparsak bunun bir zararı dıkunur mu?
    Cevap: Hayır dokunmaz.

    Soru: Kategorik sütunlara (0 ve 1'lerden oluşan ya da one-hot-encoding yapılmış) özellik ölçeklemesi uygulayabilir miyiz?
    Cevap: Bu sütunlara özellik ölçeklemesi uygulanmayabilir. Ancak uygulamanın bir sakıncası olmaz. Genellikle veri bilimcisi
    tüm sütunlara özellik ölçeklemesi uyguladığı için bunlara da uygulamaktadır. 

    Özellik ölçeklemesi yalnızca x verilerine uygulanmalıdır, y verilerine özellik ölçeklemesi uygulamanın genel olarak faydası 
    ve anlamı yoktur. Ayrıca bir nokta önemlidir: Biz ağımızı nasıl eğitmişsek öyle test ve kestirim yapmalıyız. Yani eğı 
    eğitmeden önce özellik ölçeklemesi yapmışsak test işleminden önce test verilerini de kestirim işleminden önce kestirim 
    verilerini de aynı biçimde ölçeklendirip işleme sokmalıyız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                33. Ders - 28/04/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    En çok kullanılan özellik ölçeklendirmesi yöntemlerinden biri "standart ölçekleme (standard scaling)" yöntemidir. Bu yöntemde
    sütunlar diğerlerden bağımsız olarak kendi aralarında standart normal dağılıma uydurulmaktadır. Bu işlem şöyle yapılmaktadır:

    result = (x - mean(x)) / std(x)

    Tabii burada biz formülü temsili kod (pseudo code) olarak verdik. Burdaki "mean" sütunun ortalamasını "std" ise standart 
    sapmasını belirtmektedir. Sütunu bu biçimde ölçeklendirdiğimizde değerler büyük ölçüde 0'ın etrafında toplanır. Standart 
    ölçeklemenin değerleri standart normal dağılma uydurmaya çalıştığına dikkat ediniz. 

    Standart ölçeklemeye "standardizasyon (standardization)" da denilmektedir. Standart ölçekleme aşırı uçtaki değerlerden (outliers) 
    olumsuz bir biçimde etkilenme eğilimindedir. Veri bilimcileri genellikle standart ölçeklemeyi default ölçekleme olarak 
    kullanmaktadır. Bir NumPy dizisindeki sütunları aşağıdaki gibi bir fonksiyonla standart ölçeklemeye sokabiliriz.

    Standart ölçekleme yapan bir fonksiyonu aşağıdaki gibi yazabiliriz:

    def standard_scaler(dataset):
        scaled_dataset = np.zeros(dataset.shape)
        for col in range(dataset.shape[1]):
            scaled_dataset[:, col] = (dataset[:, col] - np.mean(dataset[:, col])) / np.std(dataset[:, col])
        return scaled_dataset

    Tabii aslında NumPy'ın eksensel işlem yapma özelliğinden faydalanarak yukarıdaki işlemi aşağıdaki gibi tek satırla 
    da yapabiliriz:
    
    def standard_scaler(dataset):
        return (dataset - np.mean(dataset, axis=0)) / np.std(dataset, axis=0)

    Aşağıda standart ölçeklemeye ilişkin bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------
import numpy as np

data= np.array([4, 7, 1, 90, 45, 70, 23, 12, 6, 9, 45, 82, 65])
mean = np.mean(data)
std = np.std(data)
print(f'Mean: {mean}, Standard Deviation: {std}')

result = (data - mean) / std

print(f'Data: {data}')
print(f'Scaled Data: {result}')

print('--------------------------------')

dataset = np.array([[1, 2, 3], [2, 1, 4], [7, 3, 8], [8, 9, 2], [20, 12, 3]])

def standard_scaler(dataset):
    scaled_dataset = np.zeros(dataset.shape)
    for col in range(dataset.shape[1]):
        scaled_dataset[:, col] = (dataset[:, col] - np.mean(dataset[:, col])) / np.std(dataset[:, col])
    return scaled_dataset

result = standard_scaler(dataset)     
print(dataset)
print(result)
       
#----------------------------------------------------------------------------------------------------------------------------
    Asında scikit-learn kütüphanesinde sklearn.preprocessing modülü içerisinde zaten standart ölçekleme yapan StandardScaler 
    isimli bir sınıf vardır. Bu sınıf diğer scikit-learn sınıfları gibi kullanılmaktadır. Yani önce StandardScaler sınıfı 
    türünden bir nesne yaratılır. Sonra bu nesne ile fit ve transform metotları çağrılır. Tabii fit ve transform metotlarında
    aynı veri kümesi kullanılacaksa bu işlem tek hamlede fit_transform metoduyla yapılabilir. Örneğin:

    from sklearn.preprocessing import StandardScaler

    ss = StandardScaler()
    ss.fit(dataset)
    scaled_dataset = ss.tranform(dataset)

    fit işlemi sütunların ortalamasını ve standart sapmasını nesnenin içerisinde saklamaktadır. transform işleminde bu bilgiler
    kullanılmaktadır. fit işleminden sonra nesnenin özniteliklerinden sütunlara ilişkin bu bilgiler elde edilebilir. Örneğin
    fit işleminden sonra sınıfın mean_ örnek özniteliğinden sütun ortaalamaları, scale_ örnek özniteliğinden sütun standart
    sapmaları ve var_ örnek özniteliğinden sütunların varyansları elde edilebilir. 

    Aşağıda StandardScaler sınıfının kullanımına ilişkin bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

dataset = np.array([[1, 2, 3], [2, 1, 4], [7, 3, 8], [8, 9, 2], [20, 12, 3]])
print(dataset)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(dataset)

print(f'{ss.mean_}, {ss.scale_}')
scaled_dataset = ss.transform(dataset)
print(scaled_dataset)

#----------------------------------------------------------------------------------------------------------------------------
    Sinir ağlarında özellik ölçeklemesi yapılırken şu noktaya dikkat edilmelidir: Özellik ölçeklemesi önce eğitim veri kümesinde
    gerçekleştirilir. Sonra eğitim veri kümesindeki sütunlardaki bilgiler kullanılarak test veri kümesi ve kestirim veri kümesi
    ölçeklendirilir. (Yani test veri kümesi ve kestirim veri kümesi kendi arasında ölçeklendirilmez. Eğitim veri kümesi referans
    alınarak ölçeklendirilir. Çünkü modelin test edilmesi ve kestirimi eğitim şartlarında yapılmalıdır.) Bu durumu kodla şöyle 
    ifade edebiliriz:

    ss = StandardScaler()
    ss.fit(training_dataset_x)
    
    scaled_training_dataset_x = ss.transform(training_dataset_x)
    scaled_test_dataset_x = ss.transform(test_dataset_x)
    scaled_predict_dataset_x = ss.transform(predict_dataset_x)
    
    Örneğin "diabetes.csv" veri kümesi üzerinde standart ölçekleme yapmak isteyelim. Bunun için önce veri kümesini dataset_x 
    ve dataset_y biçiminde sonra da "eğitim" ve "test" olmak üzere ikiye ayırırız. Ondan sonra eğitim veri kümesi üzerinde 
    özellik ölçeklemesi yapıp eğitimi uygularız. Yukarıda da belirttiğimiz gibi eğitim veri kümesinden elde edilen ölçekleme 
    bilgisinin test işlemi işlemi öncesinde test veri kümesine de, kestirim işlemi öncesinde kestirim veri kümesine de 
    uygulanması gerekmektedir.

   Aşağıda "dibates" örneğini standart ölçeklendirme uygulayarak yeniden düzenliyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import numpy as np

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

scaled_predict_dataset = ss.transform(predict_dataset)
predict_result = model.predict(scaled_predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok kullanılan bir özellik ölçeklemesi yöntemi de "min-max" ölçeklemesi denilen yöntemdir. Bu ölçeklemede sütun 
    değerleri [0, 1] arasında noktalı sayılarla temsil edilir. Min-max ölçeklemesi aşağıdaki temsili kodda olduğu gibi yapılmaktadır:

    (a - min(a)) / (max(a) - min(a))

    Örneğin sütun değerleri aşağıdaki gibi olsun:

    2
    5
    9
    4
    12

    Burada min-max ölçeklemesi şöyle yapılmaktadır:

    2 => (2 - 2) / 10
    5 => (5 - 2) / 10
    9 => (9 - 2) / 10
    4 => (4 - 2) / 10
    12 => (12 - 2) / 10

    Min-max ölçeklemesinde en küçük değerin ölçeklenmiş değerinin 0 olduğuna, en büyük değerin ölçeklenmiş değerinin 1 olduğuna, 
    diğer değerlerin ise 0 ile 1 arasında ölçeklendirildiğine dikkat ediniz. 
    
    Min-max ölçeklemesi yapan bir fonksiyon şöyle yazılabilir:

    import numpy as np

    def minmax_scaler(dataset):
        scaled_dataset = np.zeros(dataset.shape)
        for col in range(dataset.shape[1]):
            min_val, max_val = np.min(dataset[:, col]), np.max(dataset[:, col])
            scaled_dataset[:, col] = 0 if max_val - min_val == 0 else (dataset[:, col] - min_val) / (max_val - min_val)
        return scaled_dataset

    Bir sütundaki tüm değerlerin aynı olduğunu düşünelim. Böyle bir sütunun veri kümesinde bulunmasının bir faydası olabilir 
    mi? Tabii ki olmaz. Min-max ölçeklemesi yaparken sütundaki tüm değerler aynı ise sıfıra bölme gibi bir anomali oluşabilmektedir.
    Yukarıdak kodda bu durum da dikkate alınmıştır. (Tabii aslında böyle bir sütun önişleme aşamasında veri kümesinden zaten 
    atılması gerekir.) Yukarıdaki fonksiyonu yine NumPy'ın eksensel işlem yapma yenetiğini kullanarak tek satırda da yazabiliriz:

    def minmax_scaler(dataset):
        return (dataset - np.min(dataset, axis=0)) / (np.max(dataset, axis=0) - np.min(dataset, axis=0))

    Aşağıda min-max ölçeklemesine ilişkin bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

def minmax_scaler(dataset):
    scaled_dataset = np.zeros(dataset.shape)
    
    for col in range(dataset.shape[1]):
        min_val, max_val = np.min(dataset[:, col]), np.max(dataset[:, col])
        scaled_dataset[:, col] = 0 if max_val - min_val == 0 else (dataset[:, col] - min_val) / (max_val - min_val)
    return scaled_dataset

"""
def minmax_scaler(dataset):
        return (dataset - np.min(dataset, axis=0)) / (np.max(dataset, axis=0) - np.min(dataset, axis=0))
"""

dataset = np.array([[1, 2, 3], [2, 1, 4], [7, 3, 8], [8, 9, 2], [20, 12, 3]])

result = minmax_scaler(dataset)
print(dataset)
print(result)     

#----------------------------------------------------------------------------------------------------------------------------
    sckit-learn kütüphanesinde sklearn.preprocessing modülünde min-max ölçeklemesi yapan MinMaxScaler isimli bir sınıf da
    bulunmaktadır. Sınıfın kullanımı tamamen benzerleri gibidir. Örneğin:

    from sklearn.preprocessing import MinMaxScaler

    mms = MinMaxScaler()
    mms.fit(dataset)
    scaled_dataset = mms.transform(dataset)

    Yine sınıfın fit ve tarnsform işlemini birlikte yapan fit_transform isimli metodu bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

dataset = np.array([[1, 2, 3], [2, 1, 4], [7, 3, 8], [8, 9, 2], [20, 12, 3]])

from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
mms.fit(dataset)
scaled_dataset = mms.transform(dataset)

print(dataset)
print(scaled_dataset)

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de "dibates" örneğini min-max ölçeklemesi ile yeniden gerçekleştirelim. Aşağıdaki programı çalıştırdığımızda min-max 
    ölçeklemesinin standart ölçeklemeden bu veri kümesi için daha iyi sonuçlar verdiğini görmekteyiz. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()

mms.fit(training_dataset_x)
scaled_training_dataset_x = mms.transform(training_dataset_x)
scaled_test_dataset_x = mms.transform(test_dataset_x)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import numpy as np

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

scaled_predict_dataset = mms.transform(predict_dataset)
predict_result = model.predict(scaled_predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
                                            34. Ders - 04/05/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Diğer bir özellik ölçeklemesi de "maxabs" ölçeklemesi denilen ölçeklemedir. Maxabs ölçeklemesinde sütundaki değerler sütundaki
    değerlerin mutlak değerlerinin en büyüğüne bölünmektedir. Böylece sütun değerleri [-1, 1] arasına ölçeklenmektedir. Maxabs 
    ölçeklemesi şöyle yapılmaktadır:

    x / max(abs(x))

    Burada sütundaki tüm değerler en büyük mutlak değere bölündüğüne göre ölçeklenmiş değerler -1 ile +1 arasında olacaktır.

    scikit-learn kütüphanesinde "maxabs" ölçeklemesi MaxAbsScaler isimli sınıfla temsil edilmiştir. Bu sınıf diğer scikit-learn 
    sınıflarında olduğu gibi kullanılmaktadır. 

    Aşağıda "maxabs" ölçeklemesinin manuel ve scikit-learn kütüphanesiyle yapılmasına bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------
import numpy as np

def maxabs_scaler(dataset):
    scaled_dataset = np.zeros(dataset.shape)
    
    for col in range(dataset.shape[1]):
        maxabs_val = np.max(np.abs(dataset[:, col]))
        scaled_dataset[:, col] = 0 if maxabs_val == 0 else  dataset[:, col] / maxabs_val       
    return scaled_dataset

"""
def maxabs_scaler(dataset):
        return dataset /  np.max(np.abs(dataset), axis=0)
"""

dataset = np.array([[1, 2, 3], [2, 1, 4], [7, 3, 8], [8, 9, 2], [20, 12, 3]])

result = maxabs_scaler(dataset)
print(dataset)
print(result)

print('------------------------------------------------')

dataset = np.array([[1, 2, 3], [2, 1, 4], [7, 3, 8], [8, 9, 2], [20, 12, 3]])

from sklearn.preprocessing import MaxAbsScaler

mas = MaxAbsScaler()
mas.fit(dataset)
scaled_dataset = mas.transform(dataset)

print(dataset)
print(scaled_dataset))

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda "diabetes" örneğinde maxabs ölçeklemesinin kullanılmasına ilişkin örnek verilmiştir. Diabetes örneğinde aslında 
    maxabs ölçeklemesi diğerlerine göre uygun bir ölçekleme değildir. Ancak biz burada ölçeklemenin kullanımına ilişkin bir
    örnek vermek istiyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from sklearn.preprocessing import MaxAbsScaler

mas = MaxAbsScaler()

mas.fit(training_dataset_x)
scaled_training_dataset_x = mas.transform(training_dataset_x)
scaled_test_dataset_x = mas.transform(test_dataset_x)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))

model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import numpy as np

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

scaled_predict_dataset = mas.transform(predict_dataset)
predict_result = model.predict(scaled_predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda üç ölçeklendirmeyi tanıttık. Bunlar "standart ölçekleme", "minmax ölçeklemesi" ve "maxabs ölçeklemesi" idi.
    Aslında bunların dışında başka ölçeklemeler de kullanılabilmektedir. Bu konuda başka kaynaklara başvurabilirsiniz. 
    Peki elimizdeki veri kümesi için hangi ölçeklemenin daha iyi olduğuna nasıl karar verebiliriz? Aslında bu kararı vermenin
    çok prtaik yolları yoktur. En iyi yöntem yine de "deneme yanılma yoluyla" kıyaslama yapmaktır. Fakat yine de ölçekleme 
    türünü seçerken aşağıdaki durumlara dikkat edilmelidir:

    - Sütunlarda aşırı uç değerlerin (outliers) bulunduğu durumda minmax ölçeklemesi ölçeklenmiş değerlerin birbirinden 
    uzaklaşmasına yol açabilmektedir. Bu durumda bu ölçeklemenin performansı düşürebileceğine dikkat etmek gerekir. 
    
    - Sütunlardaki değerler normal dağılıma benziyorsa (örneğin doğal birtakım olgulardan geliyorsa) standart ölçekleme 
    diğerlerine göre daha iyi performans gösterebilmektedir. 
    
    - Sütunlardaki değerler düzgün dağılmışsa ve aşırı uç değerler yoksa minmax ölçeklemesi tercih edilebilir. 

    Ancak yine uygulamacılar veri kümeleri hakkında özel bir bilgiye sahip değilse ve deneme yanılma yöntemini kullanmak 
    istemiyorlarsa standart ölçeklemeyi default ölçekleme olarak tercih etmektedir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki özellik ölçeklemesi yaptığımız bir modeli nasıl saklayıp geri yükleyebiliriz? Yukarıdaki örneklerde biz özellik 
    ölçeklemesini scikit-learn kullanarak yaptık. Sequential sınıfının save metodu modeli save ederken bu ölçeklemeler modelin 
    bir parçası olmadığı için onları saklayamamaktadır. Bu nedenle bizim bu bilgileri ayrıca save etmemiz gerekmektedir. Ancak 
    Keras'ta özellik ölçeklendirmeleri bir katman nesnesi olarak da bulundurulmuştur. Eğer özellik ölçeklemeleri bir katman ile 
    yapılırsa bu durumda modeli zaten save ettiğimizde bu bilgiler de save edilmiş olacaktır. Biz burada scikit-learn ölçekleme 
    bilgisinin nasıl saklanacağı ve geri yükleneceği üzerinde duracağız. 

    Programalamada bir sınıf nesnesinin diskteki bir dosyaya yazılmasına ve oradan geri yüklenmesine "nesnelerin seri hale 
    getirilmesi (object serialization)" denilmektedir. scikit-learn içerisinde "object serialiazation" işlemine yönelik özel 
    sınıflar yoktur. Ancak seri hale getirme işlemi Python'un standart kütüphanesindeki pickle modülü ile yapılabilmektedir. 
    Örneğin scikit-learn ile standard ölçekleme yapmış olalım ve bu ölçekleme bilgisini Python'un standart pickle modülü ile 
    bir dosyada saklamak isteyelim. Bu işlemi şöyle yapabiliriz:

    import pickle

    with open('diabetes-scaling.dat', 'wb') as f:
        pickle.dump(ss, f)    

    Nesneyi dosyadan geri yükleme işlemi de şöyle yapılmaktadır:

    with open('diabetes-scaling.dat', 'rb') as f:
        ss = pickle.load(f)    

    Aşağıdaki örnekte model "diabetes.h5" dosyası içerisinde, MinMaxScaler nesnesi de "diabetes-scaling.dat" dosyası içerisinde 
    saklanmıştır ve geri yüklenmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

# diabetes-scaling-save.py

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')


model.save('diabetes.h5')

import pickle

with open('diabetes-scaling.dat', 'wb') as f:
    pickle.dump(ss, f)    

# diabetes-scaling-load.py

import numpy as np

from tensorflow.keras.models import load_model
import pickle 

model = load_model('diabetes.h5')

with open('diabetes-scaling.dat', 'rb') as f:
    ss = pickle.load(f)

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

scaled_predict_dataset = ss.transform(predict_dataset)
predict_result = model.predict(scaled_predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi eğer biz ölçekleme işlemini Keras'ın bir katmanı gibi ifade edip uygularsak Keras modelini
    sakladığımızda zaten bu bilgi de saklanmış olacaktır. Son zamanlara kadar Keras'ta bu işlemi yapacak hazır katmanlar 
    bulunmuyordu. Ancak daha sonraları Keras da diğer framewok'ler gibi böylesi katmanları bulundurmaya başlamıştır. Tabii 
    bu biçimdeki hazır katmanlar yokken programcılar yine bu işlemleri yapan kendi katman nesnelerini oluşturabiliyordu.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Keras'a sonradan eklenen Normalization isimli katman özellik ölçeklemesi yapabilmektedir. Normalization katmanı default 
    olarak "standart ölçekleme" için düşünülmüştür. Ancak "minmax ölçeklemesi" için de kullanılabilir. Bu katmanı kullanabilmek
    için önce Normalization türünden bir nesnenin yaratılması gerekir. Normalization sınıfının __init__ metodunun parametrik 
    yapısı şöyledir:

    tf.keras.layers.Normalization(axis=-1, mean=None, variance=None, invert=False, **kwargs)

    Burada mean sütunların ortalama değerlerini, variance ise sütunların varysans değerlerini almaktadır. Ancak programıcnın 
    bu değerleri girmesine gerek yoktur. Normalization sınıfının adapt isimli metodu bizden bir veri kümesi alıp bu değerleri 
    o kümeden elde edebilmektedir. Bu durumda standart ölçekleme için Normalization katmanı aşağıdaki gibi oluşturulabilir. 

    from tensorflow.keras.layers import Normalization

    norm_layer = Normalization()
    norm_layer.adapt(traininf_dataset_x)

    Tabii nu katmanı input katmanından sonra modele eklememiz gerekir. Örneğin:

    model = Sequential(name='Diabetes')

    model.add(Input((training_dataset_x.shape[1],)))
    model.add(norm_layer)
    model.add(Dense(16, activation='relu', name='Hidden-1'))
    model.add(Dense(16, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Örneğin:

    import numpy as np

    dataset = np.array([[1, 2, 3], [4, 5, 6], [3, 2, 7], [5, 9, 5]])
    print(dataset)

    from tensorflow.keras.layers import Normalization

    norm_layer = Normalization()
    norm_layer.adapt(dataset)

    print(norm_layer.mean)
    print(norm_layer.variance)

    Tabii biz ölçeklemeyi bir katman biçiminde modele eklediğimizde artık test ve predict işlemlerinde ayrıca ölçekleme 
    yapmamıza gerek kalmamaktadır. Bu katman zaten modele dahil olduğuna göre işlemler ölçeklendirilerek yapılacaktır.
    Yukarıda da belirttiğimiz gibi özellik ölçeklemesini Keras'ın bir katmanına yaptırdırğımızda ayrıca ölçekleme bilgilerinin
    saklanmasına gerek olmadığına dikkat ediniz. Çünkü ölçekleme bilgileri artık modelin bir parçası durumundadır. 

    Aşağıda "diabetes" örneğinin Keras'ın hazır Normalization katmanı kullanılarak gerçekleştirimi verilmiştir. Burada biz 
    konunun anlaşılması için gerekmediği halde modeli saklayıp geri de yükledik.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)
        
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Normalization, Dense

norm_layer = Normalization()
norm_layer.adapt(training_dataset_x)

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(norm_layer)
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import numpy as np

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

model.save('diabetes.h5')

from tensorflow.keras.models import load_model

model = load_model('diabetes.h5')

predict_result = model.predict(predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Keras'ta minmax ölçeklemesi için hazır bir katman bulunmamaktadır. Ancak böyle bir katman nesnesi programcı tarafından da 
    oluşturulabilir. Aşağıdaki makalede buna ilişkin bir örnek verilmiştir:

    https://fdnieuwveldt.medium.com/implementing-sklearn-like-transformers-in-keras-a-custom-preprocessing-layer-example-60688ba821e0

    Tabii biz henüz TensorFlow kütüphanesini incelemediğimiz için şu anda böyle bir örnek vermeyeceğiz.

    Aslında hazır Normalization sınıfını biz minmax ölçeklemesi için de kullanabiliriz. Standart ölçeklemenin aşağıdaki gibi 
    yapıldığını anımsayınız:

    (X - mu) / sigma

    Burada mu ve sigma ilgili sütunun ortalamasını ve standart sapmasını belirtmektedir. Minmax ölçeklemesinin ise şöyle yapıldığını 
    anımsayınız:

    (X - min ) / (max - min)

    O halde biz aslında standart ölçeklemeyi minmax ölçeklemesi haline de getirebiliriz. Burada mu değerinin min, 
    sigma değerinin ise max - min olması gerektiğine dikkat ediniz. Örneğin.

    mins = np.min(training_dataset_x, axis=0)
    maxmin_diffs = np.max(training_dataset_x, axis=0) - np.min(training_dataset_x, axis=0)
    norm_layer = Normalization(mean=mins, variance=maxmin_diffs ** 2)

    Aşağıda Normalization sınıfının minmax ölçeklemesi için kullanımına örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('diabetes.csv')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=0)

impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[impute_features] = si.fit_transform(df[impute_features])

dataset = df.to_numpy()

dataset_x = dataset[:, :-1]
dataset_y = dataset[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)
        
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Normalization, Dense
import numpy as np

mins = np.min(training_dataset_x, axis=0)
maxmin_diffs = np.max(training_dataset_x, axis=0) - np.min(training_dataset_x, axis=0)

norm_layer = Normalization(mean=mins, variance=maxmin_diffs ** 2)

model = Sequential(name='Diabetes')

model.add(Input((training_dataset_x.shape[1],)))
model.add(norm_layer)
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)
eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

import numpy as np

predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],
                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],
                            [3, 190, 65, 25, 130, 34, 0.271, 26],
                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],
                            [7, 106, 92, 18, 200, 35, 0.300, 48]])

model.save('diabetes.h5')

from tensorflow.keras.models import load_model

model = load_model('diabetes.h5')

predict_result = model.predict(predict_dataset)
print(predict_result)

for result in predict_result[:, 0]:
    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')

#----------------------------------------------------------------------------------------------------------------------------
    Biz şimdiye kadar Keras ve sinir ağı modeli üzerinde temel konuları anlayabilmek için hep diabetes örneği üzerinde çalıştık. 
    Şimdi başka veri kümelerini kullanarak başka örnekler üzerinde çalışacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi çıktının sınıfsal olmadığı modellere "regresyon modelleri" deniyordu. Biz kursumuzda bu durumu vurgulamak
    için "lojistik olmayan regresyon modelleri" terminini de kullanıyorduk. Regreson modellerinde sinir ağının
    çıktı katmanındaki aktivasyon fonksiyonu "linear" olmalıdır. Linear aktivasyon fonksiyonu bir şey yapmayan fonksiyondur. 
    Başka bir deyişle f(x) değerinin x ile aynı olduğu fonksiyondur. (Zaten anımsanacağı gibi Dense katmanında activation 
    parametresi girilmezse default durumda aktivasyon fonksiyonu "linear" alınmaktaydı.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            35. Ders - 05/05/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    "Auto-MPG" otomobillerin bir galon benzinle kaç mil gidebildiklerininin (başka bri deyişle yakıt tüketiminin) tahmin edilmesi 
    amacıyla oluşturulmuş bir veri kümesidir. Regresyon problemleri için sık kullanılan veri kümelerinden biridir. Veriler 80'li
    yılların başlarında toplanmıştır. O zamanki otomobil teknolojisi dikkate alınmalıdır. Veri kümesi aşağıdaki adresten indirilebilir:

    https://archive.ics.uci.edu/dataset/9/auto+mpg

    Veri kümesi bir zip dosyası olarak indirilmektedir. Buradaki "auto-mpg.data" dosyasını kullanabilirsiniz. Zip dosyasındaki
    "auto-mpg.names" dosyasında veri kümesi hakkında açıklamalar ve sütunların isimleri ve anlamları bulunmaktadır. 

    Veri kümsindeki sütunlar SPACE karakterleriyle ve son sütun da TAB karakteriyle biribirlerinden ayrılmıştır. Sütunların 
    anlamları şöyledir:
    
    1. mpg:           continuous
    2. cylinders:     multi-valued discrete
    3. displacement:  continuous
    4. horsepower:    continuous
    5. weight:        continuous
    6. acceleration:  continuous
    7. model year:    multi-valued discrete
    8. origin:        multi-valued discrete
    9. car name:      string (unique for each instance)

    Burada orijin kategorik bir sütundur. Buradaki değer 1 ise araba Amerika orijinli, 2 ise Avrupa orijinli ve 3 ise Japon
    orijinlidir.

    Veri kümesinin text dosyadaki görünümü aşağıdaki gibidir:

    18.0   8   307.0      130.0      3504.      12.0   70  1	"chevrolet chevelle malibu"
    15.0   8   350.0      165.0      3693.      11.5   70  1	"buick skylark 320"
    18.0   8   318.0      150.0      3436.      11.0   70  1	"plymouth satellite"
    16.0   8   304.0      150.0      3433.      12.0   70  1	"amc rebel sst"
    17.0   8   302.0      140.0      3449.      10.5   70  1	"ford torino"
    25.0   4   98.00      ?          2046.      19.0   71  1	"ford pinto"
    19.0   6   232.0      100.0      2634.      13.0   71  1	"amc gremlin"
    16.0   6   225.0      105.0      3439.      15.5   71  1	"plymouth satellite custom"
    17.0   6   250.0      100.0      3329.      15.5   71  1	"chevrolet chevelle malib
    ....

    Veri kümesi incelendiğinde dördüncü sütunda (horsepower) '?' karakteri ile belirtilen eksik verilerin bulunduğu görülmektedir. 
    Veri kümesindeki veriler az olmadığı için bu eksik verilerin bulunduğuğu satırlar tamamen atılabilir. Ya da daha önceden 
    de yaptığımız gibi ortalama değerle imputation uygulanabilir. Ayrıca arabanın yakıt tüketimi arabanın markası ile ilişkili 
    olsa da arabaların pek çok alt modelleri vardır. Bu nednele son sütundaki araba isimlerinin kestirimde faydası dokunmayacaktır. 
    Bu sütun da veri kümesinden atılabilir. 

    Veri kümesinin bir başlık kısmı içermediğine dikkat ediniz. read_csv default durumda veri kümesinde başlık kısmının olup 
    olmadığına kendi algoritması ile karar vermektedir. Ancak başlık kısmının bu biçimde read_csv tarafından otomatik belirlenmesi
    sağlam bir yöntem değildir. Bu nedenle bu veri kümesi okunurken read_csv fonksiyonunda header parametresi None geçilmelidir. 
    read_csv default olarak sütunlardaki ayıraçların ',' karakteri olduğunu varsaymaktadır. Halbuki bu veri kümesinde sütun 
    ayıraçları ASCII SPACE ve TAB karakterleridir. Bu nedenle dosyanın read_csv tarafından düzgün parse edilebilmesi için 
    delimeter parametresine r'\s+' biçiminde "düzenli ifade (regular expression)" kalıbı girilmelidir. (Düzenli ifadeler "Python
    Uygulamaları" kursunda ele alınmaktadır.) read_csv fonksiyonu eüer dosyada başlık kısmı yoksa sütun isimlerini 0, 1, ...
    biçiminde nümerik almaktadır. 
    
    Bu durumda yukarıdaki veri kümsinin okunması şöyle yapılabilir:

    df = pd.read_csv('auto-mpg.data', delimiter=r'\s+', header=None)

    Şimdi bizim araba markalarının bulunduğu son sütundan kurtulmamız gerekir. Bu işlemi şöyle yapabiliriz:

    df = df.iloc[:, :-1]

    Tabii bu işlemi DataFrame sınıfının drop metodu ile de yapabiliriz:

    df.drop(8, axis=1, inplace=True)

    Aslında bu sütun read_csv ile okuma sırasında usecols parametresi yardımıyla da atılabilir. 

    Şimdi de 3'üncü indeksli sütundaki eksik verileri temsil eden '?' bulunan satırlar üzerinde çalışalım. Yukarıda da 
    belirttiğimiz gibi bu sütundaki eksik verilerin sayıları çok az olduğu için veri kümesinden atılabilirler. Bu işlem 
    şöyle yapılabilir:

    df = df[df.iloc[:, 3] != '?']
    
    Ancak biz geçmiş konuları da kullanabilmek için bu eksik verileri sütun ortalamaları ile doldurmaya (imputation) çalışalım.
    Burada dikkat edilmesi gereken nokta DataFrame nesnesinin 3'üncü indeksli sütununun türünün nümerik olmamasıdır. Bu nedenle
    öncelikle bu sütunun türünü nümerik hale getirmek gerekir. Tabii sütunda '?' karakterleri olduğuna göre önce bu karakterler 
    yerine 0 gibi nümerik değerleri yerleştirmeliyiz:

    df.iloc[df.loc[:, 3] == '?', 3] = 0

    Tabii eğer ilgili sütunda zaten 0 değerleri varsa bu durumda 0 ile doldurmak yerine np.nan değeri ile dolduma yolunu tercih
    edebilirsiniz. Örneğin:

    df.iloc[df.loc[:, 3] == '?', 3] = np.nan

    Artık sütunun türünü nümerik hala getirebiliriz:

    df[3] = df[3].astype('float64')

    Aslında read_csv ile okuma sırasında da fonksiyonun na_values parametresi yardımıyla işin başında '?' karakterleri yerine
    fonksiyonun np.nan değerlerini yerleştirmesini de sağlayabiliriz.

    Burada doğrudan indekslemede sütun isimlerinin kullanılması gerektiğine, sütun isimlerinin de sütun başlığı olmadığı için 
    sayısal biçimde verildiğine dikkat ediniz. Artık 3'üncü indeksli sütun üzerinde imputation uygulayabiliriz:

    from sklearn.impute import SimpleImputer

    si = SimpleImputer(strategy='mean', missing_values=0)
    df[3] = si.fit_transform(df[[3]])

    Henüz NumPy'a dönmeden önce 7'inci sütundaki kategorik verileri Pandas'ın get_dummis fonksiyonu ile "one-hot-encoding"
    biçimine dönüştürebiliriz:

    df = pd.get_dummies(df, columns=[7], dtype='uint8')
    
    Artık NumPy'a dönebiliriz:

    dataset = df.to_numpy()

    Şimdi de veri kümesini x ve y olarak ayrıştıracağız. Ancak y verilerinin son sütunda değil ilk sütunda olduğuna dikkat 
    ediniz:

    dataset_x = dataset[:, 1:]
    dataset_y = dataset[:, 0]   

    Bundan sonra veri kümesi eğitim ve test amasıyla train_test_slit fonkisyonu ile ayrıştrılabiliriz

    from sklearn.model_selection import train_test_split

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)      

    Artık özellik ölçeklemesi yapabiliriz. Özellik ölçeklemesini scikit-learn kullanarak ya da yukarıda da bahsettiğimiz gibi 
    Normailzation isimli Keras katmanı kullanarak da yapabiliriz. Sütun dağılımlarına bakıldığında standart ölçekleme yerine 
    minmax ölçeklemesinin daha iyi performans verebileceği izlenimi edinilmektedir. Ancak bu konuda deneme yanılma yöntemi 
    uygulamak gerekir. Biz default standart ölçekleme uygulayalım:

    from sklearn.preprocessing import StandardScaler

    ss = StandardScaler()
    ss.fit(training_dataset_x)
    scaled_training_dataset_x = ss.transform(training_dataset_x)
    scaled_test_dataset_x = ss.transform(test_dataset_x)

    Artık modelimizi oluşturabiliriz. Bunun için yine iki saklı katman kullanacağız. Saklı katmanlardaki aktivasyon fonksiyonlarını 
    yine "relu" olarak alacağız. Ancak çıktı katmanındaki aktivasyonun "linear" olması gerektiğini anımsayınız:

    model = Sequential(name='Auto-MPG')

    model.add(Input((training_dataset_x.shape[1],)))
    model.add(Dense(32, activation='relu', name='Hidden-1'))
    model.add(Dense(32, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Şimdi de modelimizi compile edip fit işlemi uygulayalım. Modelimiz için optimizasyon algoritması yine "rmsprop" seçilebilir. 
    Regresyon problemleri için loss fonksiyonunun genellikle "mean_squared_error" biçiminde alınabileceğini belirtmiştik. Yine 
    regresyon problemleri için "mean_absolute_error" metrik değeri kullanılabilir:

    odel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)
    
    Modelimizi test veri kümesiyle test edebiliriz:

    eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

    for i in range(len(eval_result)):
        print(f'{model.metrics_names[i]}: {eval_result[i]}')

    Şimdi kestirim yapmaya çalışalım. Kesitirilecek veriler üzerinde de one-hot-encoding dönüştürmesinin ve özellik ölçeklemesinin
    yapılması gerektiğini anımsayınız. Kestirilecek verileri bir "predict.csv" isimli bir dosyada aşağıdaki gibi oluşturmuş olalım:

    8,307.0,130.0,3504,12.0,70,1	
    4,350.0,165.0,3693,11.5,77,2	
    8,318.0,150.0,3436,11.0,74,3

    Bu dosyayı okuduktan predict işlemi yapmadan önce sonra sırasıyla "one-hot-encoding" ve standart ölçeklemenin uygulanması
    gerekir:
    
    predict_df = pd.read_csv('predict.csv', header=None)
    predict_df = pd.get_dummies(predict_df, columns=[6])
    predict_dataset_x = predict_df.to_numpy() 
    scaled_predict_dataset_x = ss.transform(predict_dataset_x)

    predict_result = model.predict(scaled_predict_dataset_x)

    for val in predict_result[:, 0]:
        print(val)

    Aşağıda tüm işlemler bir bütün halinde verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('auto-mpg.data', delimiter=r'\s+', header=None)

df = df.iloc[:, :-1]
df.iloc[df.loc[:, 3] == '?', 3] = np.nan

df[3] = df[3].astype('float64')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=np.nan)
df[3] = si.fit_transform(df[[3]])

df = pd.get_dummies(df, columns=[7], dtype='uint8')

dataset = df.to_numpy()

dataset_x = dataset[:, 1:]
dataset_y = dataset[:, 0]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)
        
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)
       
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Auto-MPG')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(32, activation='relu', name='Hidden-1'))
model.add(Dense(32, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)
eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Absolute Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Measn Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

# prediction

predict_df = pd.read_csv('predict.csv', header=None)
predict_df = pd.get_dummies(predict_df, columns=[6])
predict_dataset_x = predict_df.to_numpy() 
scaled_predict_dataset_x = ss.transform(predict_dataset_x)

predict_result = model.predict(scaled_predict_dataset_x)

for val in predict_result[:, 0]:
    print(val)
    
#----------------------------------------------------------------------------------------------------------------------------
                                                36. Ders - 11/05/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    One-hot-encoding işleminde Pandas'ın get_dummies fonksiyonunu kullanırken dikkat ediniz. Bu fonksiyon one-hot-encoding 
    yapılacak sütundaki kategorileri kendisi belirlemektedir. (Bu fonksiyon "one-hot-encoding" yapılacak sütunda unique olan
    değerlerden hareketle kategorileri belirler.) Eğer predict yapacağınız CSV dosyasındaki satırlar tüm kategorileri içermezse 
    bu durum bir sorun yaratır. scikit-learn içerisindeki OneHotEncoder sınıfı bu tür durumlarda "categories" isimli parametreyle 
    bizlere yardımcı olmaktadır. Maalesef get_dummies fonksiyonun böyle bir parametresi yoktur. 

    OneHotEncoder sınfının __init__ metodunda categories isimli parametre ilgili sütundaki kategorileri belirtmek için düşünülmüştür. 
    Biz "one-hot-encoding" yapılacak sütundaki kategorileri kendimiz belirlemek istiyorsak bu parametreyi kullanabiliriz. categories 
    parametresi iki boyutlu bir liste olarak girilmelidir. Çünkü birden fazla sütun "one-hot-encoding" işlemine sokulabilmektedir. 
    Bu durumda bu iki boyutlu listenin her elemanı sırasıyla sütunlardaki kategorileri belirtir. Örneğin:

    ohe = OneHotEncoder(sparse=False, categories=[[0, 1, 2], ['elma', 'armut', 'kayısı']])

    Burada biz iki sütunlu bir veri kümesinin iki sütununu da one-hot-encoding yapmak istemekteyiz. Bu veri tablosunun ilk 
    sütunundaki kategoriler 0, 1, 2 biçiminde, ikinci sütunundaki kategoriler 'elma', 'armut', 'kayısı' biçimindedir. Eğer 
    bu sütunlarda daha az kategori varsa burada belirtilen sayıda ve sırada sütun oluşturulacaktır. 

    Tabii OneHotEncoder sınıfı ile fit işlemi yaptığımızda zaten bu kategoriler nesnenin içerisinde oluşturulmaktadır. Anımsanacağı
    gibi OneHotEncoder sınıfının categories_ örnek özniteliği bize fit işleminden elde edilen kategorileri vermektedir. Bu nesne
    eğer saklanmışsa doğrudan predict işleminde kullanılabilir. Bu durumda bir sorun oluşmayacaktır. Peki eğer bu nesne 
    saklanmamışsa ne yapabiliriz?

    predict işlemi yapılırken uygulanan "one-hot-encoding" işlemindeki sütunların eğitimdeki "one-hot-encoding" sütunlarıyla 
    uyuşması gerekir. Aslında kütüphanelerde "one-hot-encoding" işlemini yapan fonksiyonlar ve metotlar önce sütunlardaki
    unique elemanları belirleyip sonra onları sıraya dizip sütunları bu sırada oluşturmaktadır. Örneğin Pandas'daki get_dummies
    fonksiyonu ve scikit-learn'deki OneHotEncoder sınıfı böyle davranmaktadır. Fakat yine de tam uyuşma için one-hot-encoding 
    işlemlerini farklı sınıflarla yapmamaya çalışınız. Predict işlemindeki "one-hot-encoding" sütunların eğitimde kullanılan 
    one-hot-encoding sütunlarıyla uyuşmasını sağlamak için eğitimde kullanılan sütunlardaki değerleri saklabilirsiniz. OneHotEncoder
    sınıfının categories_ örnek özniteliği zaten bu kategorileri bize vermektedir.  

    Burada NumPy ve Pandas ile ilgili bir ayrıntı üzerinde durmak istiyoruz. NumPy'dki unique fonksiyonu ve metodu unique
    elemanları elde ettikten sonra onları sırasyı dizip vermektedir. Ancak Pandas'taki unique fonksiyonu ve metodu sıraya dizme
    işlemini ayrca yapmamaktadır.

    Aşağıda Auto-MPGveri kümesinde predict işleminin nasıl yapıldığı gösterilmektedir. Burada predict sırasında yeni bir 
    OneHotEncoder nesnesi oluşturulmamış eğitim sırasındaki OneHotEncoder nesnesi  kullanılmıştır. Ayrıca bu örnekte modelde 
    kullanılan nesnelerin disk'te saklandığına da dikkat ediniz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('auto-mpg.data', delimiter=r'\s+', header=None)

df = df.iloc[:, :-1]
df.iloc[df.loc[:, 3] == '?', 3] = np.nan

df[3] = df[3].astype('float64')

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean', missing_values=np.nan)
df[3] = si.fit_transform(df[[3]])

df_1 = df.iloc[:, :7]
df_2 = df.iloc[:, [7]]

dataset_1 = df_1.to_numpy()
dataset_2 = df_2.to_numpy()

import numpy as np
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
dataset_2 = ohe.fit_transform(dataset_2)

dataset = np.concatenate([dataset_1, dataset_2], axis=1)

dataset_x = dataset[:, 1:]
dataset_y = dataset[:, 0]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)
        
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)
       
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Auto-MPG')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(32, activation='relu', name='Hidden-1'))
model.add(Dense(32, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)
eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Absolute Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Measn Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

model.save('auto-mpg.h5')

import pickle

with open('auto-mpg.pickle', 'wb') as f:
    pickle.dump((ohe, ss), f)

# prediction

predict_df = pd.read_csv('predict.csv', header=None)
predict_df_1 = predict_df.iloc[:, :6]
predict_df_2 = predict_df.iloc[:, [6]]

predict_dataset_1 = predict_df_1.to_numpy()
predict_dataset_2 = predict_df_2.to_numpy()
predict_dataset_2  = ohe.transform(predict_dataset_2)

predict_dataset = np.concatenate([predict_dataset_1, predict_dataset_2], axis=1)
scaled_predict_dataset = ss.transform(predict_dataset)
predict_result = model.predict(scaled_predict_dataset)

for val in predict_result[:, 0]:
    print(val)

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte save edilmiş olan model nesnesinin ve sckit-learn nesnelerinin başka bir Python grogramında nasıl 
    yüklenerek kullanılacağı gösterilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import pickle
import numpy as np
import pandas as pd
from tensorflow.keras.models import load_model

# prediction

model = load_model('auto-mpg.h5')

with open('auto-mpg.pickle', 'rb') as f:
    ohe, ss = pickle.load(f)    

predict_df = pd.read_csv('predict.csv', header=None)
predict_df_1 = predict_df.iloc[:, :6]
predict_df_2 = predict_df.iloc[:, [6]]

predict_dataset_1 = predict_df_1.to_numpy()
predict_dataset_2 = predict_df_2.to_numpy()
predict_dataset_2  = ohe.transform(predict_dataset_2)

predict_dataset = np.concatenate([predict_dataset_1, predict_dataset_2], axis=1)
scaled_predict_dataset = ss.transform(predict_dataset)
predict_result = model.predict(scaled_predict_dataset)

for val in predict_result[:, 0]:
    print(val)

#----------------------------------------------------------------------------------------------------------------------------
    Bir veri kümesinde "one-hot-encoding" yapılacak birden fazla sütun varsa -yukarıdaki örnekte olduğu gibi- veri kümesini
    önce "one-hot-encoding" yapılacak kısım ile yapılmayacak kısmı iki parçaya ayırıp one-hot-encoding işlemini tek hamlede
    birden fazla sütun için uygulayabilirsiniz. Anımsanacağı gibi scikit-learn içerisindeki OneHotEncoder sınıfı zaten tek 
    hamlede birden fazla sütunu "one-hot-encoding" yapabiliyordu. İzleyen paragraflarda bu tür örnekler üzerinde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki "one-hot-encoding" işlemi Keras'taki bir katmana yaptırılamaz mı? Biz daha önce özellik ölçeklemesini bir katmana
    yaptırmıştık. Ancak maalesef Keras bu konuda önemli bir pratiklik sunmamaktadır. one-hot-encoding yapan bir katman nesnesi 
    programcı tarafından oluşturulabilir. Ancak bunun oluşturulabilmesi için TensorFlow kütüphanesinin kullanımının biliniyor olması 
    gerekir. Keras içerisinde CategoryEncoding isimli bir katman bulunuyor olsa da bu katman tüm girdideki sütunları "one-hot 
    encoding" yapmaktadır. Bu nedenle bu katmanın kullanılabilmesi için girdi katmanın parçalara ayrılması gerekir. Bu işlemleri 
    ileride "Keras Modelinin Fonksiyonel Biçimde Oluşturulması" konusu içerisinde ele alacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Regresyon problemlerinde çok kullanılan veri kümelerinden biri de "Boston Housing Prices (BHP)" isimli veri kümesidir. 
    Bu veri kümesi daha önce görümüş olduğumuz "Melbourne Housing Snapshot (MHS)" veri kümesine benzemektedir. Bu veri kümesinde 
    evlerin çeşitli bilgileri sütunlar halinde kodlanmıştır. Amaç yine evin fiyatının kestirilmesidir. Veriler 1970 yılında 
    toplanmıştır. Bu nedenle ev fiyatları size düşükmüş gibi gelebilir. (Türkiye'de yaşayan kişilerin bir bölümü ABD'de ve Avrupa
    ülkelerinde hiç enflasyon olmadığını sanabiliyorlar. Biz paradan 6 sıfır attık. Ancak örneğin dolardan hiçbir zaman sıfır 
    atılmadı. ABD'deki enflasyon çok düşük olsa bile 70'li yıllardan bu yana enflasyondaki bileşik artış aradaki farkı dikkate 
    değer hale gelmektedir.) BHP veri kümesi aşağıdaki bağlantıdan indirilebilir:

    https://www.kaggle.com/datasets/vikrishnan/boston-house-prices

    Buradan veri kümesini bir zip dosyası biçiminde indireceksiniz. Zip dosyası açıldığında "housing.csv" isimli dosya elde 
    edilmektedir. Veri kümesi aşağıdaki görünümdedir:

    0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00
    0.02731   0.00   7.070  0  0.4690  6.4210  78.90  4.9671   2  242.0  17.80 396.90   9.14  21.60
    0.02729   0.00   7.070  0  0.4690  7.1850  61.10  4.9671   2  242.0  17.80 392.83   4.03  34.70
    0.03237   0.00   2.180  0  0.4580  6.9980  45.80  6.0622   3  222.0  18.70 394.63   2.94  33.40
    0.06905   0.00   2.180  0  0.4580  7.1470  54.20  6.0622   3  222.0  18.70 396.90   5.33  36.20
    0.02985   0.00   2.180  0  0.4580  6.4300  58.70  6.0622   3  222.0  18.70 394.12   5.21  28.70
    0.08829  12.50   7.870  0  0.5240  6.0120  66.60  5.5605   5  311.0  15.20 395.60  12.43  22.90
    0.14455  12.50   7.870  0  0.5240  6.1720  96.10  5.9505   5  311.0  15.20 396.90  19.15  27.10
    ............

    Burada da görüldüğü gibi her ne kadar dosyasının uzantısı ".csv" ise de aslında sütunlar virgüllerle değil SPACE karakterleriyle
    ayrıştırılmıştır. Tüm sütunlarda sayısal bilgiler olduğu için aslında bu dosya en kolay bir biçimde NumPy'ın loadtxt
    fonksiyonuyla okunabilir. Örneğin:
    
    dataset = np.loadtxt('housing.csv')

    Ancak biz kursumuzda ilk aşamaları hep Pandas ile yaptığımızdan aynı süreçleri izlemek için okumayı da yine Pandas'ın read_csv
    fonksiyonuyla yapacağız. Tabii read_csv fonksiyonunda yine delimiter parametresi boşlukları belirten "düzenli ifade (regular
    expression)" biçiminde olmalıdır. Dosyada bir başlık kısmının olmadığına da dikkat ediniz. read_csv fonksiyonu ile okumayı 
    şöyle yapabiliriz:

    df = pd.read_csv('housing.csv', header=None, delimiter=r'\s+')

    Veri kümesindeki sütunlar için İngilizce olarak aşağıdaki açıklamalar yapılmıştır:

    1. CRIM: per capita crime rate by town
    2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
    3. INDUS: proportion of non-retail business acres per town
    4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    5. NOX: nitric oxides concentration (parts per 10 million)
    6. RM: average number of rooms per dwelling
    7. AGE: proportion of owner-occupied units built prior to 1940
    8. DIS: weighted distances to ﬁve Boston employment centers
    9. RAD: index of accessibility to radial highways
    10. TAX: full-value property-tax rate per $10,000
    11. PTRATIO: pupil-teacher ratio by town 
    12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town 
    13. LSTAT: % lower status of the population
    14. MEDV: Median value of owner-occupied homes in $1000s

    Buradaki son sütun evin fiyatını 1000 dolar cinsinden belirtmektedir. Veri kümesinde eksik veri yoktur. Veri kümesinin 
    4'üncü sütununda kategorik bir bilgi bulunmaktadır. Ancak bu alanda yalnızca 0 ve 1 biçiminde iki değer vardır. İki değerli 
    sütunlar için "one-hot-encoding" işlemine gerek olmadığını anımsayınız. Ancak 9'uncu sütunda ikiden daha fazla sınıf içeren 
    kategorik bir bilgi bulunmaktadır. Dolayısıyla bu sütun üzerinde "one-hot-encoding" dönüştürmesi uygulamalıyız. Sütunlar 
    arasında önemli basamaksal farklılıklar göze çarpmaktadır. Yani veri kümesi üzerinde özellik ölçeklemesinin yapılması 
    gerekmektedir. Veri kümesinin sütunlarında aşırı uç değerler (outliers) de bulunmamaktadır. Özellik ölçeklemesi için standart 
    ölçekleme ya da min-max ölçeklemesi kullanılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                37. Ders - 12/05/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    BHP veri kümesi için oluşturulmuş olan CSV dosyasında bir başlık kısmı olmadığına dikkat ediniz. Pandas'ın read_csv fonksiyonu 
    ile CSV dosyası okunurken eğer dosyada başlık kısmı yoksa Pandas sütun başlık isimlerini 0'dan başlayarak sayısal biçimde 
    vermektedir. 

    8'inci sütun kategorik olduğu için o sütunu "one-hot-encoding" işlemine sokalım:
    
    highway_class = df.iloc[:, 8].to_numpy()

    ohe = OneHotEncoder(sparse_output=False)
    ohe_highway = ohe.fit_transform(highway_class.reshape(-1, 1))

    Şimdi "one-hot-encoding" işleminden elde ettiğimiz matrisi DataFrame nesnesinin sonuna yerleştirebiliriz. Tabii bundan 
    önce bu sütunu silip "y" verilerini de ayırmalıyız:

    dataset_y = df.iloc[:, -1].to_numpy()
    df.drop([8, 13], axis=1, inplace=True)
    dataset_x = pd.concat([df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()

    Şimdi elimizde NumPy dizisine dönüştürülmüş olan dataset_x ve dataset_y verileri var.  Artık veri kümesini eğitim ve test 
    olmak üzere ikiye ayırabiliriz:

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

    Bu işlemden sonra diğer adımları uygulayabiliriz. Ancak önce eğitim veri kümesini ölçeklememiz gerekir:

    ss = StandardScaler()
    ss.fit(training_dataset_x)
    scaled_dataset_x = ss.transform(training_dataset_x)

    Burada standart ölçekleme uyguladık. Ancak bu veri kümesine min-max ölçeklemesi de uygulayabilirsiniz. 

    Veri kümesi için iki saklı katman içeren klasik model kullanılabilir:

    model = Sequential(name='Boston-Housing-Prices')
    model.add(Input((training_dataset_x.shape[1], ), name='Input'))
    model.add(Dense(64, activation='relu', name='Hidden-1'))
    model.add(Dense(64, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    model.compile('rmsprop', loss='mse', metrics=['mae'])
    hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)

    Modelin çıktı katmanındaki aktivasyon fonksiyonunun "linear" olarak, loss fonksiyonunun "mean_sequred_error", metrik 
    değerin de "mean_absolute_error" olarak  dikkat ediniz.

    Aşağıda BHP veri kümesi uygulamasının tüm kodlar verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------
import pandas as pd

df = pd.read_csv('housing.csv', delimiter=r'\s+', header=None)

highway_class = df.iloc[:, 8].to_numpy()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
ohe_highway = ohe.fit_transform(highway_class.reshape(-1, 1))

dataset_y = df.iloc[:, -1].to_numpy()
df.drop([8, 13], axis=1, inplace=True)
dataset_x = pd.concat([df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential(name='Boston-Housing-Prices')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(64, activation='relu', name='Hidden-1'))
model.add(Dense(64, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile('rmsprop', loss='mse', metrics=['mae'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Mean Absolute Error - Validation Mean Absolute Error Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

"""
import pickle

model.save('boston-housing-prices.h5')
with open('boston-housing-prices.pickle', 'wb') as f:
    pickle.dump([ohe, ss], f)
"""

predict_df = pd.read_csv('predict-boston-housing-prices.csv', delimiter=r'\s+', header=None)

highway_class = predict_df.iloc[:, 8].to_numpy()
ohe_highway = ohe.transform(highway_class.reshape(-1, 1))
predict_df.drop(8, axis=1, inplace=True)
predict_dataset_x = pd.concat([predict_df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()
scaled_predict_dataset_x = ss.transform(predict_dataset_x )
predict_result = model.predict(scaled_predict_dataset_x)

for val in predict_result[:, 0]:
    print(val)
    
#  predict-boston-hosing-prices.csv

 0.13554  12.50   6.070  0  0.4090  5.5940  36.80  6.4980   4  345.0  18.90 396.90  13.09  
 0.12816  12.50   6.070  0  0.4090  5.8850  33.00  6.4980   4  345.0  18.90 396.90   8.79  
 0.08826   0.00  10.810  0  0.4130  6.4170   6.60  5.2873   4  305.0  19.20 383.73   6.72  
 0.15876   0.00  10.810  0  0.4130  5.9610  17.50  5.2873   4  305.0  19.20 376.94   9.88  
 0.09164   0.00  10.810  0  0.4130  6.0650   7.80  5.2873   4  305.0  19.20 390.91   5.52  
 0.19539   0.00  10.810  0  0.4130  6.2450   6.20  5.2873   4  305.0  19.20 377.17   7.54

#----------------------------------------------------------------------------------------------------------------------------
                                            38. Ders 18/05/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesi ve veri bilimine ilişkin pek çok kğtğphane ve framework'te hazır bazı veri kümeleri bulunabilmektedir. 
    Örneğin Keras içerisinde tensorflow.keras.datasets modülünde aşağıdaki veri kümeleri hazır biçimde bulunmaktadır:

    boston_housing
    california_housing
    cifar10 
    cifar100 
    fashion_mnist 
    imdb 
    mnist
    reuters

    Keras dokümanlarında BHS veri kümesi için etik bir uyarı da eklenmiştir. Bu veri kümelerinin hepsi benzer biçimde 
    kullanılmaktadır: Önce yukarıdaki modüller import edilir sonra da modüllerin load_data isimli fonksiyonu çağrılır. 
    Bu fonksiyonlar ikişer elemandan oluşan ikili bir demet vermektedir. Dolayısıyla bu load_data fonksiyonu aşağıdaki gibi
    açılabilir:
    
    (training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = boston_housing.load_data()

    Görüldüğü gibi load_data fonksiyonu zaten eğitim ve test veri kümelerini ayrıştırılmış olarak bize vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi Keras içerisindeki hazır boston_housing veri kümesini kullanan bir örnek yapalım. Veri kümesi aşağıdaki gibi elde
    edilebilir:

    from tensorflow.keras.datasets import boston_housing

    (training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = boston_housing.load_data()

    8'inci sütunu OneHotEncoding yapabiliriz:

    ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
    ohe_result_training = ohe.fit_transform(training_dataset_x[:, 8].reshape(-1, 1))
    ohe_result_test = ohe.transform(test_dataset_x[:, 8].reshape(-1, 1))

    Artık 8'inci sütunu silip one-hot-encoding edilmiş biçimi veri kümelerine eklememiz gerekir. Tabii bu tür durumlarda aslında
    one-hot-encoding yapılacak sütunların hepsini tek hamlede one-hot-encoding yapıp bunları veri kümesinin sonuna ekleyebiliriz. 
    Ancak burada bir değişiklik olsun diye one-hot-encoding yapılacak verileri kategorik sütunun yerine (8'inci sütun) insert 
    edelim:

    training_dataset_x = np.delete(training_dataset_x, 8, axis=1)
    test_dataset_x = np.delete(test_dataset_x, 8, axis=1)

    training_dataset_x = np.insert(training_dataset_x, [8], ohe_result_training, axis=1)
    test_dataset_x = np.insert(test_dataset_x, [8], ohe_result_test, axis=1)

    Yukarıda da belirttiğimiz gibi bu işlem aslında append fonksiyonuyla ya da concatenate fonksiyonuyla da yapılabilir. 
    Geri kalan işlemler önceki örnekte olduğu gibi yürütülebilir. Aşağıda tüm örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.datasets import boston_housing

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = boston_housing.load_data()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
ohe_result_training = ohe.fit_transform(training_dataset_x[:, 8].reshape(-1, 1))
ohe_result_test = ohe.transform(test_dataset_x[:, 8].reshape(-1, 1))

import numpy as np

training_dataset_x = np.delete(training_dataset_x, 8, axis=1)
test_dataset_x = np.delete(test_dataset_x, 8, axis=1)

training_dataset_x = np.insert(training_dataset_x, [8], ohe_result_training, axis=1)
test_dataset_x = np.insert(test_dataset_x, [8], ohe_result_test, axis=1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential(name='Boston-Housing-Prices')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(64, activation='relu', name='Hidden-1'))
model.add(Dense(64, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile('rmsprop', loss='mse', metrics=['mae'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Mean Absolute Error - Validation Mean Absolute Error Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

"""
import pickle

model.save('boston-housing-prices.h5')
with open('boston-housing-prices.pickle', 'wb') as f:
    pickle.dump([ohe, ss], f)
"""

import pandas as pd

predict_df = pd.read_csv('predict-boston-housing-prices.csv', delimiter=r'\s+', header=None)
predict_dataset_x = predict_df.to_numpy()

ohe_result_predict = ohe.transform(predict_dataset_x [:, 8].reshape(-1, 1))

predict_dataset_x = np.delete(predict_dataset_x, 8, axis=1)
predict_dataset_x = np.insert(predict_dataset_x, [8], ohe_result_predict, axis=1)

scaled_predict_dataset_x = ss.transform(predict_dataset_x )
predict_result = model.predict(scaled_predict_dataset_x)

for val in predict_result[:, 0]:
    print(val)
    
# predict-boston-housing-pripces.csv

 0.13554  12.50   6.070  0  0.4090  5.5940  36.80  6.4980   4  345.0  18.90 396.90  13.09  
 0.12816  12.50   6.070  0  0.4090  5.8850  33.00  6.4980   4  345.0  18.90 396.90   8.79  
 0.08826   0.00  10.810  0  0.4130  6.4170   6.60  5.2873   4  305.0  19.20 383.73   6.72  
 0.15876   0.00  10.810  0  0.4130  5.9610  17.50  5.2873   4  305.0  19.20 376.94   9.88  
 0.09164   0.00  10.810  0  0.4130  6.0650   7.80  5.2873   4  305.0  19.20 390.91   5.52  
 0.19539   0.00  10.810  0  0.4130  6.2450   6.20  5.2873   4  305.0  19.20 377.17   7.54

#----------------------------------------------------------------------------------------------------------------------------
    Biz regresyon terimini daha çok "çıktının bir sınıf değil sayısal bir değer olduğu" modeller için kullanıyorduk. Bu paragrafta
    daha genel bir terim olarak kullanacağız. Regresyon çeşitli biçimlerde yani çeşitli yöntemlerle gerçekleştirilebilmektedir. 
    Aslında yapay sinir ağları da regresyon için bir yöntem grubunu oluşturmaktadır. 
    
    Regresyon en genel anlamda girdi ile çıktı arasında bir ilişki kurma sürecini belirtmektedir. Matematiksel olarak regresyon 
    y = f(x) biçiminde bir f fonksiyonunun elde edilme süreci olarak da tanımlanabilir. Eğer biz böyle bir f fonksiyonu bulursak 
    x değerlerini fonksiyonda yerine koyarak y değerini elde edebiliriz. Tabi y = f(x) fonksiyonunda x değişkeni aslında 
    (x0, x1, x2, ..., xn) biçiminde birden fazla değişkeni de temsil ediyor olabilir. Bu durumda f fonksiyonu f((x0, x1, x2, ..., xn)) 
    biçiminde çok değişkenli bir fonksiyon olacaktır. Benzer biçimde y = f(x) eşitliğinde f fonksiyonu birden fazla değer de 
    veriyor olabilir. Yani buradaki y değeri (y0, y1, y2, ..., ym) biçiminde de olabilir. 
 
    İstatistikte regresyon işlemleri tipik olarak aşağıdaki gibi sınıflandırılmaktadır:

    - Eğer bağımsız değişken (x değişkeni) bir tane ise buna genellikle "basit regresyon (simple regression)" denilmektedir. 

    - Eğer bağımsız değişken (x değişleni) birden fazla ise buna da genellikle "çoklu regresyon (mulptiple regression)" 
    denilmektedir. 

    - Eğer girdiyle çıktı arasında doğrusal bir ilişki kurulmak isteniyorsa (yani regresyon işleminden doğrusal bir fonksiyon 
    elde edilmek isteniyorsa) bu tür regresyonlara "doğrusal regresyon (linear regression)" denilmektedir. Doğrusal regresyon 
    da bağımsız değişken bir tane ise "basit doğrusal regresyon (simple linear regression)", bağımsız değişken birden fazla 
    ise "çoklu doğrusal regresyon (multiple linear regression)" biçiminde ikiye ayrılabilmektedir. 
   
    - Bağımsız değişken ile bağımlı değişken arasında polinomsal ilişki kurulmaya çalışılabilir. (Yani regresyon sonucunda 
    bir polinom elde edilmeye çalışılabilir). Buna da "polinomsal regresyon (polynomial regression)" denilmektedir. Bu da yine basit 
    ya da çoklu olabilir. Aslında işin matematiksel tarafında polinomsal regresyon bir transformasyonla doğrusal regresyon haline 
    dönüştürülebilmektedir. Dolayısıyla doğrusal regresyonla polinomsal regresyon arasında aslında işlem bakımından önemli bir 
    fark yoktur.
    
    - Girdiyle çıktı arasında doğrusal olmayan bir ilişki de kurulmak istenebilir. (Yani doğrusal olmayan bir fonksiyon da oluşturulmak 
    istenebilir). Bu tür regresyonlara "doğrusal olmayan regresyon (nonlinear regressions)" denilmektedir. Yukarıda da belirttiğimiz 
    gibi her ne kadar polinomlar doğrusal fonksiyonlar olmasa da bunlar transformasyonla doğrusal hale getirilebildikleri için doğrusal 
    olmayan regresyon denildiğinde genel olarak polinomsal regresyonlar kastedilmemektedir. Örneğin logatirmik, üstel regresyonlar 
    doğrusal olmayan regresyonlara örnektir. 
    
    - Bir regresyonda çıktı da birden fazla olabilir. Genellikle (her zaman değil) bu tür regresyonlara "çok değişkenli (multivariate)" 
    regresyonlar denilmektedir. Örneğin:

    (y1, y2) = f((x1, x2, x3, x4, x5))

    Regresyon terminolojisinde "çok değişkenli" sözcüğü bağımsız değişkenin birden fazla olmasını değil (buna "çoklu" denilmektedir) 
    bağımlı değişkenin birden fazla olmasını anlatan bir terimdir. İngilizce bu bağlamda "çok değişkenli" terimi "multivariate"
    biçiminde ifade edilmektedir. 

    - Eğer regresyonun çıktısı kategorik değerler ise yani f fonksiyonu kategorik bir değer üretiyorsa buna "lojistik regresyon 
    (logictic regression)" ya da "logit regresyonu" denilmektedir. Lojistk regresyonda çıktı iki sınıftan oluşuyorsa (hasta-sağlıklı 
    gibi, olumlu-olumsuz gibi, doğru-yanlış gibi) böyle lojistik regresyonlara "iki sınıflı lojistik regresyon (binary logistic 
    regression)" denilmektedir. Eğer çıktı ikiden fazla sınıftan oluşuyorsa böyle lojistik regresyonlara da "çok sınıflı lojistik 
    regresyon (multi-class/multinomial logistic regression)" denilmektedir. Tabii aslında makine öğrenmesinde ve sinir sinir ağlarında 
    "lojistik regresyon" terimi yerine "sınıflandırma (classification)" terimi tercih edilmektedir. Bizim de genellikle (ancak her 
    zaman değil) kategorik kestirim modellerine "lojistik regresyon modelleri" yerine "sınıflandırma problemleri" dediğimizi 
    anımsayınız.
    
    - Sınıflandırma problemlerinde bir de "etiket (label)" kavramı sıklıkla karşımıza çıkmaktadır. Etiket genellikle çok 
    değişkenli (multivariate) sınıflandırma problemlerinde (yani çıktının birden fazla olduğu ve kategorik olduğu problemlerde) 
    her çıktı için kullanılan bir terimdir. Örneğin biz bir sinir ağından üç bilgi elde etmek isteyebiliriz: "kişinin hasta olup 
    olmadığı", "kişinin obez olup olmadığı", "kişinin mutlu olup olmadığı". Burada üç tane etiket vardır. Sınıf kavramının belli 
    bir etiketteki kategorileri belirtmek için kullanıldığına dikkat ediniz. Etiketlerin sayısına göre lojistik regresyon modelleri
    (yani "multivariate lojistik regresyon" modelleri) genellikle aşağıdaki gibi sınıflandırılmaktadır:

        - Tek Etiketli İki Sınıflı Sınıflandırma (Single Label Binary Classification) Modelleri: Bu modellerde çıktı yani etiket 
        bir tanedir. Etiket de iki sınıftan oluşmaktadır. Örneğin bir tümörün iyi huylu mu kötü huylu mu olduğunu kestirimeye çalışan 
        model tek etiketli iki sınıflı modeldir.

        - Tek Etiketli Çok Sınıflı Sınıflandırma (Single Label Multiclass) Modelleri: Burada bir tane çıktı vardır. Ancak çıktı
        ikiden fazla sınıftan oluşmaktadır. Örneğin bir resmin "elma mı, armut mu, kayısı mı" olduğunu anlamaya çalışan sınıflandırma
        problemi tek etiketli çok sınıflı bir modeldir. 

        - Çok Etiketli İki Sınıflı Sınıflandırma (Multilabel Binary Classification) Modelleri: Çok etiketli modeller denildiğinde
        zaten default olarak iki sınıflı çok etiketli modeller anlaşılmaktadır. Örneğin bir yazının içeriğine göre yazıyı 
        tag'lamak istediğimizde her tag ayrı bir etikettir. O tag'ın olması ya da olmaması da iki sınıflı bir çıktı belirtmektedir. 

        - Çok Etiketli Çok Sınıflı Sınıflandırma (Multilabel Multiclass / Multidimentional Classification) Modelleri: Bu tür
        modellere genellikle "çok boyutlu (multidimentional)" modeller denilmektedir. Yani çıktı birden fazladır. Her çıktı da
        ikiden fazla sınıfa ilişkin olabilmektedir. Bu modelleri çok etiketli sınıflandırma modellerinin genel biçimi olarak 
        düşünebilirsiniz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de tek etiketli çok sınıflı bir sınıflandırma problemine örnek verelim. Örneğimizde "iris (zambak)" isimli bir veri 
    kümesini kullanacağız. Bu veri kümesi bu tür uygulamalarda örnek veri kümesi olarak çok sık kullanılmaktadır. Veri kümesi a
    şağıdaki bağlantıdan indirilebilir:

    https://www.kaggle.com/datasets/uciml/iris?resource=download

    Yukarıdaki bağlantıdan Iris veri kümesi ibir zip dosyası biçiminde indirilmektedir. Bu zip dosyası açıldığında "Iris.csv" 
    dosyası elde edilecektir.

    Veri kümesi aşağıdaki görünümdedir:

    Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
    1,5.1,3.5,1.4,0.2,Iris-setosa
    2,4.9,3.0,1.4,0.2,Iris-setosa
    3,4.7,3.2,1.3,0.2,Iris-setosa
    4,4.6,3.1,1.5,0.2,Iris-setosa
    5,5.0,3.6,1.4,0.2,Iris-setosa
    6,5.4,3.9,1.7,0.4,Iris-setosa
    7,4.6,3.4,1.4,0.3,Iris-setosa
    8,5.0,3.4,1.5,0.2,Iris-setosa
    9,4.4,2.9,1.4,0.2,Iris-setosa
    ......

    Veri kümesinde üç grup zambak vardır: "Iris-setosa", "Iris-versicolor" ve "Iris-virginica". x verileri ise çanak (sepal) 
    yaprakların ve taç (petal) yaprakların genişlik ve yüksekliklerine ilişkin dört değerden oluşmaktadır. Veri kümesi içerisinde 
    "Id" isimli ilk sütun sıra numarası belirtir. Dolayısıyla kestirim sürecinde bu sütunun bir faydası yoktur.
 
    Çok sınıflı sınıflandırma problemlerinde çıktıların (yani y verilerinin) "one-hot-encoding" işlemine sokulması gerekir. Çıktı 
    sütunu one-hot-encoding yapıldığında uygulamacının hangi sütunların hangi sınıfları belirttiğini biliyor olması gerekir. 
    (Anımsanacağı gibi Pandas'ın get_dummies fonksiyonu aslında unique fonksiyonunu ile elde ettiği unique değerleri sort ettikten 
    sonra "one-hot-encoding" işlemi yapmaktadır. (Dolayısıyla aslında get_dummies fonksiyonu sütunları kategorik değerleri küçükten 
    büyüğe sıraya dizerek oluşturmaktadır. scikit-learn içerisindeki OneHotEncoder sınıfı zaten kendi içerisinde categories_ özniteliği 
    ile bu sütunların neler olduğunu bize vermektedir. Tabii aslında OneHotEncoder sınıfı da kendi içerisinde unique işlemini 
    uygulamaktadır. NumPy'ın unique fonksiyonunun aynı zamanda sıraya dizmeyi de yaptığını anımsayınız. Yani aslında categories_ 
    özniteliğindeki kategoriler de leksikografik olarak sıraya dizilmiş biçimdedir.)

    Veri kümesini aşağıdaki gibi okuyabiliriz:

    df = pd.read_csv('Iris.csv')

    x verilerini aşağıdaki gibi ayrıştırabiliriz:

    dataset_x = df.iloc[:, 1:-1].to_numpy(dtype='float32')

    y verilerini aşağıdaki gibi onet hot encoding yaparak ayrıştırabiliriz:

    ohe = OneHotEncoder(sparse= False)
    dataset_y = ohe.fit_transform(df.iloc[:, -1].to_numpy().reshape(-1, 1))

    Anımsanacağı gibi çok sınıflı sınıflandırma problemlerindeki loss fonksiyonu "categorical_crossentropy", çıktı katmanındaki 
    aktivasyon fonksiyonu "softmax" olmalıdır. Metrik değer olarak "binary_accuracy" yerine "categorical_accuracy" kullanılmalıdır.
    (Keras metrik değer olarak "accuracy" girildiğinde zaten problemin türüne göre onu "binary_accuracy" ya da "categorical_accuracy"
    biçiminde ele alabilmektedir.) Veri kümesi yine özellik ölçeklemesine sokulmalıdır. Bunun için standart ölçekleme kullanılabilir.
    Sinir ağı modelini şöyle oluşturulabiliriz:

    model = Sequential(name='Iris')
    model.add(Input((training_dataset_x.shape[1], ), name='Input'))
    model.add(Dense(64, activation='relu', name='Hidden-1'))
    model.add(Dense(64, activation='relu', name='Hidden-2'))
    model.add(Dense(dataset_y.shape[1], activation='softmax', name='Output'))
    model.summary()

    Çok sınıflı modellerin çıktı katmanında sınıf sayısı kadar nöron olması gerektiğini belirtmiştik. Çıktı katmanında aktivasyon 
    fonksiyonu olarak softmax alındığı için çıktı değerlerinin toplamı 1 olmak zorundadır. Bu durumda biz kestirim işlemi yaparken 
    çıktıdaki en büyük değerli nöronu tespit etmemiz gerekir. Tabii aslında bizim en büyük çıktıya sahip olan nöronun çıktı değerinden 
    ziyade onun çıktıdaki kaçıncı nöron olduğunu tespit etmemiz gerekmektedir. Bu işlem tipik olarak NumPy kütüphanesindeki 
    argmax fonksiyonu ile yapılabilir. Peki varsayalım ki ilki 0 olmak üzere 2 numaralı nöronun değeri en yüksek olmuş olsun. 
    Bu 2 numaralı nöron hangi sınıfı temsil etmektedir? İşte bu 2 numaralı nöron aslında eğitimdeki dataset_y sütununun one hot 
    encoding sonucundaki 2 numaralı sütununu temsil eder. O halde bizim dataset_y değerlerini one-hot-encoding yaparken hangi 
    sütunun hangi sınıfa karşı geldiğini biliyor olmamız gerekir. Zaten OneHotEncoder sınıfının bu bilgiyi categories_ örnek
    özniteliğinde sakladığını anımsayınız.

    Aşağıda örneğin tüm kodu bütünsel olarak verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('Iris.csv')

dataset_x = df.iloc[:, 1:-1].to_numpy(dtype='float32')

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse= False)
dataset_y = ohe.fit_transform(df.iloc[:, -1].to_numpy().reshape(-1, 1))

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential(name='Iris')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(64, activation='relu', name='Hidden-1'))
model.add(Dense(64, activation='relu', name='Hidden-2'))
model.add(Dense(dataset_y.shape[1], activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

scaled_test_dataset_x = ss.transform(test_dataset_x)
eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_dataset_x = pd.read_csv('predict-iris.csv').to_numpy(dtype='float32')
scaled_predict_dataset_x = ss.transform(predict_dataset_x)

import numpy as np

predict_result = model.predict(scaled_predict_dataset_x)
predict_indexes = np.argmax(predict_result, axis=1)

for pi in predict_indexes:
    print(ohe.categories_[0][pi])

"""
predict_categories = ohe.categories_[0][predict_indexes]
print(predict_categories)
"""

#predict-iris.csv

4.8,3.4,1.6,0.2
4.8,3.0,1.4,0.1
4.3,3.0,1.1,0.1
6.6,3.0,4.4,1.4
6.8,2.8,4.8,1.4
6.7,3.0,5.0,1.7
6.0,2.9,4.5,1.5
5.7,2.6,3.5,1.0
6.3,2.5,5.0,1.9
6.5,3.0,5.2,2.0
6.2,3.4,5.4,2.3
5.9,3.0,5.1,1.8

#----------------------------------------------------------------------------------------------------------------------------
                                            39. Ders 25/05/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesinin en önemli uygulama alanlarından biri de "doğal dil işleme (natuaral lanuage processing)" 
    alanıdır. Doğal dil işleme denildiğinde Türkçe, İngilizce gibi konuşma dilleri üzerindeki her türlü işlemler kastedilmektedir. 
    Bugün doğal dil işlemede artık ağırlıklı olarak makine öğrenmesi teknikleri kullanılmaktadır. Örneğin makine çevirisi (machine 
    translation) süreci aslında doğal dil işlemenin bir konusudur ancak bugün artık makine çevirileri artık neredeyse tamamen 
    makine öğrenmesi teknikleriyle  yapılmaktadır. Doğal dil işleme alanı ile ilişkili olan diğer bir alan da "metin madenciliği 
    (text mining)" denilen alandır. Metin madenciliği metinler içerisinden faydalı bilgilerin çekilip alınması ve onlardan faydalanılması 
    ile ilgili süreçleri belirtmektedir. Bugün metin madenciliğinde de yine veri bilimi ve makine öğrenmesi teknikleri yoğun 
    olarak kullanılmaktadır. 

    Metinler üzerinde makine öğrenmesi teknikleri uygulanırken metinlerin önişlemlere sokularak sayısal biçime dönüştürülmesi 
    gerekir. Çünkü makine öğrenmesi tekniklerinde yazılar üzerinde değil sayılar üzerinde işlemler yapılmaktadır. Bu nedenle
    makine öğrenmesinde yazıların önişleme sokulması ve sayısal hale dönüştürülmesi sürecinde doğal dil işleme ve metin madenciliği 
    alanlarındaki daha önceden elde edilmiş bilgiler ve deneyimlerden faydalanılmaktadır. Örneğin bir film hakkında aşağıdaki gibi 
    bir yazı olsun:

    "Filmi pek beğenmedim. Oyuncular iyi oynayamamışlar. Filmde pek çok abartılı sahneler de vardı. Neticede filmin iyi mi 
    kötü mü olduğu konusunda kafam karışık. Size tavsiyem filme gidip boşuna para harcamayın!"

    Bu yazıyı sayısal hale dönüştürmeden önce yazı üzerinde bazı önişlemlerin yapılması gerekebilmektedir. Tipk önişlemler 
    şunlardır:

    - Yazıyı "atom (token)" denilen en parçalara ayırmak ve noktalama işaretlerini atmak (tokenizing). Metinlerin atomlarına 
    ayrılmasında çeşitli yöntemler kullanılabiliyorsa da en fazla kullanılan "sözcüklerden atomlarına ayırma" yöntemidir. Buna 
    "word tokenization" denilmektedir. Yazılar atomlarına karakterlerden de ayrılabilmektedir. Buna da "character tokenization" 
    denilmektedir. Bu yöntemde her karakter bir atom haline gelmektedir. Yazıların atomlarına ayrılmasında kullanılan diğer bir 
    tekniğe de "subword tokenization" denilmektedir. Bu teknikte atomlar sözcük parçalarından da oluşabilmektedir. Biz kursumuzda 
    yazıları sözcüklerinden atomlarına ayıracağız. Yani "word tokenization" tekniğini kullanacağız. Bu nedenle kursumuzda "atom
    (token)" yerine "sözcük (word)" de diyeceğiz.  Yazları atomlarına ayırmak için kullanılan diğer teknikler Derneğimizde 
    "Doğal Dil İşleme ve Büyük Dil Modelleri" kursumuzda ele alınmaktadır.
    
    - Sözükleri küçük harfe ya da büyük harfe dönüştürmek (transformation).
    
    - Kendi başına anlamı olyaman edat gibi, soru ekleri gibi sözcüklerin atılması (bunlara İngilizce "stop words") 
    denilmektedir. 
    
    - Sözcüklerin köklerini elde etmek ve köklerini kullanmak (stemming).
    
    - Bağlam içerisinde farklı sözcükleri aynı sözcükle yer değiştirmek (lemmatization).

    Yukarıdaki işlemleri yapabilen çeşitli kütüphaneler de bulunmaktadır. Bunlardan Python'da en çok kullanılanlardan biri 
    NLTK isimli kütüphanedir. 

    Sözcükleri birbirinden bağımsız sayılarmış gibi ele alarak denetimli ya da denetimsiz modeller oluşturulabilmektedir. 
    Ancak son 20 yılda yazılardaki sözcüklerin bir bağlam içerisinde ele alınabilmesine yönelik sinir ağları geliştirilmiştir.
    Bunun ağlarda sözcüklerin sırası dikkate alınmakta ve sinir ağına bir hafıza kazandırılmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sınıflandırma problemlerinde üzerinde çokça çalışılan İngilizce "sentiment analysis" denilen bir problem grubu vardır.  
    Biz buna Türkçe "duygu analizi" diyeceğiz. Duygu analizi problemlerinde kişiler bir olgu hakkında kanılarını belirten yazılar 
    yazarlar.  Bu yazılar tipik olarak "olumlu", "olumsuz" biçiminde iki sınıflı etiketlenmektedir. Ancak çok sınıflı 
    etiketlendirmeler de söz konusu olabilmektedir. Böylece eğitim sonrasında bir yazının olumulu mu olumsuz mu olduğu yönünde
    kestirimler yapılabilmektedir.
    
    Duygu analizi için oluşturulmuş çeşitli örnek veri kümeleri vardır. Bunlardan en çok kullanılanlarından biri "IMDB (Internet 
    Movie Database)" isimli veri kümesidir. Bu veri kümesinde kişiler filmler hakkında yorum yazıları yazmışlardır. Bu yorum
    yazıları "olumlu (positive)" ya da "olumsuz (negative)" olarak etiketlendirilmiştir. Böylece eğitim sonrasında birisinin
    yazdığı yazının olumlu ya da olumsuz yargı içerdiği otomatik olarak tespit edilebilmektedir. 

    IMDB veri kümesindeki girdiler (yani dataset_x) yazılardan oluşmaktadır. Çıktılar ise (yani dataset_y) "olumlu" ya da 
    "olumsuz" biçiminde ikili bir çıktıdır. IMDB veri kümesini aşağıdaki bağlantıdan indirebilirsiniz:

    https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

    Buradan veri kümesi zip dosyası olarak indirilir. Açıldığında "IMDB Dataset.csv" isimli CSV dosyası elde edilmektedir. 
    Bu CSV dosaysında "review" ve "sentiment" isimli iki sütun vardır. "review" sütunu film hakkındaki yorum yazılarını 
    "sentiment" sütunu ise "positive" ya da "negative" biçiminde kanıları içermektedir. Buradaki modelin "ikili sınflandırma" 
    problemi olduğuna dikkat ediniz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi metin sınıflandırma problemlerinde önce metinlerin sayısal biçime dönüştürülmesi gerekir. 
    Metinlerin sayısal hale dönüştürülmesi için tipik olarak üç yöntem kullanılmaktadır:

    1) Sözcük Çantası (Bag of Words) Yöntemi. (Bu yöntem kısaca "BoW" ile temsil edilir. Biz bu yönteme "vektörizasyon" da
    diyeceğiz. 
    2) TF-IDF (Term Frequency - Inverse Document Frequencey) Yöntemi
    2) Sözcük Gömme (Word Embedding) Yöntemi

    Bu yöntemlerin hepsinde yazılar önce faydalı bazı önişlemlere sokulur ve "atom (token) denilen küçük parçalara ayrılır. 
    Sonra bu atomlar sayısal biçime dönüştürülür. Yazıdaki atomlar tipik olarak sözcüklerdir. Ancak yan yana birkaç sözcük 
    de tek bir atom olarak ele alınabilmektedir. 
    
    Yazıların atomlarına ayrılmasında çeşitli teknikler kullanılmaktadır. Ancak biz bu kursumuzda yazıları sözcük temelinde 
    atomlarına ayıracağız. Derneğimizde "Doğal Dil İşleme ve Büyük Dil Modelleri" kursumuzda atomlar ayırma işleminin ayrıntıları
    üzerinde durulmaktadır. 

    Yazıların sayısal biçime dönüştürülmesi için en basit yöntem "sözcük çantası (BoW)" ve "TF-IDF" yöntemleridir. Biz de bu 
    bölümde bu yöntemler üzerinde duracağız. Sözcük gömme yöntemini daha ileride ele alacağız.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------    
    Sözcük çantası (BoW) yöntemi şöyle uygulanmaktadır:
    
    1) Tüm yorumlardaki tüm sözcüklerin (atomların) kümesine "kelime haznesi (vocabulary)" denilmektedir. Örneğin IMDB veri 
    kümesinde tek olan (unique) tüm sözcüklerin sayısı 50000 ise kelime haznesi bu 50000 sözcükten oluşmaktadır. 

    2) Veri kümesindeki x verileri yorum sayısı kadar satırdan, sözcük haznesindeki sözcük (atom) sayısı kadar sütundan oluşan 
    iki boyutlu bir matris biçiminde oluşturulur. Örneğin sözcük haznesindeki sözcük (atom) sayısı 50000 ise ve toplamda veri 
    kümesinde 10000 yorum varsa x veri kümesi 10000x50000 büyüklüğünde bir matris biçimindedir. Her yorum bu matriste bir satır 
    ile temsil edilmektedir. Burada yoruma ilişkin satırda eğer sözcük haznesindeki bir sözcük (atom) kullanılmışsa o sözcüğe 
    ilişkin sütun 1, kullanılmamışsa 0 yapılmaktadır. Böylece yorum yazıları 0'lardan ve 1'lerden oluşmuş olan eşit uzunluklu 
    sayı dizilerine dönüştürülmüş olur. Bu ikili (binary) bir vektörüzasyondur. Tabii satırın sütunlarında ilgili sözcüğün 
    (atomun) yorumda kaç kere kullanıldığına ilişkin bir frekans sayısı da tutulabilir. Bu durumda satırın sütunları sözcük 
    (atom) frekanslarından oluşacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                40. Ders - 26/05/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Örneğin elimizdeki yorum yazıları şunlar olsun:

    "Film güzeldi. Ancak uzundu."
    "Film güzel değildi."
    "Film kötüydü. Tavsiye etmem."
    "Film iyiydi. Tavsiye ederim."
    "Film kötü değildi."

    Yukarıda da belirttiğimiz gibi yazılar üzerinde doğal dil işlemede kullanılan bazı tekniklerle önişlem yapılması uygun 
    olmaktadır. Ancak biz burada yalnızca noktalama işaretlerini atıp sözcükleri küçük harfe dönüştürüp atomları Bu durumda 
    sözcük haznesi şöyle olacaktır:

    film 
    güzeldi
    ancak
    güzel
    uzundu
    değildi
    kötüydü
    tavsiye
    etmem
    iyiydi
    ederim
    kötü

    Burada tüm bu sözcükleri bir sözlükte toplayarak bunlara birer numara verelim:

    film            0
    güzeldi         1
    ancak           2
    uzundu          3
    güzel           4
    değildi         5
    kötüydü         6
    tavsiye         7
    etmem           8
    iyiydi          9
    ederim          10
    kötü            11

    Örneğimizde toplam 5 tane yorum vardır ve sözcük haznesinin büyüklüğü de 12'dir. Bu durumda örneğin birinci yorum aşağıdaki 
    gibi ikili bir vektöre dönüştürülür:

    0 1 2 3 4 5 6 7 8 9 0 1 
    ----------------------------
    1 1 1 1 0 0 0 0 0 0 0 0  

    Diğer yorumlar da aşağıdaki gibi dönüştürülecektir:

    0 1 2 3 4 5 6 7 8 9 0 1 
    ----------------------------
    1 1 1 1 0 0 0 0 0 0 0 0         (1. Yorum: "Film güzeldi. Ancak uzundu.")
    1 0 0 0 1 1 0 0 0 0 0 0         (2. Yorum: "Film güzel değildi.")
    1 0 0 0 0 0 1 1 1 0 0 0         (3. Yorum: "Film kötüydü. Tavsiye etmem.")
    1 0 0 0 0 0 0 1 0 1 1 0         (4. Yorum: "Film iyiydi. Tavsiye ederim.")
    1 0 0 0 0 1 0 0 0 0 0 1         (5. Yorum: "Film kötü değildi.")

    Tabii yorum yazılarını sözcüklerin numaralarından oluşan sayılar biçiminde de ifade edebiliriz. Örneğin:

    0, 1, 2, 3                      (1. Yorum: "Film güzeldi. Ancak uzundu.")
    0, 4, 5                         (2. Yorum: "Film güzel değildi.")
    0, 6, 7, 8                      (3. Yorum: "Film kötüydü. Tavsiye etmem.")
    0, 7, 9, 10                     (4. Yorum: "Film iyiydi. Tavsiye ederim.")
    0, 11, 5                        (5. Yorum: "Film kötü değildi.")

    BoW yöntemi fazlaca bellek kullanma eğilimindedir. Veri yapıları dünyasında çok büyük kısmı 0 olan, az kısmı farklı değerde 
    bulunan matrislere "seyrek matris (sparse matrix)" denilmektedir. Buradaki vektörlerin seyrek biçimde olduğuna dikkat ediniz. 
    Eğer sözcük haznesi çok büyükse gerçekten de tüm girdilerin yukarıda belirtildiği gibi bir matris içerisinde toplanması zor 
    ve hatta imkansız olabilmektedir. Çünkü fit metodu bizden training_dataset_x ve training_dataset_y yi bir bütün olarak 
    istemektedir. Bu durumda değişik teknikler kullanılabilmektedir. İzleyen paragraflarda bu teknikler üzerinde de duracağız. 

    Yukarıdaki gibi vektörizasyon işleminde sözcükler (atomlar) arasında sırasal bir ilişkinin ortadan kaldırıldığına dikkat 
    ediniz. Bu biçimde uygulanan vektörüzasyon sözcükleri (atomları) bağlamı içerisinde değerlendirmeye olanak sağlamayacaktır. 
    Ayrıca yukarıdaki vektörizasyon ikili (binary) biçimdedir. Yani yazı içerisinde aynı sözcükten birden fazla kez kullanılmış 
    olsa da o sözcüğe ilişkin sütun elemanı 1 yapılmıştır. Ancak yukarıda da belirttiğimiz gibi istenirse vektör ikili olmaktan 
    çıkartılıp sözcüklerin frekanslarıyla da oluşturulabilir. Örneğin "film" sözcüğü yazı içerisinde 10 kere geçmişse vektörde 
    ona karşılık gelen📨eleman 1 yapılmak yerine 10 yapılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de IMDB örneğinde yukarıda açıkladığımız ikili vektörüiasyon işlemini programlama yoluyla yapalım. Önce veri kümesini 
    okuyalım:

    df = pd.read_csv('IMDB Dataset.csv')

    Şimdi tüm yorumlardaki farklı olan tüm sözcüklerden bir sözcük haznesi (vocabulary) oluşturalım:

    import re

    vocab =  set()
    for text in df['review']:
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        vocab.update(words)

    Burada Python'daki "düzenli ifade (regular expression)" kütüphanesinden faydalanılmıştır. re modülündeki findall fonksiyonu 
    ile yazı içerisindeki sözcükler elde edilmiştir. Şimdi de sözcük haznesindeki her bir sözcüğe bir numara verelim. Sözcüğe 
    göre arama yapılacağı için bir sözlük nesnesinin kullanılması uygun olacaktır. Bu işlem sözlük içlemi ile tek hamlede 
    gereçekleştirilebilir:

    vocab_dict = {word: index for index, word in enumerate(vocab)}

    Aslında burada yapılan şey aşağıdaki ile aynıdır:

    vocab_dict = {} 
    for index, word in enumerate(vocab):
        vocab_dict[word] = index

    Şimdi artık x veri kümesini oluşturalım. Bunun için önce içi sıfırlarla dolu bir matris oluşturalım. Bu matrisin satır sayısı 
    len(df) kadar (yani yorum sayısı kadar) sütun sayısı ise sözcük haznesi kadar (yani len(vocab) kadar) olmalıdır:

    dataset_x = np.zeros((len(df), len(vocab)), dtype='uint8')  

    Şimdi yeniden tüm yorumları tek tek sözcüklere ayırıp onları sayısallaştırıp dataset_x matrisinin ilgili satırının ilgili
    sütunlarını 1 yapalım:

    for row, text in enumerate(df['review']):
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        word_numbers = [vocab_dict[word] for word in words]
        dataset_x[row, word_numbers] = 1

    y değerlerini de "positive" için 1, "negatif" için 0 biçiminde oluşturabiliriz:

    dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

    Veri kümsini eğitim ve test biçiminde ikiye ayırabiliriz:
        
    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

    Artık dataset_x ve dataset_y hazırlanmıştır. Bundan sonra ikili sınıflandırma problemi için daha önce kullandığımız sinir 
    ağı modellerini aynı biçimde oluşturabiliriz Ancak girdi katmanında çok fazla nöron olduğu için katmanlardaki nöron sayılarını 
    yükseltebiliriz.

    model = Sequential(name='IMDB')

    model.add(Input((training_dataset_x.shape[1],)))
    model.add(Dense(128, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
    hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

    Bu veri kümesinden bu haliyle %88 civarında bir başarı elde edilmektedir. Veri kümesi  için predict işlemi de yine benzer 
    biçimde yapılabilir. Bunun için aşağıdaki gibi "predict.imdb" isimli CSV dosyası oluşturabiliriz:

    review
    The movie was very bad. The players showed a poor performance. The script is not interesting.
    I liked the movie very much. The players also played well. The ending of the movie was very surprising.
    The movie is a work of medium quality. I think it's neither good nor bad.
    Great movies should always leave you with a feeling during or after seeing them.
    The worst movie I've ever seen in my life
    I can't say that I am very happy that I went to this movie.

    Bu CSV dosyasının okunması ve hazır hale getirilmesi işlemi de şöyle yapılmıştır:

    predict_df = pd.read_csv('predict-imdb.csv')

    predict_dataset_x = np.zeros((len(predict_df), len(vocab)))
    for row, text in enumerate(predict_df['review']):
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        word_numbers = [vocab_dict[word] for word in words]
        predict_dataset_x[row, word_numbers] = 1

    Kestirim işlemini de şöyle yapabiliriz:

    predict_result = model.predict(predict_dataset_x)
    for presult in predict_result[:, 0]:
        if (presult > 0.5):
            print('Positive')
        else:
            print('Negative')

    Peki biz binary vektör haline geitirilmiş yazıyı bu vektörden hareketle yeniden orijinal haline getirebilir miyiz? Hayır
    getiremeyiz. Çünkü biz burada binary vektör oluşturduğumuz için sözük sıralarını ve sıklıklarını kaybetmiş durumdayız. Ancak 
    anlamsız olsa da bir vektörü bir yazı haline aşağıdaki gibi getirebiliriz:

    rev_vocab_dict = {index: word for word, index in vocab_dict.items()}

    word_indices = np.argwhere(dataset_x[0] == 1).flatten()
    words = [rev_vocab_dict[index] for index in word_indices]
    text = ' '.join(words)
    print(text)

    NumPy'ın where ya da argwhere fonksiyonları belli koşulu sağlayan elemanların indekslerini bize verebilmektedir. Buradaki
    argwhere fonksiyonu bize iki boyutlu bir dizi geri döndürür. Biz de onu flatten (ya da reshape) fonkisyonu ile tek boyutlu 
    dizi haline getirdik, sonra da liste işlemiyle bu indekslere karşı gelen sözcükleri bir liste biçiminde elde ettik. Nihayetinde 
    de bunları oin metodu aralarına SPACE karakterlerini koyarak join ile tek bir yazı biçimine dönüştürdük.

    Aşağıda örnek bir bütün olarak verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

vocab =  set()

import re

for text in df['review']:
    words = re.findall('[a-zA-Z0-9]+', text.lower())
    vocab.update(words)
   
vocab_dict = {word: index for index, word in enumerate(vocab)}

"""
vocab_dict = {} 

for index, word in enumerate(vocab):
    vocab_dict[word] = index
"""  
      
import numpy as np

dataset_x = np.zeros((len(df), len(vocab)), dtype='uint8')  

for row, text in enumerate(df['review']):
    words = re.findall('[a-zA-Z0-9]+', text.lower())
    word_numbers = [vocab_dict[word] for word in words]
    dataset_x[row, word_numbers] = 1

dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
# prediction

predict_df = pd.read_csv('predict-imdb.csv')

predict_dataset_x = np.zeros((len(predict_df), len(vocab)))
for row, text in enumerate(predict_df['review']):
    words = re.findall('[a-zA-Z0-9]+', text.lower())
    word_numbers = [vocab_dict[word] for word in words]
    predict_dataset_x[row, word_numbers] = 1
    
predict_result = model.predict(predict_dataset_x)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

# dataset_x'teki birinci yorumun yazı haline getirilmesi

rev_vocab_dict = {index: word for word, index in vocab_dict.items()}

word_indices = np.argwhere(dataset_x[0] == 1).flatten()
words = [rev_vocab_dict[index] for index in word_indices]
text = ' '.join(words)
print(text)
 
#----------------------------------------------------------------------------------------------------------------------------   
                                            41. Ders - 01/06/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yazıların yukarıda uyguladığımız gibi gibi BoW yöntemiyle binary bir vektöre dönüştürülmesinin görünen açık dezavantajları 
    şunlardır:

    - Aynı sözcük birden fazla kez yazı içerisinde kullanılmışsa bunun eğitimde bir anlamı kalmamaktadır. Oysa gerçek hayattaki 
    yazılarda örneğin "mükemmel" gibi bir sözcük çokça tekrarlanıyorsa bu durum onun olumlu yorumlanma olasılığını artırmaktadır.
    Yani yukarıdaki örnekte sözcük frekansları dikkate alınmamıştır. Tabii biz vektörüzasyonu sözcük frekanslarını kullanarak 
    da yapabiliriz. Bu durumda bu dezavantaj ortadan kalkabilecektir.
    
    - BoW yöntemi bir bağlam oluşturamamaktadır. Şöyle ki: Biz bir yazıda "çok kötü" dersek buradaki "çok" aslında "kötüyü" 
    nitelemektedir. Ancak bunu bizim ağımız anlayamaz. Örneğin biz yorumdaki sözcüklerin sırasını değiştirsek de elde 
    ettiğimiz vektör değişmeyecektir. 

    - BoW işlemi çok yer kaplama potansiyelinde olan bir işlemdir. Bu durumda ağı parçalı olarak eğitmek zorunda kalabiliriz. 
    Parçalı eğitimde training_dataset_x ve training_dataset_y fit metoduna tek hamlede verilmez. Parça parça verilir. Keras'ta 
    parçalı eğitim izleyen paragraflarda ele alınacaktır.

    - Biz yukarıdaki örnekte doğal dil işlemede kullanılan bazı önişlemleri hiç yapmadık. Bu önişlemlerin yapılması modelin
    performansını artıracaktır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında vektörizasyon işlemi pratik bir biçimde scikit-learn kütüphanesindeki sklearn.feature_extraction.text modülünde 
    bulunan CountVectorizer sınıfıyla yapılabilmektedir. Sınıfın kullanımı şöyledir: 

    1) Önce CountVectorizer sınıfı türünden bir nesne yaratılır. Nesne yaratılırken sınıfın __init__ metodunda bazı önemli 
    belirlemeler yapılabilmektedir. Sınıfın __init__ metodunun parametrik yapısı şöyledir:

    class sklearn.feature_extraction.text.CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', 
            strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, 
            token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, 
            max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)
    
    Metodun dtype parametresi elde edilecek vektörün elemanlarının türünü belirtmektedir. Bu parametreyi elde edilecek matrisin 
    kaplayacğı yeri azaltmak için 'uint8' gibi küçük bir tür olarak geçmek istebilirsiniz. Default durumda bu dtype parametresi 
    'float64' biçimindedir. Sınıf yine default durumda tüm sözcükleri küçük harfe dönüştürmektedir. Ancak metodun lowercase 
    parametresi False geçilirse bu dönüştürme yapılmamaktadır. Metodun diğer önemli parametreleri de vardır. Metodun stop_words 
    parametresi "stop word" denilen anlamsız sözcükleri atmak için kullanılabilir. Bu parametreye stop words'lerden oluşan bir 
    liste ya da NumPy dizisi girilirse bu sözcükler sözcük haznesinden atılmaktadır. Başka bir deyişle yokmuş gibi ele alınmaktadır. 
    Metodun binary parametresi default olarak False biçimdedir. Bu durumda bir yazı içerisinde aynı sözcükten birden fazla kez 
    geçerse vektörün ilgili elemanı 1 değil o sözcüğün sayısı olacak biçimde set edilmektedir. Biz eğer yukarıkdaki örneğimizde 
    olduğu gibi binary bir vektör oluşturmak istiyorsak bu parametreyi True yapabiliriz. Metodun diğer parametreleri için scikit-learn 
    dokümanlarına başvurabilirsiniz.
    
    Örneğin:

    cv = CountVectorizer(dtype='uint8', stop_words=['de', 'bir', 've', 'mu'], binary=False)

    2)  Bundan sonra scikit-learn kütüphanesinin diğer sınıflarında olduğu gibi fit ve trasform işlemleri yapılır. fit işleminde biz 
    fit metoduna yazılardan oluşan dolaşılabilir bir nesne veriririz. fit metoudu tüm yazılardan bir "sözlük haznesi (vocabulary)" 
    oluşturur. Biz de bu sözlük haznesini bir sözlük nesnesi biçiminde nesnenin vocabulary_ özniteliğinden elde edebiliriz. Bu 
    vocabulary_ tıpkı bizim yukarıdaki örnekte yaptığımız gibi anahtarları sözcükler değerleri de sözcüklerin indeksinden oluşan bir
    sözlğk biçimindedir. Örneğin:

    texts = ["film güzeldi ve senaryo iyidi", "film berbattı, tam anlamıyla berbattı", "seyretmeye değmez", 
                "oyuncular güzel oynamışlar", "senaryo berbattı, böyle senaryo olur mu?", "filme gidin de bir de siz görün"]

    cv = CountVectorizer(dtype='uint8', stop_words=['de', 'bir', 've', 'mu'], binary=True)
    cv.fit(texts)

    fit işlemi sonrasında elde edilen vocabulary_ sözlüğü şöyledir:

    {'film': 4, 'güzeldi': 9, 'senaryo': 14, 'iyidi': 10, 'berbattı': 1, 'tam': 17, 'anlamıyla': 0, 'seyretmeye': 15, 'değmez': 3, 
    'oyuncular': 13, 'güzel': 8, 'oynamışlar': 12, 'böyle': 2, 'olur': 11, 'filme': 5, 'gidin': 6, 'siz': 16, 'görün': 7}
        
    fit medodunun yalnızca sözük haznesi oluşturduğuna dikkat ediniz. Asıl dönüştürmeyi transform metodu yapmaktadır. Ancak tranform 
    bize vektörel hale getirilmiş olan yazıları "seyrek matris (sparse matrix)" biçiminde csr_matrix isimli bir sınıf nesnesi olarak 
    vermektedir. Bu sınıfın todense metodu ile biz bu seyrek matrisi normal matrise dönüştürebiliriz. Örneğin:

    dataset_x = cv.transform(dataset).todense()

    Aslında fit metodu herhangi bir dolaşılabilir nesneyi parametre olarak kabul etmektedir. Örneğin yazılar satır satır 
    bulunuyorsa biz doğrudan dosye nesnesini bu fit metoduna verebiliriz. Bu durumda tüm yazıları belleğe okumak zorunda kalmayız. 
    Örneğin:

    from sklearn.feature_extraction.text import CountVectorizer

    f = open('text.csv')
    cv = CountVectorizer()
    cv.fit(f)

    Artık bu CountVectorizer nesnesi predict işleminde de aynı biçimde kullanılabilir. 

    Aşağıda CountVectorizer sinıfının kullanımına ilişkin bir örnek verilmiştir. 
 #----------------------------------------------------------------------------------------------------------------------------

from sklearn.feature_extraction.text import CountVectorizer

texts = ["film güzeldi ve senaryo iyidi", "film berbattı, tam anlamıyla berbattı", "seyretmeye değmez", "oyuncular güzel oynamışlar", 
        "senaryo berbattı, böyle senaryo olur mu?", "filme gidin de bir de siz görün"]

cv = CountVectorizer(dtype='uint8', stop_words=['de', 'bir', 've', 'mu'])
cv.fit(texts)

print(cv.vocabulary_)

dataset_x = cv.transform(texts).todense()

print(dataset_x)

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi CountVectorizer sınıfının __init__ metodunun binary parametresi default olarak False durumdadır. 
    Bu parametrenin False olması yazı içerisinde belli bir sözcük n defa geçtiğinde o sözcüğe ilişkin sütun elemanın n olacağı 
    anlamına gelmektedir. Eğer bu parametre True yapılırsa bu durumda binary bir vector elde edilir. Peki biz vektörizasyon 
    yaparken "binary" mi yoksa frekanslı mı vektörizsyon yapmalıyız? Aslında frekanslı vektörizasyon yapmak toplamda daha iyidir. 
    Ancak binary bilgilerin tutulma biçiminden özel olarak bir kazanç sağlanmaya çalışılabilir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de IMDB veri kümesi için vektörizasyonu CountVectorizer sınıfı ile yapalım. Örneğimizde binary parametresini False 
    geçeceğiz. Dolaysıyla BoW satırlarının sütunları sözcük frekanslarından oluşacaktır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text  import CountVectorizer

cv = CountVectorizer(dtype='uint8', binary=False)

cv.fit(df['review'])
dataset_x = cv.transform(df['review']).todense()
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
# prediction

predict_df = pd.read_csv('predict-imdb.csv')

predict_dataset_x = cv.transform(predict_df['review']).todense()
predict_result = model.predict(predict_dataset_x)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

# dataset_x'teki birinci yorumun yazı haline getirilmesi

import numpy as np

rev_vocab_dict = {index: word for word, index in cv.vocabulary_.items()}

word_indices = np.argwhere(dataset_x[0] == 1)[:, 1]
words = [rev_vocab_dict[index] for index in word_indices]
text = ' '.join(words)
print(text)

#----------------------------------------------------------------------------------------------------------------------------
    Keras içerisinde tensorflow.keras.datasets modülünde IMDB veri kümesi de hazır biçimde bulunmaktadır. Diğer hazır veri 
    kümelerinde olduğu gibi bu IMDB veri kümesi de modüldeki load_data fonksiyonu ile yüklenmektedir. Örneğin:

    from tensorflow.keras.datasets import imdb

    (training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = imdb.load_data()

    Burada load_data fonksiyonunun num_words isimli parametresine eğer bir değer girilirse bu değer toplam sözcük haznesinin 
    sayısını belirtmektedir.  Örneğin:

    (training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = imdb.load_data(num_words=1000)

    Burada artık yorumlar en sık kullanılan 1000 sözcükten hareketle oluşturulmaktadır. Yani bu durumda vektörizasyon sonrasında 
    vektörlerin sütun uzunlukları 1000 olacaktır. Bu parametre girilmezse IMDB yorumlarındaki tüm sözükler dikkate alınmaktadır.

    Bize load_data fonksiyonu x verileri olarak vektörizasyon sonucundaki vektörleri vermemektedir. Sözcüklerin indekslerine ilişkin 
    vektörleri bir liste olarak vermektedir. (Bunun nedeni uygulamacının vektörizasyon yerine başka işlemler yapabilmesine olanak 
    sağlamaktır.) load_data fonksiyonun verdiği index listeleri için şöyle bir ayrıntı da vardır: Bu fonksiyon bize listelerdeki
    sözcük indekslerini üç fazla vermektedir. Bu sözcük indekslerindeki 0, 1 ve 2 indeksleri özel anlam ifade etmektedir. 
    Dolayısıyla aslında örneğin bize verilen 1234 numaralı indeks 1231 numaralı indekstir. Bizim bu indekslerden 3 çıkartmamız 
    gerekmektedir. 

    imdb modülündeki get_word_index fonksiyonu bize sözcük haznesini bir sözlük olarak vermektedir. num_words ne olursa olsun bu 
    sözlük her zaman tüm kelime haznesini içermektedir. Başka bir deyişle buradaki get_word_index fonksiyonu bizim kodlarımızdaki
    vocab_dict sözlüğünü vermektedir. Örneğin:

    vocab_dict = imdb.get_word_index()

    Bu durumda biz training_dataset_x ve test_dataset_x listelerini aşağıdaki gibi binary vector haline getiren bir fonksiyon
    yazabiliriz:
    
    def vectorize(sequence, colsize):
        dataset_x = np.zeros((len(sequence), colsize), dtype='uint8')
        for index, vals in enumerate(sequence):
            dataset_x[index, vals] = 1           
        return dataset_x

    Burada vectorize fonksiyonu indekslerin bulunduğu liste listesini ve oluşturulacak matrisin sütun uzunluğunu parametre 
    olarak almıştır. Fonksiyon binary biçimde vektörize edilmiş NumPy dizisi ile geri dönmektedir. Ancak biz bu fonksiyonu 
    kullanırken colsize parametresine get_word_index ile verilen sözlüğün eleman sayısından 3 fazla olan değeri geçirmeliyiz. 
    Çünkü bu indeks listelerinde 0, 1 ve 2 değerleri özel bazı amaçlarla kullanılmıştır. Dolayısıyla buradaki sözcük indeksleri 
    hep 3 fazladır. Yapay sinir ağımızda bu indekslerin 3 fazla olmasının bir önemi yoktur. Ancak ters dönüşüm uygulanacaksa 
    tüm indeks değerleriden 3 çıkartılmalıdır. O halde vektörizasonu şöyle yapabiliriz:

    training_dataset_x = vectorize(training_dataset_x, len(vocab_dict) + 3)
    test_dataset_x = vectorize(test_dataset_x, len(vocab_dict) + 3)

    Artık her şey tamadır. Yukarıda yaptığımız işlemleri yapabiliriz. 

    Kestirim işleminde de aynı duruma dikkat edilmesi gerekir. Biz eğitimi sözcük indekslerinin 3 fazla olduğu duruma göre 
    yaptık. O halde kestirim işleminde de aynı şeyi yapmamız gerekir. Yani kestirim yapılacak yazıyı get_word_index sözlüğüne
    sokup onun numarasını elde ettikten sonra ona 3 toplamalıyız. Bu biçimde liste listesi oluşturursak bunu yine yukarıda
    yazmış olduğumuz vectorize fonksiyonuna sokabiliriz. 

    predict_df = pd.read_csv('predict-imdb.csv')

    predict_list = []
    for text in predict_df['review']:
        index_list = []
        words = re.findall('[A-Za-z0-9]+', text.lower())
        for word in words:
            index_list.append(vocab_dict[word] + 3)
        predict_list.append(index_list)
        
    predict_dataset_x = vectorize(predict_list, len(vocab_dict) + 3)

    predict_result = model.predict(predict_dataset_x)

    Aşağıda örneğin tüm kodları verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.datasets import imdb

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = imdb.load_data()

vocab_dict = imdb.get_word_index()

import numpy as np

def vectorize(sequence, colsize):
    dataset_x = np.zeros((len(sequence), colsize), dtype='uint8')
    for index, vals in enumerate(sequence):
        dataset_x[index, vals] = 1       
    return dataset_x

training_dataset_x = vectorize(training_dataset_x, len(vocab_dict) + 3)
test_dataset_x = vectorize(test_dataset_x, len(vocab_dict) + 3)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

import pandas as pd
import re
    
predict_df = pd.read_csv('predict-imdb.csv')

predict_list = []
for text in predict_df['review']:
    index_list = []
    words = re.findall('[A-Za-z0-9]+', text.lower())
    for word in words:
        index_list.append(vocab_dict[word] + 3)
    predict_list.append(index_list)
    
predict_dataset_x = vectorize(predict_list, len(vocab_dict) + 3)

predict_result = model.predict(predict_dataset_x)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

# dataset_x'teki birinci yorumun yazı haline getirilmesi

rev_vocab_dict = {index: word for word, index in vocab_dict.items()}

word_indices = np.argwhere(training_dataset_x[0] == 1).flatten()
words = [rev_vocab_dict[index - 3] for index in word_indices if index > 2]
text = ' '.join(words)
print(text)
    
#----------------------------------------------------------------------------------------------------------------------------
    <KERAS'IN Tokenizer SINIFI BURAYA EKLENECEK>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            42. Ders - 02/06/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yazıların sınıflandırılması için çok kullanılan diğer bir veri kümesi de "Reuters" isimli veri kümesidir. Bu veri kümesi 
    80'lerin başlarında Reuters haber ajansının haber yazılarından oluşmaktadır. Bu haber yazıları birtakım konulara ilişkindir. 
    Dolayısıyla bu veri kmesi "çok sınıflı sınıflandırma" problemleri için örnek amacıyla kullanılmaktadır. Haberler toplam 
    46 farklı konuya ilişkindir. Veri kümesinin orijinali "çok etiketli (multilabel)" biçimdedir. Yani veri kümesindeki bazı 
    yazılara birden fazla etiket iliştirilmiştir. Ancak biz burada bir yazıya birden fazla etiket iliştirilmişse onun yalnızca 
    ilk etiketini alacağız. Böylece veri kümesini "çok etiketli (multilabel)" olmaktan çıkartıp "çok sınıflı (multiclass)" 
    biçimde kullanacağız
    
    Reuters veri kümesinin orijinali ".SGM" uzantılı dosyalar biçiminde "SGML" formatındadır. Dolayısıyla bu verilerin kullanıma 
    hazır hale getirilmesi biraz yorucudur. Ancak aşağıdaki bağlantıda Reuters veri kümesindeki her yazı bir dosya biçiminde 
    kaydedilmiş biçimde sunulmaktadır:

    https://www.kaggle.com/datasets/nltkdata/reuters

    Buradaki dosya indirilip açıldığında aşağıdaki gibi bir dizin yapısı oluşacaktır:

    training    <DIR>
    test        <DIR>
    cats.txt
    stopwords

    Ancak veri kümesini açtığınızda iç içe bazı dizinlerin olduğunu göreceksiniz. Bu dizinlerden yalznıca bri tanesini alıp 
    diğerlerini atabilirsiniz. 
    
    Buradaki "cats.txt" dosyası tüm yazıların kategorilerinin belirtildiği bir dosyadır. training dizininde ve test dizininide 
    her bir yazı bi rtext dosya biçiminde oluşturulmuştur. Buradaki text dosyaları okumak için "latin-1" encoding'ini 
    kullanmalısınız. Biz yukarıdaki dizin yapısını çalışma dizininde "ReutersData" isimli bir dizine çektik. Yani veri kümesinin
    dizin yapısı şu hale getirilmiştir:

    ReutersData
        training    <DIR>
        test        <DIR>
        cats.txt
        stopwords

    Burada veri kümesi "eğitim" ve "test" biçiminde zaten ikiye ayrılmış durumdadır. Dolayısıyla bizim veri kümesini ayrıca "eğitim"
    ve "test" biçiminde ayırmamıza gerek yoktur. Buradaki "cats.txt" dosyasının içeriği aşağıdaki gibidir:

    test/14826 trade
    test/14828 grain
    test/14829 nat-gas crude
    test/14832 rubber tin sugar corn rice grain trade
    test/14833 palm-oil veg-oil
    test/14839 ship
    test/14840 rubber coffee lumber palm-oil veg-oil
    ...
    raining/5793 nat-gas
    training/5796 crude
    training/5797 money-supply
    training/5798 money-supply
    training/5800 grain
    training/5803 gnp
    training/5804 gnp
    training/5805 gnp
    training/5807 gnp
    training/5808 acq
    training/5810 trade
    training/5811 money-fx
    training/5812 carcass livestock
    ...

    Reuters veri kümesinde ayrıca "stopwords" isimli bir dosya içersinde stop word'lerin listesi de verilmiştir. Bu sözcüklerin 
    sözcük haznesinden çıkartılması (yani stop word'lerin atılması) daha iyi bir sonucun elde edilmesine yol açabilecektir. 

    Biz burada vektörizasyon işlemini önce manuel bir biçimde ve binary olarak yapıp sonra CountVectorizer sınıfını kullanacağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Önce "cats.txt" dosyasını açıp buradaki bilgilerden training_dict ve test_dict isimli iki sözlük nesnesi oluşturalım. Bu 
    sözlük nesnelerinin anahtarları dosya isimleri değerleri ise o dosyadaki yazının sınıfını belirtiyor olsun:

    training_dict = {}
    test_dict = {}
    cats = set()

    with open('ReutersData/cats.txt') as f:
        for line in f:
            toklist = line.split()
            ttype, fname = toklist[0].split('/')
            if ttype == 'training':
                training_dict[fname] = toklist[1]
            else:
                if ttype == 'test':
                    test_dict[fname] = toklist[1]
            cats.add(toklist[1])
        
    vocab =  set()
    training_texts = []
    training_y = []
                    
    for fname in os.listdir('ReutersData/training'):
        with open('ReutersData/training/' + fname, encoding='latin-1') as f:
            text = f.read()
            training_texts.append(text)
            words = re.findall('[a-zA-Z0-9]+', text.lower())
            vocab.update(words)
            training_y.append(training_dict[fname])
            
    test_texts = []
    test_y = []
    for fname in os.listdir('ReutersData/test'):
        with open('ReutersData/test/' + fname, encoding='latin-1') as f:
            text = f.read()
            test_texts.append(text)
            words = re.findall('[a-zA-Z0-9]+', text.lower())
            vocab.update(words)
            test_y.append(test_dict[fname])
            
    Burada tüm sözcük haznesinin vocab isimli bir kümede tüm kategorilerin de cats isimli bir kümede toplandığına dikkat ediniz. 
    Hazır dosyaları açmışken dosyalar içerisindeki yazıları da training_texts ve test_texts isimli listelerde topladık. Ayrıca 
    her yazının kategorilerini de training_y ve test_y listelerinde topladığımıza dikkat ediniz. Artık sözcüklere numaralar 
    verebiliriz:

    vocab_dict = {word: index for index, word in enumerate(vocab)}

    Şimdi manuel olarak binary vektörizasyon uygulayalım:

    training_dataset_x = np.zeros((len(training_texts), len(vocab)), dtype='uint8')  
    test_dataset_x = np.zeros((len(test_texts), len(vocab)), dtype='uint8')  

    for row, text in enumerate(training_texts):
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        word_numbers = [vocab_dict[word] for word in words]
        training_dataset_x[row, word_numbers] = 1
        
    for row, text in enumerate(test_texts):
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        word_numbers = [vocab_dict[word] for word in words]
        test_dataset_x[row, word_numbers] = 1

    Problem çok sınıflı bir sınıflandırma problemidir. Bunun için y değerleri üzerinde one-hot-encoding dönüştürmesi uygulayabiliriz:

    ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
    ohe.fit(np.array(list(cats)).reshape(-1, 1))

    training_dataset_y = ohe.transform(np.array(training_y).reshape(-1, 1))
    test_dataset_y = ohe.transform(np.array(test_y).reshape(-1, 1))

    Artık modelimizi kurup eğitebiliriz:

    model.add(Input((training_dataset_x.shape[1],)))
    model.add(Dense(128, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(len(cats), activation='softmax', name='Output'))
    model.summary()
                
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
    hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=10, validation_split=0.2)

    Kestirim işlemi için eğitimdeki veri kümesine benzer bir veri kümesi oluşturulabilir. Biz örneğimizde kestirim için 
    "PredictData" isimli bir dizin oluşturup o dizine yazılardan oluşan dosyalar yerleştirdik. O dosyaların da olması gereken
    tiketlerini dosya ismine ek yaptık. PredictData dizinindeki dosya isimleri şöyledir:

    14829-nat-gas
    14841-wheat
    14849-interest
    14854-ipi
    14860-earn
    14862-bop
    14876-earn
    21394-acq

    Kestirim kodu şöyle oluşturulabilir:

    word_numbers_list = []
    fnames = []
    for fname in os.listdir('PredictData'):
        with open('PredictData/' + fname, encoding='latin-1') as f:
            text = f.read()
            words = re.findall('[a-zA-Z0-9]+', text.lower())
            word_numbers = [vocab_dict[word] for word in words]
            word_numbers_list.append(word_numbers)
            fnames.append(fname)
        
    predict_dataset_x = np.zeros((len(word_numbers_list), len(vocab)), dtype='uint8')
    for row, word_numbers in enumerate(word_numbers_list):
        predict_dataset_x[row, word_numbers] = 1
        
    predict_result = model.predict(predict_dataset_x)
    predict_indexes = np.argmax(predict_result, axis=1)

    for index, pi in enumerate(predict_indexes):
        print(f'{fnames[index]} => {ohe.categories_[0][pi]}')

    Aşağıda örneğin tüm kodu verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import os
import re

training_dict = {}
test_dict = {}
cats = set()

with open('ReutersData/cats.txt') as f:
    for line in f:
        toklist = line.split()
        ttype, fname = toklist[0].split('/')
        if ttype == 'training':
            training_dict[fname] = toklist[1]
        else:
            if ttype == 'test':
                test_dict[fname] = toklist[1]
        cats.add(toklist[1])
    
vocab =  set()
training_texts = []
training_y = []
                
for fname in os.listdir('ReutersData/training'):
    with open('ReutersData/training/' + fname, encoding='latin-1') as f:
        text = f.read()
        training_texts.append(text)
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        vocab.update(words)
        training_y.append(training_dict[fname])

test_texts = []
test_y = []
for fname in os.listdir('ReutersData/test'):
    with open('ReutersData/test/' + fname, encoding='latin-1') as f:
        text = f.read()
        test_texts.append(text)
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        vocab.update(words)
        test_y.append(test_dict[fname])
            
vocab_dict = {word: index for index, word in enumerate(vocab)}

import numpy as np

training_dataset_x = np.zeros((len(training_texts), len(vocab)), dtype='uint8')  
test_dataset_x = np.zeros((len(test_texts), len(vocab)), dtype='uint8')  
        
for row, text in enumerate(training_texts):
    words = re.findall('[a-zA-Z0-9]+', text.lower())
    word_numbers = [vocab_dict[word] for word in words]
    training_dataset_x[row, word_numbers] = 1
    
for row, text in enumerate(test_texts):
    words = re.findall('[a-zA-Z0-9]+', text.lower())
    word_numbers = [vocab_dict[word] for word in words]
    test_dataset_x[row, word_numbers] = 1

import numpy as np
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
ohe.fit(np.array(list(cats)).reshape(-1, 1))

training_dataset_y = ohe.transform(np.array(training_y).reshape(-1, 1))
test_dataset_y = ohe.transform(np.array(test_y).reshape(-1, 1))

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Reuters')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(len(cats), activation='softmax', name='Output'))
model.summary()
            
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=10, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction 
           
word_numbers_list = []
fnames = []
for fname in os.listdir('PredictData'):
    with open('PredictData/' + fname, encoding='latin-1') as f:
        text = f.read()
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        word_numbers = [vocab_dict[word] for word in words]
        word_numbers_list.append(word_numbers)
        fnames.append(fname)
    
predict_dataset_x = np.zeros((len(word_numbers_list), len(vocab)), dtype='uint8')
for row, word_numbers in enumerate(word_numbers_list):
    predict_dataset_x[row, word_numbers] = 1
    
predict_result = model.predict(predict_dataset_x)
predict_indexes = np.argmax(predict_result, axis=1)

for index, pi in enumerate(predict_indexes):
    print(f'{fnames[index]} => {ohe.categories_[0][pi]}')

#----------------------------------------------------------------------------------------------------------------------------
                                            43. Ders - 08/06/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Reuters örneğini CountVectorizer sınıfını kullanılarak gerçekleştirelim. Anımsanacağı gibi CountVectorizer sınıfı
    zaten vektörizasyon işlemini kendisi yapmaktaydı. O halde biz Reuters yazılarını bir listede topladıktan sonra CountVectorizer
    sınıfı ile fit işlemini yapabiliriz. Orijinal Reuters veri kümesinde ayrıca "stopwords" dosyası içerisinde "stop word'ler"
    satır satır sözcükler biçiminde verilmiştir. (Anımsanacağı gibi CountVectorizer sınıfında biz stop word'leri de ayrıca 
    belirtebiliyorduk.) Reuters veri kümesinde verilen stop word'leri bir Python listesi biçiminde şöyle elde edebiliriz:

    import pandas as pd

    df_sw = pd.read_csv('ReutersData/stopwords', header=None)
    sw = df_sw.iloc[:, 0].to_list()

    CountVectorizer sınıfı önce yazıları sözcüklere ayırıp (tokenizing) sonra stop word'leri atmaktadır. Ancak sınıfın sözcüklere
    ayırmada default kullandığı düzenli ifade kalıbı tek tırnaklı sözcüklerdeki tırnaklardan da ayrıştırma yapmaktadır. (Fakat 
    tırnaktan sonraki kısmı da atmaktadır.) Orijinal veri kümesinde verilen stop word'ler tek tırnaklı yazı içerdiği için burada 
    bir uyumsuzluk durumu ortaya çıkmaktadır. (Yani stop'word'ler içerisindeki tek tırnak içeren sözcükler zaten sınıfın ayırdığı 
    sözcükler içerisinde bulunmayacaktır.) İşte CountVectorizer sınıfının fit metodu bu durumu fark edip bize bir uyarı mesajı 
    biçiminde bunu aşağıdaki gibi iletmektedir:

    "C:\Users\aslan\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py:409: UserWarning: Your stop_words may be 
    inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 
    'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words."

    O halde biz ya sınıfın kullandığı sözcüklere ayırma düzenli ifadesini (token_pattern parametresini kastediyoruz) tırnakları 
    kapsayacak biçimde değiştirmeliyiz ya da bu tırnaklı stop word'lerdeki tırnakları silmeliyiz. Dosyanın orijinalini bozmamak 
    için uyarıda sözü edilen sözcükleri de listeye ekleyerek problemi pratik bir biçimde çözebiliriz:

    sw +=  ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 
            'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']

    Ayrıca CountVectorizer sınıfının stop_words parametresine 'english' girilirse scikit-learn içerisindeki İngilizce için 
    oluşturulmuş default stop word listesi kullanılmaktadır. Tabii veri kümesindeki orijinal listenin kullanılması daha uygun 
    olacaktır. Yukarıda da belirttiğimiz gibi biz CountVectorizer sınıfının token_pattern parametresine tırnakları da alacak 
    biçimde bir düzenli ifade kaalıbını da girebiliriz. Örneğin:

    cv = CountVectorizer(token_pattern="[a-zA-Z-0-9']+")

    Burada artık CountVectorizer bizim belirlediğimiz düzenli ifadeyi kullanarak sözcükleri ayrıştırcaktır. Bu düzenli ifade 
    "sözcüklerin içerisinde tek tırnakların da olabileceğini" belirtmektedir. 
    
    Şimdi vektörizasyon işlemini sınıfın kendi default token_pattern kalıbını kullanarak yapalım:

    df_sw = pd.read_csv('ReutersData/stopwords', header=None)
    sw = df_sw.iloc[:, 0].to_list()
    sw +=  ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 
            'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']

    cv = CountVectorizer(dtype='uint8', stop_words=sw, encoding='latin-1')
    cv.fit(training_texts + test_texts)

    training_dataset_x = cv.transform(training_texts).todense()
    test_dataset_x = cv.transform(test_texts).todense()

    Kesitirim işlemi için yine daha önce yaratmış olduğumuz CountVectorizer nesnesini kullanabiliriz:

    predict_texts = []
    fnames = []
    for fname in os.listdir('PredictData'):
        with open('PredictData/' + fname, encoding='latin-1') as f:
            text = f.read()
            predict_texts.append(text)
            fnames.append(fname)
            
    predict_dataset_x = cv.transform(predict_texts).todense()  

    CountVectorizer sınıfında binary parametresinin default olarak False biçimde olduğunu anımsayınız. Yani default durumda 
    vektörize edilmiş matris sözcüklerin sıklıkları tutacaktır.

    Örneğin bütünsel hali aşağıda verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import os
import re

training_dict = {}
test_dict = {}
cats = set()

with open('ReutersData/cats.txt') as f:
    for line in f:
        toklist = line.split()
        ttype, fname = toklist[0].split('/')
        if ttype == 'training':
            training_dict[fname] = toklist[1]
        else:
            if ttype == 'test':
                test_dict[fname] = toklist[1]
        cats.add(toklist[1])
    
vocab =  set()
training_texts = []
training_y = []
                
for fname in os.listdir('ReutersData/training'):
    with open('ReutersData/training/' + fname, encoding='latin-1') as f:
        text = f.read()
        training_texts.append(text)
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        vocab.update(words)
        training_y.append(training_dict[fname])

test_texts = []
test_y = []
for fname in os.listdir('ReutersData/test'):
    with open('ReutersData/test/' + fname, encoding='latin-1') as f:
        text = f.read()
        test_texts.append(text)
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        vocab.update(words)
        test_y.append(test_dict[fname])
            
vocab_dict = {word: index for index, word in enumerate(vocab)}

import pandas as pd

df_sw = pd.read_csv('ReutersData/stopwords', header=None)
sw = df_sw.iloc[:, 0].to_list()

sw +=  ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 
        'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']
    
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(dtype='uint8', stop_words=sw, encoding='latin-1')
cv.fit(training_texts + test_texts)

training_dataset_x = cv.transform(training_texts).todense()
test_dataset_x = cv.transform(test_texts).todense()

import numpy as np
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
ohe.fit(np.array(list(cats)).reshape(-1, 1))

training_dataset_y = ohe.transform(np.array(training_y).reshape(-1, 1))
test_dataset_y = ohe.transform(np.array(test_y).reshape(-1, 1))

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Reuters')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(len(cats), activation='softmax', name='Output'))
model.summary()
            
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=10, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction 
           
predict_texts = []
fnames = []
for fname in os.listdir('PredictData'):
    with open('PredictData/' + fname, encoding='latin-1') as f:
        text = f.read()
        predict_texts.append(text)
        fnames.append(fname)
        
predict_dataset_x = cv.transform(predict_texts).todense()       

predict_result = model.predict(predict_dataset_x)
predict_indexes = np.argmax(predict_result, axis=1)

for index, pi in enumerate(predict_indexes):
    print(f'{fnames[index]} => {ohe.categories_[0][pi]}')

#----------------------------------------------------------------------------------------------------------------------------
    Aslında "Reuters" veri kümesi zaten tensorflow.keras.datasets paketi içerisinde hazır bir biçimde bulunmaktadır. Veri 
    kümesinin yüklenmesi yine diğer veri kümelerinde olduğu gibi load_data fonksiyonuyla yapılabilir. Fonksiyonun num_words 
    parametresi yine kelime haznesini belli bir sayıda tutmak için kullanılabilir. Eğer bu parametre için argüman girilmezse 
    bütün Reuters verileri kullanılacaktır. Bu modüldeki Reuters verilerinde toplam 46 farklı kategori vardır. Ancak Keras 
    dokümanları bu kategorileri herhangi bir biçimde kullanıcıya vermemiştir. Fakat yapılan incelemeler sonucunda kategorilerin 
    sırasıyla şunlar olduğu tespit edilmiştir:

    cats = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',   
    'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',
    'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',
    'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',
    'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']

    Ancak TensorFlow kütüphanesinin son versiyonlarında (öreğin 2.16.1) artık Reuters veri kümesindeki kategorilerin isimleri de 
    get_label_names fonksiyonu ile verilmektedir. 

    Tıpkı imdb modülünde olduğu gibi bu modülde de get_word_index isimli fonksiyon tüm sözüklerin numaralarını bize bir sözlük 
    nesnesi biçiminde vermektedir. load_data fonksiyonu da yine bize sözcük numaralarından oluşan listeler verir. Yine imdb 
    modülünde olduğu gibi x verilerindeki sayılar yine 3 fazla olarak kodlanmıştır. 

    Biz bu hazır Reuters verilerinde CountVectorizer sınıfını kullanamayız. Çünkü CountVectorizer sınıfı yazı dizilerinden 
    vektörizasyon yapmaktadır. Halbuki burada bizim elimizde yazılar değil yazılara karşı gelen sayılar vardır. Tabii biz 
    sayıları yazılara dönüştürüp CountVectorizer sınıfını kullanabiliriz. Ancak bu işlem yavaş olur. Burada doğrudan sayılardan 
    vektörizasyon matrisini elde etmek daha uygun olacaktır. Binary vektörizasyon işlemini yapan fonksiyonu şöyle yazabiliriz:

    def vectorize(sequence, colsize):
        dataset_x = np.zeros((len(sequence), colsize), dtype='uint8')
        for index, vals in enumerate(sequence):
            dataset_x[index, vals] = 1          
        return dataset_x

    Aslında bu fonksiyon one-hot-encoding işlemini de yapabilecek yetenektedir. 

    Aşağıda örneğin tüm kodları verilmiştir.    
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.datasets import reuters

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = reuters.load_data()

vocab_dict = reuters.get_word_index()

import numpy as np

def vectorize(sequence, colsize):
    dataset_x = np.zeros((len(sequence), colsize), dtype='uint8')
    for index, vals in enumerate(sequence):
        dataset_x[index, vals] = 1        
    return dataset_x

training_dataset_x = vectorize(training_dataset_x, len(vocab_dict) + 3)
test_dataset_x = vectorize(test_dataset_x, len(vocab_dict) + 3)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False, dtype='uint8')
ohe.fit(np.concatenate([training_dataset_y, test_dataset_y]).reshape(-1, 1))

training_dataset_y = ohe.transform(training_dataset_y.reshape(-1, 1))
test_dataset_y = ohe.transform(test_dataset_y.reshape(-1, 1))

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='Reuters')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(len(ohe.categories_[0]), activation='softmax', name='Output'))
model.summary()
            
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=10, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
 # prediction 
   
cats = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',   
'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',
'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',
'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',
'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']

import os
import re
         
word_numbers_list = []
fnames = []
for fname in os.listdir('PredictData'):
    with open('PredictData/' + fname, encoding='latin-1') as f:
        text = f.read()
        words = re.findall('[a-zA-Z0-9]+', text.lower())
        word_numbers = [vocab_dict[word] + 3 for word in words]
        word_numbers_list.append(word_numbers)
        fnames.append(fname)
    
predict_dataset_x = vectorize(word_numbers_list, len(vocab_dict) + 3)
    
predict_result = model.predict(predict_dataset_x)
predict_indexes = np.argmax(predict_result, axis=1)

for index, pi in enumerate(predict_indexes):
    print(f'{fnames[index]} => {cats[pi]}')
    
# reverse transformation test
    
rev_vocab_dict = {index: word for word, index in vocab_dict.items()}
convert_text = lambda text_numbers: ' '.join([rev_vocab_dict[tn - 3] for tn in text_numbers if tn > 2])
print(convert_text(word_numbers_list[0]))

#----------------------------------------------------------------------------------------------------------------------------
    Aslında vektörizasyon işlemi daha sonraları Keras'a eklenmiş olan TextVectorization isimli katman sınıfı yoluyla da 
    yapılabilmektedir. Uygulamacı Input katmanından sonra bu katman nesnesini, daha sonra da diğer katman nesnelerini 
    modele ekler. Böylece alınan girdiler önce TextVectorization katmanı yoluyla vektörel hale getirilip diğer katmanlara 
    iletilir. Tabii bu durumda bizim ağa girdi olarak vektörleri değil yazıları vermemiz gerekir. Çünkü bu katmanın kendisi 
    zaten yazıları vektörel hale getirmektedir. Örneğin:

    tv = TextVectorization(...)
    ...
    model = Sequential(name='IMDB')

    model.add(Input((1, ), dtype='string'))
    model.add(tv)
    model.add(Dense(128, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Burada ağın girdi katmanında tek sütunlu bir veri kümesi olduğuna dikkat ediniz. Çünkü biz ağa artık yazıları girdi olarak
    vereceğiz. Bu yazılar TextVectorization katmanına sokulacak ve bu katmandan sözcük haznesi kadar sütundan oluşan matris 
    çıktısı elde edilecektir. Bu çıktıların da sonraki Dense katmana verildiğini görüyorsunuz. Yani TextVectorization katmanı 
    aldığı bir batch yazıyı o anda vektörel haline getirip sonraki katmana iletmektedir. Ancak Input katmanında girdilerin 
    yazısal olduğunu belirtmek için Input fonksiyonunun dtype paramatresi 'string' biçiminde girilmelidir. (Defult durumda Keras 
    girdi katmanındaki değerlerin float türünden olduğunu kabul etmektedir.)

    TextVectorization sınıfı diğer katman nesnelerinde olduğu gibi tensorflow.keras.layers modülü içerisinde bulunmaktadır. 
    Sınıfın __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.layers.TextVectorization(
        max_tokens=None,
        standardize='lower_and_strip_punctuation',
        split='whitespace',
        ngrams=None,
        output_mode='int',
        output_sequence_length=None,
        pad_to_max_tokens=False,
        vocabulary=None,
        idf_weights=None,
        sparse=False,
        ragged=False,
        encoding='utf-8',
        name=None,
        **kwargs
    )

    Görüldüğü gibi bu parametrelerin hepsi default değer almıştır. max_tokens parametresi en fazla yinelenen belli sayıda 
    sözüğün vektörel hale getirilmesi için kullanılmaktadır. Yani adeta sözcük haznesi burada belirtilen miktarda sözcük 
    içeriyor gibi olmaktadır. standardize parametresi yazılardaki sözcüklerin elde edildikten sonra nasıl önişleme sokulacağını 
    belirtmektedir. Bu parametrenin default değerinin 'lower_and_strip_punctuation' biçiminde olduğuna dikkat ediniz. Bu durumda
    yazılardaki sözcükler küçük harflere dönüştürülecek ve sözcüklerin içerisindeki noktalama işaretleri atılacaktır. (Yani
    örneğin yazıdaki "Dikkat!" sözcüğü "dikkat" olarak ele alınacaktır.) Bu parametre için "çağrılabilir (callable)" bir nesne 
    de girilebilmektedir. Girilen fonksiyon eğitim sırasında çağrılıp buradan elde edilen yazılar vektörizasyon işlemine sokulmaktadır.
    split parametresi sözcüklerin nasıl birbirinden ayrılacağını belirtmektedir. Default durumda sçzcükler boşluk karakterleriyle 
    birbirinden ayrılmaktadır. output_mode parametresinin default değeri int biçimindedir. Default durumda yazıdaki sözcükler 
    sözcük haznesindeki numaralar biçiminde verilecektir. Bu parametrenin "count" biçiminde girilmesi uygundur. Eğer bu parametre 
    "count" biçiminde girilirse bu durumda yazı bizim istediğimiz gibi frekanslardan oluşan vektörler biçimine dönüştürülecektir. 
    vocabulary parametresi doğrudan sözcük haznesinin programcı tarafından metoda verilmesini sağlamak için buludurulmuştur. 
    Bu durumda adapt işleminde sözcük haznesi adapt tarafından oluşturulmaz, burada verilen sözcük haznesi kullamılır.

    TextVectorization sınıfının get_vocabulary metodu adapt işleminin sonucunda oluşturulmuş olan sözcük haznesini bize 
    vermektedir. set_vocabulary metodu ise sözcük haznesini set etmek için kullanılmaktadır.
    
    TextVectorization nesnesi yaratıldıktan sonra sözcük haznesinin ve dönüştürmede kullanılacak sözcük nesnesinin oluşturulması
    için sınıfın adapt metodu çağrılmalıdır. Örneğin:

    tv = TextVectorization(output_mode='count')
    tv.adapt(texts)

    Aşağıda sınıfın kullanımına ilişkin basit bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.layers import TextVectorization

texts = ['film çok güzeldi', 'film güzeldi', 'film çok kötüydü', 'film ortalama bir filmdi']

tv = TextVectorization(output_mode='count')
tv.adapt(texts)

result = tv(['film güzeldi, film', 'film kötüydü'])
print(result)

result = tv.get_vocabulary()
print(result)

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de TextVectorization katmanını IMDB veri kümesinde kullanalım. Burada yapmamız gereken şey Input katmanından sonra 
    bu TextVectoriation katmanını modele eklemektir.  Örneğin:

    tv = TextVectorization(output_mode='count')
    tv.adapt(dataset_x)

    model = Sequential(name='IMDB')

    model.add(Input((1, ), dtype='string'))
    model.add(tv)
    model.add(Dense(128, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Tabii artık kestirim işlemi için manuel vektörizasyon uygulamaya gerek yoktur. Doğrudan kestirim işleminde kullanılacak
    yazılar predict metoduna verilebilir. Örneğin:

    predict_df = pd.read_csv('predict-imdb.csv') 
    predict_result = model.predict(predict_df)

    Aşağıdaki örnekte IMDB veri kümesine TextVectorization katmanı uygulanmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']  
dataset_y = (df['sentiment'] == 'positive').astype(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(df['review'], dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense, TextVectorization

tv = TextVectorization(output_mode='count')
tv.adapt(dataset_x)

model = Sequential(name='IMDB')

model.add(Input((1, ), dtype='string'))
model.add(tv)
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
# prediction

predict_df = pd.read_csv('predict-imdb.csv') 
predict_result = model.predict(predict_df)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Sözlük çantası (BoW) yönteminde bir bağlam etkisinin oluşturulması için "n-gram" denilen bir teknik de kullanılmaktadır. 
    n-gram yazıdaki sözcükleri tek tek değil de n'li olarak atomlarına ayrılması anlamına gelir. Örneğin aşağıdaki gibi iki 
    yazı söz konusu olsun:

    texts = ['Bugün hava çok güzel', 'bugün hava çok sıcak']

    Burada BoW yöntemine göre sözcükler tek tek ele alınıp vektörüzasyon yapılırsa buna "unigram" denilmektedir. Biz yukarıdaki 
    örneklerde hep "unigram" kullandık. Eğer yan yana n tane sözcük sanki tek bir atom (token) gibi ele alınırsa buna "n-gram" 
    denilmektedir. n sayısı 2 ise bu özel olarak "bigram" biçiminde de isimlendirilmektedir. Uygulamalarda yalnızca "unigram", 
    yalnızca "bigram" ya da hem "unigram" hem de "bigram" kullanılabilmektedir. CountVectorizer sınıfının ngram_range parametresi
    n-gram'daki n değerlerini belirtmektedir. Bu parametre iki elemanlı bir demet biçiminde girilir. Demetin ilk elemanından
    ikinci elemanına kadarki (bu değer de dahil) bütün n değerleri vektörüzasyonda kullanılır. Örneğin ngram=(1, 3) girilirse
    n değeri hem 1 hem 2 hem 3 anlamına gelmektedir. ngram=(2, 2) girilirse bu durum "bigram" anlamına gelir. Bu parametrenin
    default değeir ngram=(1, 1) biçimindedir. Yani default durumda unigram uygulanmaktadır. Aşağıdaki örneği inceleyiniz:

    texts = ['Bugün hava çok güzel', 'bugün hava çok sıcak']
        
    from sklearn.feature_extraction.text import CountVectorizer

    cv = CountVectorizer(ngram_range=(1, 2))
    result = cv.fit_transform(texts).todense()

    feature_names = cv.get_feature_names_out()
    print(feature_names)
    print(result)

    Burada biz hem "unigram" hem de "bigram" uygulamış olduk. Elde edilen çıktı şöyledir:

    ['bugün' 'bugün hava' 'güzel' 'hava' 'hava çok' 'sıcak' 'çok' 'çok güzel' 'çok sıcak']
    [[1 1 1 1 1 0 1 1 0]
    [1 1 0 1 1 1 1 0 1]]

    Gördüğünüz gibi yazıların atomları (tokens) hem tek sözcükten hem de yan yana iki sözcükten oluşyormuş gibi vektörüzasyon
    yapılmıştır. Burada kullandığımız CountVectorizer sınıfının get_feature_names_out metodu sütunlara karşı gelen atomların
    neler olduğunu bize vermektedir.

    Peki "unigram" uygulamakla hem "unigram" hem de "bigram" uygulamak arasındaki fark tam olarak nedir?

    - Yalnızca "unigram" uygulandığında daha az özellik (yani sütun) oluşturulmuş olur. Model daha basit hale gelir. Bu da 
    eğitim zamanını kısaltır. Ancak art arda gelen sözcükler arasında bir bağlam ilişkisi ortadan kalkar. 

    - Hem "unigram" hem "bigram" uygulanırsa daha fazla özellik söz konusu olur. Bu bazı dezavantajlar doğurabilir. Ancak 
    peş peşe gelen iki sözcük arasında nispeten bir bağlam oluşturulmuş olur. Bağlamın önemli olduğu veri kümesinin büyük 
    olduğu durumlarda hem "unigram" hem de "bigram" kullanımını tercih edebilirsiniz.

    Aşağıdaki örnekte IMDB veri kümesi hem "unigram" hem de "bigram" biçiminde vektörize edilerek sınıflandırılmıştır:
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text  import CountVectorizer

cv = CountVectorizer(dtype='uint8', ngram_range=(1, 2))

cv.fit(df['review'])
dataset_x = cv.transform(df['review']).todense()

dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1],)))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy®'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
# prediction

predict_df = pd.read_csv('predict-imdb.csv')

predict_dataset_x = cv.transform(predict_df['review']).todense()
predict_result = model.predict(predict_dataset_x)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

# dataset_x'teki birinci yorumun yazı haline getirilmesi

import numpy as np

rev_vocab_dict = {index: word for word, index in cv.vocabulary_.items()}

word_indices = np.argwhere(dataset_x[0] == 1)[:, 1]
words = [rev_vocab_dict[index] for index in word_indices]
text = ' '.join(words)
print(text)

#----------------------------------------------------------------------------------------------------------------------------
    Keras'ın TextVectorization sınıfında da n-gram kullanılabilmektedir. Sınıfın __init__ metodunun ngram parametresi None
    geçilirse (default durum) "unigram" anlaşılmaktadır. eğer bu parametreye belli bir n değeri geçilirse yalnızca o n değerine
    ilişkin n-gram işlemi yapılır. Bu parametreye bir demet girilirse demetin her elemanı bağımsız biçimde n değerlerini belirtmektedir. 
    Örneğin:

    tv = TextVectorization(output_mode='count', ngrams=(1, 2))

    Burada hem "unigram" hem de "bigram" vektörizasyon uygulanmaktadır. Buradaki ngrams parametresinin bir aralık belirtmediğine 
    tek tek değerler belirttiğine dikkat ediniz. 

    Aşağıda IMDB örneği TextVectorization sınıfı kullanılarak hem "unigram" hem de "bigram" vektörizasyonla gerçekleştirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']  
dataset_y = (df['sentiment'] == 'positive').astype(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(df['review'], dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense, TextVectorization

tv = TextVectorization(output_mode='count', ngrams=(1, 2))
tv.adapt(dataset_x)

model = Sequential(name='IMDB')

model.add(Input((1, ), dtype='string'))
model.add(tv)
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=5, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
# prediction

predict_df = pd.read_csv('predict-imdb.csv') 
predict_result = model.predict(predict_df)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Metinlerin sınıflandırılmasında yazıların sayısal biçime dönüştürülmesi sürecinde kullanılan diğer bir yöntem de "TF-IDF 
    (Term Frequency - Inverse Document Frequence)" yöntemidir. Bu yöntemde tüm atomlara (tipik olarak sözcük haznesindeki tüm
    sözcüklere) birer TF-IDF değeri karşılık getirilir. Böylece yorumlar birer TF-IDF vektörüne dönüştürülür. 

    TF-IDF yönteminde her sözcük için önce TF ve IDF değerleri hesaplanır. Bu iki değer çarpılarak TF-IDF değeri elde edilir:,
    
    TF-IDF = TF * IDF

    Bir sözcüğün TF değeri şöyle hesaplanmaktadır:

    TF = sözcüğün yorumdaki sayısı / yorumdaki sözcük sayısı

    Örneğin yorum şöyle olsun:

    "Bu film çok güzel. Film aynı zamanda çok güzel yerlerde çekilmiş."

    Yorumda toplam 11 sözcük var. Bu durumda "film", "çok", "güzel" sözcüklerinin TF değerleri 2/11 diğer sözcüklerin TF 
    değerleri ise 1/11 olur.

    Sözcüklerin IDF değerleri ise şöyle hesaplanmaktadır:

    IDF = log (N / toplam yorum sayısı)

    Buradaki N değeri ilgili sözcüğün geçtiği farklı yorum sayısını belirtmektedir. Örneğin "muhteşem" sözcüğü 5 farklı yorumda 
    geçiyor olsun. Toplamda da 100 tane yorum olsun. Bu durumda "muhteşem" sözcüğünün IDF değeri 5 / 100 olacaktır. 

    Yukarıda da belirttiğimiz gibi sözcüğün TF-IDF değeri TF değeri ile IDF değerinin çarpımından elde edilmektedir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    TF-IDF yönteminde yine tüm atomlar için (sözcük haznesindeki tüm sözcükler için) bir TF-IDF değeri elde edilir. Böylece 
    tüm yorumlar sözcük haznesi uzunluğu kadar sütuna, yorum sayısı kadar satıra sahip olan bir matrisle temsil edilir. Tabii
    bu matris de yine seyrek (sparse) biçimde olur. Çünkü bir yorumda sözcük haznesindeki çok az sözcük kullanılmış olacaktır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yazıların TF-IDF yöntemi ile vektörize edilmesi scikit-learn içerisinde TfidfVectorizer sınıfı kullanılarak yapılabilmektedir.
    Sınıfın kullanımı diğer scikit-learn sınıflarında olduğu gibidir.  TfidfVectorizer sınıfının __init__ metodunun parametrik 
    yapısı şöyledir:

    class sklearn.feature_extraction.text.TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', 
            strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, 
            token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, 
            vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, 
            sublinear_tf=False)

    Örnek bir kullanımı şöyle olabilir:

    from sklearn.feature_extraction.text import TfidfVectorizer

    documents = [
        "Bu bir örnek belgedir",
        "Bu başka bir belgedir",
        "Bu belge TF-IDF kullanılarak vektörize edilecek"
    ]

    vectorizer = TfidfVectorizer()

    tfidf_matrix = vectorizer.fit_transform(documents).todense()
    print(tfidf_matrix)
    print(vectorizer.idf_)
    print(vectorizer.vocabulary_)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            44. Ders - 09/06/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Çok büyük verilerle eğitim, test hatta predict işlemi sorunlu bir konudur. Çünkü örneğin fit işleminde büyük miktarda  
    veri kümeleriyle eğitim ve test yapılırken bu veri kümeleri bir bütün olarak metotlara verilmektedir. Ancak büyük veri 
    kümeleri eldeki belleğe sığmayabilir. (Her ne kadar 64 bit Windows ve Linux sistemlerinde prosesin sanal bellek alanı
    çok büyükse de bu teorik sanal bellek alanını kullanabilmek için swap alanlarının büyütülmesi gerekmektedir.) Örneğin 
    IMDB ya da Reuters örneklerinde vektörizasyon işlemi sonucunda çok büyük matrisler oluşmaktadır. Gerçi bu matrislerin 
    çok büyük kısmı 0'lardan oluştuğu için "seyrek (sparse)" durumdadır. Ancak Keras kütüphanesinin eski sürümlerinde 
    fit, evaluate ve predict metotları seyrek matrislerle çalışmamaktadır. İşte bu nedenden dolayı Keras'ta modeller parçalı 
    verilerle eğitilip test ve predict edilebilmektedir. Parçalı eğitim, test ve predict işlemlerinde eğitim, test ve predict 
    sırasında her batch işleminde fit, evaluate ve predict metotları o anda bziden bir batch'lik verileri istemekte ve eğitim 
    batch-bacth verilere tedarik edilerek yapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------  

#----------------------------------------------------------------------------------------------------------------------------
    Parçalı eğitim ve test işlemi için fit, evaluate ve predict metotlarının birinci parametrelerinde x verileri yerine bir 
    "üretici fonksiyon (generator)" ya da "PyDataset sınıfından türetilmiş olan bir sınıf nesnesi" girilir. Biz burada önce 
    üretici fonksiyon yoluyla sonra da PyDataset sınıfından türetilmiş olan sınıf yoluyla parçalı işlemlerin nasıl yapılacağını 
    göreceğiz. Eskiden Keras'ta normal fit, evaluate ve predict metotlarının ayrı fit_generator, evalute_generator ve 
    predict_generator biçiminde parçalı eğitim için kullanılan biçimleri vardı. Ancak bu metotlar daha sonra kaldırıldı. Artık 
    fit, evaluate ve predcit metotları hem bütünsel hem de parçalı işlemler yapabilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Üretici fonksiyon yoluyla parçalı eğitim yaparken fit metodunun birinci parametresine bir üretici fonksiyon nesnesi girilir. 
    Artık fit metodunun batch_size ve validation_split parametrelerinin bir önemi kalmaz. Çünkü batch miktarı zaten üretici 
    fonksiyonden elde edilen verilerle yapılmaktadır. Kaldı ki bu yöntemde aslında her batch işleminde aynı miktarda verinin 
    kullanılması da zorunlu değildir. Yine bu biçimde eğitim yapılırken fit metodunun validation_split parametresinin de bir 
    anlamı yoktur. Çünkü sınama işlemi de yine parçalı verilerle yapılmaktadır. Dolayısıyla sınama verilerinin parçalı olarak 
    verilmesi de yine uygulamacının sorumluluğundadır. Ancak bu yöntemde programcının iki parametreyi yine açıkça belirlemesi 
    gerekir. Birincisi steps_per_epoch parametresidir. Bu parametre bir epoch işleminin kaç batch işleminden oluşacağını belirtir. 
    İkinci parametre ise epochs parametresidir. Bu da yine toplam epoch sayısını belirtir. (epoch parametresinin default değerinin 
    1 olduğunu anımsayınız.) Bu durumda programcının üretici fonksiyon içerisinde epochs * steps_per_epoch kadar yield uygulaması 
    gerekir. Çünkü toplam batch sayısı bu kadardır. fit metodu aslında epochs * steps_per_epoch kadar işlemden sonra son kez 
    bir daha next işlemi yaparak üretici fonksiyonun bitmesine yol açmaktadır.

    Aşağıdaki toplam 100 epoch'tan oluşan her epoch'ta 20 tane batch işlemi yapılan ikili sınıflandırma örneği verilmiştir. 
    Ancak bu örnekte x ve y verileri üretici fonksiyonlardan elde edilmiştir. Üretici fonksiyon içerisinde toplam 
    epochs * steps_per_epoch kadar yield işlemi yapılmıştır. Ancak bu örnekte bir sınama işlemi yapılmamıştır. Biz bu örneği 
    rastgele verilerle yalnızca mekanizmayı açıklamak için veriyoruz. (Rastgele verilerde rastgele sayı üreticisi uygun bir 
    biçimde oluşturulmuşsa bir kalıp söz konusu olmayacağı için "binary_accuracy" metrik değerinin 0.50 civarında olması 
    beklenir.)
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

EPOCHS = 100
NFEATURES = 10
STEPS_PER_EPOCH = 20
BATCH_SIZE = 32

def data_generator():
    for _ in range(EPOCHS):
        for _ in range(STEPS_PER_EPOCH):
            x = np.random.random((BATCH_SIZE, NFEATURES))
            y = np.random.randint(0, 2, BATCH_SIZE)
            yield x, y

model = Sequential(name='Diabetes')

model.add(Input((NFEATURES, )))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(data_generator(), epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıdaki örnekte bir sınama işlemi yapılmamamıştır. İşte sınama işlemleri için veriler de tek hamlede değil parça parça
    verilebilmektedir. Bunun için yine fit metodunun validation_data parametresine bir üretici fonksiyon nesnesi girilir. fit 
    metodu da her epcoh sonrasında validation_steps parametresinde belirtilen miktarda bu üretici fonksiyon üzerinde iterasyon 
    yaparak bizden sınama verilerini almaktadır. Böylece biz her epoch sonrasında kullanılacak sınama verilerini fit metoduna 
    üretici fonksiyon nesnesi yoluyla parça parça vermiş oluruz. 
    
    Aşağıda sınama verilerinin parçalı bir biçimde nasıl verildiğine ilişkin bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

EPOCHS = 100
NFEATURES = 10
STEPS_PER_EPOCH = 20
BATCH_SIZE = 32
VALIDATION_STEPS = 10

def data_generator():
    for _ in range(EPOCHS):
        for _ in range(STEPS_PER_EPOCH):
            x = np.random.random((BATCH_SIZE, NFEATURES))
            y = np.random.randint(0, 2, BATCH_SIZE)
            yield x, y   

def validation_generator():
    x = np.random.random((BATCH_SIZE, NFEATURES))
    y = np.random.randint(0, 2, BATCH_SIZE)
    for _ in range(EPOCHS):
        for _ in range(VALIDATION_STEPS):
            yield x, y

model = Sequential(name='Diabetes')

model.add(Input((NFEATURES,)))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(data_generator(), epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, 
        validation_data=validation_generator(), validation_steps=VALIDATION_STEPS)

#----------------------------------------------------------------------------------------------------------------------------
    Parçalı eğitimin yanı sıra test işlemi de parçalı bir biçimde yapılabilmektedir. Bunun için evaluate metodunda test_dataset_x 
    yerine yine bir üretici fonksiyon nesnesi girilir. evaluate metodunun steps parametresi bizden test verilerinin kaç parça 
    olarak isteneceğini belirtmektedir. Yani programcının burada belirtilen sayıda yield işlemi yapması gerekir. Aşağıda buna 
    bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

EPOCHS = 100
NFEATURES = 10
STEPS_PER_EPOCH = 20
BATCH_SIZE = 32
VALIDATION_STEPS = 10
EVALUATION_STEPS = 15

def data_generator():
    for _ in range(EPOCHS):
        for _ in range(STEPS_PER_EPOCH):
            x = np.random.random((BATCH_SIZE, NFEATURES))
            y = np.random.randint(0, 2, BATCH_SIZE)
            yield x, y   

def validation_generator():
    x = np.random.random((BATCH_SIZE, NFEATURES))
    y = np.random.randint(0, 2, BATCH_SIZE)
    for _ in range(EPOCHS):
        for _ in range(VALIDATION_STEPS + 1):
            yield x, y

def evaluation_generator():
    for _ in range(EVALUATION_STEPS):
        x = np.random.random((BATCH_SIZE, NFEATURES))
        y = np.random.randint(0, 2, BATCH_SIZE)
        yield x, y

model = Sequential(name='Diabetes')

model.add(Input(NFEATURES, ))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(data_generator(), epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, 
        validation_data=validation_generator(), validation_steps=VALIDATION_STEPS)

eval_result = model.evaluate(evaluation_generator(), steps=EVALUATION_STEPS)

#----------------------------------------------------------------------------------------------------------------------------
    Benzer biçimde predict metodunda da kesitirimi yapılacak veriler bizden parça parça istenebilmektedir. Bunun için yine
    predict metodunun x parametresine bir üretici fonksiyon nesnesi girilir. predcit metonunun da kestirim verilerininin kaç 
    parça olarak isteneceğine yönelik steps parametresi vardır. Tabii sınama işlemi sırasında üretici fonksiyon artık yalnızca 
    x değerlerini vermelidir. Aşağıda buna ilişkin bir örnek verilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

EPOCHS = 100
NFEATURES = 10
STEPS_PER_EPOCH = 20
BATCH_SIZE = 32
VALIDATION_STEPS = 10
EVALUATION_STEPS = 15
PREDICTION_STEPS = 5

def data_generator():
    for _ in range(EPOCHS):
        for _ in range(STEPS_PER_EPOCH):
            x = np.random.random((BATCH_SIZE, NFEATURES))
            y = np.random.randint(0, 2, BATCH_SIZE)
            yield x, y   

def validation_generator():
    x = np.random.random((BATCH_SIZE, NFEATURES))
    y = np.random.randint(0, 2, BATCH_SIZE)
    for _ in range(EPOCHS):
        for _ in range(VALIDATION_STEPS + 1):
            yield x, y

def evaluation_generator():
    for _ in range(EVALUATION_STEPS):
        x = np.random.random((BATCH_SIZE, NFEATURES))
        y = np.random.randint(0, 2, BATCH_SIZE)
        yield x, y
        
def prediction_generator():
    for _ in range(PREDICTION_STEPS):
        x = np.random.random((BATCH_SIZE, NFEATURES))
        yield x

model = Sequential(name='Diabetes')

model.add(Input(NFEATURES, ))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(data_generator(), epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, 
        validation_data=validation_generator(), validation_steps=VALIDATION_STEPS)

eval_result = model.evaluate(evaluation_generator(), steps=EVALUATION_STEPS)
predict_result = model.predict(prediction_generator(), steps=PREDICTION_STEPS)

#----------------------------------------------------------------------------------------------------------------------------
                                        45. Ders - 29/06/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de gerçekten bellekte büyük bir yer kaplayan ve vektörizasyon işlemi gerektiren bir örneği parçalı bir biçimde eğitelim
    ve test edelim. Bu örnek için IMDB veri kümesini kullanalım. Burada karşılaşacağımız önemli bir sorun "karıştırma (shuffling)"
    işleminin nasıl yapılacağına ilişkindir. Bilindiği gibi fit işlemi sırasında her epoch işleminden sonra eğitim veri kümesi
    karıştırılmaktadır. (Aslında epoch sonrasında veri kümesinin karıştırılıp karıştırımayacağı fit metodundaki shuffle parametresi
    ile belirlenebilmektedir. Bu paramerenin default değeri True biçimindedir.) Parçalı eğitim yapılırken fit metodunun shuffle 
    parametresinin bir etkisi yoktur. Dolayısıyla veri kümesinin epoch sonrasında karıştırılması programcı tarafından üretici
    fonksiyon içerisinde yapılmalıdır. Peki programcı bu karıştırmayı nasıl yapabilir? CSV dosyasının dosya üzerinde karıştırılması
    uygun bir yöntem değildir. Bu durumda birkaç yöntem izlenebilir:

    1) Veri kümesi yine read_csv fonksiyonu ile tek hamlede belleğe okunabilir. Her batch işleminde bellekteki ilgili batch'lik
    kısım verilebilir. Karıştırma da bellek üzerinde yapılabilir. Ancak veri kümesini belleğe okumak yapılmak istenen şeye 
    bir tezat oluşturmaktadır. Fakat yine de text işlemlerinde asıl bellekte yer kaplayan unsur vektörizasyon işleminden elde 
    edilen matris olduğu için bu yöntem kullanılabilir. 

    2) Veri kümesi bir kez okunup dosyadaki kaydın offset numaraları bir dizide saklanabilir. Sonra bu dizi karıştırılıp 
    dizinin elemanlarına ilişkin kayıtlar okunabilir. Eğer veritabanı üzerinde doğrudan çalışılıyorsa da işlemler benzer biçimde
    yürütülebilir. 

    Peki biz vektörizasyon işlemini TextVectorization sınıfı türünden katman nesnesiyle yapmaya çalışırken tüm verileri yine
    belleğe çekmek zorunda değil miyiz? Aslında TextVectorization sınıfının adapt metodu henüz görmediğimiz TensorFlow kütüphanesindeki
    Dataset nesnesini de parametre olarak alabilmektedir. Bu sayede biz bir Dataset nesnesi oluşturarak adapt metodunun da 
    verileri parçalı bir biçimde almasını sağlayabiliriz. scikit-learn kütüphanesindeki CountVectorizer sınıfının fit metodu 
    da aslında dolaşılabilir nesneyi parametre olarak alabilmektedir. Dolayısıyla CountVectorizer kullanılırken yine üretici 
    fonksiyonlar yoluyla biz verileri fit metoduna parça parça verebilmekteyiz. Dosya nesneslerinin de dolaşılabilir nesneler
    olduğunu anımsayınız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        46. Ders - 30/06/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Örnek üzerinde çalışıldı.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        47. Ders - 06/07/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte IMDB veri kümesi tek hamlede değil parça parça okunarak eğitim yapılmıştır. Her epcoh'tan sonra karıştırma
    yapılacağı için asıl dosyanın karıştırılması uygun olmadığından dolayı satırın offset'lerii bir listede toplanıp bu liste
    karıştırılmıştır. Ancak orijinal IMDB veri kümesinde maalesef bazı yorumlar birden fazla satırdan oluşmaktadır. Bu yorumların
    sayısı çok azdır. Bu nedenle biz orijinal IMDB veri kümesinden bu satırları atan ve bu bu veri kümesini dönüştüren bir program
    yazdık. Program şöyldir:

    # convert_imdb.py

    import pandas as pd

    df = pd.read_csv('IMDB Dataset.csv', encoding='latin-1')

    for index, text in enumerate(df['review']):
        rtext = text.replace('\n', ' ')
        rtext = rtext.replace('\r', ' ')
        df.iloc[index, 0] = rtext

    df.to_csv('imdb.csv', index=False)

    Burada program hedef olarak "imdb.csv" dosyasını oluşturmaktadır. Bu dosyadaki satırların offset'lerini aşağıdaki gibi bir liste 
    biçiminde oluşturabiliriz:

    def record_offsets(f, skiprows=1):
        offsets = []
        for i in range(skiprows):
            f.readline()
        offsets.append(f.tell())
        while f.readline() != '':
            offsets.append(f.tell())           
        offsets.pop()           
        return offsets

    Burada biz her satırın offset numarasını offsets isimli listede toplayıp bu listeyle geri döndük. Ancak offset bilgisini okuma 
    işleminden sonra sakladığımız için EOF offset'i de listenin sonunda bulunmaktadır. pop işlemi ile bu son elemanının sildiğimize 
    dikkat ediniz. Programımızdaki üretici fonksiyon şöyle yazılmıştır:
    
    def data_generator(f, epochs, steps_per_epoch, batch_size, offsets):
        reader = csv.reader(f)
        for _ in range(epochs):
            random.shuffle(offsets)
            for batch_no in range(steps_per_epoch):
                x = []
                y = []
                for offset in offsets[batch_no * batch_size: batch_no * batch_size + batch_size]:
                    f.seek(offset, 0)
                    result = next(reader)
                    x.append(result[0])
                    y.append(1 if result[1] == 'positive' else 0)
                yield tf.convert_to_tensor(x), tf.convert_to_tensor(y)
            
    Bu örneğimizde sınama veri kümesi ve test veri kümesi kullanılmadığı için yalnızca epoch-loss grafiği çizdirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import math
import random
import csv
import pandas as pd
import tensorflow as tf

EPOCHS = 5
BATCH_SIZE = 32

def record_offsets(f, skiprows=1):
    offsets = []
    for i in range(skiprows):
        f.readline()
    offsets.append(f.tell())
    while f.readline() != '':
        offsets.append(f.tell())    
    offsets.pop()      
    return offsets
        
f = open('imdb.csv', encoding='latin-1')
offsets = record_offsets(f)   

def data_generator(f, epochs, steps_per_epoch, batch_size, offsets):
    reader = csv.reader(f)
    for _ in range(epochs):
        random.shuffle(offsets)
        for batch_no in range(steps_per_epoch):
            x = []
            y = []
            for offset in offsets[batch_no * batch_size: batch_no * batch_size + batch_size]:
                f.seek(offset, 0)
                result = next(reader)
                x.append(result[0])
                y.append(1 if result[1] == 'positive' else 0)
            yield tf.convert_to_tensor(x), tf.convert_to_tensor(y)
           
df = pd.read_csv('imdb.csv')

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense, TextVectorization

tv = TextVectorization(output_mode='count')
tv.adapt(df['review'])

model = Sequential(name='IMDB')

model.add(Input((1, ), dtype='string'))
model.add(tv)
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(data_generator(f, EPOCHS, math.ceil(len(offsets) / BATCH_SIZE), BATCH_SIZE, offsets), 
        batch_size=BATCH_SIZE, steps_per_epoch=math.ceil(len(offsets) / BATCH_SIZE), epochs=EPOCHS)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss'])
plt.show()

# prediction

predict_df = pd.read_csv('predict-imdb.csv') 
predict_result = model.predict(predict_df)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki parçalı verilerle eğitim yapılırken sınama, test ve kestirim işlemlerini de parçalı bir biçimde yapabilir miyiz?
    Evet bu işlemlerin hepsi parçalı bir biçimde yapılabilmektedir. fit metodunda validation_data parametresi bir üretici 
    nesne olarak (ya da bir PyDataset nesnesi olarak) girilirse bu durumda her epoch'tan sonra sınama verileri bu üretici 
    fonksiyondan (ya da PyDataset nesnesinden) elde edilecektir. Ancak bu durumda fit metodunun validation_steps parametresinde 
    kaç dolaşımla (yield işlemi ile) sınama verilerinin elde edileceği de girilmelidir. Örneğin:

    model.fit(..., validation_data=validation_generator(), validation_steps=100)

    Burada sınama verilerinin elde edilmesi için toplam 100 kez dolaşım (yield işlemi) yapılacaktır. Her epoch sonrasındaki 
    sınamada sınama veri kümesinin karıştırılmasına gerek yoktur. 

    Test işlemi de benzer biçimde parçalı olarak yapılabilir. Bunun için Sequential sınıfının evaluate metodunun x parametresine 
    bir üretici nesne (ya da bir PyDataset nesnesi) girilir. Test işlemi yapılırken kaç kere dolaşım uygulanacağı da steps 
    parametresiyle belirtilmektedir. Örneğin:

    eval_result = model.evalute(test_generator(), steps=32)

    Kestirim işleminde parçalı veri kullanılmasına genellikle gereksinim duyulmuyor olsa da kestirim işlemi yine parçalı 
    verilerle yapılabilir. Bunun için Sequential sınıfının predict metdounda x parametresine bir üretici nesne (ya da 
    PyDataset nesnesi) nesne gerilir. Yine metodun steps parametresi sınama verileri için kaç kez dolaşım uygulanacağını 
    (yani yield yapılacağını) beelirtir. Örneğin:

    predict_result = model.predict(predict_generator(), steps=32)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda IMDB veri kümesi üzerinde üretici fonksiyonlar yoluyla parçalı üretim, parçalı sınama, parçalı test ve parçalı 
    kestirim işlemlerine örnek verilmiştir. Sınamaların (validation) her epoch sonrasında yapıldığını anımsayınız. Burada parçalı 
    sınama yapılırkesn iç içe iki döngü kullanılmıştır. Birinci döngü epoch döngüsü, ikinci döngü batch döngüsüdür:

    def training_validation_test_generator(f, epochs, steps, batch_size, offsets, shuffle=False):
        reader = csv.reader(f)
        for _ in range(epochs):
            if shuffle:
                random.shuffle(offsets)
            for batch_no in range(steps):
                x = []
                y = []
                for offset in offsets[batch_no * batch_size: batch_no * batch_size + batch_size]:
                    f.seek(offset, 0)
                    result = next(reader)
                    x.append(result[0])
                    y.append(1 if result[1] == 'positive' else 0)
                yield tf.convert_to_tensor(x), tf.convert_to_tensor(y)
                        
    Fonksiyonda eopchs * steps kadar yield işlemi yapıldığına dikkat ediniz. Bu fonksiyon hem eğitim, hem sınama hemi de test 
    amacıyla kullanılmıştır. Tabii test işleminde bir epoch kavramı yoktur. Bu nedenle test işleminde burada epochs parametresi 
    1 olarak grilmiştir. Yukarıda da belirttiğimiz gibi kestirimlerde genellikle parçalı işlemlere gereksinim duyulmamaktadır. 
    Ancak biz burada parçalı kestirime de bir örnek vermek istedik. Parçalı kestirim için kullanılan üretici fonksiyon şöyledir:

    def predict_generator(f, steps, batch_size, offsets):
        reader = csv.reader(f)
        for batch_no in range(steps):
            x = []
            for offset in offsets[batch_no * batch_size:batch_no * batch_size + batch_size]:
                f.seek(offset, 0)
                result = next(reader)
                x.append(result[0])
            yield tf.convert_to_tensor(x), 
                    
    Parçalı kestirimde de bir epcoh kavramı yoktur. Dolayısıyla fonksiyonda bir epoch döngüsü oluşturulmamıştır. 
#----------------------------------------------------------------------------------------------------------------------------

import math
import random
import csv
import pandas as pd
import tensorflow as tf

EPOCHS = 5
BATCH_SIZE = 32
TEST_RATIO = 0.2
VALIDATION_RATIO = 0.2

def record_offsets(f, skiprows=1):
    offsets = []
    for i in range(skiprows):
        f.readline()
    offsets.append(f.tell())
    while f.readline() != '':
        offsets.append(f.tell())
    offsets.pop()      
    return offsets
        
f = open('imdb.csv', encoding='latin-1')
offsets = record_offsets(f)   

random.shuffle(offsets)

test_split_index = int(len(offsets) * (1 - TEST_RATIO))
validation_split_index = int(test_split_index * (1 - VALIDATION_RATIO))

training_offsets = offsets[:validation_split_index]
validation_offsets = offsets[validation_split_index:test_split_index]
test_offsets = offsets[test_split_index:]

training_steps = math.ceil(len(training_offsets) / BATCH_SIZE)
validation_steps = math.ceil(len(validation_offsets)/BATCH_SIZE)
test_steps = math.ceil(len(test_offsets)/BATCH_SIZE)

def training_validation_test_generator(f, epochs, steps, batch_size, offsets, shuffle=False):
    reader = csv.reader(f)
    for _ in range(epochs):
        if shuffle:
            random.shuffle(offsets)
        for batch_no in range(steps):
            x = []
            y = []
            for offset in offsets[batch_no * batch_size: batch_no * batch_size + batch_size]:
                f.seek(offset, 0)
                result = next(reader)
                x.append(result[0])
                y.append(1 if result[1] == 'positive' else 0)
            yield tf.convert_to_tensor(x), tf.convert_to_tensor(y)
                
def predict_generator(f, steps, batch_size, offsets):
    reader = csv.reader(f)
    for batch_no in range(steps):
        x = []
        for offset in offsets[batch_no * batch_size:batch_no * batch_size + batch_size]:
            f.seek(offset, 0)
            result = next(reader)
            x.append(result[0])
        yield tf.convert_to_tensor(x), 
                  
df = pd.read_csv('imdb.csv')

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense, TextVectorization

tv = TextVectorization(output_mode='count')
tv.adapt(df['review'])

model = Sequential(name='IMDB')

model.add(Input((1, ), dtype='string'))
model.add(tv)
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_validation_test_generator(f, 
                 EPOCHS, training_steps, BATCH_SIZE, training_offsets, True), 
                 batch_size=BATCH_SIZE, 
                 steps_per_epoch=training_steps, 
                 epochs=EPOCHS, 
                 validation_data=training_validation_test_generator(f, EPOCHS, validation_steps, BATCH_SIZE, validation_offsets), 
                 validation_steps=validation_steps)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(training_validation_test_generator(f, 1, test_steps, BATCH_SIZE, test_offsets), steps=test_steps)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_f = open('predict-imdb.csv')
predict_offsets = record_offsets(predict_f)
predict_steps = int(math.ceil(len(predict_offsets) / BATCH_SIZE))

predict_result = model.predict(predict_generator(predict_f, predict_steps, BATCH_SIZE, predict_offsets), steps=predict_steps)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------   
                                        48. Ders - 07/07/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------   

#----------------------------------------------------------------------------------------------------------------------------   
    Parçalı biçimde eğitim, test ve kestirim işlemi üretici fonksiyon yerine sınıfsal bir biçimde de yapılabilmektedir. Bunun 
    için tensorflow.keras.utils modülü içerisindeki PyDataset sınıfından türetilmiş sınıflar kullanılmaktadır. (Aslında bir süre 
    öncesine kadar bu amaçla Sequence isimli bir sınıftan türetme yapılıyordu. Ancak Keras ekibi bunun yerine TenserFlow kütüphanesi 
    içerisindeki Dataset sınıfını Keras'tan kullanılabilir hale getirdi. Dokümantasyondan da Sequence sınıfı kaldırıldı.) 
    
    Parçalı eğitim PyDataset sınıfı yoluyla şöyle yapılmaktadır: Programcı önce PyDataset sınıfındna bir sınıf türetir. türemiş 
    sınıf içerisinde __len__ ve __getiitem__ metotlarını yazar. fit metodu bir epoch'un kaç tane batch içerdiğini tespit etmek için 
    sınıfın __len__ metodunu çağırmaktadır. Daha sonra fit metodu eğitim sırasında her batch bilgiyi elde etmek için sınıfın 
    __getitem__ metodunu çağırır. Bu metodu çağırırken batch numarasını metoda parametre olarak geçirir. Bu metottan programcı batch 
    büyüklüğü kadar x ve y verisinden oluşan bir demetle geri dönmelidir. (Tabii aslında metodun geri döndürdüğü x ve y değerleri her 
    defasında aynı uzunlukta olmak zorunda da değildir.) Sonra programcı fit metodunun training_dataset_x parametresine bu sınıf 
    türünden bir nesne yaratarak o nesneyi girer. Örneğin:

    class DataGenerator(PyDataset):
        def __init__(self):
            super().__init__()
            pass

        def __len__(self):
            pass
        
        def __getitem__(self, batch_no):
            pass
    ...
    model.fit(DataGenrator(...), epochs=100)

    Tabii artık bu yöntemde fit metodunun steps_per_epoch parametresi kullanılmamaktadır. Metodun bath_size parametresinin de 
    bu yöntemde bir anlamı yoktur. Ancak epochs parametresi kaç epoch uygulanacağını belirtmek içn kullanılmaktadır.

    Sınama işlemi yine benzer bir biçimde yapılmaktadır. Yani programcı yine PyDataset sınıfından sınıf türetip __len__ ve 
    __getitem__ metotlarını yazar. Tabii durumda her epoch sonrasında bu sınama verileri __getitem__ metodu yoluyla elde 
    edilecektir. Programcı yine bunun için validation_data parametresine PyDataset sınıfından türettiği sınıf türünden bir 
    nesne girer. Örneğin:

    model.fit(DataGenrator(...), epochs=100, validation_data=DataGenerator(....))

    Ayrıca PyDataset sınıfından türetilmiş olan sınıfta on_epoch_end isimli bir metot da yazılabilmektedir. Eğitim sırasında her 
    epoch bittiğinde fit tarafından bu metot çağrılmaktadır. Programcı da tipik olarak bu metotta eğitim veri kümesini karıştırır.  
    Bu biçimdeki parçalı eğitimde artık fit metodunun steps_per_epoch parametresi kullanılmamaktadır. Çünkü zaten bu bilgi fit 
    metodu tarafından __len__ metodu çağrılarak elde edilmektedir. Benzer biçimde evaluate işleminde de yine PyDataset sınıfından 
    sınıf türetilerek test edilecek veriler parçalı bir biçimde evaluate metoduna verilebilmektedir. 

    Bu yöntemde predict işlemi de yine benzer biçimde parçalı olarak gerçekleştirilebilmektedir. Ancak her ne kadar predict 
    işleminde bir epoch kavramı olmasa da Keras predict işleminin sonunda yine on_epoch_end metodunu çağırmaktadır.

    Uygulamada parçalı verilerle eğitim işleminde aslında üretici fonksiyonlardan ziyada bu sınıfsal yöntem daha çok tercih 
    edilmektedir. Bu yöntemi uygulamak daha kolaydır. Ayrıca toplamda bu yöntem daha hızlı olma eğilimindedir. 

    Aşağıda rastgele verilerle bu işlemin nasıl yapılacağına yönelik bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.utils import PyDataset

EPOCHS = 2
NFEATURES = 10
BATCH_SIZE = 32

class DataGenerator(PyDataset):
    def __init__(self, batch_size, nfeatures, *, steps):
        super().__init__()
        self.batch_size = batch_size
        self.nfeatures = nfeatures
        self.steps = steps
        
    def __len__(self):
       return self.steps
    
    def __getitem__(self, batch_no):
        x = np.random.random((self.batch_size, self.nfeatures))
        y = np.random.randint(0, 2, self.batch_size)
        return x, y
    
    def on_epoch_end(self):
        print('shuffle')

model = Sequential(name='Test')

model.add(Input((NFEATURES, )))
model.add(Dense(16, activation='relu', name='Hidden-1'))
model.add(Dense(16, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.fit(DataGenerator(BATCH_SIZE, NFEATURES, steps=50), epochs=EPOCHS, validation_data=DataGenerator(BATCH_SIZE, NFEATURES, steps=5))

eval_result = model.evaluate(DataGenerator(BATCH_SIZE, NFEATURES, steps=10))
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_result = model.predict(DataGenerator(BATCH_SIZE, NFEATURES, steps=1))

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda IMDB veri kümesinin PyDataset sınıfından türetme yapılarak parçalı eğitilmesine bir örnek verilmiştir. Bu örnekte
    biz bir tane DataGenerator sınıfı oluşturkduk. Hem eğitim işleminde, hem sınama işleminde, hem test işleminde hem de kestirim
    işleminde aynı sınıfı kullandık. Aslında bu örnekte yapılanlar genel mantık olarak üretici fonksiyon örneğinde yapılanlarla
    benzerdir. Yukarıda da belirttiğimiz gibi parçalı eğitim için üretici fonksiyonlar yerine bu yöntemin uygulanmasını tavsiye 
    etmekteyiz. 
#----------------------------------------------------------------------------------------------------------------------------

import math
import random
import csv
import pandas as pd
import tensorflow as tf
from tensorflow.keras.utils import PyDataset

EPOCHS = 5
BATCH_SIZE = 32
TEST_RATIO = 0.2
VALIDATION_RATIO = 0.2

def record_offsets(f, skiprows=1):
    offsets = []
    for i in range(skiprows):
        f.readline()
    offsets.append(f.tell())
    while f.readline() != '':
        offsets.append(f.tell())
    offsets.pop()       
    return offsets
        
f = open('imdb.csv', encoding='latin-1')
offsets = record_offsets(f)   

random.shuffle(offsets)

test_split_index = int(len(offsets) * (1 - TEST_RATIO))
validation_split_index = int(test_split_index * (1 - VALIDATION_RATIO))

training_offsets = offsets[:validation_split_index]
validation_offsets = offsets[validation_split_index:test_split_index]
test_offsets = offsets[test_split_index:]

training_steps = math.ceil(len(training_offsets) / BATCH_SIZE)
validation_steps = math.ceil(len(validation_offsets)/BATCH_SIZE)
test_steps = math.ceil(len(test_offsets)/BATCH_SIZE)

class DataGenerator(PyDataset):
    def __init__(self, f, steps, batch_size, offsets, *, shuffle=False, predict=False):
        super().__init__()
        self.f = f
        self.steps = steps
        self.batch_size = batch_size
        self.offsets = offsets
        self.shuffle = shuffle
        self.predict = predict
        self.reader = csv.reader(f)
        
    def __len__(self):
        return self.steps
    
    def __getitem__(self, batch_no):
        x = []
        if not self.predict:
            y = []
        for offset in self.offsets[batch_no * self.batch_size: batch_no * self.batch_size + self.batch_size]:
            f.seek(offset, 0)
            result = next(self.reader)
            x.append(result[0])
            if not self.predict:
                y.append(1 if result[1] == 'positive' else 0)
                
        if not self.predict:
            return tf.convert_to_tensor(x), tf.convert_to_tensor(y)
        return tf.convert_to_tensor(x),
    
    def on_epoch_end(self):
        random.shuffle(self.offsets)  

df = pd.read_csv('imdb.csv')

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense, TextVectorization

tv = TextVectorization(output_mode='count')
tv.adapt(df['review'])

model = Sequential(name='IMDB')
model.add(Input((1, ), dtype='string'))
model.add(tv)
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(DataGenerator(f, training_steps, BATCH_SIZE, training_offsets, shuffle=True), 
                epochs=EPOCHS, 
                validation_data = DataGenerator(f, validation_steps, BATCH_SIZE, validation_offsets)) 
                 
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(DataGenerator(f, test_steps, BATCH_SIZE, test_offsets))
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_f = open('predict-imdb.csv')
predict_offsets = record_offsets(predict_f)
predict_steps = int(math.ceil(len(predict_offsets) / BATCH_SIZE))

predict_result = model.predict(DataGenerator(f, predict_steps, BATCH_SIZE, predict_offsets, predict=True))

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Aslında Keras'ta uzunca bir süredir hem dizin içerisindeki resimlerden önişlem yapan, veri artırımında da kullanılabilecek 
    ImageDataGenerator isimli bir sınıf bulunuyordu. Ancak Keras'ın yeni versiyonlarında bu sınıf "deprecated" yapılmıştır. 
    Yani "artık uygulamacıların bu sınıfı kullanmaması gerektiği ileride bu sınıfın tamamen kütüphaneden kaldırılabileceği"
    belirtilmiştir. Bu nedenle biz artık kursumuzda bu sınıf üzerinde ayrıntılı bir biçimde durmayacağız. Ancak eski kodları 
    incelerken bu sınıfla karşılaşabilirsiniz. Burada sınıfın temel kullanım abacı üzerinde bazı şeyler söylemek istiyoruz. 
    Sınıfın __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.preprocessing.image.ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        zca_whitening=False,
        zca_epsilon=1e-06,
        rotation_range=0,
        width_shift_range=0.0,
        height_shift_range=0.0,
        brightness_range=None,
        shear_range=0.0,
        zoom_range=0.0,
        channel_shift_range=0.0,
        fill_mode='nearest',
        cval=0.0,
        horizontal_flip=False,
        vertical_flip=False,
        rescale=None,
        preprocessing_function=None,
        data_format=None,
        validation_split=0.0,
        interpolation_order=1,
        dtype=None
    )

    Bu sınıfın kabaca kullanımı şöyledir:

    1) Uygulamacı önce ImageDataGenerator sınıfı türünden bir nesne yaratır. Bu nesneyi yaratırken yapılacak artırım (augmentation)
    işlemlerini parametrelere argümanlar girerek belirler. Örneğin:

    idg = ImageDataGenerator(
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    2) Daha sonra sınıfın flow metotları çağrılır. Aslında asıl dönüştürme işlemleri bu flow metotları yoluyla yapılmaktadır. 
    Yani flow metodu her çağrıldığında ona verdiğimiz x ve y değerleri yukarıda belirttiğimiz biçimde dönüştürmeye sokulmaktadır.
    flow metotları üretici fonksiyon gibi çalışmaktadır. Yani her next işleminde sıradaki batch'lik veriyi işleme sokarak 
    vermektedir. üç flow metodu vardır:

    flow
    flow_from_dataframe
    flow_from_directory  

    3) Artık model kurulup fit işleminde x verileri olarak ve sınama verileri olarak bu flow metotlarından elde edilen 
    üretici fonksiyonlar verilir. Örneğin:

    hist = model.fit(idg.flow(x_train, y_train, batch_size=64), epochs=50)

    Peki bu sınıf neden deprecated yapılmıştır? Bu sınıf tek hamlede pek çok veri artırımını yapmaya çalışmaktadır. Bunun 
    yerine bu işlemlerin tek tek katmanlara yaptırılması daha modüler bir yaklaşımdır. Yukarıda açıkladığımız veri artırımına 
    yönelik katman nesneleri ve tensorflow.keras.preprocessing.image modülündeki fonksiyonlar zaten bu sınıfın yaptıklarını
    daha modüler biçimde yapabilmektedir. Ancak ImageDataGenerator sınıfının flow_from_directory metodu doğrudan bir dizindeki 
    resimlerden önişlem yapabilmektedir. İşte bu işlemi bağımsız yapabilen image_dataset_from_directory isimli bir fonksiyon 
    da  tensorflow.keras.preprocessing modülüne eklenmiştir. Sonraki paragrafta bu fonksiyonun kullanımı açıklanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında parçalı eğitimler için kullanılan asıl sınıf Dataset isimli sınıftır. Ancak bu sınıf TensorFlow kütüphanesinin 
    aşağı seviyeli çalışması ile uyumlu biçimde hazırlanmıştır. Bu sınıfın Keras'tan kolay kullanılabilmesi için PyDataset 
    sınıfı oluşturulmuştur. Biz de yukarıdaki örneklerimizde PyDataset sınıfından türetme yaparak işlemlerimizi gerçekleştirdik. 
    TensorFlow'un aşağı seviyeli Dataset sınıfı tensorflow.data modülü içerisindedir. Bu Dataset sınıfı soyut bir sınıftır. 
    Yani bunun kullanılması için bu sınıftan türetme yapılıp türemiş sınıfta pek çok metodun yazılması gerekir. Zaten bun 
    nedenden dolayı kullanımı basitleştirmek için PyDataset sınıfı oluşturulmuştur. Ancak TensorFlow kütüphanesinde uygulamacı 
    için işlemleri kolaylaştıran yardımcı fonksiyonlar ve sınıflar da bulundurulmuştur. Bu fonksiyonlar birtakım işlemleri 
    kolaylaştıran Dataset nesneleri oluşturmaktadır. Örneğin image_dataset_from_directory fonksiyonu eğer resimler bir dizin 
    içerisinde uygun bir biçimde yerleştirilmişse onları batch batch dizinlerden alarak parçalı bir biçimde dışarıya verebilmektedir. 
    Böylece uygulamacı resimleri bir dizine uygun biçimde yerleştirdikten sonra doğrudan bu fonksiyonun çıktısı olan DataSet 
    nesnesini fit, evaluate ve predict gibi metotlara verebilmektedir. İzleyen paragraflarda işlemleri kolaylaştıran ve bize 
    parçalı eğitim için kullanabileceğimiz Dataset nesneleri veren bazı fonksiyonlar ve sınıfları göreceğiz. Bu fonksiyonlar 
    TensorFlow kütüphanesine göreli olarak yeni eklenmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    tensorflow.data modülündeki Dataset sınıfı soyut bir sınıftır. Yani bu sınıfı biz doğrudan kullanamayız. Bu sınıftan türetme
    yapılıp türemiş sınıfta taban Dataset sınıfının metotlarının override edilmesi gerekir. Tabii pratikte programcılar türetme
    yapmak yerine zaten hazırda var olan somut türemiş sınıfları ya da somut türemiş sınıf nesnelerini veren fonksiyonları 
    kullanmaktadır. Biz burada Dataset sınıfında bulunan birkaç önemli fonksiyondan bahsedeceğiz.

    Dataset sınıfları dolaşılabilir sınıflardır. Biz bir Dataset nesnesini dolaştığımızd batch batch bilgileri elde ederiz. 
    
    Sınıfın take metodunun parametrik yapısı şöyledir:

    take(
        count, 
        name=None
    ) 

    Metodun count parametresi Dataset nesnesinden kaç batch'lik alınacağını belirtmektedir. Bu parametre -1 girilirse tüm 
    elemanlar elde edilmektedir. take metodu bize bir dolaşım nesnesi (iterator) verir. Bu dolaşım nesnesi her dolaşıldığında 
    x ve y değerlerinden oluşan demetler elde edilmektedir. Bize verilen bu dolaşım nesnesi toplamda count defa dolaşılmaktadır. 
    Örneğin Dataset nesnesi için belirlenmiş olan batch_size 32 olsun. take metodunda count parametresi için 10 girersek bu 
    nesneyi her dolaştığımızda 32'lik bir x ve y veri kümesi elde ederiz. Toplamda da bu nesneyi 10 kez dolaşabiliriz. 

    Dataset sınıfının batch isimli metodu bizden bir batch_size değeri alır ve bize bir dolaşım nesnesi verir. Biz bu dolaşım 
    nesnesini dolaştığımızda batch_size kadar batch'ler elde ederiz. Metodun parametrik yapısı şöyledir:

    batch(
        batch_size,
        drop_remainder=False,
        num_parallel_calls=None,
        deterministic=None,
        name=None
    )    
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de bize Dataset nesneleri veren hazır fonksiyonlardan ve sınıflardan bahsedelim. Yukarıda da belirttiğimiz gibi 
    tensorflow.keras.preprocessing modülündeki image_dataset_from_directory isimli fonksiyon bir dizinden hareketle oradaki 
    esimleri kullanıma hazır hale getirmektedir. Uygulamacı bulduğu resimleri bir dizin içerisine belli bir düzende saklar. 
    Sonra da bu fonksiyonu çağırır. Fonksiyon da bu resimleri önişleme sokarak bize TensorFlow Dataset nesnesi olarak verir. 
    image_dataset_from_directory fonksiyonunun parametrik yapısı şöyledir:

    tensorflow.keras.preprocessing.image_dataset_from_directory(
        directory,
        labels='inferred',
        label_mode='int',
        class_names=None,
        color_mode='rgb',
        batch_size=32,
        image_size=(256, 256),
        shuffle=True,
        seed=None,
        validation_split=None,
        subset=None,
        interpolation='bilinear',
        follow_links=False,
        crop_to_aspect_ratio=False,
        pad_to_aspect_ratio=False,
        data_format=None,
        verbose=True
    )

    Bu fonksiyonu kullanmadan önce uygulamacı resimleri bir dizin içerisine yerleştirmelidir. Eğer bir sınıflandırma problemi 
    söz konusu ise her sınıftaki resimler ayrıca bir alt dizine yerleştirilmelidir. Örneğin biz elmalarla portakalları sınıflandıran 
    bir ikili sınıflandırma problemi üzerinde çalışacak olalım. Bu durumda oluşturacağımız dizin yapısı şöyle olmalıdır:

    Images
        Apple
        Orange

    Tabii burada dizinlere istediğimiz isimleri verebiliriz. Ancak sınıflara ilişkin anlamlı isimlerin kullanılması tavsiye
    edilmektedir. İşte uygulamacı bulduğu elma resimlerini Apple dizinine, portakal resimlerini Orange dizinine yerleştirir. 
    Resimlerin bulunduğu dizinin yol ifadesini fonksiyonun birinci parametresine geçirir. İkinci parametre sınıf belirten 
    etiketlerin nasıl oluşturulacağını belirtmektedir. Buradaki default 'inferred' değeri etiketlerin otomatik olarak dizin 
    yapısından oluşturulacağı anlamına gelmektedir. Bu parametre birkaç biçimde daha geçilebilmektedir. Bunun için dokümanlara
    başvurabilirsiniz. label_mode parametresi default olarak 'int' biçimdedir. Bu durumda her bir kategori bir int değerle 
    temsil edilmektedir. (Yani y değeri olarak int değerler elde edilecektir.) Eğer bu parametreye 'categorical' girilirse 
    burada y değerleri "one-hot-encoding" biçiminde oluşturulur. Eğer bu parametreye 'binary' girilirse y değerleri 0, 1
    biçiminde oluşturulmaktadır. İkili sınıflandırma problemleri için bu parametreye 'binary', çoklu sınıflandırma problemleri 
    için 'categorical' girilmelidir. class_names parametresi sınıfların yazısal isimlerini belirtmektedir. Default durumda 
    sınıfların isimleri alt dizin isimlerinden elde edilir. color_mode parametresi dizinlerdeki resimlerin renk durumlarının
    nasıl ele alınacağını belirtmektedir. Bu parametrenin default değeri 'rgb' biçimindedir. Ancak duruma göre bu parametre 
    'grayscale' ya da 'rgba' biçiminde de girilebilir. batch_size parametresi bir batch'lik resmin kaç resimden oluşacağını 
    belirtmektedir. Bu sınıf kullanılarak fit işlemi yapılırken artık fit metodunun batch_size parametresi girilmez. Bu 
    batch_size değeri bu nesnede belirtilmektedir. Fonksiyonun image_size parametresi dizinlerdeki resimlerin hangi boyuta 
    çekileceğini belirtmektedir. Bu parametrenin default değeri (256, 256) biçimindedir. shuffle parametresi dizinlerden 
    elde edilen resimlerin karıştırılıp karıştırılmayacağını belirtmektedir. Bu parametrenin default değerinin True olduğunu 
    görüyorsunuz. Fonksiyonun diğer parametreleri için dokümanlara başvurulabilir.

    Örneğin yukarıdaki gibi bir dizin  yapısı olsun:

    Images
        Apple
        Orange

    Biz bu dizinden resimleri aşağıdaki gibi Dataset biçiminde oluşturabiliriz:

    dataset = image_dataset_from_directory('Images', label_mode='binary', image_size=(128, 128), batch_size=32)    

    Artık image_dataset_from_dirictory fonksiyonuyla elde ettiğimiz Dataset nesnesini daha önce görmüş olduğumuz parçalı verilerle 
    eğitimde kullanabiliriz. Bir Dataset nesnesi içerisindeki bilgiler iteratör yoluyla, sınıfın batch ya da take metotları yoluyla
    elde edilebildiğini anımsayınız.  

    Biz image_dataset_from_directory fonksiyonunu yalnız fit işlemlerinde değil, test ve kestirim işlemlerinde de kullanabiliriz. 
    Fonksiyonun seubset parametresi 'training', 'validation' ya da 'both' biçiminde girilebilmektedir. 'training' eldeki resim 
    kümesindenki bir grup resmin eğitim amacıylai 'validation' ise sınama amacıyla kullanılacağını belirtmektedir. Ancak 
    subset parametresi girildiğinde validation_split paranetresinin ve seed parametresinin de girilmesi gerekmektedir. Örneğin 
    biz eğitim ve sınama kümeleri için ayrışırmayı şöyle yapabiliriz:

    training_dataset = image_dataset_from_directory('Images', label_mode='binary', 
            image_size=(128, 128), subset='training', seed=123, validation_split=0.2, batch_size=32)  

    validation_dataset = image_dataset_from_directory('Images', label_mode='binary', 
            image_size=(128, 128), subset='training', seed=123, validation_split=0.2, batch_size=32)  

    Bu biçimdeki kullanımda seed değerlerinin aynı girilmesi gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import tensorflow
from tensorflow.keras.preprocessing import image_dataset_from_directory

dataset = image_dataset_from_directory('Images', label_mode='binary', image_size=(128, 128), batch_size=1)

batch_data = dataset.take(10)

import matplotlib.pyplot as plt

for x, y in batch_data:
    image = tensorflow.cast(x[0], 'uint8')
    plt.title(str(int(y)))
    plt.imshow(image)
    plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    tensorflow.data modülündeki TextLineDataset isimli sınıf bir ya da birden fazla dosyanın satırlarını parçalı bir biçimde 
    dışarıya verebilmektedir. Yani bu sınıfı bir dosyanın satırlarını batch batch elde eden bir Dataset sınıfı olarak düşünebiliriz. 
    Zaten TextLineDataset sınıfı Dataset sınıfından türetilmiş durumdadır. Sınıfın as_numpy_iterator metodu bize satırları 
    NumPy nesneleri olarka verebilmektedir. Örneğin:

    from tensorflow.data import TextLineDataset

    tld = TextLineDataset('iris.txt')
    for line in tld.as_numpy_iterator():
        print(line)
    tld.batch()

    Sınıfın ayrıntıları için ilgili dokümanlara başvurulabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    tensorflow.data modülü içerisindeki Dataset sınıfının list_files isimli static metodu bir dizinde joker karakterleriyle 
    verilmiş olan dosyaları bize Dataset nesnesi biçiminde vermektedir. Örneğin:

    dataset = tensorflow.data.Dataset.list_files("/path/*.txt")

    Tabii buradan elde edilen Dataset nesnesi dosyanın içeriğini değil yalnızca dosyaların yol ifadelerini vermektedir. 

    Dataset sınıfının from_generator isimli static metodu bir üretici fonksiyonu alıp onu Dataset nesnesi haline getirmektedir. 
    Yani bu fonksiyon elimizde bir üretici fonksiyon varsa ancak bizden bir Dataset nesnesi isteniyorsa o üretici fonksiyonu 
    Dataset nesnesi haline getirmek için kullanılmaktadır. 

    Dataset sınıfının from_tensor_slices isimli static metodu ise bizden bir Tensor nesnesi ya da dolaşılabilir bir nesne alarak 
    ondan Dataset nesnesi oluşturmaktadır. Örneğin:

    dataset = Dataset.from_tensor_slices([1, 2, 3, 4, 5])
    for val in dataset.as_numpy_iterator():
        print(val)

    tensorflow.data.experimental modülü içerisindeki make_csv_dataset fonksiyonu bizden bir CSV dosyasını alıp ondan parçalı 
    biçimde satır elde etmekte kullanılmaktadır. Örneğin:

    from tensorflow.data.experimental import make_csv_dataset

    dataset = make_csv_dataset('iris.csv', batch_size=32, label_name='Species')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        49. Ders - 13/07/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki biz parçalı eğitimi fit metodunu birden fazla kez çağırarak yapamaz mıyız? Keras'ın orijinal dokümanlarında bu 
    konuda çok açık örnekler verilmemiştir. Ancak kaynak kodlar incelendiğinde fit işleminin artırımlı bir biçimde yapıldığı
    görülmektedir. Yani birden fazla kez fit metodu çağrıldığında eğitim kalınan yerden devam ettirilmektedir. Bu nedenle 
    biz eğitimi farklı zamanlarda fit işlemlerini birden fazla kez yaparak devam ettirebiliriz. Ancak fit metodunun bu biçimde
    birden fazla kez çağrılması işleminde dikkat edilmesi gereken bazı noktalar da olabilmektedir. Keras ekibi bu tür parçalı
    eğitimler için daha aşağı seviyeli xxx_on_bath isimli metotlar bulundurmuştur. Programcının birden fazla kez fit metodunu 
    çağırması yerine bu metotları kullanması tavsiye edilmektedir. Parçalı işlemler için Sequential sınıfının şu metotları 
    bulundurulmuştur:

    train_on_batch
    test_on_batch
    predict_on_batch

    Ancak bu yöntemde sınama işlemleri otomatik olarak train_on_batch içerisinde yapılmamaktadır. Programcının sınamayı kendisinin
    yapması gerekmektedir. 

    train_on_batch metodunun parametrik yapısı şöyledir:

    train_on_batch(x, y=None, sample_weight=None, class_weight=None, return_dict=False)

    Burada x ve y parametreleri parçalı eğitimde kullanılacak x ve y değerlerini almaktadır. sample_weight ve class_weight 
    parametreleri ağırlıklandırmak için kullanılmaktadır. return_dict parametresi True geçilirse metot bize geri dönüş değeri 
    olarak loss değerini ve metrik değerleri bir sözlük nesnesi biçiminde verir. 

    train_on_batch metodu ile parçalı eğitim biraz daha düşük seviyeli olarak yapılmaktadır. Bu biçimde parçalı eğitimde epoch
    döngüsünü ve batch döngüsünü programcı kendisi oluşturmalıdır. Örneğin:

    for epoch in range(EPOCHS):
        <eğitim veri kümesi karıştırılıyor>
        for batch_no in range(NBATCHES):
            <bir batch'lik x ve y elde ediliyor>
            model.train_on_batch(x, y)

    Tabii yukarıda da belirttiğimiz gibi bu biçimde çalışma aşağı seviyelidir. Yani bazı şeyleri programcının kendisinin 
    yapması gerekir. Örneğin fit metodu bize bir History sınıfı türünden bir callback nesnesi veriyordu. Bu nesnenin içerisinden 
    de biz tüm epoch'lara ilişkin metrik değerleri elde edebiliyorduk. train_on_batch işlemleriyle eğitimde bu bu değerleri
    bizim elde etmemiz gerekmektedir. train_on_batch metodunun return_dict parametresi True geçilirse batch işlemi sonucundaki 
    loss ve metik değerler bize bir sözlük biçiminde verilmektedir. Biz de bu değerlerden hareketle epoch'taki ortalama loss ve
    metrik değerleri hesaplayabiliriz. Örneğin:

    for epoch in range(EPOCHS):
        <eğitim veri kümesi karıştırılıyor>
        for batch_no in range(NBATCHES):
            <bir batch'lik x ve y elde ediliyor>
            rd = model.train_on_batch(x, y, return_dict=True)
            <batch'e ilişkin loss ve metrik değerler epoch için ortalama hesabında kullanılıyor>

    Burada her spoch sonrasında değil her batch sonrasında değerlerin elde edildiğine dikkat ediniz. Aslında biz TensorFlow 
    ya da PyTorch kütüphanelerini aşağı seviyeli olarak kullanacak olsaydık zaten yine işlemleri bu biçimde iki döngü yoluyla
    yapmak durumunda kalacaktık. Genellikle uygulamacılar her batch işleminde elde edilen değerlerin bir ortalamasını epoch 
    değeri olarak kullanmaktadır. 

    Bu yöntemde epoch sonrasındaki sınama işlemlerinin de programcı tarafından manuel olarak yapılması gerekmektedir. 
    Yani programcı sınama veri kümesini kendisi oluşturmalı ve sınamayı kendisi yapmalıdır. evaulate metodu aslen test 
    amaçlı kullanılsa da işlev bakımından sınama amaçlı da kullanılabilmektedir. Bu durumda sınama işlemi şöyle yapılabilir:

    for epoch in range(EPOCHS):
        <eğitim veri kümesi karıştırılıyor>
        for batch_no in range(NBATCHES):
            <bir batch'lik x ve y elde ediliyor>
            rd = model.train_on_batch(x, y, return_dict=True)
        val_result = model.evaluate(...)

    Tabii burada evaluate işlemini de parçalı bir biçimde yapabilirsiniz. Bu durumda yine bir döngü oluşturup test_on_batch 
    fonksiyonunu kullanabilirsiniz. test_on_batch metodunun parametrik yapısı şöyledir:

    test_on_batch(x, y=None, sample_weight=None, return_dict=False)

    Kullanım tamamen train_batch metodu gibidir. 

    Test işlemi de tüm epoch'lar bittiğinde yine parçalı bir biçimde test_on_batch metoduyla yapılabilir. Kestirim işlemi de 
    yine benzer bir biçimde predict_on_batch metoduyla yapılabilmektedir. predict_on_batch metodunun parametrik yapısı şöyledir:

    predict_on_batch(x)

    Metot kestirim değerleriyle geri dönmektedir. Örneğin:

    for batch_no in range(NBATCHES):
        <bir batch'lik x elde ediliyor>
        predict_result = model.predict_on_batch(x)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            50. Ders - 14/07/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda IMDB veri kümesi üzerinde xxx_on_batch metotlarıyla parçalı biçimde işlemler yapılmasına bir örnek verilmiştir. 
    Bu örnekte tüm veri kümesi tek hamlede DataFrame nesnesi olarak okunmuştur. Aslında burada parçalı işlemler için daha önce 
    yapmış olduğmuz işlemlerin uygulanması daha uygundur. Ancak biz örneği karmaşık hale getirmemek için tüm veri kümesini tek 
    hamlede okuyup xxx_on_batch metotlarına onu parçalara ayırarak verdik. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense, TextVectorization

EPOCHS = 5
BATCH_SIZE = 32
TEST_SPLIT_RATIO = .20
VALIDATION_SPLIT_RATIO = .20

def create_model(df):
    tv = TextVectorization(output_mode='count')
    tv.adapt(df['review'])
    
    model = Sequential(name='IMDB')
    
    model.add(Input((1, ), dtype='string'))
    model.add(tv)
    model.add(Dense(128, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()
    return model
    
def train_test_model(training_df, validation_df, epochs, verbose = 1):
    history_loss = []
    history_accuracy = []
    val_history_loss = []
    val_history_accuracy = []
    
    for epoch in range(epochs):
        training_df = training_df.sample(frac=1)
        
        print('-' * 30)
        
        mean_loss, mean_accuracy = batch_train(training_df, int(np.ceil(len(training_df) / BATCH_SIZE)), 
                model.train_on_batch, verbose=1)
        history_loss.append(mean_loss)
        history_accuracy.append(mean_accuracy)
             
        if verbose == 1:
            print(f'Epoch: {epoch + 1}')       
            print(f'Epoch mean loss: {mean_loss}, Epoch Binary Accuracy: {mean_accuracy}')
            
        val_mean_loss, val_mean_accuracy = batch_train(validation_df, int(np.ceil(len(validation_df) / BATCH_SIZE)), 
                model.test_on_batch)
        
        val_history_loss.append(val_mean_loss)
        val_history_accuracy.append(val_mean_accuracy)
                
        if verbose == 1:
            print(f'Validation Loss: {val_mean_loss}, Validation Binary Accuracy: {val_mean_accuracy}')
        
    return history_loss, history_accuracy, val_history_loss, val_history_accuracy
  
def batch_train(df, nbatches, batch_method, verbose=0):   
    loss_list = []
    accuracy_list = []
    
    for batch_no in range(nbatches):
        x = tf.convert_to_tensor(df['review'].iloc[batch_no * BATCH_SIZE: batch_no * BATCH_SIZE + BATCH_SIZE], dtype='string')
        y = tf.convert_to_tensor(df['sentiment'].iloc[batch_no * BATCH_SIZE: batch_no * BATCH_SIZE + BATCH_SIZE])
        rd = batch_method(x, y, return_dict=True)
        
        loss_list.append(rd['loss'])
        accuracy_list.append(rd['binary_accuracy'])
        
        if verbose:
            print(f'Batch No: {batch_no}')
            if verbose == 2:
                print(f"Batch Loss: {rd['loss']}, Batch Binary Accuracy: {rd['accuracy']}")
               
    mean_loss = np.mean(loss_list)
    mean_accuracy = np.mean(accuracy_list)

    return mean_loss, mean_accuracy
  
df = pd.read_csv('IMDB Dataset.csv').iloc[:10000, :]
df['sentiment'] = (df['sentiment'] == 'positive').astype(dtype='uint8')

df = df.sample(frac=1)
test_zone = int(len(df) * (1 - TEST_SPLIT_RATIO))
training_validation_df = df.iloc[:test_zone, :]
test_df = df.iloc[test_zone:, :]
validation_zone = int(len(training_validation_df) * (1 - VALIDATION_SPLIT_RATIO))
training_df = training_validation_df.iloc[:validation_zone, :]
validation_df = training_validation_df.iloc[validation_zone:, :]
    
model = create_model(df)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
   
history_loss, history_accuracy, val_history_loss, val_history_accuracy = train_test_model(training_df, validation_df, EPOCHS)
            
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, EPOCHS))
plt.plot(range(EPOCHS), history_loss)
plt.plot(range(EPOCHS), val_history_loss)
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, EPOCHS))
plt.plot(range(EPOCHS), history_accuracy)
plt.plot(range(EPOCHS), val_history_accuracy)
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

# evaluation

eval_mean_loss, eval_accuracy = batch_train(test_df, int(np.ceil(len(test_df) / BATCH_SIZE)), model.test_on_batch)   
print(f'Test Loss: {eval_mean_loss}, Test Binary Accuracy: {eval_accuracy}')

# prediction

predict_df = pd.read_csv('predict-imdb.csv') 

for i in range(int(np.ceil(len(predict_df) / BATCH_SIZE))):
    predict_result = model.predict_on_batch(predict_df)
    for presult in predict_result[:, 0]:
        if (presult > 0.5):
            print('Positive')
        else:
            print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu noktasında sözünü ettiğimiz kütüphanelerin işlevlerini yeniden anımsatmak istiyoruz:

    NumPy ---> Vektörel işlemler yapan C'de yazılmış taban bir kütüphanedir. Pek çok proje NumPy kütüphanesini kendi içerisinde
    kullanmaktadır.

    Pandas ---> Bu kütüphane sütunlu veri yapılarını (yani istatistiksel veri tablolarını) ifade etmek için kullanılmaktadır. 
    Kütüphanenin en önemli özelliği farklı türlere ilişkin sütunsal bilgilerin DataFrame isimli bir veri yapısı ile temsil 
    edilmesini sağlamasıdır. Bu kütüphane de NumPy kullanılarak yazılmıştır.

    scikit-learn ---> Bir makine öğrennesi kütüphanesidir. Ancak bu kütüphanede yapay sinir ağlarıyla ilgili özellikler yoktur 
    (minimal düzeydedir). Yani bu kütüphane yapay sinir ağlarının dışındaki makine öğrenmesi yöntemleri için kullanılmaktadır. 
    scikit-learn kendi içerisinde NumPy, Pandas ve SciPy kütüphanelerini kullanmaktadır.

    SciPy ---> Genel amaçlı matematik ve nümerik analiz kütüphanesidir. Bu kütüphanenin doğrudan makine öğrenmesiyle bir ilgisi 
    yoktur. Ancak matematiğin çeşitli alanlarına ilişkin nümerik analiz işlemleri yapan geniş bir taban kütüphanedir. Bu kütüphane 
    de kendi içerisinde NumPy ve Pandas kullanılarak yazılmıştır. 

    TensorFlow ---> Google tarafından yapay sinir ağları ve makine öğrenmesi için oluşturulmuş taban bir kütüphanedir. Bu kütüphane
    çok işlemcili ve çekirdekli sistemlerde matrisler (bunlara "tensor" de denilmektedir) üzerinde paralel programlama teknikleriyle
    işlemler yapabilmek amacıyla tasarlanmıştır.

    Keras ---> Yapay sinir ağı işlemlerini kolaylaştırmak için oluşturulmuş olan yüksek seviyeli bir kütüphanedir. Eskiden bu 
    kütüphane "backend" olarak farklı kütüphaneleri kullanbiliyordu. Eski hali devam ettirilse de kütüphane taamamen TensorFlow
    içerisine dahil edilmiştir ve TensorFlow kütüphanesinin yüksek seviyeli bir katmanı haline getirilmiştir.

    PyTorch ---> Tamamen TensorFlow kütüphanesinde hedeflenen işlemleri yapan taban bir yapay sinir ağı ve makine öğrenmesi 
    kütüphanesidir. Facebook (Meta) tarafından geliştirilmiştir. 

    Theano --> TensorFlow, PyTorch SciPy benzeri bir taban kütüphanedir. Akademik çevreler tarafından geliştirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Elemanlarının çok büyük kısmı 0 olan matrislere "seyrek matrisler (sparse matrices)" denilmektedir. Seyreklik (sparsity)
    0 olan elemanların tüm elemanlara oranıyla belirlenmektedir. Bit matrisin seyrek olarak ele alınması için herkes tarafından
    kabul edilen bir seyreklik oranı yoktur. Seyreklik oranı ne kadar yüksek olursa onların çok boyutlu diziler yerine alternatif 
    veri yapılarıyla ifade edilmeleri o kadar verimli olmaktadır. Makine öğrenmesinde seyrek matrisle sıkça karşılaşılmaktadır. 
    Örneğin bir grup yazıyı vektörel hale getirdiğimizde aslında bir seyrek matris oluşmaktadır. Benzer biçimde one-hot-encoding 
    dönüştürmesi de bir seyrek matris oluşturmaktadır. scikit-learn kütüphanesindeki OneHotEncoder sınıfının ve CountVectorizer 
    sınıfının çıktı olarak seyrek matris verdiğini anımsayınız.

    Seyrek matrislerin daha az yer kaplayacak biçimde tutulmasındaki temel yaklaşım matrisin yalnızca sıfırdan farklı elemanlarının 
    ve onların yerlerinin tutulmasıdır. Örneğin bir milyon elemana sahip bir seyrek matriste yalnızca 100 eleman sıfırdan 
    faklıysa biz bu 100 elemanın değerlerini ve matristeki yerlerini tutarsak önemli bir yer kazancı sağlayabiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------  
    Seyrek matrisleri ifade etmek için alternatif birkaç veri yapısı kullanılmaktadır. Bunlardan biri DOK (Dictionary Of Keys) 
    denilen veri yapısıdır. Bu veri yapısında matrisin yalnızca 0'dan farklı olan elemanları bir sözlükte tutulur. Sözlüğün 
    biçimi aşağıdaki gibidir:

    {(34674,17000): 1, (542001, 170): 4, ...}

    Burada sözlüğün anahtarları satır ve sütun numaralarından oluşan demetler biçimindedir. Sözlüğün değerleri ise o satır ve 
    sütundaki matris değerlerinden oluşmaktadır. Örneğin aşağıdaki gibi bir matris söz konusu olsun:

    0 0 5
    3 0 0 
    0 6 0

    Buradaki dok sözlüğü şöyle olacaktır:

    {(0, 2): 5, (1, 0): 3, (2, 1): 6}

    Aşağıda DOK veri yapısı ile seyrek matris oluşturan DokMatrix isimli bir sınıf örneği verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

class DokMatrix:
    def __init__(self, nrows, ncols):
        self._nrows = nrows
        self._ncols = ncols
        self._dok = {}
        
    def __getitem__(self, index):
        if not isinstance(index, tuple) or len(index) != 2:
            raise TypeError('index must have twho dimension')
        if index[0] < 0 or index[0] >= self._nrows:
            raise IndexError('index out of range')
        if index[1] < 0 or index[1] >= self._ncols:
            raise IndexError('index out of range')
        return self._dok.get(index, 0)
    
    def __setitem__(self, index, val):
        if not isinstance(index, tuple) or len(index) != 2:
            raise TypeError('index must have twho dimension')
        if index[0] < 0 or index[0] >= self._nrows:
            raise IndexError('index out of range')
        if index[1] < 0 or index[1] >= self._ncols:
            raise IndexError('index out of range')
        
        self._dok[index] = val
        
    @property
    def shape(self):
        return self._nrows, self._ncols
    
    @property
    def size(self):
        return self._nrows * self._ncols
    
    def __len__(self):
        return self._nrows
    
    @staticmethod
    def array(a):
        if not isinstance(a, list):
            raise TypeError('argument must be Python list')
        nrows = len(a)
        ncols = len(a[0])
        for i in range(1, nrows):
            if len(a[i]) != ncols:
                raise ValueError('matrix rows must have the same number of elements')
        dm = DokMatrix(nrows, ncols)
        for i in range(nrows):
            for k in range(ncols):
                if a[i][k] != 0:
                    dm._dok[(i, k)] = a[i][k]
        return dm
    
    def todense(self):
        array = np.zeros((self._nrows, self._ncols))
        for index, val in self._dok.items():
            array[index] = val
        return array
    
    def __str__(self):
        smatrix = ''
        for i in range(self._nrows):
            sline = ''
            for k in range(self._ncols):
                if sline != '':
                    sline += ' '
                sline += str(self._dok.get((i, k), 0))
            if smatrix != '':
                smatrix += '\n'
            smatrix += sline           
        return smatrix
    
    def __repr__(self):
        smatrix = ''
        for index, val in self._dok.items():
            if smatrix != '':
                smatrix += '\n'
            smatrix += f'({index[0]}, {index[1]}) ---> {val}'        
        return smatrix
      
a = [[1, 0, 3], [0, 0, 1], [5, 0, 0]]
dok = DokMatrix.array(a)
print(dok)
print('-' * 10)
dok[1, 1] = 8
print(dok)
print('-' * 10)
print(repr(dok))

#----------------------------------------------------------------------------------------------------------------------------
    NumPy kütüphanesi içerisinde seyrek matrislerle işlem yapan sınıflar ya da fonksiyonlar bulunmamaktadır. Ancak SciPy 
    kütüphanesi içerisinde seyrek matrislerle ilgili işlemler yapan sınıflar ve fonksiyonlar vardır. scikit_learn kütüphanesi
    doğrudan SciPy kütüphanesinin seyrek matris sınıflarını kullanmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    DOK biçimindeki seyrek matrisler SciPy kütüphanesinde scipy.sparse modülü içerisindeki dok_matrix sınıfıyla temsil edilmiştir. 
    Biz bir dok_marix sınıfı türünden nesneyi yalnızca boyut belirterek yaratabiliriz. Daha sonra bu nesneyi sanki bir NumPy 
    dizisiymiş gibi onu kullanabiliriz. Seyrek matrisi normal bir NumPy dizisine dönüştürmek için sınıfın todense ya da toarray 
    metotları kullanılmaktadır. Örneğin:

    from scipy.sparse import dok_matrix

    dok = dok_matrix((10, 10), dtype='int32')
    dok[1, 3] = 10
    dok[3, 5] = 20

    a = dok.todense()
    print(dok)
    print('-' * 20)
    print(a)

    dok_matrix sınıfının minimum, maximum, sum, mean gibi birtakım faydalı metotları bulunmaktadır. nonzero metodu sıfır 
    dışındaki elemanların indekslerini vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import dok_matrix

dok = dok_matrix((1000, 1000), dtype='int32')

dok[3, 6] = 100
dok[30, 7] = 200

print(dok[3, 6])
print(dok[30, 60])

print(dok)

a = dok.todense()
print(a)
print(dok)

#----------------------------------------------------------------------------------------------------------------------------
    Bir dok_matrix nesnesi Python listelerinden ya da NumPy dizilerinden de oluşturulabilir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import dok_matrix

dok = dok_matrix([[1, 0, 0], [1, 0, 0], [0, 0, 1]], dtype='int32')

print(dok)

#----------------------------------------------------------------------------------------------------------------------------
    Bir seyrek matris nesnesi ile biz NumPy dizileri üzerinde yaptığımız işlemlerin benzerlerini yapabiliriz. Örneğin bir 
    seyrek matrisi dilimleyebiliriz. Bu durumda yine bir seyrek matris elde ederiz. dok_matrix sınıfının keys metodu yalnızca 
    anahtarları, values metodu ise yalnızca değerleri vermektedir. 

    Biz seyrek matrislerin karşılıklı elemanları üzerinde işlemler yapabiliriz. Ancak her türlü işlem değişik veri yapılarına 
    sahip seyrek matrislerde aynı verimlilikte yapılamamaktadır. Örneğin iki dok_matrix nesnesini toplayabiliriz ya da 
    çarpabiliriz. Ancak bu işlemler yavaş olma eğilimindedir. Örneğin:

    from scipy.sparse import dok_matrix

    dok1 = dok_matrix((5, 5), dtype='int32')
    dok1[1, 2] = 10
    dok1[0, 1] = 20

    dok2 = dok_matrix((5, 5), dtype='int32')
    dok2[3, 2] = 10
    dok2[4, 1] = 20
    dok2[1, 2] = 20

    result = dok1 + dok2
    print(result)

    result = dok1 * dok2
    print(result)
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import dok_matrix
import numpy as np

a = np.random.randint(0, 2, (10, 10))
b = np.random.randint(0, 2, (10, 10))

dok1 = dok_matrix(a, dtype='float32')
dok2 = dok_matrix(b, dtype='float32')

dok3 = dok1 + dok2
print(dok3)

dok3 = dok1 * dok2
print(dok3)

#----------------------------------------------------------------------------------------------------------------------------
    Diğer bir seyrek matris veri yapısı da "LIL (List of Lists)" denilen veri yapısıdır. Bu veri yapısında matrisin satır satır 
    0 olmayan elemanları ayrı listelerde tutulur. Başka bir listede de bu sıfır olmayan elemanların sütunlarının indeksi 
    tutulmaktadır. LIL matrisler SciPy kütüphanesinde scipy.sparse modülündeki lil_matrix sınıfyla temsil edilmektedir. Bu sınıfın 
    genel kullanımı dok_matrix sınıfında olduğu gibidir.  Sınıfın data ve rows örnek öznitelikleri bize bu bilgileri vermektedir. 
    Örneğin aşağıdaki gibi bir matrisi lil_matrix yapmış olalım:

    [[ 0  0 10 20  0]
     [15  0  0  0 40]
     [12  0 51  0 16]
     [42  0 18  0 16]
     [ 0  0  0  0  0]]

    Buradaki data listesi şöyle olacaktır:

    array([list([10, 20]), list([15, 40]), list([12, 51, 16]), list([42, 18, 16]), list([])], dtype=object)

    rows lsistesi de şöyle olacaktır:

    array([list([2, 3]), list([0, 4]), list([0, 2, 4]), list([0, 2, 4]), list([])], dtype=object)

    LIL matrisler de artimetik işlemlerde yavaştır. Dilimleme işlemleri de bu matrislerde nispeten yavaş yapılmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import lil_matrix
import numpy as np

lil1 = lil_matrix((100, 100), dtype='float32')

lil1[30, 32] = 10
lil1[35, 2] = 20
lil1[38, 27] = 30
lil1[36, 42] = 40
lil1[55, 21] = 50

print(lil1)

lil2 = lil_matrix(np.random.randint(0, 2, (100, 100)))

lil3 = lil1 + lil2
print(lil3)

#----------------------------------------------------------------------------------------------------------------------------
    Aslında uygulamada DOK ve LIL matrisler seyrek kullanılmaktadır. Daha çok CSR ve CSC veri yapıları tercih edilmektedir. 
    CSR (Compressed Sparse Row), ve CSC (Compressed Sparse Column) matrisleri genel veri yapısı olarak birbirlerine çok 
    benzemektedir. Bunlar adeta birbirlerinin tersi durumundadır. 

    Bu veri yapıları seyrek matrislerin karşılıklı elemanlarının işleme sokulması durumunda DOK ve LIL veri yapılarına göre 
    daha avantajlıdır. CSR satır dilimlemesini CSC ise sütun dilimlemesi hızlı yapabilmektedir. Ancak bu matrislerde seyrek bir 
    matrisin 0 olmayan bir elemanına atama yapmak nispeten yavaş bir işlemdir. 

    CSR veri yapısı da SciPy kütüphanesinde scipy.sparse modülünde csr_matrix sınıfıyla temsil edilmektedir. CSR matrislerde 
    sıfırdan farklı elemanlar üç dizi (liste) halinde tutulmaktadır: data, indices ve indptr. Bu diziler sınıfın aynı isimli örnek 
    özniteliklerinden elde edilebilmektedir. data dizisi sıfır olmayan elemanların tutulduğu tek boyutlu dizidir. indices dizisi 
    data dizisindeki elemanların kendi satırlarının hangi sütunlarında bulunduğunu belirtmektedir. indptr dizisi ise sıfır olmayan 
    elemanların hangi satırlarda olduğuna ilişkin ilk ve son indeks (ilk indeks dahil, son indeks dahil değil) değerlerinden 
    oluşmaktadır. indptr dizisi hep yan yana iki eleman olarak değerlendirilmelidir. Soldaki eleman ilk indeksi, sağdaki eleman 
    ise son indeksi belirtir. 
    
    Örneğin:

    0, 0, 9, 0, 5
    8, 0, 3, 0, 7
    0, 0, 0, 0, 0
    0, 0, 5, 0, 9
    0, 0, 0, 0, 0

    Burada söz konusu üç dizi şöyledir:

    data: [9, 5, 8, 3, 7, 5, 9]
    indices: [2, 4, 0, 2, 4, 2, 4]
    indptr: [0, 2, 5, 5, 7, 7]

    csr_matrix sınıfının genel kullanımı diğer seyrek matris sınıflarındaki gibidir. Ancak CSR ce CSC matrislerde sıfır olan bir 
    elemana atama yapmak yavaş bir işlemdir. Çünkü bu işlemler yukarıda belirtilen üç dizide kaydırmalara yol açmaktadır. Bu tür 
    durumlarda DOK ya da LIL matrisler daha hızlı işleme yol açarlar. Bu nedenle bu matrisler kullanılırken sıfır olmayan bir 
    elemana atama yapıldığında bir uyarı mesajıyla karşılaşabilirsiniz. O halde CSR ve CSC matrisleri işin başında oluşturulmalı 
    ve sonra da onların elemanları bir daha değiştirilmemelidir.

    CSR matrislerinde satırsal, CSC matrislerinde sütunsal dilimlemeler hızlıdır. Aynı zamanda bu iki matrisin karşılıklı 
    elemanları üzerinde hızlı işlemler yapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import csr_matrix
import numpy as np

a = np.zeros((100, 100), dtype='float32')

for _ in range(100):
    row = np.random.randint(0, 100, 1)
    col = np.random.randint(0, 100, 1)
    a[row, col] = np.random.randint(0, 100, 1)
    
csr = csr_matrix(a, dtype='float32')

print(csr)

csr2 = csr[2:4, :]
print(csr2)

#----------------------------------------------------------------------------------------------------------------------------
                                            51. Ders - 20/07/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    csr matrislerin data, indices ve indptr örnek öznitelikleri yukarıda açıklanan matris bilgilerini bize vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import csr_matrix

a = [[0, 0, 9, 0, 5], [8, 0, 3, 0, 7], [0, 0, 0, 0, 0], [0, 0, 5, 0, 9], [0, 0, 0, 0, 0]]
csr = csr_matrix(a)

print(csr.todense(), end='\n\n')
print(f'data: {csr.data}')              # data: [9 5 8 3 7 5 9]
print(f'indices: {csr.indices}')        # indices: [2 4 0 2 4 2 4]
print(f'indptr: {csr.indptr}')          # indptr: [0 2 5 5 7 7]

#----------------------------------------------------------------------------------------------------------------------------
    CSC formatı aslında CSR formatına çok benzerdir. CSR formatı adeta CSC formatının sütunsal biçimidir. Yani iki format 
    arasındaki tek fark CSR formatında satır indeksleri tutulurken, CSC formatında sütun indekslerinin tutulmasıdır. Yapılan 
    işlemlerin hepsi satır-sütun temelinde terstir. Örneğin:

    0, 0, 9, 0, 5
    8, 0, 3, 0, 7
    0, 0, 0, 0, 0
    0, 0, 5, 0, 9
    0, 0, 0, 0, 0

    data: [8, 9, 3, 5, 5, 7, 9]
    indices: [1, 0, 1, 3, 0, 1, 3] 
    indptr: [0, 1, 1, 4, 4, 7]
#----------------------------------------------------------------------------------------------------------------------------

from scipy.sparse import csc_matrix

a = [[0, 0, 9, 0, 5], [8, 0, 3, 0, 7], [0, 0, 0, 0, 0], [0, 0, 5, 0, 9], [0, 0, 0, 0, 0]]
csc = csc_matrix(a)

print(csc.todense(), end='\n\n')
print(f'data: {csc.data}')              # data: [8 9 3 5 5 7 9]
print(f'indices: {csc.indices}')        # indices: [1 0 1 3 0 1 3]
print(f'indptr: {csc.indptr}')          # indptr: [0 1 1 4 4 7]

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıdaki seyrek matris sınıflarının her birinde diğer seyrek matris sınıflarına dönüştürme yapan toxxx biçiminde metotlar 
    vardır. Yani örneğin elimizde bir csr_matrix nesnesi varsa biz bu sınıfın tolil metoduyla bunu bir lil_matris nesnesine 
    dönüştürebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.sparse import csc_matrix

a = np.array([[0, 0, 10, 20, 0], [15, 0, 0, 0, 40], [12, 0, 51, 0, 16], [42, 0, 18, 0, 16], [0, 0, 0, 0, 0]])
print(a)
csc = csc_matrix(a, dtype='int32')

print(csc)
print(csc.todense())

csr = csc.tocsr()
print(csr)

dok = csr.todok()
print(dok)

lil = dok.tolil()
print(lil)

#----------------------------------------------------------------------------------------------------------------------------
    Seyrek bir matris train_test_split fonksiyonuyla ayrıştırılabilir. Çünkü zaten train_test_split fonksiyonu dilimleme yoluyla 
    işlemlerini yapmaktadır. Ancak seyrek matrislere len fonksiyonu uygulanamaz. Fakat seyrek matrislerin boyutları yine shape 
    örnek özniteliği ile elde edilebilir. train_test_split fonksiyonu seyrek matrisi de karıştırabilmektedir. 

    Aşağıda bir seyrek matris oluşturulup train_test_split fonksiyonu ile bu matris iki kısma ayrılmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.sparse import csr_matrix
from sklearn.model_selection import train_test_split

dense = np.zeros((10, 5))

for i in range(len(dense)):
    rcols = np.random.randint(0, 5, 2)
    dense[i, rcols] = np.random.randint(0, 100, 2)
    
sparse_dataset_x = csr_matrix(dense)
dataset_y = np.random.randint(0, 2, 10)

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(sparse_dataset_x, dataset_y, test_size=0.2)

print(training_dataset_x)
print(training_dataset_y)

print(test_dataset_x)
print(test_dataset_y)

#----------------------------------------------------------------------------------------------------------------------------
    Seyrek matrislerin birbirlerine göre avantaj ve dezavantajları şöyle özetlenebilir:

    - DOK matriste elemanlara okuma ya da yazma amaçlı erişim hızlı bir biçimde gerçekleştirilmektedir. Ancak DOK matrisler 
    matris işlemlerinde etkin değildir. DOK matrisler dilimleme de de etkin değildir. 

    - LIL matrisler de okuma amaçlı eleman erişimlerinde ve satırsal dilimlemelerde hızlıdırlar. Ancak sütunsal dilimlemelerde 
    ve matris işlemlerinde yavaştırlar. 0 olan elemanlara yazma amaçlı erişimlerde çok hızlı olmasalar da yavaş değillerdir. 
    Bu matrislerin matris işlemleri için CSR ve CSC formatlarına dönüştürülmesi uygundur ve bu dönüştürme hızlıdır. 

    - CSR matrisler satırsal dilimlemelerde CSC matrisler ise sütunsal dilimlemelerde hızlıdırlar. Ancak CSR sütünsal dilimlemelerde,
    CSC de satırsal dilimlemelerde yavaştır. Her iki matris de matris işlemlerinde hızlıdır. Bu matrislerde elemanların değerlerini 
    değiştirmek (özellikle 0 olan elemanların) yavaştır.     

    O halde biz eğer eleman değerleri değiştirilmeyecekse CSR ya da CSC matris kullanabiliriz. Ancak eleman değerleri 
    değiştirilecekse önce işlemlemlerimize DOK ya da LIL matrisle başlayıp değişikler yapıldıktan sonra matris işlemlerine
    başlamadan önce matrisimizi CSR ya da SCS formatına dönüştürebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında biz daha önce bazı konularda seyrek matris kavramıyla zaten karşılaşmıştık. Örneğin scikit-learn içerisindeki 
    OneHotEncoder sınıfının sparse_output parametresi False geçilmezse bu sınıf bize transform işleminde SCiPy'ın CSR formatında 
    seyrek matrisini vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

from sklearn.preprocessing import OneHotEncoder
import numpy as np

a = np.array(['Mavi', 'Yeşil', 'Kırmızı', 'Mavi', 'Kırmızı', 'Mavi', 'Yeşil'])

ohe = OneHotEncoder()
result = ohe.fit_transform(a.reshape(-1, 1))
print(result)
print(type(result))
print(result.todense())

#----------------------------------------------------------------------------------------------------------------------------
    Benzer biçimde scikit-learn kütüphanesindeki CountVectorizer sınıfı da yine bize SCiPy'ın CSR formatında seyrek matrisini 
    vermektedir.
#----------------------------------------------------------------------------------------------------------------------------

from sklearn.feature_extraction.text import CountVectorizer

texts = ['this film is very very good', 'I hate this film', 'It is good', 'I don\'t like it']

cv = CountVectorizer()
cv.fit(texts)
result = cv.transform(texts)

print(result)
print(result.todense())

#----------------------------------------------------------------------------------------------------------------------------
    Seyrek matrisleri karıştırmak için NumPy'da herhangi bir fonksiyon yoktur. (SciPy'ın NumPy'ı kullandığını, NumPy'ın SciPy'ı
    kullanmadığını anımsayınız.) Ancak scikit-learn kütüphanesinde utils modülü içerisindeki shuffle fonksiyonu SciPy'ın seyrek 
    matrislerini karıştırabilmektedir. Tabii bu fonksiyon karıştırma sonucunda karıştırılmış matrisi bize yine seyrek matris 
    olarak vermektedir. Buradaki shuffle fonksiyonu birden fazla girdi alabilmektedir. Karıştırmayı paralel biçimde yaptığı 
    için karıştırılmış x matrisiyle y matrisinin karşılıklı elemanları yine aynı olmaktadır. Örneğin:

    result_x, result_y = shuffle(x, y)

    Burada karıştırma sonrasında x'in aynı satırları yine y'nin aynı satırlarına karşı gelecektir.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.sparse import csr_matrix
from sklearn.utils import shuffle

a = np.array([[0, 0, 10, 20, 0], [15, 0, 0, 0, 40], [12, 0, 51, 0, 16], [42, 0, 18, 0, 16], [0, 0, 0, 0, 0]])

csr = csr_matrix(a)
y = [10, 20, 30, 40, 50]
result_x, result_y = shuffle(csr, y)

print(csr.todense())
print(y)
print('-' * 15)
print(result_x.todense())
print(result_y)

#----------------------------------------------------------------------------------------------------------------------------
    Seyrek matris sınıfları büyük kısmı sıfır olan matrislerin bellekte daha az yer kaplamasını sağlamak için kullanılmaktadır. 
    Aslında seyrek matrislerle işlemler normal (dense) matrislerle işlemlerden her zaman daha yavaştır. Peki bizim elimizde 
    bir seyrek matris varsa biz bunu Keras'ta nasıl kullanabiliriz? Örneğin CountVectorizer işleminden büyük bir seyrek matris 
    elde etmiş olalım ve Keras'ın TextVectorization katmanını kullanmıyor olalım. Bu durumda bu seyrek matrisi sinir ağlarında 
    nasıl kullanabiliriz? 

    Seyrek matrislerin Keras sinir ağlarında kullanılmasının temelde iki yöntemi vardır:

    1) Parçalı eğitim uygulanırken seyrek matrisin ilgili batch'lik kısımları o anda yoğun matrise dönüştürüp verilebilir.
    2) TensorFlow kütüphanesinin Input katmanına sonradan bir sparse parametresi eklenmiştir. Bu parametre True yapılarak artık 
    doğrudan dataset_x değerleri SciPy sparse matrisi olarak verilebilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi tüm verilerin vektörizasyonunu CountVectorizer sınıfı ile tek hamlede yapıp bir seyrek matris elde ettikten sonra 
    parçalı eğitim ile seyrek matrisin ilgili batch'lik bölümünü yoğun hale getirerek eğitim, sınama, test ve kestirim işlemlerini
    yapalım. Aşağıdaki örnekte IMDB veri kümesi tek hamlede okunup CountVectorizer sınıfı ile seyrek biçimde vektörize edilmiştir. 
    Sonra DataGenerator sınıfı ile bu seyrek matris batch batch yoğun matrise dönüşürülerek işlemler ypılmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

import math
import pandas as pd
from tensorflow.keras.utils import PyDataset
import tensorflow as tf
from sklearn.utils import shuffle

EPOCHS = 5
BATCH_SIZE = 32

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text  import CountVectorizer

cv = CountVectorizer(dtype='uint8')

dataset_x = cv.fit_transform(df['review'])
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

from sklearn.model_selection import train_test_split

temp_dataset_x, test_dataset_x, temp_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

training_dataset_x, validation_dataset_x, training_dataset_y, validation_dataset_y = train_test_split(temp_dataset_x, temp_dataset_y, test_size=0.2)

training_steps = math.ceil(training_dataset_x.shape[0] / BATCH_SIZE)
validation_steps = math.ceil(validation_dataset_x.shape[0] / BATCH_SIZE)
test_steps = math.ceil(test_dataset_x.shape[0] / BATCH_SIZE)

class DataGenerator(PyDataset):
    def __init__(self, sparse_x, y, steps, batch_size, predict=False):
        super().__init__()
        self.sparse_x = sparse_x
        self.y = y
        self.steps = steps
        self.batch_size = batch_size
        self.predict = predict
        
    def __len__(self):
        return self.steps
    
    def __getitem__(self, batch_no):      
        x = self.sparse_x[batch_no * self.batch_size: batch_no * self.batch_size + self.batch_size]
        if not self.predict:
            y = self.y[batch_no * self.batch_size: batch_no * self.batch_size + self.batch_size]
            return tf.convert_to_tensor(x.todense()), tf.convert_to_tensor(y)
        else:
            return tf.convert_to_tensor(x.todense()),
       
    def on_epoch_end(self):
        if not self.predict:
            self.sparse_x, self.y = shuffle(self.sparse_x, self.y)
          
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1], )))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(DataGenerator(training_dataset_x, training_dataset_y, training_steps, BATCH_SIZE), 
                 validation_data = DataGenerator(training_dataset_x, training_dataset_y, validation_steps, BATCH_SIZE), epochs=EPOCHS)
    
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(DataGenerator(test_dataset_x, test_dataset_y, test_steps, BATCH_SIZE))

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

df_predict = pd.read_csv('predict-imdb.csv')
predict_dataset_x = cv.transform(df_predict['review'])
predict_steps = math.ceil(predict_dataset_x.shape[0] / BATCH_SIZE)

predict_result = model.predict(DataGenerator(predict_dataset_x, None, predict_steps, BATCH_SIZE, True))

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')
#----------------------------------------------------------------------------------------------------------------------------

import math
import pandas as pd
from tensorflow.keras.utils import PyDataset
import tensorflow as tf
from sklearn.utils import shuffle

EPOCHS = 5
BATCH_SIZE = 32

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text  import CountVectorizer

cv = CountVectorizer(dtype='uint8')

dataset_x = cv.fit_transform(df['review'])
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

from sklearn.model_selection import train_test_split

temp_dataset_x, test_dataset_x, temp_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

training_dataset_x, validation_dataset_x, training_dataset_y, validation_dataset_y = train_test_split(temp_dataset_x, temp_dataset_y, test_size=0.2)

training_steps = math.ceil(training_dataset_x.shape[0] / BATCH_SIZE)
validation_steps = math.ceil(validation_dataset_x.shape[0] / BATCH_SIZE)
test_steps = math.ceil(test_dataset_x.shape[0] / BATCH_SIZE)

class DataGenerator(PyDataset):
    def __init__(self, sparse_x, y, steps, batch_size, predict=False):
        super().__init__()
        self.sparse_x = sparse_x
        self.y = y
        self.steps = steps
        self.batch_size = batch_size
        self.predict = predict
        
    def __len__(self):
        return self.steps
    
    def __getitem__(self, batch_no):      
        x = self.sparse_x[batch_no * self.batch_size: batch_no * self.batch_size + self.batch_size]
        if not self.predict:
            y = self.y[batch_no * self.batch_size: batch_no * self.batch_size + self.batch_size]
            return tf.convert_to_tensor(x.todense()), tf.convert_to_tensor(y)
        else:
            return tf.convert_to_tensor(x.todense()),
       
    def on_epoch_end(self):
        if not self.predict:
            self.sparse_x, self.y = shuffle(self.sparse_x, self.y)
          
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1], )))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(DataGenerator(training_dataset_x, training_dataset_y, training_steps, BATCH_SIZE), 
                 validation_data = DataGenerator(training_dataset_x, training_dataset_y, validation_steps, BATCH_SIZE), epochs=EPOCHS)
    
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(DataGenerator(test_dataset_x, test_dataset_y, test_steps, BATCH_SIZE))

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

df_predict = pd.read_csv('predict-imdb.csv')
predict_dataset_x = cv.transform(df_predict['review'])
predict_steps = math.ceil(predict_dataset_x.shape[0] / BATCH_SIZE)

predict_result = model.predict(DataGenerator(predict_dataset_x, None, predict_steps, BATCH_SIZE, True))

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi daha sonraları TensorFlow kütüphanesindeki Input katmanına sparse parametresi de eklenmiştir.
    Böylece biz artık doğrudan Input katmanına seyrek matrisin kendisini verebilmekteyiz. Geri kalan işlemleri Keras kendisi 
    halletmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

EPOCHS = 5
BATCH_SIZE = 32

df = pd.read_csv('IMDB Dataset.csv').iloc[:1000, :]

from sklearn.feature_extraction.text  import CountVectorizer

cv = CountVectorizer(dtype='uint8')

dataset_x = cv.fit_transform(df['review'])
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8') 

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

model = Sequential(name='IMDB')

model.add(Input((training_dataset_x.shape[1],), sparse=True))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])
hist = model.fit(training_dataset_x, training_dataset_y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)
    
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
# prediction

df_predict = pd.read_csv('predict-imdb.csv')
predict_dataset_x = cv.transform(df_predict['review'])

predict_result = model.predict(predict_dataset_x)

for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
                                            52. Ders - 21/07/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Resimlerle ilgili işlemler yapabilmek için öncelikle resimlerin sayısal biçimde nasıl oluşturulduğu ve görüntülendiği
    hakkında temel bazı bilgilerin edinilmiş olması gerekir. Bilgisayar ekranlarında tüm görüntü aslında pixel'lerden oluşmaktadır. 
    Pixel ("picture element" sözcüklerinden uydurulmuştur) ekranda görüntülenebilen en küçük noktasal öğedir. Yani ekrandaki tüm
    görüntü aslında pixel'lerin bir araya getirilmesiyle oluşturulmaktadır. Bilgisayar ekranını bir pixel matrisi olarak düşünebiliriz.
    Örneğin ekranımızın çözünürlüğü 1920x1080 ise (ilk yazılan sütun ikinci yazılan satır) bu ekranımızın her satırında 1920 tane
    pixel olduğu ve toplam 1080 tane satırın bulunduğu anlamına gelmektedir. (Yani bu durumda ekranımızda 1920 * 1080 = 2073600 
    pixel vardır.) Ekrandaki her pixel programlama yoluyla diğerlerinden bağımsız bir biçimde renklendirilebilmektedir. Eskiden 
    teknoloji zayıfken bilgisayar ekranlarının çözünürlükleri de çok düşüktü. Çünkü o zamanlar pixel'leri oluşturan ve renklendiren 
    grafik kartları gelişkin değildi. Günümüzde grafik kartları çok gelişmiştir dolayısıyla çözünürlükler de geçmişe göre çok 
    daha yüksektir. 
    
    Bugün kullandığımız bilgisayarlarda her pixel "kırmızı (red), yeşil (green) ve mavinin (blue)" bir byte'lık tonal birleşimleriyle 
    oluşturulmaktadır. Yani belli bir renk kırmızının 0-255 arasındaki bir değeri, yeşilin 0-255 arasındaki bir değeri ve mavinin 
    0-255 arasındaki bir değeri ile oluşturulmaktadır. Örneğin R=255, G=0, B=0 ise bu tam kırmızı olan renktir. Kırmızı ile
    yeşil ışınsal olarak bir araya getirilirse sarı renk elde edilmektedir. Bu biçimde bütün renkler aslında bu üç ana rengin 
    tonal birleşimleriyle elde edilmektedir. (Burada bir uyarıda bulunmak istiyoruz: Işıkların girişimiyle elde edilen renklerle
    boyaların karıştırılması ile elde edilen renkler aynı değildir. Çünkü bunlar farklı fiziksel etkileşimlere girmektedir.) Yani 
    örneğin Biz kırmızı ışık ile yeşil ışığı giriştirirsek sarı ışık elde ederiz. Ancak kırmızı boya ile yeşil boyayı karıştırırsak
    elde ettiğimiz renk sarı olmaz.) Peki böyle bir sistemde bir pixel kaç farklı renge boyaaabilir? R, G ve B için toplam 256 
    farklı değer olduğuna göre elde edilebilecek toplam renk sayısı 256 * 256 * 256'dır. Bunu 2 ** 8 * 2 ** 8 * 2 ** 8 biçiminde 
    de ifade edebiliriz. 2 ** 24 değeri yaklaşık 16 milyon (16777216) civarındadır. Her ne kadar doğada bu 16 milyon rengin dışında
    renkler borsa da insanın donanımsal özelliği nedeniyle algılayabildiği renklerin de bir sınırı vardır. Yani örneğin pixel 
    renklerinin milyar düzeyine çıkarılması bizim daha iyi bir görüntü algılamamıza yol açmayacaktır. 

    Ekranda iki pixel arasında bir doğru çizdiğimizde bu doğru kırıklı gibi görünebilir. Bunun nedeni doğrunun sınırlı sayıda 
    pixel ile oluşturulmasıdır. Kartezyen koordinat sisteminde sonsuz tane nokta vardır. Yani çözünürlük sonsuzdur. Ancak ekran
    koordinat sisteminde sınırlı sayıda pixel vardır. Bu nedenle ekranda doğru gibi, daire gibi en temel geometrik şekiller bile
    kırıklı gözükebilmektedir. Şüphesiz çözünürlük artırıldığı zaman bu kırıklı görünüm azalacaktır. Ancak çözünürlüğün çok 
    artırılması da başka dezavantajlar doğurabilmektedir. Örneğin notebook'larda ve mobil cihazlarda CPU dışındaki en önemli 
    güç tüketimi LCD ekranda oluşmaktadır. Çözünürlük artırıldıkça ekran kartları ve LCD birimleri daha fazla güç harcar hale 
    gelmektedir. 
    
    Peki ekran boyutunu sabit tutup çözünürlüğü küçültürsek ne olur? Bu durumda pixel'ler büyür ve görüntü daha büyür ancak
    netlik azalır. Çözünürlüğü sabit tutup ekranımızı büyütürsek de benzer durum oluşacaktır. O halde göz için belli bir büyüklükte 
    ekran ve çözünürlük daha önemlidir. İşte buna DPI (Dot Per Inch) denilmektedir. DPI bir inch'te kaç pixel olduğunu belirtmektedir. 
    Çözünürlük sabit tutulup ekran büyütülürse DPI düşer, ekran küçültülürse DPI yükselir. Bugün kullandığımız akıllı cep 
    telefonlarında DPI oldukça yüksektir. Buna "retinal çözünürlük" de denilmektedir. Gözümüzün de donanımsal (fizyolojik) bir çöznürlüğü vardır. 
    Belli bir DPI'dan daha yüksek çözünürlük sağlamanın bizim için bir faydası kalmamaktadır. 

    Bilgisayar bilimlerinin sınırlı sayıda pixelle geometrik şekillerin ve resimlerin nasıl oluşturulduğunu inceleyen ve bunlar 
    üzerinde işlemlerin nasıl yapılabileceğini araştıran bölümüne İngilizce "computer graphics" denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bilgisayar ortamındaki en doğal resim formatları "bitmap (ya da raster)" formatlardır. Örneğin BMP, GIF, TIF, PNG gibi 
    formatlar bitmap formatlardır. Bitmap dosya formatlarında resmin her bir pixel'inin rengi dosya içerisinde saklanır. 
    Dolayısıyla resmi görüntüleyecek kişinin tek yapacağı şey o pixel'leri o renklerde görüntülemektir. Ancak bitmap formatlar 
    çok yer kaplama eğilimindedir. Örneğin 100x100 pixellik bir resim kabaca 100 * 100 * 3 byte yer kaplar. Bu yüzden resimler 
    üzerinde kayıplı sıkıştırma yöntemleri oluşturulmuştur. Örneğin JPEG formatı aslında bitmap format üzerinde bir çeşit kayıplı 
    sıkıştırmanın uygulandığı bir formattır. Yani bir BMP dosyasını JPG dosyasına dönüştürdüğümüzde resim çok bozulmaz. Ama 
    aslında biraz bozulur. Sonra onu yeniden BMP dosyasına dönüştürdüğümüzde orijinal resmi elde edemeyiz. Ancak JPEG gibi 
    farmatlar resimleri çok az bozup çok iyi sıkıştırabilmektedir. O halde aslında doğal formatlar BMP formatı ve benzerleridir. 
    JPEG gibi formatlar doğal formatlar değildir. Sıkıştırılmış formatlardır. Bitmap formatlardaki resimler orijinal boyutuyla
    görüntülenmeidir. Çünkü bu resimlerin büyütülüp küçültülmesinde (scale edilmesinde) görüntü bozulabilmektedir. 

    Bugünkü bilgisayar sistemlerinde arka planda bir görüntü varken onun önüne bir görüntü getrilip arkadaki görüntü adeta bir 
    tül perdeden görünüyormuş gibi bir etki yaratılabilmektedir. Bu etki aslında ön plandaki pixel ile arka plandaki pixel'in 
    bit operasyonuna sokulmasıyla sağlanmaktadır. Bu operasyon bugünkü grafik kartlarında grafik kartının kendisi tarafından 
    yapılmaktadır. Ancak bu saydamlılık (transparency) özelliğinin de derecesi söz konusu olmaktadır. Programcı bu saydamlılık 
    derecesini grafik kartına RGB değerleriyle birlikte birlikte verebilmektedir. RGB değerlerinin yanı sıra saydamlılılık 
    belirten bu değere "alpha channel" denilmetedir. 
    
    Bazı bitmap formatlar pixel renklerinin yanı sıra her pixel için saydamlılık bilgilerini de tutmaktadır. Böylece dikdörtgensel 
    resim başka bir resmin üztüne basıldığında ön plan resmin bazı kısımlarının görüntülenmesi engellenebilmektedir. Örneğin 
    PNG formatı bu biçimde transparanlık bilgisi de tutulmaktadır. Ancak örneğin BMP formatında böyle bir transparanlık 
    bilgisi tutulmamaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Siyah beyaz demek her pixel'in yalnızca siyah ya da beyaz olabildiği resim demektir. Böyle bir resimde bir pixel bir bit 
    ile ifade edilebilir. Ancak siyah beyaz resimlerde resim bir siluet gibi gözükmektedir. Gri tonlamalı (gray scale) resimlerde
    ise her pixel siyahın (grinin) bir tonu biçiminde renkledirilmektedir. Eski siyah-beyaz fotoğraflar aslında gri tonlamalı 
    fotoğraflardır. Gri tonlamalı resimlerde aslında her pixelin RGB renkleri aynıdır. Yani R = G = B biçimindedir. Gri tonlamalı 
    resimlerde grinin 256 tonu görüntülenebilmektedir. Dolayısıyla gri tonlamalı bir resimde her pixel bir byte ile ifade 
    edilebilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Matplotlib kütüphanesinde bir resmi resim dosyasından (JEPG, BMP, PNG vs.) okuyarak onun pixel'lerini elde edip bize bir 
    NumPy dizisi biçiminde veren imread isimli bir fonksiyon vardır. Biz bir resim dosyasını imread ile okuduğumuzda artık o 
    resmin saf pixel değerlerini elde ederiz. Örneğin:

    import matplotlib.pyplot as plt

    image_data = plt.imread('AbbeyRoad.jpg')

    Bu örnekte biz bir NumPy dizisi elde etmiş olduk. Söz konusu resim renkli ir resim olduğu için elde edilen dizinin de shape 
    demeti (100, 1500, 3) biçimindedir. Yani söz konusu resim 1000x1500 pixel'lik bir resimdir, ancak resmin her pixel'i RGB 
    değerlerinden oluşmaktadır. Biz buarada image_data[i, j] biçiminde matrisin bir elemanına erişmek istersek aslında resmin 
    i'inci satır j'inci sütunundaki pixel'in RGB renklerini bir NumPy dizisi olarak elde ederiz. 

    Matplotlib kütüphanesinin imshow isimli fonksiyonu pixel bilgilerini alarak resmi görüntüler. Tabii imshow resmi orijinal 
    boyutuyla görüntülememektedir. imshow resmi ölçeklendirip figür büyüklüğünde görüntülemektedir. Örneğin:

    image_data = plt.imread('AbbeyRoad.jpg')
    plt.imshow(image_data)
    plt.show()

    Matplotlib bir resim üzerinde ondan parça almak, onu büyütmek, küçültmek gibi işlemler için uygun değildir. Bu tür işlemler 
    için Python programcıları başka kütüphanelerden faydalamaktadır. Örneğin bu bağlamda en yaygın kullanılan kütüphane "Python 
    Image Library (PIL ya da Pillow diye kısaltılmaktadır)" isimli kütüphanedir. Matplotlib yalnızca resim dosyalarını okuyup 
    bize pixel'lerini verir ve pixel'leri verilmiş resmi görüntüler. 
#----------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt

image_data = plt.imread('AbbeyRoad.jpg')
plt.imshow(image_data)
plt.show()

print(image_data.shape)

#----------------------------------------------------------------------------------------------------------------------------
    Örneğin biz bir resmi ters çevirmek için resmin tüm satırlarını ters yüz etmemiz gerekir. Bu işlemi aslında NumPy'ın flip 
    fonksiyonu pratik bir biçimde yapmaktadır. Bir resmin pixel'leri üzerinde aşağı seviyeli çalışma yapmak için Matplotlib 
    ve NumPy iyi araçlardır.  Örneğin:

    image_data = plt.imread('AbbeyRoad.jpg')

    plt.imshow(image_data)
    plt.show()
    result_image = np.flip(image_data, axis=1)
    plt.imshow(result_image)
    plt.show()

    Burada pixel verileri yatay eksende döndürülmüştür.
#----------------------------------------------------------------------------------------------------------------------------

image_data = plt.imread('AbbeyRoad.jpg')

plt.imshow(image_data)
plt.show()
result_image = np.flip(image_data, axis=1)
plt.imshow(result_image)
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    NumPy'da rot90 fonksiyonu resmim pixellerini 90 derece döndürmektedir. Fonksiyon resmin pixel verileriyle saat yönününün 
    tersinde kaç defa 90 derece döndürüleceğini (default değeri 1) bizden istemektedir. Örneğin:

    mage_data = plt.imread('AbbeyRoad.jpg')

    plt.imshow(image_data)
    plt.show()
    result_image = np.rot90(image_data, 3)
    plt.imshow(result_image)
    plt.show()

    Burada resim 3 kere 90 derece döndürülmüştür.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Renkli resimleri imread fonksiyonu ile okuduğumuzda "row x col x 3" boyutunda bir matris elde ederiz. Gri tonlamalı resimleri
    aynı fonksiyonla okuduğumuzda ise row x col x 1 boyutunda bir matris elde ederiz. Tabii aslında "row x col" biçiminde iki 
    boyutlu bir matrisin eleman sayısı ile "row x col x 1" biçiminde üç boyutlu bir matrisin eleman sayısı arasında bir farklılık 
    yoktur. Bazen gri tonlamalı resimler "row x col x 1" yerine "row x col" biçiminde de karşımıza çıkabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki renkli bir resmi gri tonlamalı bir resim haline nasıl getirebiliriz? Bunun için en basit yöntem her pixel'in RGB 
    renklerinin ortalamasını almaktır. Bunu basit bir biçimde np.mean fonksiyonunda axis=2 parametresini kullanarak sağlayabiliriz.
    Örneğin:

    import numpy as np
    import matplotlib.pyplot as plt
    image_data = plt.imread('AbbeyRoad.jpg')
    gray_scaled_image_data = np.mean(image_data, axis=2)
#----------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt

image_data = plt.imread('AbbeyRoad.jpg')

import numpy as np

gray_scaled_image_data = np.mean(image_data, axis=2)
plt.imshow(gray_scaled_image_data, cmap='gray')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    RGB bir resmi yukardaki gibi gri tonlamalı hale getirirken açık renklerin etkisi fazlalaşmaktadır. Bu nedenle çoğu kez 
    ağırlıklı ortalama uygulanır. Tipik olarak ağırlıklar R=0.3, G=0.59, B=0.11 biçiminde alınmaktadır. Aşağıda bu ağırlıklarla 
    resim gri tonlamalı hale getirilmiştir. (NumPy'ın mean fonksiyonunda ağırlıklandırma parametresi yoktur. Ancak average 
    isimli fonksiyon ağırlıklı ortalama için kullanılabilmektedir.)
#----------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt

image_data = plt.imread('AbbeyRoad.jpg')

import numpy as np

gray_scaled_image_data = np.average(image_data, axis=2, weights=[0.3, 0.59, 0.11])
plt.imshow(gray_scaled_image_data, cmap='gray')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
                                        53. Ders - 27/07/2024 - Cumartesi -     
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Resim tanıma üzerinde en sık kullanılan popüler bir veri kümelerinden biri MNIST (Modified National Institute of Standards 
    and Technology) denilen veri kümesidir. Bu veri kümesinde her biri 28x28 pixel'den oluşan gri tonlamalı resimler vardır. Bu 
    resimler çeşitli kişilerin 0 ile 9 arasındaki sayıları elle çizmesiyle oluşturulmuştur. Veri kümesi "resmin üzerindeki sayının 
    kestirilmesi" gibi resim tanıma uygulamalarında kullanılmaktadır. Veri kümesinde toplam 60000 tane resim bulunmaktadır. Veri 
    kümesini zip'lenmiş CSV dosyaları biçiminde aşağıdaki bağlantıdan indirebilrsiniz:

    https://www.kaggle.com/oddrationale/mnist-in-csv  

    Buradan minist_train.csv ve mnist_test.csv dosyaları elde edilmektedir. 

    Aşağıdaki örnekte MNIST verileri dosyadan okunmuş ve iki saklı katmanlı bir sinir ağı ile model oluşturulmuştur. Model test 
    edildiğinde %97 civarında bir başarı elde edilmektedir. Daha sonra 28x28'lik kendi oluşturduğumuz bitmap resimlerle kestirim 
    işlemi yapılmıştır. Tabii kestirim işlemi eğitim verileriyle aynı biçimde oluşturulmuş rakamlarla yapılmalıdır. Eğitim 
    verilerinde "anti-aliasing" özelliği bulunmaktadır. Biz de Microsoft Paint ile fırça kullanarak anti-aliasing eşliğinde 
    kestirilecek resimleri oluşturduk. Pixel verileri eğitime sokulmadan önce min-max ölçeklemesi de yapılmıştır. Tabii 
    [0, 255] arasındaki verilerde min-max ölçeklemesi aslında bu pixel verilerinin 255'e bölümüyle oluşturulabilmektedir. 
    Modele çok fazla epoch uygulandığında "overfitting" problemi ortaya çıkmaktadır. Bu nedenle epoch sayısı 20 olarak 
    belirlenmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

training_df = pd.read_csv('mnist_train.csv')
test_df = pd.read_csv('mnist_test.csv')

training_dataset_x = training_df.iloc[:, 1:].to_numpy(dtype='uint8')
training_dataset_y = training_df.iloc[:, 0].to_numpy(dtype='uint8')

test_dataset_x = test_df.iloc[:, 1:].to_numpy(dtype='uint8')
test_dataset_y = test_df.iloc[:, 0].to_numpy(dtype='uint8')

# one hot encoding for y data

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

# minmax scaling

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 13))
for i in range(50):
    plt.subplot(10, 5, i + 1)
    plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = training_dataset_x[i].reshape(28, 28)
    plt.imshow(picture, cmap='gray')
plt.show()

"""
seven_x = training_dataset_x[training_dataset_y == 7]
for i in range(50):
    plt.figure(figsize=(1, 1))
   # plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = seven_x[i].reshape(28, 28)
    plt.imshow(picture, cmap='gray')
    plt.show()
"""

"""
for i in range(50):
    plt.figure(figsize=(1, 1))
    plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = training_dataset_x[i].reshape(28, 28)
    plt.imshow(picture, cmap='gray')
    plt.show()
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential(name='MNIST')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=20, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os
import glob

for path in glob.glob('Predict-Pictures/*.bmp'):
    image = plt.imread(path)
    gray_scaled_image = np.average(image, axis=2, weights=[0.3, 0.59, 0.11]).reshape(1, 28 * 28)
    gray_scaled_image /= 255
    model_result = model.predict(gray_scaled_image, verbose=0)
    predict_result = np.argmax(model_result)
    print(f'Real Number: {os.path.basename(path)[0]}, Prdicted Result: {predict_result}, Path: {path}')
       
#----------------------------------------------------------------------------------------------------------------------------
    Aslında MNIST verileri tensorflow.keras.datasets paketindeki mnist isimli modülde de bulunmaktadır. Modülün load_data 
    fonksiyonu bize pixel verilerini üç boyutlu bir NumPy dizisi olarak vermektedir. Bizim bu verileri reshape ile 784'lük 
    iki boyutlu matrise dönüştürmemiz gerekir. Aşağıda aynı işlemler doğrudan Keras'ın içerisindeki mnist veri kümesi kullanılarak
    yapılmıştırç
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.datasets import mnist

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = mnist.load_data()

training_dataset_x = training_dataset_x.reshape(-1, 28 * 28)
test_dataset_x = test_dataset_x.reshape(-1, 28 * 28)

# one hot encoding for y data

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

# minmax scaling

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 13))
for i in range(50):
    plt.subplot(10, 5, i + 1)
    plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = training_dataset_x[i].reshape(28, 28)
    plt.imshow(picture, cmap='gray')
plt.show()

"""
seven_x = training_dataset_x[training_dataset_y == 7]
for i in range(50):
    plt.figure(figsize=(1, 1))
   # plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = seven_x[i].reshape(28, 28)
    plt.imshow(picture, cmap='gray')
    plt.show()
"""

"""
for i in range(50):
    plt.figure(figsize=(1, 1))
    plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = training_dataset_x[i].reshape(28, 28)
    plt.imshow(picture, cmap='gray')
    plt.show()
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential(name='MNIST')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=20, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os
import glob

for path in glob.glob('Predict-Pictures/*.bmp'):
    image = plt.imread(path)
    gray_scaled_image = np.average(image, axis=2, weights=[0.3, 0.59, 0.11]).reshape(1, 28 * 28)
    gray_scaled_image /= 255
    model_result = model.predict(gray_scaled_image, verbose=0)
    predict_result = np.argmax(model_result)
    print(f'Real Number: {os.path.basename(path)[0]}, Prdicted Result: {predict_result}, Path: {path}')

#----------------------------------------------------------------------------------------------------------------------------
                                        54. Ders - 28/07/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Evrişim (convolution) genel olarak "sayısal işaret işleme (digital signal processing)" faaliyetlerinde kullanılan bir 
    tekniktir. Bir verinin başka bir veriyle girişime  sokulması anlamına gelir. Evrişim en çok görüntü verileri üzerinde 
    uygulanan bir işlemdir. Ancak görüntünün dışında işitsel (audio) ve hareketli görüntüler (video) verileri üzerinde de 
    sıkça uygulanmaktadır. Evrişim işlemleri ile oluşturulan yapay sinir ağlarına "Evrişimsel Sinir Ağları (Convolutional 
    Neural Network)" denilmektedir ve İngilizce CNN biçiminde kısaltılmaktadır. 

    Resimlerde evrişim işlemi pixel'lerin birbirleri ile ilişkili hale gelmesini sağlamaktadır. Evrişim sayesinde pixel'ler 
    bağımsız ve birbirinden kopuk durumdan çıkıp komşu pixellerle ilişkili hale gelir. Aynı zamanda evrişim bir filtreleme 
    etkisi oluşturmaktadır. Görüntü işlemede filtreleme işlemleri evrişimlerle sağlanmaktadır.

    Bir resmin evrişim işlemine sokulması için elimizde bir resim ve bir küçük matrisin olması gerekir. Bu küçük matrise "filtre
    (filter)" ya da "kernel" denilmektedir. Kernel herhangi bir boyutta olabilir. Kare bir matris biçiminde olması gerekmez. 
    Ancak uygulamada NxN'lik kare matrisler kullanılmaktadır ve genellikle buradaki N değeri 3, 5, 7, 9 gibi tek sayı olmaktadır. 
    Evrişim işlemi şöyle yapılmaktadır: Kernel resmin sol üst köşesi ile çakıştırılır. Sonra resmin arkada kalan kısmıyla dot-product 
    işlemine sokulur. (Yani kernel'ın asıl resimle çakıştırıldığı pixel değerleri birbiriyle çarpılıp toplanır.)
    Buradan bir değer elde edilir. Bu değer yeni resmin kernel ile çakıştırılan orta noktasındaki pixel'i olur. Sonra kernel resim 
    üzerinde kaydırılır ve aynı işlem yine yapılır. Kaydırma sağa doğru ve sonra da aşağıya doğru yapılır. Böylece evrişim işlemi 
    sonucunda başka bir resim elde edilmiştir. Örneğin aşağıdaki gibi 5x5'lik gri tonlamalı bir resim söz konusu olsun:

    a11 a12 a13 a14 a15
    a21 a22 a23 a24 a25
    a31 a32 a33 a34 a35
    a41 a42 a43 a44 a45
    a51 a52 a53 a54 a55

    Kullandığımız kernel da aşağıdaki gibi 3x3'lük olsun:

    b11 b12 b13
    b21 b22 b23
    b31 b32 b33

    Bu kernel resmin sol üst köşesi ile çakıştırılıp dot product uygulanırsa şöyle bir değer elde edilir:

    c11 = b11 * a11 + b12 * a12 + b13 * a13 + b21 * a21 + b22 * a22 + b23 * a23 + b31 * a31 + b32 * a32 + b33 * a33

    Şimdi kernel'ı bir sağa kaydırıp aynı işlemi yapalım:

    c12 = b11 * a12 + b12 * a13 + b13 * a14 + b21 * a22 + b22 * a23 + b23 * a24 + b31 * a32 + b32 * a32 + b33 * a34

    Şimdi kernel'ı bir sağa daha kaydıralım:

    c13 = b11 * a13 + b12 * a14 + b13 * a15 + b21 * a23 + b22 * a24 + b23 * a25 + b31 * a33 + b32 * a34 + b33 * a35

    Şimdi kernel'ı aşağı kaydıralım:

    c21 = b11 * a21 + b12 * a22 + b13 * a23 + b21 * a31 + b22 * a32 + b23 * a33 + b31 * a41 + b32 * a42 + b33 * a43

    İşte bu biçimde işlemlere devam edersek aşağıdaki gibi bir C matrisi (resmi) elde ederiz:

    c11 c12 c13
    c21 c22 c23
    c31 c32 c33

    Eğer işlemler yukarıdaki gibi yapılırsa hedef olarak elde edilecek resmin genişlik ve yüksekliği şöyle olur:

    Hedef Resmin Genişliği = Asıl Resmin Genişliği - Kernel Genişliği + 1
    Hedef Resmin Yüksekliği = Asıl Resmin Yüksekliği - Kernel Yüksekliği + 1

    Örneğin ana resim 5X5'lik ve kernel'da 3X3'lik ise evrişim işleminin sonucunda elde edilecek resim 3X3'lük olacaktır. 

    Görüldüğü gibi hedef resim asıl resimden küçük olmaktadır. Eğer biz hedef resmin asıl resimle aynı büyüklükte olmasını 
    istersek asıl resmin soluna, sağına, yukarısına ve aşağısına eklemeler yaparız. Bu eklemelere "padding" denilmektedir. 
    Bu eklemelere İngilizce "padding" denilmektedir. Hedef resmin asıl resimle aynı büyüklükte olması için asıl resme (kernel 
    genişliği ya da yükseliği - 1) kadar padding uygulanmalıdır.

    Toplam Padding Genişliği = Kernel Genişliği - 1
    Toplam Padding Yüksekliği = Kernel Yüksekliği - 1

    Tabii bu toplam pading genişliği ve yüksekliği iki yöne eşit bir biçimde yaydırılmalıdır. Yani başka bir deyişle asıl 
    resim evrişim işlemine sokulmadan önce dört taraftan büyütülmelidir. Örneğin 5x5'lik resme 3x3'lük kernel uygulamak 
    isteyelim:
    
    a11 a12 a13 a14 a15
    a21 a22 a23 a24 a25
    a31 a32 a33 a34 a35
    a41 a42 a43 a44 a45
    a51 a52 a53 a54 a55

    Asl resmin padding'li hali şu biçimde görünecektir:

    pad pad pad pad pad pad pad
    pad a11 a12 a13 a14 a15 pad 
    pad a21 a22 a23 a24 a25 pad 
    pad a31 a32 a33 a34 a35 pad 
    pad a41 a42 a43 a44 a45 pad 
    pad a51 a52 a53 a54 a55 pad 
    pad pad pad pad pad pad pad 

    Peki padding'ler asıl resme dahil olmadığına göre hangi içeriğe sahip olacaktır? İşte tipik olarak iki yöntem kullanılmaktadır. 
    Birincisi padding'leri 0 almak ikincisi ise ilk ve son n tane satır ya da sütunu tekrarlamaktır. Genellikle bu ikinci yöntem
    tercih edilmektedir.

    Evrişim işleminde kaydırma birer birer yapılmayabilir. Örneğin ikişer ikişer, üçer üçer yapılabilir. Bu kaydırmaya "stride" 
    denilmektedir. stride değeri artırılırsa hedef resim padding de yapılmadıysa daha fazla küçülecektir. Hedef resmi küçültmek 
    için stride değeri artırılabilmektedir. 
    
    Evrişim işlemi ile ne elde edilmek istenmektedir? Resimlerde evrişim işlemi resmi filtrelemek için kullanılır. Resim filtrelenince 
    farklı bir hale gelmektedir. Görüntü işlemede resmin bazı yönlerini açığa çıkartmak için amaca uygun çeşitli filtreler 
    kullanılabilmektedir. Örneğin biz bir filtre sayesinde resmi bulanık (blurred) hale getirebiliriz. Başka bir filtre sayesinde 
    resmin içerisindeki nesnelerin sınır çizgilerini elde edebiliriz. Bu konu "sayısal görüntü işleme" ile ilgildir. Detayları 
    çeşitli kaynaklardan edinilebilir. 

    Evirişim işlemini padding uygulamadan yapan basit bir fonksiyonu şöyle yazabiliriz:

    import numpy as np

    def conv(image, kernel):
        image_height = image.shape[0]
        image_width = image.shape[1]
        kernel_height = kernel.shape[0]
        kernel_width = kernel.shape[1]
        
        target = np.zeros((image_height - kernel_height + 1, image_width - kernel_width + 1), dtype='uint8')
        
        for row in range(image_height - kernel_height + 1):
            for col in range(image_width - kernel_width + 1):
                dotp = 0
                for i in range(kernel_height):
                    for k in range(kernel_width):
                        dotp += image[row + i, col + k] * kernel[i, k]
                target[row, col] = np.clip(dotp, 0, 255)
        return target

    Evrişim işleminin bu biçimde uygulanması yavaş bir yöntemdir. Bu tür işlemlerde mümkün olduğunca NumPy içerisindeki 
    fonksiyonlardan faydalanılmalıdır. Çünkü NumPy'ın fonksiyonlarının önemli bir bölümü C'de yazılmıştır ve manuel Python
    kodlarına göre çok daha hızlı çalışmaktadır.

    Aşağıdaki örnekte evrişim işlemi yapan conv isimli bir fonksiyon yazılmıştır. Bu fonksiyonla "blur" ve "sobel" filtreleri 
    denenmiştir. Görüntü işlemede blur filtresi resmi bulanıklaştırmakta, sobel filtresi ise nesnelerin sınır çizgilerini belirgin 
    hale getirmekte kullanılmaktadır. Blur filtrelemesinde eğer resminizin pixel boyutları büyükse kernel matrisi daha büyük 
    tutmalısınız.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

def conv(image, kernel):
    image_height = image.shape[0]
    image_width = image.shape[1]
    kernel_height = kernel.shape[0]
    kernel_width = kernel.shape[1]
    
    target = np.zeros((image_height - kernel_height + 1, image_width - kernel_width + 1), dtype='uint8')
    
    for row in range(image_height - kernel_height + 1):
        for col in range(image_width - kernel_width + 1):
            dotp = 0
            for i in range(kernel_height):
                for k in range(kernel_width):
                    dotp += image[row + i, col + k] * kernel[i, k]
            target[row, col] = np.clip(dotp, 0, 255)
    return target

import matplotlib.pyplot as plt

image = plt.imread('AbbeyRoad.jpg')
gray_scaled_image = np.average(image, axis=2, weights=[0.3, 0.59, 0.11])

blur_kernel = np.full((9, 9), 1 / 100)
convoluted_image = conv(gray_scaled_image, blur_kernel)

plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.imshow(gray_scaled_image, cmap='gray')
plt.subplot(1, 2, 2)
plt.imshow(convoluted_image, cmap='gray')
plt.show()

sobel_kernel = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
convoluted_image = conv(gray_scaled_image, sobel_kernel)

plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.imshow(gray_scaled_image, cmap='gray')
plt.subplot(1, 2, 2)
plt.imshow(convoluted_image, cmap='gray')
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Peki evrişim işleminin yapay sinir ağları için anlamı nedir? İşte burada işlemler yukarıdaki filtreleme örneğinin tersi 
    olacak biçimde yürütülmektedir. Yani bir resmin sınıfını belirlemek için onu filtreye sokabiliriz. Ancak bu filtrenin nasıl 
    bir filtre olacağını da ağın bulmasını sağlayabiliriz. O halde evrişimsel ağlarda biz uygulayacağımız filtreyi bilmemekteyiz. 
    Biz kestirimin daha isabetli yapılması için resmin nasıl bir filtreden geçirilmesi gerektiğini ağın bulmasını sağlarız. 
    Yani ağ yalnızca filtreyi uygulamaz bizzat filtrenin kendisini de bulmaya çalışır. Ancak resmin yalnızca filtreden geçirilmesi 
    yeterli değildir. Resim filtreden geçirildikten sonra yine Dense katmanlara sokulur. Yani filtreleme genellikle ön katmanlarda 
    yapılan bir işlemdir. Filtreleme katmanlarından sonra yine Dense katmanlar kullanılır. Tabii resmi filtrelerden geçirmek ve 
    bu filtreleri ağın kendisinin bulmasını sağlamak modeldeki eğitilebilir parametrelerin sayısını artırmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında evrişim işlemi nöronlarla ifade edilebilir. Çünkü evrişim sırasında yapılan dot-product işlemi aslında nöron 
    girişlerinin ağırlık değerleriyle çarpılıp toplanması işlemi ile aynıdır. Örneğin:

    a11 a12 a13 a14 a15 a16 
    a21 a22 a23 a24 a25 a26
    a31 a32 a33 a34 a35 a36
    a41 a42 a43 a44 a45 a46
    a51 a52 a53 a54 a55 a56
    a61 a62 a63 a64 a65 a66

    Burada aşağıdaki gibi bir kernel kullanılmış olsun:

    b11 b12 b13
    b21 b22 b23
    b31 b32 b33

    Bu kernel'ı resmin sol üst köşesi ile çakıştırdığımızda aslında bir nöron girdisi oluşturmuş oluruz. Şöyle ki: Bu nöronun 
    girdileri kernel'ın altındaki resmin pixelleridir. Nöronun ağırlık değerleri ise kernel'ın kendisidir. Yani biz aslında 
    evrişim işlemini sanki bir katmanmış gibi ifade edebiliriz. Peki yukarıdaki örnekte padding yapılmadıysa oluşturulacak 
    evrişim katmanında kaç nöron olacaktır? Tabii ki hedef resmin büyüklüğü kadar. Bu örnekte hedef resim 4x4'lüktür. Bu durumda     
    katmandaki nöron sayısı da 4 * 4 = 16 tane olacaktır. Bu 16 nöronun sonraki katmana girdi olarak verileceğine dikkat ediniz. 
    Peki bu katmandaki 16 nöronun her birine kaç tane xi değeri uygulanmaktadır? Tabii ki kernel boyutu kadar. Yani örneğimizde
    3 * 3 = 9 tane. O halde bu katmanda toplam 16 nöron vardır ve her bir nörona 9 girdi uygulanmaktadır. Fakat burada dikkat 
    edilmesi gereken nokta bu 16 nörona uygulanan girdilerin aslında 3 * 3 = 9 ağırlık değeri ile işleme sokulmasıdır. Yani 
    Dense katmanlarda olduğu gibi her xi için farklı bir wi ağırlık değeri yoktur. Bu katmanda her nöronda aynı 9 tane ağırlık 
    değeri kullanılmaktadır. Peki bu örneğimizde toplam eğitilebilir parametrelerin (trainable parameters) sayısı kaç tanedir?
    İşte 3 * 3 = 9 tane w değerinin konumlandırılması gerekmektedir. Ancak 1 tane de bias değeri dot product işleminin sonucunda 
    toplama işlemine sokulacaktır. Fakat burada her nöron için farklı bir bias değeri kullanılmamaktadır. Toplamda hep aynı w
    değerleri dot product işlemine sokulduğu için toplamda tek bir bias değeri söz konusu olmaktadır. Bu durumda örneğimizde 
    eğitilebilir parametrelerin sayısı 3 * 3 + 1 = 10 tane olacaktır. Tabii pixel'lerle kernel matris değerlerinin dot product
    işlemine sokulup bias değeriyle toplanmasıyla elde edilen değer yine diğer katmanlarda olduğu gibi aktivasyon fonksiyonuna
    sokulmaktadır.  
    
    Şimdi yukarıdaki açıklamaları somut bir örnek üzerinde gözden geçirelim. Aşağıdaki gibi 3x3'lük gri tonlamalı bir resim olsun:

    x11 x12 x13 
    x21 x22 x23 
    x31 x32 x33

    Biz de 2x2'lik aşağıdaki kernel'ı evrişimde uygulayalım:

    w11 w12
    w21 w22

    Bu işlemin sonucu olarak padding yapılmadığı durumda bu işlemden toplamda 4 nöron elde edilecektir. Elde edilen nöronların 
    çıktıları şöyle olacaktır:

    activation(x11 * w11 + x12 * w12 + x21 * w21 + x22 * w22 + b)       ---> 
    activation(x12 * w11 + x13 * w12 + x22 * w21 + x23 * w22 + b)       --->  
    activation(x21 * w11 + x22 * w12 + x31 * w21 + x32 * w22 + b)       ---> 
    activation(x22 * w11 + x23 * w12 + x32 * w21 + x33 * w22 + b)       ---> 

    Burada toplam 4 nöron çıktısı vardır. Bu nöronların hepsinin bias değerleri aynıdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        55. Ders - 03/08/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda evrişim işleminin gri tonlamalı resimlerde nasıl yapıldığını açıkladık. Peki evrişim işlemi RGB resimlerde 
    nasıl yürütülmektedir? RGB resimler aslında R, G ve B'lerden oluşan 3 farklı resim gibi ele alınmaktadır. Dolayısıyla üç 
    farklı kernel bu R, G ve B resimlere ayrı ayrı uygulanmaktadır. Görüntü işleme uygulamalarında bu farklı kernel'ların her 
    bir kanala uygulanması sonucunda ayrı bir değer elde edilir. Bu da hedef pixel'in RGB değerleri olur. Ancak sinir ağlarında 
    genel olarak 3 farklı kernel her bir kanala uygulandıktan sonra elde edilen değerler toplanarak teke düşürülmektedir. Yani 
    adeta biz evrişimsel ağlarda renkli resimleri evrişim işlemine soktuktan sonra onlardan gri tonlamalı bir resim elde etmiş 
    gibi oluruz. Peki bu işlemde kaç tane bias değeri kullanılacaktır? Her kanal (channel) için ayrı bir bias değeri 
    kullanılmamaktadır. Bias değeri bu kanallardan evrişim sonucunda elde edilen üç değerin toplanmasından sonra işleme sokulmaktadır. 
    Dolayısıyla bias değeri yalnızca bir tane olacaktır. 
    
    Örneğin biz 10x10'luk bir RGB resme evrişim uygulamak isteyelim. Kullanacağımız filtre matrisi (kernel) 3x3'lük olsun. 
    Burada her kanal için ayrı bir 3x3'lük filtre matrisi kullanılacaktır. Bu durumda evrişim katmanında eğitilebilir 
    parametrelerin sayısı 3 * 3 * 3 + 1 = 28 tane olacaktır. Eğer biz bu örnekte padding kullanmıyorsak ve stride değeri de 1 ise
    (yani kaydırma birer birer yapılıyorsa) bu durumda elde edilen hedef resim 8x8x1'lik olacaktır. Uygulanan evrişim sonucunda 
    resmin RGB olmaktan çıkıp adreta gray scale hale getirildiğine dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında uygulamada resim tek bir filtreye de sokulmamaktadır. Birden fazla filteye sokulmaktadır. Örneğin biz bir resimde 
    3x3'lük 32 farklı filtre kullanabiliriz. Bu durumda ağın bu 32 filtreyi de belirlemesini isteyebiliriz. Filtre sayısı 
    artırıldıkça her filtre resmin bir yönünü keşfedeceğinden resmin anlaşılması da iyileştirilmektedir. Şimdi 10x10'luk 
    resmi 3x3'lük filtre kullanarak padding uygulamadan 32 farklı filtreye sokmuş olalım. Biz bu resmi tek bir filtreye soktuğumuzda
    8x8x1'lik bir hedef resim elde ediyorduk. İşte 32 farklı filtreye soktuumuzda her filtreden 8x8x1'lik bir resim elde edileceğine
    göre toplamda 8x8x32'lik bir resim elde edilmiş olur. 
    
    Şimdi de 32 filtre kullandığımız durumda 10x10x3'lük RGB resim için eğitilebilir parametrelerin sayısını hesaplayalım. 
    Bir tane filtre için yukarıda toplam eğitilebilir parametrelerin sayısını 3 * 3 * 3 + 1 olarak hesaplamıştık. Bu filtrelerden 
    32 tane olduğuna göre toplam eğitilebilir parametrelerin sayısı 32 * (3 * 3 * 3 + 1) = 32 * 27 + 32 = 32 * 28 = 896 tane
    olacaktır. 

    Peki 10x10'luk resmimiz gri tonlamalı olsaydı 32 filtre ve 3x3'lük kernel için toplam eğitilebilir parametrelerin sayısı 
    ne olurdu? Bu durumda 3x3'lük toplam 32 farklı filtre kullanılacağı için ve her filtrede bir tane bias değeri söz konusu
    olacağı için toplam eğitilebilir parametrelerin sayısı da 32 * (3 * 3 + 1) = 32 * 10 = 320 tane olacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki evrişimsel sinir ağlarında tek bir evrişim katmanı mı bulunmalıdır? Aslında evrişim işlemi komşu pxelleri birbirleriyle 
    ilişkilendirmektedir. Yani onlara bir bağlam kazandırmaktadır.  Evrişim işlemiyle pixel'ler birbirinden bağımsız değil 
    komşu pixel'lerle ilişkili hale gelmektedir. Evrişimin çıktısının yeniden evrişime sokulması pixel'lerin daha uzak pixel'lerle 
    ilişkilendirilmesini sağlar. İşte bu nedenle genel olarak evrişim katmanları birden fazla katman olarak bulundurulur. Bu da 
    ağın derinleşmesine yol açmaktadır. Anımsanacağı gibi ara katmanların sayısı 2'den fazla ise böyle ağlara "derin ağlar (deep 
    neural network)" denilmektedir. Bu durumda ağa evrişim katmanlarını eklediğimizde artık derin ağlar yani derin öğrenme 
    uygulaması yapmış oluruz. Pek çok uygulamacı evrişim katmanlarındaki filtre sayısını önceki evrişimin iki katı olacak biçimde 
    artırmaktadır. Ancak uygulamacılar bu değerleri modelden modele kalbire edebilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de evrişimsel ağların Keras'ta nasıl oluşturulacağı üzerinde duralım. Keras'ta evrişimsel ağların oluşturulması için 
    tipik olarak Conv2D isimli bir sınıf kullanılmaktadır. Conv2D sınıfı resim girdisini iki boyutla bizden ister. Zaten 2D soneki
    bu anlama gelmektedir. Aslında benzer işlemi yapan Conv1D isimli bir sınıf da vardır. Tabii resimsel uygulamalarda resimler 
    iki boyutlu olduğu için Conv2D katmanı kullanılmaktadır. 
    
    Conv2D sınıfının __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.layers.Conv2D(
        filters,
        kernel_size,
        strides=(1, 1),
        padding='valid',
        data_format=None,
        dilation_rate=(1, 1),
        groups=1,
        activation=None,
        use_bias=True,
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        kernel_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        bias_constraint=None,
        **kwargs
    )
    
    Metodun ilk 4 parametresi önemlidir. Bu 4 parametre sırasıyla uygulanacak filtrelerin sayısını, kernel'ın genişlik ve yüksekliğini,
    stride miktarını ve padding yapılıp yapılmayacağını belirtir. Örneğin:

    conv2 = Conv2D(32, (3, 3), padding='same', activation='linear')

    Burada toplam 32 filtre uygulanmıştır. Kernel (3, 3) olarak alınmıştır. padding parametresi default "valid" durumdadır. 
    Bu "valid" değeri "padding yapılmayacağı" anlamına gelir. Bu parametre "same" geçilirse padding yapılır. Yani hedef resim 
    kaynak resimle aynı büyüklükte olur. Padding yapıldığı durumda padding satırları ve sütunları tamamen sıfırlarla doldurmaktadır. 
    Biz burada strides parametresine bir şey girmedik. Bu parametrenin default değeri (1, 1) biçimindedir. Yani kaydırma yatayda 
    ve düşeyde birer birer yapılacaktır. Evrişim katmanlarındaki aktivasyon fonksiyonları da Dense katmanlarda olduğu gibi 
    genellikle "relu" alınmaktadır. Eğer aktivasyon fonksiyonu hiç girilmezse sanki "linear" girilmiş gibi bir işlem söz konusu 
    olur. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Evrişim katmanlarından sonra modele genellikle yine Dense katmanlar eklenmektedir. Ancak Conv2D katmanın çıktısı çok boyutlu 
    olduğu için ve Dense katmanı da girdi olarak tek boyut istediği için Conv2D çıktısının Dense katmana verilmedne önce tek 
    boyuta indirgenmesi gerekmektedir. Çok boyutlu girdileri tek boyuta indirgemek için Keras'ta Flatten isimli bir katman 
    bulundurulmuştur. Örneğin:

    model = Sequential(name='MNIST') 
    model.add(Input((28, 28, 1)))
    model.add(Conv2D(32, (3, 3), name='Conv2D-1', activation='relu'))
    model.add(Conv2D(64, (3, 3), name='Conv2D-2', activation='relu'))
    model.add(Flatten(name='Flatten'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(10, activation='softmax', name='Output'))

    model.summary()

    Burada giri tonlamalı resim için tepik bir evirişim katmanının kullanım örneğini görüyorsunuz. Nodelin girdisi 28x28'lik 
    gri tonlamalı resimlerden oluşmaktadır. Sonra bu resimler üzerinde 3x3'lük filtreler uygulanmıştır. İlk Conv2D katmanında 
    32 filtre sonraki Conv2D katmanında ise 64 filtre kullanılmıştır. Daha sonra Flatten katmanıyla çok boyutlu çıktının tek
    boyuta indirgendiğini görüyorsunuz. Aslında Flatten katmanı yerine genel Reshape katmanı da çok boyutlu verileri tek boyuta 
    dönüştürmek için eReshape((-1, )) biçiminde kullanılabilmektedir.

    Bu modelden şöyle bir özet elde edilmiştir:

    Model: "MNIST"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv2D-1 (Conv2D)               │ (None, 26, 26, 32)     │           320 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv2D-2 (Conv2D)               │ (None, 24, 24, 64)     │        18,496 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Flatten (Flatten)               │ (None, 36864)          │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 128)            │     4,718,720 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 128)            │        16,512 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 10)             │         1,290 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 4,755,338 (18.14 MB)
    Trainable params: 4,755,338 (18.14 MB)
    Non-trainable params: 0 (0.00 B)
        
    Burada birinci evrişim katmanında 32 filtre uygulandığı için bu evrişim katmanının çıktısı 26x26x32 tane nörondan oluşacaktır. 
    Ancak Conv2D katmanı sanki bu nöronları 26x26'lık 32 kanaldan oluşan bir resim gibi vermektedir. Bu evrişim katmanının 
    çıktısı diğer evrişim katmanına bağlandığında sanki diğer evrişim katmanı 26x26'lık 32 kanala sahip bir resim girdisi almış 
    gibi olur. Birinci katmandaki eğitilebilir parametrelerin sayısı şöyle hesaplanmaktadır: Bu katmanda 3x3'lük kernel kullanılmıştır. 
    Toplamda 1 tane bias değeri evrişim sonucundaki değerle toplanacağından bir filtre için eğitilebilir parametrelerin sayısı 
    3 * 3 + 1 = 10 tane olacaktır. Bu katmanda 32 filte kullanıldığına göre bu katmandaki toplam eğitilebilir parametrelerin 
    sayısı (3 * 3 + 1) * 32 = 320 tane olacaktır.  Yukarıda da belirtildiği gibi ikinci evrişim katmanının girdisi sanki 26x26'lık 
    32 kanallı resim gibidir. Buna evrişim uygularken her kanal için ayrı bir filtre matrisi kullanılır. Bu durumda 32 tane 
    3 x 3'lük filtreye ihtiyaç duyulacaktır. En nihayetinde bu 32 filtereden elde edilen değer tek bias değeri ile toplanacağı 
    için bir filtre söz konusu olduğunda ikinci evrişim katmanındaki eğitilebilir parametrelerin sayısı 3 * 3 * 32 + 1 tane 
    olacaktır. İkinci evrişim katmanında 64 tane filtre kullanıldığına göre ikinci evirişim katmanındaki toplam eğitilebilir 
    parametrelerin sayısı 64 * (3 * 3 * 32 + 1) = 18496 olur. İkinci evirişim katmanının çıktısının 24x24x64'lük bir matris 
    olduğuna dikkat ediniz. Yani adeta ikinci evirişm katmanı bize sanki 24x24'lük 64 kanallı resimler vermektedir. Yukarıda 
    da belirttiğimiz gibi evrişim katmanlarından sonra bu çok boyutlu çıktının tek boyuta indirgenmesi  gerekir. Bunun için 
    Flatten karmanı kullanılmıştır. Flatten katmanında hiç eğitilebilir parametre yoktur. Flatten katmanının çıktısının  
    24 * 24 * 64 = 36864 nörondan oluştuğuna dikkat ediniz. Artık bu nöronlar ilk Dense katmana girdi yapılacaktır. Bu durumda 
    ilk Dense katmandaki eğitilebilir parametrelerin sayısı 36864 * 128 + 128 = 4718720 tane olacaktır. Birinci Dense katmanın 
    çıktısında 128 nöron vardır. Bu 128 nöron ikinci Dense katmana girdi yapılmıştır. Dolayısıyla ikinci katmandaki eğitilebilir 
    parametrelerin sayısı 128 * 128 + 128 = 16512 tane olacaktır. Nihayet son Dense katmana 128 nöron girip bu katmandan 10 
    nöron çıktısı elde edilecektir. Bu durumda bu son katmandaki eğitilebilir parametrelerin sayısı 128 * 10 + 10 = 1290 
    olacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda daha önce yapmış olduğumuz MNIST örneğini evrişimsel katmanlar kullanarak yeniden yapıyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

training_df = pd.read_csv('mnist_train.csv')
test_df = pd.read_csv('mnist_test.csv')

training_dataset_x = training_df.iloc[:, 1:].to_numpy(dtype='uint8')
training_dataset_y = training_df.iloc[:, 0].to_numpy(dtype='uint8')

test_dataset_x = test_df.iloc[:, 1:].to_numpy(dtype='uint8')
test_dataset_y = test_df.iloc[:, 0]

# one hot encoding for y data

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

# reshape as 28x28x1

training_dataset_x = training_dataset_x.reshape(-1, 28, 28, 1)
test_dataset_x = test_dataset_x.reshape(-1, 28, 28, 1)

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 13))
for i in range(50):
    plt.subplot(10, 5, i + 1)
    plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = training_dataset_x[i]
    plt.imshow(picture, cmap='gray')
plt.show()

# minmax scaling

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten

model = Sequential(name='MNIST')
model.add(Input((28, 28, 1), name='Input'))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=20, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os
import glob

for path in glob.glob('Predict-Pictures/*.bmp'):
    image = plt.imread(path)
    gray_scaled_image = np.average(image, axis=2, weights=[0.3, 0.59, 0.11])
    gray_scaled_image = gray_scaled_image.reshape(1, 28, 28, 1)
    gray_scaled_image /= 255
    
    model_result = model.predict(gray_scaled_image, verbose=0)
    predict_result = np.argmax(model_result)
    print(f'Real Number: {os.path.basename(path)[0]}, Prdicted Result: {predict_result}, Path: {path}')

#----------------------------------------------------------------------------------------------------------------------------
    Evrişimsel ağlarda evrişim katmanlarında çok fazla eğitilebilir parametre oluşmaktadır. Yukarıdaki MNIST örneğinde toplam 
    eğitilebilir parametrelerin sayısı 4.5 milyon civarındadır. Üstelik bu örnekteki resimler 28x28'lik gri tonlamalı resimlerdir.
    Pratikte 28x28 gibi resimler çok küçük olduğundan kullanılmazlar. Yani resimler pratikte 28x28'den çok daha büyük olma 
    eğilimindedir.  Ayrıca resimler genellikle karşımıza renkli biçimde gelmektedir. Eğitilebilir parametrelerin sayısının 
    fazla olmasının şu dezavantajları vardır:

    - Eğitim için gereken zaman fazlalaşır. 
    - Çok nöron olmasından kaynaklanan overfitting durumları oluşabilir.
    - Eğitim sonucunda eğitim bilgilerinin saklanması için gerekli olan disk alanı büyür.
        
    İşte bu tür resim tanıma işlemlerinde eğitilebilir parametrelerin sayısını düşürmek için çeşitli teknikler kullanılmaktadır. 
    Bunun için ilk akla gelecek yöntem evrişim katmanlarındaki kaydırma değerlerini (strides) artırmaktır. Ancak kaydırma değerlerinin 
    artırılması resmin tanınması için dezavantaj da oluşturmaktadır. Nöron sayılarını azaltmak için diğer bir yöntem ise "pooling" 
    denilen yöntemdir. Bu bağlamda genellikle pooling yöntemi tercih edilmektedir. 

    Pooling bir grup dikdörtgensel bölgedeki pixel'ler yerine onları temsil eden tek bir pixel'in elde edilmesi yöntemidir. (Tabii 
    aslında pooling yöntemi yalnızca resimsel verilerde kullanılmamaktadır. Ancak biz burada pooling işlemiin resimler üzerinde 
    uyguladığımız için pixel terimini kullanıyoruz.) Pooling işleminin İki önemli biçimi vardır: "Max Pooing" ve "Average Pooling".  
    "Max Pooling" yönteminde dikdörtgensel bölgedeki en büyük eleman alınır. Average Pooling yönteminde ise dikdörtgensel bölgedeki 
    elemanların ortalamaları alınmaktadır. Uygulamada daha çok Max Pooling yöntemi tercih edilmektedir. MaxPooling yöntemi ilgili 
    dikdörtgensel bölgedeki en belirgin özelliğin elde edilmesine yol açmaktadır. Örneğin 4x4'lük pixel'lerden oluşan gri tonlamalı 
    resimdeki pixel değerleri aşağıdaki gibi olsun:

    112     62      41      52
    200     15      217     21
    58      92      81      117
    0       21      45      89

    Pooling uygulayacağımız çerçevimiz 2x2'lik olsun. Bu 2x2'lik çerçeve resim üzerinde sağdan iki aşağıdan olacak şekilde kaydırılır 
    ve toplam 4 bölge elde edilir:

    112     62     
    200     15      

    41      52
    217     21

    58      92      
    0       21      

    81      117
    45      89

    Görüldüğü gibi pooling işleminde kaydırma (yani stride) genellikle 1 değil pooling çerçevesi kadar yapılmaktadır. İşte Max 
    Pooling yönteminde bu çerçevelerin en büyük elemanları alınır ve aşağıdaki matris elde edilir:

    200     217 
    92      117

    Bu işlemin sonucunda elde edilen matrisin ilkinin karekökü kadar olduğuna dikkat ediniz. Yani pooling çerçevesi resmi
    üstel olarak küçültmektedir. 

    Tabii pooling işlemleri üç boyutlu matrisler üzerinde de uygulanabilir. Örneğin evrişim katmanının çıktısı birden fazla
    filtre kullanıldığı için genel olarak N kanallı resim gibidir. Bu durumda her kanal için ayrı ayrı pooling uygulanacaktır. 
    Örneğin 26x26x32'lik bir matris üzerinde 2x2 çerçeveli pooling işlemi yapıldığında hedef matris 13x13x32'lik olur.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Resimsel verilerde pooling işlemleri Keras'ta tipik olarak MaxPooling2D ve AveragePooling2D sınıflarıyla temsil edilmiştir. 
    Sınıfların __init__ metotlarının parametrik yapıları şöyledir:
    
    MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)
    AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)

    Metotların pool_size parametreleri çerçevenin büyüklüğünü belirtmektedir. Default olarak 2x2'lik çerçeve kullanılmaktadır. 
    2x2'lik çerçeve kullanımı tipiktir. Metotların strides parametreleri yine kaydırma miktarını belirtir. Default durumda 
    kaydırma pools_size parametresiyle aynı değerdedir. Yani bu parametreye None geçersek aslında stride değerinin pool_size 
    ile aynı olmaktadır. padding parametresi yine "valid" ya da "same" olabilir. "valid" padding yapılmayacağı, "same" ise 
    padding yapılacağı anlamına gelmektedir. Örneğin elimizdeki resim 5x5'lik olsun. Biz padding kullanmazsak çerçevimiz 2x2 ise 
    toplamda yatayda ve düşeyde 2 kaydırma yapabiliriz. Hedef resim de 2x2'lik olur. Ancak padding uygularsak artık 3 yatayda 
    ve düşeyde 3 kaydırma yapabiliriz. Hedef resmizimde 3x3'lük olur. Yani padding işlemi sonda kalan artık alanların da kullanılmasına 
    yol açmaktadır. 

    Tipik olarak Pooling katmanları her evrişim katmanından sonra uyhulanmaktadır. Yani tipik olarak her Conv2D katmanından 
    sonra bir tane de MaxPooling2D ya da AveragePooling2D katmanı bulundurulur. Örneğin:

    model = Sequential(name='MNIST')
    model.add(Input((28, 28, 1), name='Input'))
    model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
    model.add(MaxPooling2D(name='MaxPooling2D-1'))
    model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
    model.add(MaxPooling2D(name='MaxPooling2D-2'))
    model.add(Flatten(name='Flatten'))    
    model.add(Dense(128, activation='relu', name='Hidden-1'))
    model.add(Dense(128, activation='relu', name='Hidden-2'))
    model.add(Dense(10, activation='softmax', name='Output'))
    model.summary()

    Modelden şöyle bir özet bilgi edilmiştir:

   Model: "MNIST"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv2D-1 (Conv2D)               │ (None, 26, 26, 32)     │           320 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ MaxPooling2D-1 (MaxPooling2D)   │ (None, 13, 13, 32)     │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv2D-2 (Conv2D)               │ (None, 11, 11, 64)     │        18,496 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ MaxPooling2D-2 (MaxPooling2D)   │ (None, 5, 5, 64)       │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Flatten (Flatten)               │ (None, 1600)           │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 128)            │       204,928 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 128)            │        16,512 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 10)             │         1,290 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 241,546 (943.54 KB)
    Trainable params: 241,546 (943.54 KB)
    Non-trainable params: 0 (0.00 B)

    Burada katmanlardaki eğitilebilir parametrelerin sayısının ve katmanların çıktılarındaki toplam nöron sayısının nasıl 
    hesaplandığı üzerinde duralım:
    
    - Burada yine birinci Conv2D katmanındaki eğitilebilir parametrelerin sayısı (3 * 3 + 1) * 32 = 320 olacaktır. Bu katmanın
    çıktısı 26x26x32'lik bir nöron matrisidir. Birinci MaxPooling2D katmanının girdisi de bu biçimde olacaktır. 

    - Birinci MaxPoolin2D katmanında eğitilebilir parametre yoktur. Bu katmanın amacı zaten nöron sayılarını düşürmektir. 
    Pooling işlemindeki pencere genişliği 2x2 olduğu için bu katmanın çıktısı 13x13x32'lik bir nöron matrisidir. 

    - İkinci Conv2D katmanında yine (3 * 3 * 32 + 1) * 64 =  18496 tane eğitilebilir parametre vardır. Bu katmanın çıktısı da 
    11x11x64'lük bir nöron matrisidir (padding uygulanmadığına dikkat ediniz). 
    
    - İkinci MaxPooling2D katmanının giridisi 11x11x64 nöron matrisinden çıktısı ise 5x5x64'lük nöron matrisinden oluşmaktadır.
    Tabiibu katmanda da eğitilebilir parameteler yoktur.

    - Birinci Dense katmanın girdisi Flatten işleminden sonra artık 5 * 5 * 64 = 1600 nörondan oluşmaktadır. Bu katmanda 128
    nöron vardır. Bu durumda birinci Dense katmandaki eğitilebilir parametrelerin sayısı 1600 * 128 + 128 = 204928 tanedir. 

    - İkinci Dense katmana 128 nöron girmekte ve bu katmandan 128 nöron çıkmaktadır. Bu durumda ikinci Dense katmandaki eğitilebilir
    parametrelerin sayısı 128 * 128 + 128 = 16512 tane olacaktır.

    - Çıktı katmanına giren nöron sayısı 128, çıktı nöron sayısı ise 10 tanedir. Bu durumda çıktı katmanındaki eğitilebilir 
    parametrelerin sayısı 128 * 10 + 10 = 1290 tane olacaktır. 

    Toplam eğitilebilir parametrelerin sayısı da 241546 tanedir. Bu değeri pooling işlemini uygulamadığımız örnekteki 4755338
    değeri ile karşılaştırdığımızda uyguladığımız MaxPooling işleminin bu örnekte eğitilebilir parametrelerin sayısını 20 
    kat civarında düşürdüğü görülmektedir. Resimler büyüdükçe bu parametrelerin sayısının azaltılmasının etkisi çok daha iyi 
    anlaşılacaktır.

    MNIST örneğinin pooling uygulanmış hali ile pooling uygulanmamış hali karşılaştırıldığında pooling uygulanmış halinin 
    her bakımdan biraz daha iyi performans gösterdiği görülmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            56. Ders - 04/08/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    MNIST veri kümesi için evrişim ve pooling uygulanmış sinir ağı modeli aşağıda bir bütün olarak verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

EPOCHS = 5

training_df = pd.read_csv('mnist_train.csv')
test_df = pd.read_csv('mnist_test.csv')

training_dataset_x = training_df.iloc[:, 1:].to_numpy(dtype='uint8')
training_dataset_y = training_df.iloc[:, 0].to_numpy(dtype='uint8')

test_dataset_x = test_df.iloc[:, 1:].to_numpy(dtype='uint8')
test_dataset_y = test_df.iloc[:, 0]

# one hot encoding for y data

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

# reshape as 28x28x1

training_dataset_x = training_dataset_x.reshape(-1, 28, 28, 1)
test_dataset_x = test_dataset_x.reshape(-1, 28, 28, 1)

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 13))
for i in range(50):
    plt.subplot(10, 5, i + 1)
    plt.title(str(training_dataset_y[i]), fontsize=12, fontweight='bold')
    picture = training_dataset_x[i]
    plt.imshow(picture, cmap='gray')
plt.show()

# minmax scaling

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential(name='MNIST')
model.add(Input((28, 28, 1), name='Input'))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(MaxPooling2D(name='MaxPooling2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(MaxPooling2D(name='MaxPooling2D-2'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(128, activation='relu', name='Hidden-1'))
model.add(Dense(128, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os
import glob

for path in glob.glob('Predict-Pictures/*.bmp'):
    image = plt.imread(path)
    gray_scaled_image = np.average(image, axis=2, weights=[0.3, 0.59, 0.11])
    gray_scaled_image = gray_scaled_image.reshape(1, 28, 28, 1)
    gray_scaled_image /= 255
    
    model_result = model.predict(gray_scaled_image, verbose=0)
    predict_result = np.argmax(model_result)
    print(f'Real Number: {os.path.basename(path)[0]}, Prdicted Result: {predict_result}, Path: {path}')
   
#----------------------------------------------------------------------------------------------------------------------------
    Peki toplamda pooling işleminin bize sağladığı katkı nedir? İşte eğitilebilir parametrelerin sayısı pooling işlemi 
    ile oldukça azaltılmaktadır. Bu da eğitimin daha hızlı gerçekleşmesini ve overfitting olgusunun daha az düzeye çekilmesini 
    sağlamaktadır. Eğitilebilir parametrelerin fazla olması ağdaki nöronların ağırlıklarının daha uzun eğitim sürecinde 
    konumlandırılmasına yol açar. Aynı zamanda yüksek sayıda parametre modelin yanlış şeyleri öğrenmesine (overfitting) zemin 
    hazırlamaktadır. Bu nedenle resim tanıma gibi işlemlerde uygulamacılar evirişim katmanından sonra hemen her zaman bir 
    pooling katmanı kullanırlar.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki MaxPooling işlemi ile AveragePooling işlemi arasında ne fark vardır? Hangi durumlarda hangisi tercih edilmelidir?
    Aslında bu tür tercihlerin sezgisel yapılması iyi bir yöntem olmayabilir. Bu tür durumlarda daha çok deneme yöntemi uygulanmaktadır.
    Genel olarak MaxPooling işleminin resim içeisindeki belirgin öğeleri daha iyi tespit ettiği söylenebilir. Bu da pek çok
    resim tanıma işleminde fayda sağlamaktadır. AveragePooling ise pixel'lerin ortalamasını aldığı için daha pürüssüz ve daha
    ortalama bilginin elde edilmesini sağlamaktadır. Ancak MaxPooling işlemi resimdeki belirgin özellikleri alırken diğer özellikleri 
    kaybetme eğilimindedir. MaxPoooling işleminin işlem maliyetinin AveragePooling işleminden daha az olduğuna da dikkat ediniz. 
    Uygulamada genellikle MaxPooling işlemi tercih edilmektedir. Ancak yukarıda da belittiğimiz gibi bu durum amaca bağlı olarak 
    değişebilir. Eğer mümkünse deneme yönteminin uygulanması önerilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    MaxPooling2D ve AveragePooling2D katmanlarının dışında bu işlemleri global düzeyde yapan GlobalAveragePooling2D ve 
    GlobalMaxPooling2D katman sınıfları da bulunmaktadır. Bu katman sınıfları ile yapılan pooling işlemlerinde toplamda tek 
    bir değer elde edilmektedir. Yani bu katmanlar girdi olarak aldığı tüm değerlerden tek bir değeri çıktı olarak vernmektedir. 
    Dolayısıyla bu sınıfların __init__ metotlarının parametrelerinde onlara verilecek bir şey yoktur:

    tf.keras.layers.GlobalAveragePooling2D(
        data_format=None, keepdims=False, **kwargs
    )

    tf.keras.layers.GlobalMaxPool2D(
        data_format=None, keepdims=False, **kwargs
    )

    Tabii bu katman nesnelerinden önce Conv2D katmanı kulanılmışsa ve o katmanda N tane filtre belirtilmişse bu katmanın 
    çıktısının da 1 değil N olması gerekir. Uygulamada GLobalMaxPooling ve GlobalAvreagePooling işlemleri nihai olarak 
    tek bir değerin elde edilmesi gerektiğinde kullanılmaktadır. Tabii bu katmanlar evrişim katmanlarının en sonunda hemen 
    Dense katmanlardan önce uygulanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki pooling çerçevesi hangi büyüklükte olmalıdır? Aslında bu da üzerinde çalıştığımız resimlerin büyüklüklerine ve
    onların niteliklerine bağlı olarak değişebilir. Keras'ın default pooling çerçevesinin 2x2'lik olduğunu belirtmiştik.
    Bu çerçevenin artırılması bazı uygulamalarda daha iyi sonuçların elde edilmesini sağlayabilmektedir. Genel olarak bu 
    çerçeve genişliğinin de üzerinde çalışılan veri kümesi eşliğinde deneme yoluyla belirlenmesi uygun olmaktadır. Fakat 
    eğer bu yönteme sapmayacaksanız 2x2'lik default çerçeve büyüklüğünü kullanabilirsiniz. Resimler büyüdükçe 2x2 yerine
    3x3'lük ya da 4x4'lik çerçeveleri tercih edebilirsiniz. Çünkü büyük resimlerde eğitilebilir parametrelerin sayısı ciddi 
    boyuta gelebilmektedir. Büyük çerçeveler bunların daha fazla azaltılmasına katkı sağlayacaktır. Çerçeve büyütüldükçe
    ayrıntıların daha fazla göz ardı edileceğine dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Renkli resimlerin sınıflandırılması için sık kullanılan deneme veri kümelerinden biri CIFAR-10 (Canadian Institute for 
    Advanced Research) isimli veri kümesidir. Bu veri kümesi tensorflow.keras.datasets paketi içerisindeki cifar10 modülünde 
    de bulunmaktadır. CIFAR-10 veri kümesinde her biri 32x32 pixel olan 3 kanallı RGB resimler bulunmaktadır. Bu RGB resimler 
    10 farklı sınıfa ayrılmıştır. Sınıflar şunlardır:

    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

    Veri kümesinin orijinal https://www.cs.toronto.edu/~kriz/cifar.html adresinden indirilebilir. (Bu bağlantıya tıklandığında 
    veri kümesinin farklı programlama dilleri için farklı versiyonlarının bulunduğunu göreceksiniz. Burada Python'a ilşkin veri
    kümesini indirebilirsiniz.) Veri kümesinin CSV biçimi de https://www.kaggle.com/datasets/fedesoriano/cifar10-python-in-csv 
    bağlantısından indirilebilir. Veri kümesinin ilk bağlantıda bulunan orijinalinde 5 dosya eğitim verilerini, bir dosya da 
    test verilerini bulundurmaktadır. (Veri kümesini kullanıma sunanlar dosyalar çok büyümesin diye bunları 5 dosyaya bölmüş 
    olabilirler. Ya da verileri 5 dosyaya bölmelerinin nedeni az veriyle denemeler yapacak kişilerin küçük bir dosyayı kullanmalarını 
    sağlamak da olabilir.) Ancak bu dosyalar Python'un pickle modülü ile seri hale getirilmiştir. Bu dosyalar pickle.load ile 
    deserialize yapıldığında 5 tane anahtardan oluşan sözlük nesneleri elde edilmektedir. Sözlüğün 5 anahtarı şöyledir:

    dict_keys([b'batch_label', b'labels', b'data', b'filenames'])
    
    Bizim bu 5 eğitim dosyasındaki x ve y verilerini ayrı ayrı elde edip birleştirmemiz gerekir. Bu işlemi şöyle yapabiliriz:
    
  import pickle
    import glob

    x_lst = []
    y_lst = []

    for path in glob.glob('cifar-10-batches-py/data_batch_*'):
        with open(path, 'rb') as f:
            d = pickle.load(f, encoding='bytes')
            x_lst.append(d[b'data'])
            y_lst.append(d[b'labels'])     

    import numpy as np
            
    training_dataset_x = np.concatenate(x_lst)
    training_dataset_y = np.concatenate(y_lst)

    with open('cifar-10-batches-py/test_batch', 'rb') as f:
        d = pickle.load(f, encoding='bytes')
        test_dataset_x = d[b'data']
        test_dataset_y = d[b'labels']
        
    Buradan elde ettiğimiz matrisler iki boyutludur. Evrişim katmanları için bunların üç boyutlu hale getirilmesi gerekir. 
    Ancak resmin orijinalleri maalesef tek boyutlu hale getirilirken standrat bir eksen sistemi uygulanmamış aşağıdaki gibi 
    boyutlar uç uca eklenmiştir:

    Tek boyutlu resmin 0'ınci boyutu ---> Gerçek resmin 2'üncü boyutu
    Tek boyutlu resmin 1'inci boyutu ---> Gerçek resmin 0'ıncı boyutu
    Tek boyutlu resmin 2'inci boyutu ---> Gerçek resmin 1'inci boyutu

    Bu nedenle tek boyut olarak elde ettiğmiz resimlerin klasik RGB boyutlarına dönüştürülmesi için NumPy'ın transpose 
    fonksiyonundan faydalanılması gerekmektedir. Dönüştürme işlemi şöyle yapılabilir:

    training_dataset_x = training_dataset_x.reshape(-1, 3, 32, 32)
    training_dataset_x = np.transpose(training_dataset_x, [0, 2, 3, 1])

    test_dataset_x = test_dataset_x.reshape(-1, 3, 32, 32)
    test_dataset_x = np.transpose(test_dataset_x, [0, 2, 3, 1])

    Burada transpose işleminde 3 boyut değil 4 boyut kullanıldığına dikkat ediniz. Çünkü aslında matrisler resimlerden 
    oluşmaktadır. Bu işlemlerden sonra bir grup resmi fikir vermesi için aşağıdaki gibi çizdirebiliriz:

    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
  
    import matplotlib.pyplot as plt
        
    plt.figure(figsize=(4, 20))
    for i in range(30):
        plt.subplot(10, 3, i + 1)
        plt.title(class_names[training_dataset_y[i]], pad=10)    
        plt.imshow(training_dataset_x[i])
    plt.show()

    Tabii yine resimler üzerinde minmax ölçeklemesinin yapılması uygundur:

    training_dataset_x = training_dataset_x / 255
    test_dataset_x = training_dataset_x / 255

    Artık modelimizi kurup eğitebiliriz. Modeli MNIST örneğinde olduğu gibi oluşturulabiliriz. Ancak burada resim 3 kanallı 
    olduğu için ve biraz daha büyük olduğu için iki yerine üç evrişim katmanı kullanabiliriz. Filtre sayılarını da artırabiliriz. 
    Dense katmanlardaki nöronları da artırmak daha iyi sonucun elde edilmesine yol açabilecektir:

    model = Sequential(name='CIFAR10')
    model.add(Input((32, 32, 3), name='Input'))
    model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
    model.add(MaxPooling2D(name='MaxPooling2D-1'))
    model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
    model.add(MaxPooling2D(name='MaxPooling2D-2'))
    model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
    model.add(MaxPooling2D(name='MaxPooling2D-3'))
    model.add(Flatten(name='Flatten'))    
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(10, activation='softmax', name='Output'))
    model.summary()

    Kestirim işlemini Internet'ten rastgele resimler bulup onları 32x32'lik boyuta getirerek yapabiliriz. Örneğimizde kestirilecek 
    resimler "Predict-Pictures" isimli bir dizinine yerleştirilmiştir. Bulunan resimlerin 32x32'lik boyuta ölçeklenmesi hazır 
    programlarla yapılabilir. (Microsoft'un Paint programı bunun biraz zahmetledir.) Resimler üzerinde bu türlü manipülasyonlar
    yapmak için sık kullanılan kütüphanelerden biri "PIL (Python Image Library)" denilen kütüphanedir. Kütüphane aşağıdaki 
    gibi kurulabilir:

    pip install pillow
        
    Kütüphanenin dokümantasyonuna aşağıdaki bağlantıdan ulaşabilirsiniz:

    https://pillow.readthedocs.io/en/stable/

    PIL kütüphanesini kullanarak bir resmin ölçeklendirilip save edilmesi kabaca şöyle yapılmaktadır:

    # rescale-image.py

    from PIL import Image
    import glob

    for path in glob.glob('Predict-Pictures/*.*'):
        image = Image.open(path)
        resized_image = image.resize((32, 32))
        image.close()
        resized_image.save(path)
    
    Burada önce glob fonksiyonu ile dizindeki tüm resim dosyaları elde edilmişl sonra PIL ile yeniden boyutlandırılmış,
    sonra da yeni boyuttaki resim orijinal formatta save edilmiştir.
    
    Aşağıda tüm örneği bir bütün olarak veriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pickle
import glob

EPOCHS = 100

x_lst = []
y_lst = []

for path in glob.glob('cifar-10-batches-py/data_batch_*'):
    with open(path, 'rb') as f:
        d = pickle.load(f, encoding='bytes')
        x_lst.append(d[b'data'])
        y_lst.append(d[b'labels'])     

import numpy as np
        
training_dataset_x = np.concatenate(x_lst)
training_dataset_y = np.concatenate(y_lst)

with open('cifar-10-batches-py/test_batch', 'rb') as f:
    d = pickle.load(f, encoding='bytes')
    test_dataset_x = d[b'data']
    test_dataset_y = d[b'labels']

training_dataset_x = training_dataset_x.reshape(-1, 3, 32, 32)
training_dataset_x = np.transpose(training_dataset_x, [0, 2, 3, 1])

test_dataset_x = test_dataset_x.reshape(-1, 3, 32, 32)
test_dataset_x = np.transpose(test_dataset_x, [0, 2, 3, 1])

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
  
import matplotlib.pyplot as plt
     
plt.figure(figsize=(4, 20))
for i in range(30):
    plt.subplot(10, 3, i + 1)
    plt.title(class_names[training_dataset_y[i]], pad=10)    
    plt.imshow(training_dataset_x[i])
plt.show()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential(name='CIFAR10')
model.add(Input((32, 32, 3), name='Input'))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(MaxPooling2D(name='MaxPooling2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(MaxPooling2D(name='MaxPooling2D-2'))
model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
model.add(MaxPooling2D(name='MaxPooling2D-3'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')
    
#----------------------------------------------------------------------------------------------------------------------------
                                        57. Ders - 10/08/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında CIFAR-10 veri kümesi tensorflow.keras.datasets paketi içerisinde cifar10 modülü biçiminde hazır olarak da bulunmaktadır. 
    Keras'taki hazır CIFAR-10 veri kümesi zaten 32x32x3'lük resim verilerini bize vermektedir. Aşağıdaki örnekte aynı CIFAR-10 
    veri kümesi hazır olarak kullanılmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

import glob

EPOCHS = 5

from tensorflow.keras.datasets import cifar10

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar10.load_data()

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
  
import matplotlib.pyplot as plt
     
plt.figure(figsize=(4, 20))
for i in range(30):
    plt.subplot(10, 3, i + 1)
    plt.title(class_names[training_dataset_y[i, 0]], pad=10)    
    plt.imshow(training_dataset_x[i])
plt.show()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential(name='CIFAR10')
model.add(Input((32, 32, 3), name='Input'))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(MaxPooling2D(name='MaxPooling2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(MaxPooling2D(name='MaxPooling2D-2'))
model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
model.add(MaxPooling2D(name='MaxPooling2D-3'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')
   
#----------------------------------------------------------------------------------------------------------------------------
    CIFAR-10 vei kümesinin 100 sınıf içeren CIFAR-100 isimli başka bir versiyonu da vardır. CIFAR-100 veri kümesi tamamen CIFAR-10 
    veri kümesi gibidir. Ancak resimler 10 tane sınıfa değil 100 tane sınıfa ayrılmış durumdadır. Yani biz CIFAR-100 veri kümesinde 
    resimlerin 10 tane sınıftan hangisine ilişkin olduğunu değil 100 tane sınıftan hangisine ilişkin olduğunu tahmin etmeye 
    çalışırız. Bu veri kümesi de yine aşağıdaki bağlantıdan indirilebilir:

    https://www.cs.toronto.edu/~kriz/cifar.html

    Buradaki dosya indirilip açıldığında "cifar-100-python" dizini içerisinde "train" ve "test" isimli iki dosya bulunacaktır. 
    Bu dosyalar yine Python'un pickle modülü ile seri hale getirilmiştir. Bunların açılması gerekmektedir. Seri hale getirilmiş 
    veriler açıldığında yine bir sözlük elde edilir. y verilerini temsil eden  "fine_labels" "coarse_labels" anahtarları bulunmaktadır. 
    Buradaki "fine_labels" 100 sınıf, "coarse_labels" ise 20 sınıf belirtmektedir. Açım işlemi şöyle yapılabilir:

    import pickle

    with open('cifar-100-python/train', 'rb') as f:
        training_dataset = pickle.load(f, encoding='bytes')
        
        training_dataset_x = training_dataset[b'data']
        training_dataset_y = training_dataset[b'fine_labels']
            
    with open('cifar-100-python/test', 'rb') as f:
        test_dataset = pickle.load(f, encoding='bytes')

        test_dataset_x = test_dataset[b'data']
        test_dataset_y = test_dataset[b'fine_labels']

    Resimlerin ilişkin olduğu 100 sınıf şöyledir:

    class_names = [
        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',
        'worm'
    ]

    Bu örnekte de yine kestirim için kullanılacak dosyalar "Predict-Pictures" isimli dizinde bulunmalıdır. Resimleri o 
    dizine çektikten sonra aşağıdaki programı çalıştırarak onları 32x32x3 olarak yeniden boyutlandırabilirsiniz:

    # rescale-image.py

    from PIL import Image
    import glob

    for path in glob.glob('Predict-Pictures/*.*'):
        image = Image.open(path)
        resized_image = image.resize((32, 32))
        image.close()
        resized_image.save(path)
    
    Aşağıda CIFAR-100 örneği bütün olarak verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pickle
import numpy as np

EPOCHS = 5

with open('cifar-100-python/train', 'rb') as f:
    training_dataset = pickle.load(f, encoding='bytes')
    
    training_dataset_x = training_dataset[b'data']
    training_dataset_y = training_dataset[b'fine_labels']
        
with open('cifar-100-python/test', 'rb') as f:
    test_dataset = pickle.load(f, encoding='bytes')

    test_dataset_x = test_dataset[b'data']
    test_dataset_y = test_dataset[b'fine_labels']

training_dataset_x = training_dataset_x.reshape(-1, 3, 32, 32)
training_dataset_x = np.transpose(training_dataset_x, [0, 2, 3, 1])

test_dataset_x = test_dataset_x.reshape(-1, 3, 32, 32)
test_dataset_x = np.transpose(test_dataset_x, [0, 2, 3, 1])

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
    ]

import matplotlib.pyplot as plt
     
plt.figure(figsize=(4, 20))
for i in range(30):
    plt.subplot(10, 3, i + 1)
    plt.title(class_names[training_dataset_y[i]], pad=10)    
    plt.imshow(training_dataset_x[i])
plt.show()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential(name='CIFAR100')
model.add(Input((32, 32, 3), name='Input'))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(MaxPooling2D(name='MaxPooling2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(MaxPooling2D(name='MaxPooling2D-2'))
model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
model.add(MaxPooling2D(name='MaxPooling2D-3'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(512, activation='relu', name='Hidden-1'))
model.add(Dense(512, activation='relu', name='Hidden-2'))
model.add(Dense(100, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import glob
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')

#----------------------------------------------------------------------------------------------------------------------------
                                        58. Ders - 10/08/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında CIFAR-100 veri kümesi de tensorflow.keras.datasets paketi içerisindeki cifar100 modülünde hazır bir biçimde 
    bulunmaktadır. Aşağıda aynı örneği bu modüldeki hazır verilerle gerçekleştiriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pickle

with open('cifar-100-python/train', 'rb') as f:
   training_dataset = pickle.load(f, encoding='bytes')
   
training_dataset_x = training_dataset[b'data']
training_dataset_y = training_dataset[b'fine_labels']
    
with open('cifar-100-python/test', 'rb') as f:
   test_dataset = pickle.load(f, encoding='bytes')

test_dataset_x = test_dataset[b'data']
test_dataset_y = test_dataset[b'fine_labels']

training_dataset_x = training_dataset_x / 255
test_dataset_x = test_dataset_x / 255

import numpy as np

training_dataset_x = training_dataset_x.reshape(-1, 3, 32, 32)
training_dataset_x = np.transpose(training_dataset_x, [0, 2, 3, 1])

test_dataset_x = test_dataset_x.reshape(-1, 3, 32, 32)
test_dataset_x = np.transpose(test_dataset_x, [0, 2, 3, 1])

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)
    
class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',
    'worm']

import matplotlib.pyplot as plt
     
plt.figure(figsize=(5, 25))
for i in range(30):
    plt.subplot(10, 3, i + 1)
    plt.title(class_names[training_dataset_y[i]], pad=10)    
    plt.imshow(training_dataset_x[i])
plt.show()

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D

model = Sequential(name='CIFAR-100') 
model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), name='Conv2D-1', activation='relu'))
model.add(MaxPooling2D(name='Pooling-1'))
model.add(Conv2D(64, (3, 3), name='Conv2D-2', activation='relu'))
model.add(MaxPooling2D(name='Pooling-2'))
model.add(Conv2D(128, (3, 3), name='Conv3D-3', activation='relu'))
model.add(MaxPooling2D(name='Pooling-3'))
model.add(Flatten(name='Flatten'))
model.add(Dense(512, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(100, activation='softmax', name='Output'))

model.summary()

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(training_dataset_x, ohe_training_dataset_y, epochs=20, batch_size=32, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.xticks(range(0, 210, 10))

plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Categorical Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.xticks(range(0, 210, 10))

plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, ohe_test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
 
import glob

for path in glob.glob('test-images/*.jpg'):
    image_data = plt.imread(path)
    image_data = image_data / 255
    predict_result = model.predict(image_data.reshape(1, 32, 32, 3))
    result = np.argmax(predict_result)
    print(f'{path}: {class_names[result]}')

import itertools

for picture_data in itertools.islice(training_dataset_x[np.array(training_dataset_y) == class_names.index('bowl')], 100):
    plt.figure(figsize=(1, 1))
    plt.imshow(picture_data)
    plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Verilerin Artırılması (Data Augmentation) makine öğrenmesi ve genel olarak veri bilimi için önemli yardımcı konulardan 
    biridir. Elimizdeki eğitim veri kümesi kısıtlı olabilir. Biz de elimizdeki veri kümesinden hareketle veri kümemizi 
    büyütmek isteyebiliriz. Bu konuya genel olarak "verilerin artırılması (data augmentation)" denilmektedir. 

    Verilerin artırılması değişik veri grupları için farklı tekniklerle gerçekleştirilmektedir. Yani bu bakımdan genel tekniklerle
    değil ilgili konuya özgü tekniklerle veri artırımı yapılmaktadır. Örneğin resimsel verilerin artıırılması ile metinsel 
    verilerin artırılması farklı tekniklerle yapılmaktadır. O halde verilerin artırılması için tipik şu alt gruplar sıralanabilir:

    - Resimsel verilerin artırılması
    - Metinsel verilerin artırılması
    - İşitsel (audio) verilerin artırılması
    - Hareketli görüntü verilerinin artırılması
    - Veri tabloları biçimindeki (Boston Hausing Price veri kümesinde olduğu gibi) verilerin artırılması
    - Zamansal (temporal/time series) verilerinin artırılması

    Verilerin artırılması için ilgili framework'ler ve kütüphaneler özel sınıflar ve fonksiyonlar bulundurabilmektedir. Örneğin
    sinir ağları için kullandığımız Keras kütüphanesi ve dolayısıyla TensorFlow kütüphanesi veri artırımı için çeşitli fonksiyonlar
    ve katman sınıfları bulundurmaktadır. Aynı durum PyTorch kütüphanesi için de geçerlidir. 

    Verilerin artırılması sırasında bazı genel unsurlara dikkat edilmesi gerekir. Örneğin artırım sırasındaki "yanlılık (bias)" 
    önemli sorunlardan biridir. Veriler artırılırken onların özellikleri belli bir yöne kaydırılmamalıdır. 

    Biz bu bölümde resimsel verilerin artırılması üzerinde duracağız.
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Örneğin CIFAR-100 veri kümesinde eğitim için kullanabileceğimiz toplam 50000 resim vardır. Resimlerin sınıfları 100 tane 
    olduğuna göre her sınıf için ortalama 500 resim söz konusudur. Peki bu 500 resim ilgili resim sınıfı için genelleme 
    yapabilir mi? Örneklerimizde "categorical accuracy" değerinin %30 ile %40 arasında değişebildiğini gördük. Bu da her 100
    resmin 30 ile 40 arasındaki kısmının doğru sınıflandırıldığı diğerlerinin yanlış sınıflandırıldığı anlamına gelmektedir. 
    Bu veri kümesindeki "yengeç (crab)" resimlerini dikkate alalım. Buradai yengeçlerin bize doğru konumu değişebilmektedir. 
    Burada ters dönmüş bir yengeç yoktur. Buradaki yengeç resimleri hep dik bir açıdan elde edilmiş resimlerdir. Ancak 
    kestirim yapılırken gerçek resimlerin eğitimdeki resimlerle aynı koşulda oluşturulması mümkün olamayabilir. İşte biz bu 
    yengeç resimleri üzerinde manipülasyonlar yaparak garklı özelliklere sahip yengeç resimleri oluşturabiliriz. Veri kümesine 
    bu resimleri de dahil edebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Verilerin artırılması konusu genellikle kitapların belli bölümlerinde karışık bir biçimde ele alınmaktadır. Konusu tam 
    olarak bu olan kitapların sayısı çok azdır. Ancak bu alanda yazılmış akademik olan ve akademik olmayan çok sayıda makale
    bulmak mümkündür. Bu konuya odaklanmış az sayıda kitaptan biri "Data Augmentatiın in Python (Packt Yayınevi), Duc Haba
    (2023)" isimli kitaptır. Buradaki notlarda bu kitaptaki konu başlıklarından alıntı yapacağız. Ancak bu kitap uygulamalı 
    bir kitap değildir.   
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Resimsel verilerin artırılması için pek çok teknik kullanılmaktadır. Önemli teknikler şunlardır:

    - Resmin Çevrilmesi (Flipping): Resimlerin yatay ve düşey yönde çevrilmesiyle yeni resimlerin elde edilmesi tekniğidir. 
    Örneğin bir resimde bir kişi sola bakarken o resmi yatay biçimde çevirirsek o kişi sağa bakar hale gelir.

    - Resmin Kırpılması (Cropping): Bir resmin bir bölgesinin alınarak yeni bir resim haline getirilmesine ilişkin bir tekniktir. 
    Crop işlemi genellikle merkeze yönelik yapılır. Ancak diğer bölgeler üzerinde (özellikle merkezden kayıklık yaratarak) crop 
    işlemleri de yapılabilmektedir.

    - Yeniden Boyutlandırma (Resizing): Resmin yatay düşey oranını (aspect ratio) değiştirerek başka resimler elde edilmesine 
    yönelik tekniklerdir. Örneğin böylece bir insan daha uzun boylu, daha zayıf hale getirilebilmektedir. 

    - Resmi Tamamlama (Padding): REsmin kenarlarına ekler yaparak resmi farklılaştırma tekniğidir. 

    - Resmi Döndürme (Rotating): Resmi belirli bir açıyla döndürerek yeni resimler elde etme tekniğidir. 

    - Resmin Transpose Edilmesi (Translation): Resmin eksenlere göre değişik bir biçime dönüştürülmesi tekniğidir. Burada 
    geometrik dönüştürmeler yapılmaktadır. 

    - Gürültü Eklemesi (Noise Injection): Resme resimde olmayan gürültülerin eklenmesi tekdiğidir. Örneğin resim sanki
    bir sis içerisinde çekilmiş gibi bir etki oluşturulabilir. Resme dumanlar eklenebilir. Resimdeki netliğin bozulması 
    sağlanabilir. 

    - Resmin Zoom Edilmesi (Zooming): Resmin zoom-in ya da zoom-out yapılarak başka resimlerin elde edilmesine ilişkin tekniklerdir. 

    - Resmin Karanlık ya da Aydınlık Hale Getirilmesi (Darken and Lighten): Resmi sanki daha karanlık bir ortamda ya da 
    faha aydınlık bir ortamda çekilmiş gibi değiştirme tekniğini belirtmektedir. 

    - Resmin Saturasyonun Değiştirilmesi (Color Saturation): Resimdeki renk dougunluklarının değiştirilmesi tekniğidir. Yani 
    örneğin kırmızılar dah akırmızı, siyahlar daha siyah hale getirilebilir. 

    - Resimdeki Renklerin Ötelenmesi (Hue Shifting): Resimdeki renklerin frekanslarını değiştirip b-aşka renkler haline 
    getirilmesine ilişkin tekniklerdir. 

    - Resimdeki Bazı Renklerin Değiştirilmesi (Color Casting): Resimdeki bazı renkler başka renklerle yer değiştirilebilir. 
    Örneğin koyu beyaz daha açık beyaz yapılabilir. REsimdeki yeşil alanlar gri olarak değiştirilebilir.

    - Resimdeki Bazı Kısımların Rastgele Silinmnesi (Random Erasing): Resimdeki bazı alanların silinerek onlar yerine başka 
    dolguların kullanılmasına ilişkin tekniklerdir. 

    - Resimlerin Birleştirilmesi (Combining): Farklı küçük resimlerin bir araya getirilerek farklı bir resim haline getirilmesine
    ilişkin tekniklerdir. 

    Resimsel verilerin artırılmasında burada belirttiğimiz tekniklerin hepsinin uygulanması gerekmemektedir. Genellikle 
    uygulamacılar yalnızca birkaç tekniği kullanmaktadır. Bu teknikler uygulanırken abartıya kaçılmamalıdır. Örneğin resim 
    zoom edilirken çok küçük büyütme küçültme uygulanabilir. Resme gürültü eklenirken küçük gürültüler tercih edilebilir. 
    Resim döndürülürken küçük döndürmeler uygulanabilir. Abartılı işlemler gerçekle bağlantının kesilmesine yol açıp modelin
    performasnını düşürebilmektedir. 

    Genellikle uygulamacılar resimleri üzt üste birden fazla kez yukarıda belirttiğimiz işlemlere sokarlar. Örneğin önce bir 
    flip işlemi arkaından bir zoom işlemi arkasından bir döndürme işlemi peşi sıra yapılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki bir resim tanıma problemi söz konusu olduğunda veri artırmayı nasıl uygulamalıyız? Önce resimleri yukarıdaki 
    tekniklerle çoğaltıp onları saklamak mı yoksa eğitime sokarken onları hiç saklamadan o anda çoğaltmak mı daha iyi bir 
    yöntemdir? İşte genellikle ikinci yöntem tercih edilmektedir. Yani çoğaltma işlemi eğitimin bir önişlemi olarak eğitim 
    sırasında yapılmaktadır. Çaoğaltılmış verilerin saklanması fazlaca disk hacmi gerekterirebilmektedir. Yalnızca orijinal 
    resimlerin saklanması daha uygun bir yöntem olabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Keras'ta resimlerin artırımına ilişkin TensorFlow kütüphanesinden gelen fonksiyonlar ve sınıflar bulunmaktadır. Bu amaçla 
    keras.layers modülünde bulundurulmuş olan katman nesneleri şunlardır:
    
    RandomFlip
    RandomRotation
    RandomZoom
    RandomContrast
    RandomCrop
    RandomBrightness
    RandomTranslation
    Resize
    Rescaling

    RandomFlip katmanı "horizontol", "vertical" ya da "horizontal_and_vertical" değerlerini parametre olarak almaktadır. Resmi
    rastgele yatay, düşey ya da her iki yönde tam çevirmektedir. RandomRtotation katmanı parametre olarak maksimum radyan cinsinden 
    dönüş açısı alır. Resmi ratgele bu maksimum açıyı geçmeyecek biçimde döndürür. RandomZoom makismum zoom faktörünü parametre 
    olarak almaktadır. Sıfırdan büyük değerler zoom-in sıfırdan küçük değerler zoom-out anlamına gelir. Bu katman resmi bu 
    maksimum değeri dikkate alarak rastgele biçimde zoom eder. RandomContrast resmin kontrastını rastgele bir biçimde değiştirmek
    için kullanılmaktadır. RandomCrop ise resmin rastgele bir bölgesini elde etmekte kullanılmaktadır. Ancak RandomCrop belli bir 
    em-boy parametresi almaktadır. Resim rastgele bir biçimde bizim istediğimiz en-boy halinde crop edilmektedir. Tabii bizim
    bu işlem sonucunda resmi yeniden Resize sınıfı ile eski büyüklüğüne getirmemiz gerekir. RandomBrightness katmanı ise resmin
    açıklık-koyuluk durumunu rastgele değiştirmektedir. Yani bu katman sayesinde resin sanki farklı ışık şiddetleri altında (gece, 
    akşam, öğleni sabah) çekilmiş gibi bir etki oluşmaktadır. RandormTranslation koordinat eksininde rastgele dönüüştürmeler 
    yapmaktadır. Resize ve Rescaling sınıfları sırasıyla resmin boyutunu değiştirmek için ve ölçekleme yapmak için kullanılmaktadır. 
    Tabii biz ölçeklemeyi resmin tüm pizellerini 255'e bölerek daha önceden yapmışsak bu Rescaling katmanına gereksinim duymayız. 
    Ancak bu işlem bu sınıfla bir katman nesnesi olarak da gerçekleştirilebilmektedir. 

    Aşağıda bu sınıfların yarattığı etkileri gösteren bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt

picture = plt.imread('Sample-Pictures/AbbeyRoad.jpg')
plt.figure(figsize=(9, 16))
plt.imshow(picture);
plt.show()

from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, Rescaling, RandomCrop, Resizing, 
    RandomContrast, RandomBrightness

rf = RandomFlip('horizontal')
result = rf(picture).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

rr = RandomRotation(0.1)
result = rr(picture).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

rz = RandomZoom(0.2)
result = rz(picture).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

rs = Rescaling(0.50)
result = rs(picture).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

rc = RandomCrop(500, 500)
result = rc(picture).numpy()
plt.figure(figsize=(9, 16))
rs = Resizing(1000, 1000)
result = rs(result).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

rc = RandomContrast(0.2)
result = rc(picture).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

rc = RandomBrightness(0.7)
result = rc(picture).numpy()
plt.figure(figsize=(9, 16))
plt.imshow(result.astype('uint8'));
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
                                            59. Ders - 18/08/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de yukarıdaki augmentation katman nesnelerini daha önce yapmış olduğumuz CIFAR-100 veri kümesinde kullanalım. Aslında
    tek yapacağımız şey Input katmanından sonra bu katman nesnelerini modele eklemektir. Örneğin:
    
    model = Sequential(name='CIFAR100')
    model.add(Input((32, 32, 3), name='Input'))
    model.add(RandomFlip('horizontal'))
    model.add(RandomRotation(0.1))
    model.add(RandomZoom(0.2))
    model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
    model.add(MaxPooling2D(name='MaxPooling2D-1'))
    model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
    model.add(MaxPooling2D(name='MaxPooling2D-2'))
    model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
    model.add(MaxPooling2D(name='MaxPooling2D-3'))
    model.add(Flatten(name='Flatten'))    
    model.add(Dense(512, activation='relu', name='Hidden-1'))
    model.add(Dense(512, activation='relu', name='Hidden-2'))
    model.add(Dense(100, activation='softmax', name='Output'))
    model.summary()

    Burada Input katmanından sonra aşağıdaki üç augmentation katmanı modele eklenmiştir:

    model.add(RandomFlip('horizontal'))
    model.add(RandomRotation(0.1))
    model.add(RandomZoom(0.2))

    Böylece aslında her epoch'ta her resim rastgele bir biçimde çevrilip, döndürülüp zoom edilmektedir. Tabii bu biçimdeki 
    uygulamalarda artık eğitimdeki epoch sayısını artırmalıyız. Çünkü artık her epoch'ta aslında aynı veri kümesi işleme 
    sokulmamaktadır. Rastgelelikten dolayı farklı veri kümeleri işleme sokulmaktadır. Bu tür veri artırma işlemlerinde 
    artık veri kümesine çok fazla epoch uygulamalıyız. Çünkü epoch'lar sırasında aslında gerçek veri kümesinin aynısı değil
    rastgele biçimleri işleme sokulmaktadır. Eğer bu tür modellere az epoch uygularsak modelin başrısını büyütmek bir yana
    muhtemelen düşürmüş oluruz. 

    Aşağıda CIFAR-100 örneğinin augmentation uygulanmış biçimini veriyoruz. Burada EPOCHS sayısını 1000 olarak ayarladık. 
    Ancak bu örneği denerken bu sayıyı düşürebilirsiniz. Bu tür modellerede eğitim zamanı çok uzayacağı için bulut (cloud) 
    sistemlerinden faydalanılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import glob

EPOCHS = 5

from tensorflow.keras.datasets import cifar100

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar100.load_data()

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
    ]

import matplotlib.pyplot as plt
     
plt.figure(figsize=(4, 20))
for i in range(30):
    plt.subplot(10, 3, i + 1)
    plt.title(class_names[training_dataset_y[i, 0]], pad=10)    
    plt.imshow(training_dataset_x[i])
plt.show()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, RandomFlip, RandomRotation, RandomZoom, Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential(name='CIFAR100')
model.add(Input((32, 32, 3), name='Input'))
model.add(RandomFlip('horizontal'))
model.add(RandomRotation(0.1))
model.add(RandomZoom(0.2))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(MaxPooling2D(name='MaxPooling2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(MaxPooling2D(name='MaxPooling2D-2'))
model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
model.add(MaxPooling2D(name='MaxPooling2D-3'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(512, activation='relu', name='Hidden-1'))
model.add(Dense(512, activation='relu', name='Hidden-2'))
model.add(Dense(100, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')

#----------------------------------------------------------------------------------------------------------------------------
    Aslında Keras'ta veri artırma (data augmentation) işlemleri programcı tarafından özelleştirilerek (customize edilerek) de
    gerçekleştirilebilmektedir. Ancak bu işlemlerde TensorFlow kütüphanesinin başka özelliklerinin de kullanılması gerekmektedir. 
    Biz henüz TensorFlow kütüphanesinin taban kullanımını görmediğimiz için burada bu konu üzerinde durmayacağız. Bu özelleştirme 
    sürecinde Keras'ın tensorflow.keras.preprocessing.image modülündeki fonksiyonlardan faydalanılmaktadır. Uygulamacı bu fonksiyonları 
    manuel biçimde kullanabilir ya da bu fonksiyonlardan kendi özelleştirilmiş (custom) katman nesneleri oluşturabilir. Bu 
    fonksiyonların doküsmantasyonuna aşağıdaki bağlantıdan erişebilirsiniz:

    https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                    60. Ders - 07/09/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir modeli eğtirken ne kadar epoch uygulamak gerekir? Epoch uygularken şu durumları göz önüne almalıyız?

    - Modeldeki loss ya da metrik değerler iyileşmedikten sonra (örneğin loss değeri düşmedikten sonra) fazla epoch uygulamanın 
    bir yararı olmadığı gibi zararı olabilmektedir. Yani ne kadar çok epoch ygulanırsa daha iyi sonucun elde edileceği gibi 
    bir yargı doğru değildir. Modeli iyileştirmeyen epoch'ların uygulanması overfitting sorunlarına yol açabilmektedir. 

    - Modeli eğitirken eğitimdeki loss ya da metrik değerlerin sınamadaki loss ya da metrik değerlerden kopması (yani biri 
    iyileşirken diğerinin iyileşmemesi) epoch kaynaklı bir overfitting oluşumuna yol açabilmektedir. 

    - Modelin eğitilmesi sırasında loss ya da metrik değerler dalgalanabilmektedir. Bu dalgalanmanın kötü bir noktasında 
    epoch'lar bittiğinden dolayı eğitimin sonlanması da arzu edilen bir durum değildir. Çünkü modelde geçmiş epoch'larda daha 
    iyi değerler oluştuğu halde son durumda daha kötü değerler oluşmuş durumdadır.

    Peki bu durumda uygun epoch sayısı nasıl belirlenmelidir? Yöntemlerden biri modeli yüksek bir epoch sayısı ile eğitip 
    loss ve metirk değerleri gözle inceleyerek uygun epoch değerinin ne olacağına gözle karar vermek olabilir. Tabii bu yöntemin
    kusurları vardır. Bu yöntemde epoch sayısı gözle tespit edilip modelin eğitilmesi uzun eğitim zamanına yol açabilir. Dalgalı
    durumlarda bu yöntem genellikle çalışmaz. Çünkü her eğitimde birtakım değerlerin rastgele alınması nedeniyle dalgalanmalar
    değişebilmektedir. Gözle belirleme yöntemi yerine her epoch'ta callback mekanizması yoluyla uygulamacının değerlere bakıp 
    modeli manuel bir biçimde sonlandırması daha iyi bir yöntemdir. Biz Keras'taki callback mekanizmalarını daha önce görmüştük.
    Ancak bu işlemler için kullanılabilecek iki hazır callback sınıfı da bulundurulmuştur. Bu sınıflar EarlyStopping ve 
    ModelCheckpoint isimli sınıflardır. İzleyen paragraflarda bu sınıfların kullanımları üzerinde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    EarlyStopping callback sınıfının amacı loss ya da metrik değerlerde istenilen kadar iyileşmenin sağlanmadığı durumlarda 
    eğitimin otomatik sonlandırılmasını sağlamaktır. Normal olarak epoch'lar sırasında loss ve metrik değerlerin iyileşmesi 
    beklenir. Yukarıda da belirttiğimiz gibi bu değerlerin iyileşmemesi durumunda eğitime devam etmek iyi bir fikir değildir. 
    EarlyStopping callback sınıfının __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        min_delta=0,
        patience=0,
        verbose=0,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )

    Burada monitor parametresi izlenecek metrik değeri belirtir. Loss ya da metrik değerin başında "val_" öneki varsa bunun 
    sınamaya ilişkin değer olduğu kabul edilmektedir. Örneğin bu parametreye "loss" değeri girilirse bu eğitimdeki loss değerini 
    "val_loss" girilirse bu dasınamadaki loss değerini belirtmektedir. Örneğin sınamadaki accuracy metrik değeri için bu 
    parametreye "val_accuracy" girilmelidir. min_delta parametresi iyileşme için minimum aralığı belirtmektedir. (Örneğin bu 
    değer "val_loss" için 0.01 girilirse ancak 0.01'den daha fazla bir düşüş iyileşme kabul edilir.) patience parametresi üst 
    üste kaç kez iyileşme olmazsa eğitimin sonlandırılacağını belirtir. Buraya tipik olarak 3, 5 gibi değerler girilebilir. 
    verbose parametresi 1 girilirse ekrana bilgi yazıları basılır. verbose parametresi 0 ya da 1 biçiminde girilebilir. Eğer
    bu parametre 1 olarak girilirse ekrana daha fazla bilgi yazısı çıkartılmaktadır. mode parametresi ise "min", "max" ya da 
    "auto" biçiminde girilebilir. "min" iyileşmenin düşüşle sağlandığını, "max" iyileşmenin yükselişle sağlandığını belirtir. 
    "auto"" ise monitor parametresine göre bunun otomatik belirleneceği anlamına gelmektedir.  baseline parametresi sonlandırma 
    için eşik değerin belirlenmesini sağlamaktadır. restore_best_weights parametresi True geçilirse eğitim sonlandırılana kadar 
    en iyi loss ya da metrik değerin bulunduğu epoch'a ilişkin nöron ağırklık değerleri modele set edilir. Bu parametre False 
    geçilirse (default durum) modelin sonlandırılması sırasındaki değerler model nesnesinde bırakılır. start_from_epoch
    parametresi yeni versiyonlarda eklenmiştir. Bu parametre bu mekanizmanın kaçıncı epoch'tan itibaren başlatılacağını 
    belirtmektedir. Örneğin:

    esc = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=EPOCHS, 

    Burada "val_loss" değerinde üst üste 3 kez iyileşme olmadığnda eğitim otomatik sonlandırılacaktır.

    Aşağıdaki örnekte Boston Housing Prices veri kümesinde "val_loss" metrik değeri üst üste 3 kez iyileşmediği zaman eğitim 
    sonlandırılmıştır. restore_best_weights=True yapıldığı için model son epoch'taki ağırlık değerleriyle değil tüm epoch'lar 
    arasındaki en iyi ağırlık değeriyle set edilecektir. Bu programı çalıştırdığımızda aşağıdaki gibi bir çıktı elde edilmiştir:

    ...
    val_loss: 16.5277 - val_mae: 2.9099
    Epoch 17/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 15.6860 - mae: 2.7044 - val_loss: 16.0524 - val_mae: 2.8168
    Epoch 18/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 15.7328 - mae: 2.6971 - val_loss: 16.0958 - val_mae: 2.8007
    Epoch 19/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 19.8981 - mae: 2.8945 - val_loss: 15.7974 - val_mae: 2.7772
    Epoch 20/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 13.1439 - mae: 2.5510 - val_loss: 17.2526 - val_mae: 2.8939
    Epoch 21/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 14.5947 - mae: 2.5541 - val_loss: 15.7460 - val_mae: 2.7190
    Epoch 22/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 11.9902 - mae: 2.4701 - val_loss: 17.3226 - val_mae: 2.8476
    Epoch 23/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 12.7140 - mae: 2.4168 - val_loss: 17.7918 - val_mae: 2.8993
    Epoch 24/200
    12/12 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 15.0356 - mae: 2.5725 - val_loss: 16.6013 - val_mae: 2.6930
    Epoch 24: early stopping
    Restoring model weights from the end of the best epoch: 21.

    Burada epoch'lardaki "val_loss" değerlerini inceleyiniz. Bu "val_loss" değerleri üst üste 3 kez iyileşmediğinde eğitim
    sonlandırılmıştır ve en iyi değere ilişkin ağırlıklar modele yüklenmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

EPOCHS = 200

df = pd.read_csv('housing.csv', delimiter=r'\s+', header=None)

highway_class = df.iloc[:, 8].to_numpy()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
ohe_highway = ohe.fit_transform(highway_class.reshape(-1, 1))

dataset_y = df.iloc[:, -1].to_numpy()
df.drop([8, 13], axis=1, inplace=True)
dataset_x = pd.concat([df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = \
        train_test_split(dataset_x, dataset_y, test_size=0.1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential(name='Boston-Housing-Prices')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(64, activation='relu', name='Hidden-1'))
model.add(Dense(64, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile('rmsprop', loss='mse', metrics=['mae'])

esc = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=EPOCHS, 
                validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Mean Absolute Error - Validation Mean Absolute Error Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-boston-housing-prices.csv', delimiter=r'\s+', header=None)

highway_class = predict_df.iloc[:, 8].to_numpy()
ohe_highway = ohe.transform(highway_class.reshape(-1, 1))
predict_df.drop(8, axis=1, inplace=True)
predict_dataset_x = pd.concat([predict_df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()
scaled_predict_dataset_x = ss.transform(predict_dataset_x )
predict_result = model.predict(scaled_predict_dataset_x)

for val in predict_result[:, 0]:
    print(val)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte CIFAR-10 veri kümesinde "val_categorical_accuracy" metirk değeri üst üste 5 kez iyileşmediği zaman eğitim 
    sonlandırılmıştır. Bu örnekte de restore_best_weigts=True yapıldığı için model en iyi ağırlık değerleriyle set edilecektir. 
    Programı çalıştırdığımızda aşağıdaki gibi bir çıktı elde edilmiştir:

    ...
    Epoch 11/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 14s 11ms/step - categorical_accuracy: 0.8262 - loss: 0.5247 - 
    val_categorical_accuracy: 0.6999 - val_loss: 1.1722
    Epoch 12/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 15s 12ms/step - categorical_accuracy: 0.8330 - loss: 0.5094 - 
    val_categorical_accuracy: 0.7127 - val_loss: 1.0260
    Epoch 13/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 14s 11ms/step - categorical_accuracy: 0.8380 - loss: 0.4906 - 
    val_categorical_accuracy: 0.6487 - val_loss: 1.5351
    Epoch 14/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 14s 11ms/step - categorical_accuracy: 0.8349 - loss: 0.4990 - 
    val_categorical_accuracy: 0.7041 - val_loss: 1.2637
    Epoch 15/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 14s 11ms/step - categorical_accuracy: 0.8427 - loss: 0.4849 - 
    val_categorical_accuracy: 0.7015 - val_loss: 1.2270
    Epoch 16/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 14s 11ms/step - categorical_accuracy: 0.8454 - loss: 0.4817 - 
    val_categorical_accuracy: 0.6969 - val_loss: 1.2923
    Epoch 17/200
    1250/1250 ━━━━━━━━━━━━━━━━━━━━ 14s 12ms/step - categorical_accuracy: 0.8418 - loss: 0.4924 - 
    val_categorical_accuracy: 0.6973 - val_loss: 1.7678
    Epoch 17: early stopping
    Restoring model weights from the end of the best epoch: 12.
#----------------------------------------------------------------------------------------------------------------------------

import glob

EPOCHS = 200

from tensorflow.keras.datasets import cifar10

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar10.load_data()

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
  
import matplotlib.pyplot as plt
     
plt.figure(figsize=(4, 20))
for i in range(30):
    plt.subplot(10, 3, i + 1)
    plt.title(class_names[training_dataset_y[i, 0]], pad=10)    
    plt.imshow(training_dataset_x[i])
plt.show()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential(name='CIFAR10')
model.add(Input((32, 32, 3), name='Input'))
model.add(Conv2D(32, (3, 3), activation='relu', name='Conv2D-1'))
model.add(MaxPooling2D(name='MaxPooling2D-1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='Conv2D-2'))
model.add(MaxPooling2D(name='MaxPooling2D-2'))
model.add(Conv2D(128, (3, 3), activation='relu', name='Conv2D-3'))
model.add(MaxPooling2D(name='MaxPooling2D-3'))
model.add(Flatten(name='Flatten'))    
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(10, activation='softmax', name='Output'))
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])

esc = EarlyStopping(monitor='val_categorical_accuracy', patience=5, restore_best_weights=True, verbose=1)
hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, 
        epochs=EPOCHS, validation_split=0.2, callbacks=[esc])

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')
 
#----------------------------------------------------------------------------------------------------------------------------
    ModelCheckpoint sınıfı epoch'lar sırasında modelin belli durumlarda save edilmesi için kullanılmaktadır. Sınıfın __init__ 
    metodunun parametrik yapısı şöyledir:

    tf.keras.callbacks.ModelCheckpoint(
        filepath,
        monitor='val_loss',
        verbose=0,
        save_best_only=False,
        save_weights_only=False,
        mode='auto',
        save_freq='epoch',
        initial_value_threshold=None
    )

    Metodun birinci parametresi modelin save edileceği dosyanın yol ifadesini alır. Bu parametredeki isim formatlı (yani kalıp 
    içeren biçimde) olabilmektedir. Metot birden fazla save işlemi yapacaksa bu parametrede dosya ismi kalıp içeren biçimde 
    kullanılmalıdır. İkinci parametre yine izlenecek loss ya da metrik değeri belirtmektedir. Yani bu parametre save işleminin
    hangi loss ya da metrik değere dayalı olarak yapılacağını belirtmektedir. Yine verbose parametresi 1 geçilirse daha fazla 
    bilgi ekrana yazdırılmaktadır. Metodun save_best_only parametresi True girilirse yalnızca en iyi model save edilir. mode 
    parametresi yine EarlyStopping sınıfındaki gibidir. save_weights_only parametresi default durumda False biçimdedir. Bu 
    parametre True geçilirse tüm model değil yalnızca katmanlardaki nöron ağırlıkları save edilir. Örneğin bizim amacımız 
    val_loss değerinin en iyi olduğu durumdaki modeli save etmekse ModelCheckpoint nesnesini aşağıdaki gibi yaratabiliriz:

    mcp = ModelCheckpoint('boston-checkpoint.keras', monitor='val_loss', save_best_only=True)

    Burada save_best_only parametresi True girildiği için yalnızca en iyi model save edilecektir. 
    
    Bu callback sınıfının amacı eğitimi erkenden sonlandırmak değildir. Ancak tabii bu callback sınıfı EarlyStopping callback 
    sınıfıyla birlikte de kullanılabilir. Metot aslında birden fazla save işlemi yapabilecek biçimde tasarlanmıştır. Ancak bunun 
    için metodun birinci parametresine bir kalıp girilmelidir. Eğer metodun birinci parametresine bir kalıp girilirse ve metodun 
    save_best_only parametresi False geçilirse tüm epoch'lardaki ağırlıklar formatlama kalıba uygun dosya isimleri ile save edilir. 
    Eğer save_best_only parametresi True geçilirse bu durumda yalnızca daha öncekine göre daha iyi olan epoch değerleri kalıba 
    uygun dosya isimleriyle save edilmektedir. Eğer dosya isminde bir kalıp kullanılmazsa bu durumda save_best_only parametresi 
    False geçilirse son epoch'taki değerler save edilir. Eğer dosya isminde kalıp kullanılmazsa fakat save_best_only parametresi 
    True geçilirse bu durumda en iyi model save edilmiş olacaktır. Başka bir deyişle dosya ismindeki kalıp aslında "save işlemini 
    başka bir dosya üzerinde yap" anlamına gelmektedir. Bu durumu özetle şöyle ifade edebiliriz:

    - Metodun birinci parametresine kalıp girilirse ve save_best_only parametresi False geçilirse: Bu durumda her epoch'ta
    kalıba uygun save işlemi yapılmaktadır.

    - Metodun birinci parametresine kalıp girilirse ve save_best_only parametresi True geçilirse: Bu durumda yalnızca daha 
    öncekine göre daha iyi olan epoch değerleri kalıba uygun dosya isimleriyle save edilmektedir. 

    - Metodun birinci parametresine kalıp girilmezse ve save_best_only parametresi False geçilirse: Bu durumda son epoch'taki 
    değerler save edilir. 

    Metodun birinci parametresine kalıp girilmezse ve save_best_only parametresi True geçilirse: Bu durumda yalnızca en iyi 
    monitor değerleri save edilir.

    Kalıp olulştururken "{epoch}" ifadesi o andaki epoch değerini temsil eder. Örneğin "{epoch:03d}" gibi bir kalıp epoch 
    değerini iki basamak olarak (tek basamaksa 0 ile doldurarak)" oluşturma anlamına gelir. Diğer kalıp ifadeleri için sınıfın 
    dokümanlarına başvurabilirsiniz. Örneğin:

    mcp = ModelCheckpoint('boston-checkpoint-{epoch:03d}.keras', monitor='val_loss', save_best_only=True)

    Burada "val_loss" metrik değeri daha öncekilere göre iyi olan epoch ile karşılaşıldığında "boston-checkpoint-NNN" gibi 
    (burada NN epoch numarasını belirtir) bir dosyaya save işlemi yapılacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        61. Ders - 08/09/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte "Boston Housing Prices" veri kümesinde EarlyStopping ve ModelCheckpoint sınıfları bir arada kullanılmıştır. 
    Kodun ilgili kısmı şöyledir:

    mcp = ModelCheckpoint('Boston-Housing-{epoch:03d}.keras', monitor='val_loss', save_best_only=True)
    esc = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1, mode='min')

    hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=EPOCHS, 
                     validation_split=0.2, callbacks=[mcp, esc])

    Burada "val_loss" değerinde her yeni iyileşmede model "Boston-Housing-NNN.keras" ismiyle save edilecektir. Aynı zamanda
    5 kez üst üste "val_loss" değeri iyileşmediği takdirde eğitim sonlandırılacaktır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

EPOCHS = 200

df = pd.read_csv('housing.csv', delimiter=r'\s+', header=None)

highway_class = df.iloc[:, 8].to_numpy()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse=False)
ohe_highway = ohe.fit_transform(highway_class.reshape(-1, 1))

dataset_y = df.iloc[:, -1].to_numpy()
df.drop([8, 13], axis=1, inplace=True)
dataset_x = pd.concat([df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

model = Sequential(name='Boston-Housing-Prices')
model.add(Input((training_dataset_x.shape[1], ), name='Input'))
model.add(Dense(64, activation='relu', name='Hidden-1'))
model.add(Dense(64, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile('rmsprop', loss='mse', metrics=['mae'])

mcp = ModelCheckpoint('Boston-Housing-{epoch:03d}.keras', monitor='val_loss', save_best_only=True)
esc = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1, mode='min')

hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=EPOCHS, 
        validation_split=0.2, callbacks=[mcp, esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Mean Absolute Error - Validation Mean Absolute Error Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-boston-housing-prices.csv', delimiter=r'\s+', header=None)

highway_class = predict_df.iloc[:, 8].to_numpy()
ohe_highway = ohe.transform(highway_class.reshape(-1, 1))
predict_df.drop(8, axis=1, inplace=True)
predict_dataset_x = pd.concat([predict_df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()
scaled_predict_dataset_x = ss.transform(predict_dataset_x )
predict_result = model.predict(scaled_predict_dataset_x)

for val in predict_result[:, 0]:
    print(val)

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu noktasında "Aktarım Öğrenmesi (Transfer Learning)" konusuna bir giriş yapacağız. Sonraki bölümlerde aktarım 
    öğrenmesini çeşitli biçimlerde kullanacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aktarım öğrenmesi (transfer learning) psikolojiden aktarılmış bir terimdir. Psikolojide aktarım öğrenmesi "daha önce öğrenilmiş
    olan şeylerin başka öğrenmeleri etkilemesi sürecini" belirtmektedir. Örneğin İngilizce bilen bir kişi Almanca'yı (farklı 
    diller olduğu halde) daha kolay öğrenebilmektedir. Psikolojide aktarım öğrenmesi pozitif ya da negatif olabilmektedir. 
    Eğer önceden öğrenilen malzemeler sonradan öğrenilecekleri olumlu biçimde destekliyorsa buna "pozitif aktarım" olumsuz 
    bir biçimde etkiliyorsa buna da "negatif aktarım" denilmektedir. Örneğin Q-Klavyede yazan bir kişinin F-Klavyeye geçmesi
    hiç klavye kullanmamış kişilere göre daha zor olabilmektedir. İşte makine öğrenmesinde "aktarım öğrenmesi" de psikolojide 
    olduğu gibi önceden öğrenilmiş malzemenin sonraki öğrenmede olumu bir biçimde kullanılması anlamına gelmektedir. Aktarım 
    öğrenmesi sayesinde önceden eğitilmiş (pretrained) ağların başka amaçlarla kullanılması sağlanmaktadır. Örneğin çok geniş 
    bir resim veritabanı kullanılarak sınıflandırma amacıyla bir eğitim yapılmış olabilir. Bu eğitimdeki nöron ağırlıkları 
    save edilmiş olabilir. Biz de kendi resim sınıflandırmamızda bu eğitilmiş modelden faydalanabiliriz. Tabii buradaki 
    eğitilmiş modelin bizim hedefimize yönelik eğitilmiş olması da aslında gerekmemektedir. Örneğin eğitilmiş model resimleri 
    100 farklı sınıfa ayırmak üzere eğitilmiş olabilir. Biz bu modeli farklı sınıflar için de yine kullanabiliriz. Çünkü 
    bu tür modellerde aslında gerekli olan pek çok faaliyet (filtreleme, evirişim gibi) zaten yapılmış durumdadır. Her ne kadar 
    eğitilmiş model bizim hedeflerimiz için eğitilmemiş olsa da yine bizim modelimizde önemli faydalar sağlayabilecektir. 

    Aktarım öğrenmesi resimsel uygulamalarda, metinsel uygulamalarda, işitsel uygulamalarda yaygın bir biçimde kullanılmaktadır.
    Şüphesiz aktarım öğrenmesi konusu "önceden eğitilmiş (pre-trained)" modeller konusuyla iç içe girmiş bir konudur. Tabii biz 
    Keras'ta önceden eğitilmiş modelleri kullanamdan da başkalarının oluşturdğu modelleri kendi modelimize monte ederek 
    kullanabiliriz. 
    
    Tipik olarak önceden eğitilmiş modellerle aktarım öğrenmesi şu aşamalardan geçilerek gerçekleştirilmektedir:

    1) Aktarım öğrenmesi için uygun eğitilmiş modelin belirlenmesi: Çeşitli kurumlar tarafından farklı amaçlarla farklı modeller 
    kullanılarak önceden eğitilmiş modeller oluşturulmuştur. Bunlardan uygun olanını uygulamacının seçmesi gerekmektedir. 

    2) Önceden eğitilmiş modelin çıktısının uygulamacının özel modeline bağlanması: Genellikle önceden eğitilmiş modeller
    sinir ağının ilk katmanları olarak kullanılmaktadır. Uygulamacı kendi modeli için kendi sinir ağı katmanlarını oluşturup 
    önceden eğitilmiş modelin çıktısını kendi modeline bağlamalıdır. 

    Girdiler ---> önceden eğitilmiş model ---> uygulamacının kendi amaçları için oluşturduğu model ---> çıktılar

    3) Modelin uygulamacının hedeflerine yönelik eğitilmesi: Her ne kadar uygulamacı modelinin önüne önceden eğitilmiş modeli 
    eklemiş olsa da modelin yine uygulamacının hedeflerine yönelik eğitilmesi gerekmektedir. Yani uygulamacı yine modelini
    kendi verileriyle ayrıca eğitmelidir. Tabii şüphesiz eğer önceden eğitilmiş model zaten uygulamacının hedefleriyle tam 
    örtüşüyorsa ayrıca böyle bir eğitimin yapılmasına gerek de kalmaz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Keras'ın Sequential modelinde Sequential sınıfının add metoduyla modele katman nesnelerini ekliyorduk. Ancak bu katman 
    nesneleri hep modelin sonuna ekleniyordu. Ayrıca Sequential modelde yalnızca bir tane girdi katmanı ve yalnızca bir tane 
    çıktı katmanı bulunabiliyordu. Oysa bazı uygulamalarda girdiler ve çıktılar birden fazla çeşit olabilmektedir. Örneğin 
    ağın girdisi hem bir resim hem de bir yazı hem de birtakım sayısal verilerden oluşabilmektedir. Benzer biçimde ağın çıktısı 
    da hem bir kategorik değer hem de gerçek bir değerden oluşabilmektedir. Örneğin bir resim ve bir yazı içeren girdiler söz 
    konusu olsun. Kişi resme balkıp ilgili soruyu yanıtlıyor olsun. Burada girdi yalnızca bir resim değil aynı zamanda bir metin 
    de içermektedir. Biz şimdiye kadar yalnızca resimlerden ve yazılardan girdiler oluşturduk. Bunların ikisini bir arada 
    kullanmadık. Böyle bir modelin girdisi için iki girdi katmanının bulunuyor olması gerekmektedir. Halbuki Sequential modelde 
    modelin tek bir girdi katmanı olmak zorundadır. Benzer biçimde bazen çıktının da birden fazla olması istenebilmektedir. Örneğin 
    ağ hem bir yazının kategorisini belirleyebilir hem de yazıdaki beğeni miktarını tespit etmeye çalışabilir. Birden fazla 
    çıktı katmanına sahip olan modeller de Sequential sınıfı ile oluşturulamamaktadır. İşte bu tür gereksinimlerden dolayı 
    Sequential model yetersiz kalabilmektedir. Bu nedenle bu tür uygulamalarda daha aşağı seviyeli olan "fonksiyonel model" 
    tercih edilmektedir. Fonksiyonel model aslında TensorFlow'daki gerçek modeldir. Yani aslında TensorFlow zaten bu biçimde 
    tasarlanmış olan temel (base) bir kütüphanedir. Sequential model aslında bazı işlemleri kolaylaştırmak için düşünülmüş 
    olan yüksek seviyeli bir tasarımdır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        62. Ders - 14/09/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Asında TensorFlow'daki katman nesneleri girdiyi işleme sokup çıktı oluşturmaktadır. Bu katman nesnelerinde bu işlem ilgili 
    katman sınıfının fonksiyon çağırma operatör metodu ile (yani __call__ metodu ile) yapılmaktadır. Örneğin:

    dense1 = Dense(256, activation='relu', name='Dense-1')
    dense2 = Dense(256, activation='relu', name='Dense-2')

    Burada aslında asıl nöron işlemlerini Dense sınıfının __call__ metodu yapmaktadır. Bu __call__ metoduna nöronların girdi 
    değerleri verilir. Metot da onları nöral işlemlere sokarakbir çıktı verir. Örneğin:

    result = dense1(data)

    Şimdi bu çıktıyı biz diğer Dense katman nesnesine girdi olarak verebiliriz:

    result = dense2(result)

    Yani aslında Sequential model yukarıdaki gibi bir katmanın çıktısı diğer katmana girdi yapılarak oluşturulmuştur. Örneğin:

    inp = Input(...)
    d1 = Dense(...)
    d2 = Dense(...)
    d3 = Dense(...)
    d4 = Dense(...)

    result = d1(inp)
    result = d2(result)
    result = d3(result)
    out = d4(result)

    Yukarıdaki işlemleri daha kompakt olarak aşağıdaki gibi de yapabiliriz:

    inp = Input(...)
    result = Dense(...)(inp)
    result = Dense(...)(result)
    result = Dense(...)(result)
    out = Dense(...)(result)

    Burada önemli bir nokta üzerinde durmak istiyoruz. TensorFlow ve PyTorch gibi kütüphaneler bir çeşit "meta programlama" 
    kütüphaneleridir. Yani bu programlama modelinde önce işlemi yapacak kodlar oluşturulur. Sonra onlar çalıştırılır. Biz yukarıda
    hangi işlemlerin yapılacağını tanımlamış olduk. Ancak gerçekte henüz bu modele bir veri verip çıktısını almadık. Başka bir 
    deyişle biz yukrıda istediğimiz işlemleri yapan bir program oluşturmuş olduk. Fakat henüz onu çalıştırmadık. TensorFlow
    kütüphanesinin 2'li versiyonlarıyla birlikte "eager tensor" adı altında doğrudan çalıştırmalı tensör modeli de kütüphaneye 
    eklenmiştir. Bu konuların ayrıntıları TensorFlow kütüphanesinin anlatıldığı bölümde ele alınacaktır.

    Örneğin biz bir Dense katmanı tamamen ayrı bir biçimde işletmek isteyelim. Bu durumda TensorFlow'un 2'li versiyonlarından 
    sonra artık biz bu işlemi sanki Dense nesnesiyle fonksiyon çağırıyormuş gibi yapabiliriz. Örneğin:

    import numpy as np
    from tensorflow.keras.layers import Dense

    data = np.random.random((32, 8))

    d = Dense(16, activation='relu', name='Dense')
    result = d(data).numpy()

    Katman nesnelerinin bir grup satırı (batch) alıp işlem yaptığını anımsayınız. Yani biz Dense katmana tek bir satırı değil 
    bir grup satırı girdi olarak vermeliyiz. Yukarıdaki örnekte her biri 8 sütundan 32 satırdan oluşan rastgele bir NumPy dizisi 
    oluşturulup bu dizi Dense katmana verilmiştir. TensorFlow'da katman nesneleri Tensor alıp Tensor vermektedir. Ancak Tensor 
    yerine bazı katman nesneleri NumPy dizilerini de girdi olarak alabilmektedir. Örneğimizde çıktı olarak aslında bir Tensor 
    nesnesi elde edilmiştir. Biz de bu Tensor nesnesini yeniden NumPy dizisine dönüştürdük. Yukarıdaki örnekte elde ettiğimiz 
    NumPy dizisi (32, 16) boyutlarında olacaktır. 

    Fonksiyonel olarak oluşturduğumuz yapıya dikkat ediniz:

    inp = Input(...)
    result = Dense(...)(inp)
    result = Dense(...)(result)
    result = Dense(...)(result)
    out = Dense(result)

    Burada sonuçta bir girdi bir de çıktı tensörü oluşturulmuştur. İşlemlerin yapılabilmesi için bu girdi ve çıktı tensörleri ile 
    bir Model nesnesinin yaratılması gerekmektedir. Bunun için tensorflow.keras modülündeki Model sınıfı kullanılmaktadır. Model 
    sınıfının __init__ metodunun iki önemli parametresi vardır: inputs ve outputs. inputs girdi tensörünü, outputs ise çıktı çıktı 
    tensörünü almaktadır. Yukarıdaki bağlantıyı biz model nesnesi haline şöyle getirebiliriz:

    model = Model(inputs=inp, outputs=out, name='MyModel')

    Aslında burada oluşturmaya çalıştığımız modelin Sequential eşdeğeri şöyledir:

    model = Sequential('MyModel')
    model.add(Input(...))
    model.add(Dense(...)
    model.add(Dense(...))
    model.add(Dense(...))   
    model.add(Dense(...))

    Aslında asıl olan model fonksiyonel modeldir. Sequential sınıfı fonksiyonel model kullanılarak yazılmış olan yüksek seviyeli 
    yardımcı bir sınıftır. Ancak önceki paragraflarda da belirttiğimiz gibi Sequential model bazı uygulamalarda yetersiz kalmaktadır. 
    Yani aslında Sequential sınıfında add işlemi yapıldıkça yukarıdaki gibi fonksiyonel modele fonksiyon çağırma operatöryle 
    eklemeler yapılmaktadır. Bir fikir vermesi için Sequential sınıfının aşağıdaki biçimde yazılmış olduğunu varsayabilirsiniz:

    class Sequential:
        def __init__(self):
            self.result = None
        
        def add(self, layer):
            if self.result is None:
                self.inp = layer
                self.result = self.inp
            else:
                self.result = self.result(layer)
                
        def compile(self, *args):
            self.model = Model(inputs=self.inp, outputs=self.result)
            # ....
    
    Tensör kavramı ve konunun ayrıntıları kurusumuzu TensorFlow kütüphanesinin anlatıldığı bölümde ele alınacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi daha önce yapmış olduğumuz "Zambak (iris)" örneğini fonksiyonel modelle yeniden yapalım. Model şöyle kurulabilir:

    from tensorflow.keras import Model
    from tensorflow.keras.layers import Input, Dense

    inp = Input((training_dataset_x.shape[1], ), name='Input')
    result = Dense(64, activation='relu', name='Hidden-1')(inp)
    result = Dense(64, activation='relu', name='Hidden-2')(result)
    out = Dense(dataset_y.shape[1], activation='softmax', name='Output')(result)

    model = Model(inputs=inp, outputs=out, name='FunctionalModel')

    Görüldüğü gibi modelde bir girdi katmanı iki saklı katman ve bir de çıktı katmanı bulunmaktadır. Aşağıda örneğin tamamı
    verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('Iris.csv')

dataset_x = df.iloc[:, 1:-1].to_numpy(dtype='float32')

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output= False)
dataset_y = ohe.fit_transform(df.iloc[:, -1].to_numpy().reshape(-1, 1))

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)

from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input

inp = Input((training_dataset_x.shape[1], ), name='Input')
result = Dense(64, activation='relu', name='Hidden-1')(inp)
result = Dense(64, activation='relu', name='Hidden-2')(result)
out = Dense(dataset_y.shape[1], activation='softmax', name='Output')(result)

model = Model(inputs=inp, outputs=out, name='FunctionalModel')
model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

scaled_test_dataset_x = ss.transform(test_dataset_x)
eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_dataset_x = pd.read_csv('predict-iris.csv').to_numpy(dtype='float32')
scaled_predict_dataset_x = ss.transform(predict_dataset_x)

import numpy as np

predict_result = model.predict(scaled_predict_dataset_x)
predict_indexes = np.argmax(predict_result, axis=1)

for pi in predict_indexes:
    print(ohe.categories_[0][pi])

"""
predict_categories = ohe.categories_[0][predict_indexes]
print(predict_categories)
"""

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda daha önce yapmış olduğumuz "Boston Housing Prices" örneği fonksiyonel bir biçimde oluşturulmuştur. Modelin 
    oluşturulma biçimi önceki örnekle benzerdir:

    from tensorflow.keras import Model
    from tensorflow.keras.layers import Dense, Input

    inp = Input((training_dataset_x.shape[1], ), name='Input')
    result = Dense(64, activation='relu', name='Hidden-1')(inp)
    result = Dense(64, activation='relu', name='Hidden-2')(result)
    out = Dense(1, activation='linear', name='Output')(result)

    model = Model(inputs=inp, outputs=out, name='BostonHousingPrices')
    model.summary()

#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('housing.csv', delimiter=r'\s+', header=None)

highway_class = df.iloc[:, 8].to_numpy()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
ohe_highway = ohe.fit_transform(highway_class.reshape(-1, 1))

dataset_y = df.iloc[:, -1].to_numpy()
df.drop([8, 13], axis=1, inplace=True)
dataset_x = pd.concat([df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)

from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input

inp = Input((training_dataset_x.shape[1], ), name='Input')
result = Dense(64, activation='relu', name='Hidden-1')(inp)
result = Dense(64, activation='relu', name='Hidden-2')(result)
out = Dense(1, activation='linear', name='Output')(result)

model = Model(inputs=inp, outputs=out, name='BostonHousingPrices')
model.summary()

model.compile('rmsprop', loss='mse', metrics=['mae'])
hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Mean Absolute Error - Validation Mean Absolute Error Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()


eval_result = model.evaluate(scaled_test_dataset_x , test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

"""
import pickle

model.save('boston-housing-prices.h5')
with open('boston-housing-prices.pickle', 'wb') as f:
    pickle.dump([ohe, ss], f)
"""

predict_df = pd.read_csv('predict-boston-housing-prices.csv', delimiter=r'\s+', header=None)

highway_class = predict_df.iloc[:, 8].to_numpy()
ohe_highway = ohe.transform(highway_class.reshape(-1, 1))
predict_df.drop(8, axis=1, inplace=True)
predict_dataset_x = pd.concat([predict_df, pd.DataFrame(ohe_highway)], axis=1).to_numpy()
scaled_predict_dataset_x = ss.transform(predict_dataset_x )
predict_result = model.predict(scaled_predict_dataset_x)

for val in predict_result[:, 0]:
    print(val)
    
#----------------------------------------------------------------------------------------------------------------------------
    Bir kestirim modelinde veriler farklı duyusal alanlara ilişkin olabilir. Örneğin veri kümesindeki sütunlardan biri bir 
    yazı olabilir, diğerleri sayısal sütunlar olabilir. Bu tür veri kümelerine "çok modaliteye sahip (multimodal)" ya da "karışık 
    (mixed)" veri kümeleri de denilmektedir. ("Multimodal" sözcüğü aslında "psikoloji" ve "bişilsel bilimlerden" aktarılmış bir 
    terimdir. Buradaki "modalite"" farklı duyu organlarına hitap eden bilgiler anlamına gelmektedir.) Önceki paragraflarda da 
    belirttiğimiz gibi karışık veri kümelerinde Sequential model kullanılamamaktadır. Bu tür durumlarda mecburen fonksiyonel 
    modelin kullanılması gerekmektedir. Farklı alanlardaki girdilerin fonksiyonel modelle oluşturulabilmesi birden fazla girdi 
    katmanının bulundurulması gerekir. Tipik olarak bu girdi karmanlarına farklı işlemler uygulandıktan sonra bunlar birleştirilirler. 
    Birleştirme işlemi için Concatenate katmanı kullanılmaktadır. Concatenate katmanı yine fonksiyonel biçimde kullanılabilmektedir. 
    Örneğin:
   
    inp1 = Input(...)
    ...
    inp2 = Input(...)
    ...
    result = Concatenate()([inp1, inp2])
    result = Dense(...)(result)
    result = Dense(...)(result)
    out = Dense(...)(result)

    Aslında Concatenate katmanının yanı sıra tensorflow.keras modülünde aynı zamanda concatenate isminde bir fonksiyon da vardır. 
    Concatenate katmanı yerine concatenate fonksiyonu da kullanılabilir:

    inp1 = Input(...)
    ...
    inp2 = Input(...)
    ...
    result = concatenate([inp1, inp2])
    result = Dense(...)(result)
    result = Dense(...)(result)
    out = Dense(...)(result)

    Bu biçimde birden fazla girdi katmanının olduğu durumda Model nesnesi yaratılırken inputs parametresine girdi katmanları 
    bir liste biçiminde (liste olması şart değil)verilmelidir. Örneğin:

    model = Model(inputs=[inp1, inp2], outputs=out)

    Burada şöyle bir model oluşturulmuştur:

    inp1 ---> .... ---> 
                            Dense ---> Dense ----> Dense (output)
    inp2 ---> ... ---->
  
    Peki yukarıdaki gibi iki girişli bir modelin eğitimi, testi ve kestirimi nasıl yapılacaktır? İşte bu işlemlerde bizim 
    girdileri bir liste ile (liste olmak zorunda değil) ayrı ayrı vermemiz gerekir. 
    
    model.fit([training_dataset_x1, training_dataset_x2], training_dataset_y, ...)

    Tabii yukarıdaki gibi iki girişli bir modelde aslında x verileri de iki parçadan oluşacaktır. Burada training_dataset_x1 ve
    training_dataset_x2 veri kümeleri bu parçaları temsil etmektedir. Benzer biçimde modelin test edilmesi sırasında da evaluate 
    metodunda yine x verileri bir liste biçiminde verilmelidir:
    
    eval_result = model.evaluate([test_dataset_x1, test_dataset_x2], test_dataset_y)

    Burada test verilerinin de iki parça haline oluşturulduğuna dikkat ediniz. test_datset_x1 ve test_dataset_x2 bu parçaları
    temsil etmektedir. 
    
    Benzer biçimde kestirim işleminde de predict metodunda x verileri bir liste biçiminde (liste olmak zorunda değil) girilir.
    Örneğin:

    predict_result = model.predict([predict_dataset_x1, predict_dataset_x2])
    
    Burada predict_dataset_x1 ve predict_dataset_x2 bu parçaları temsil etmektedir. 

    Birden fazla girdiye sahip olan modellerde özellik ölçeklemesi iki model birleştirildiğinde uyumlu olacak biçimde yapılmalıdır. 
    Bunun için girişlere tür olarak aynı özellik ölçeklemesini uygulayabilirsiniz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        63. Ders - 15/09/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda hem nümerik sütunların hem de yazısal sütunların kullanıldığı bir veri kümesinde lojistik olmayan regresyon 
    örneği verilmiştir. Örnekte veri kümesinin nümerik kısmı ve yazısal kısmı farklı işlemlere sokulmuştur. Sonra bu kısımlar
    Concateb-nate katmanıyla birleştirilmiş ve Dense katmanlardan sonra çıktı katmanı elde edilmiştir. Kodun ilgili kısmı
    şöyledir:

    inp1 = Input(shape=(training_dataset_x1.shape[1], ), name='Numeric-Input')
    inp2 = Input(shape=(1, ), dtype='string', name='Text-Input')

    tv = TextVectorization(output_mode='count')
    tv.adapt(training_dataset_x2)

    result = tv(inp2)
    result = Concatenate()([inp1, result])
    result = Dense(128, activation='relu', name='Hidden-1')(result)
    result = Dense(128, activation='relu', name='Hidden-2')(result)
    out = Dense(1, activation='linear', name='Output')(result)

    model = Model(inputs=[inp1, inp2], outputs=[out], name='MixedRandomModel')
    model.summary()

    model.compile('rmsprop', loss='mse', metrics=['mae'])
    hist = model.fit([scaled_training_dataset_x1, training_dataset_x2], training_dataset_y, 
        batch_size=32, epochs=100, validation_split=0.2)

    Bu örnekte "dataset.csv" dosyası da "create-random-mixed-data.py" isimli program tarafından oluşturulmuştur. Bu programın
    oluşturduğuğu CSV dosyası aşağıdaki gibi bir görünümdedir:

    "Yaş","Gelir","Harcamalar","Kredi_Skoru","İnternet_Aboneliği","Yorum","Puan"
    56,39930,21657,733,3,"Güzel hizmet, ama daha iyi olabilir.",326
    69,33285,35347,622,19,"Harika ürünler!",889
    46,65863,15314,721,10,"Ürünler kaliteli, ama fiyatlar yüksek.",481
    32,46704,28840,740,14,"Yine de memnun kaldım.",770
    60,48705,18662,683,19,"Yine de memnun kaldım.",803
    25,30555,27754,705,11,"Beklentilerimin altında.",930
    38,51323,17629,663,21,"Güzel hizmet, ama daha iyi olabilir.",946
    56,96922,21324,724,6,"Beklediğimden daha iyi.",376
    36,83915,35247,651,19,"Güzel hizmet, ama daha iyi olabilir.",651
    40,40619,27558,647,14,"Kaliteli, ama fiyat biraz yüksek.",14
    28,89274,26494,660,8,"İnternetten daha iyi bekliyordum.",899
    28,51225,29943,616,2,"Harika ürünler!",695
    41,56739,16692,639,22,"Harika ürünler!",480
    .....

    Kestirim için kullanılan CSV dosyasının içeriği de şöyledir:

    "Yaş","Gelir","Harcamalar","Kredi_Skoru","İnternet_Aboneliği","Yorum"
    61,37642,24599,716,18,"Yine de memnun kaldım."
    47,85556,14872,735,11,"Beklediğimden daha iyi."
    55,63760,37547,672,9,"İnternetten daha iyi bekliyordum."
    19,68192,20573,644,8,"Yine de memnun kaldım."
    38,34381,23691,616,21,"Hizmet çok iyi, teşekkürler."
    50,49558,21482,676,8,"Kaliteli, ama fiyat biraz yüksek."
    29,60782,14753,736,5,"Ürünler kaliteli, ama fiyatlar yüksek."
    39,34782,11833,690,13,"Harika ürünler!"
    61,84712,15928,705,7,"Beklentilerimin altında."
    42,49462,18645,700,21,"İnternetten daha iyi bekliyordum."
    66,90030,16816,702,5,"Yeterli bir deneyim."
    44,98519,27124,623,13,"Ürünler kaliteli, ama fiyatlar yüksek."
    59,86411,17256,672,20,"Beklentilerimin altında."
    45,85206,28764,720,16,"Yine de memnun kaldım."
    33,76950,20549,705,14,"Yeterli bir deneyim."

    Aşağıda her iki program da verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

# create-random-mixed-data.py

import pandas as pd
import numpy as np
import csv
import random

# Random veri üretimi için seed
np.random.seed(42)
random.seed(42)

num_rows = 10000

data = {
    'Yaş': np.random.randint(18, 70, size=num_rows),
    'Gelir': np.random.randint(30000, 100000, size=num_rows),
    'Harcamalar': np.random.randint(10000, 40000, size=num_rows),
    'Kredi_Skoru': np.random.randint(600, 750, size=num_rows),
    'İnternet_Aboneliği': np.random.randint(1, 25, size=num_rows),
    'Yorum': [random.choice(["Harika ürünler!", "Güzel hizmet, ama daha iyi olabilir.", 
         "Beklentilerimin altında.", "Yine de memnun kaldım.", "Ürünler kaliteli, ama fiyatlar yüksek.", 
         "Yeterli bir deneyim.", "İnternetten daha iyi bekliyordum.", "Hizmet çok iyi, teşekkürler.", 
         "Beklediğimden daha iyi.", "Kaliteli, ama fiyat biraz yüksek."]) 
         for _ in range(num_rows)],
    'Puan': np.random.randint(0, 1000, size=num_rows),
    }

df = pd.DataFrame(data)

df.to_csv('dataset.csv', quoting=csv.QUOTE_NONNUMERIC, index=False)

print("Veri kümesi 'dataset.csv' olarak kaydedildi.")

# mixed-numeric-text-random.py

import pandas as pd

dataset = pd.read_csv('dataset.csv')
dataset_x = dataset.iloc[:, :-1]
dataset_y = dataset.iloc[:, -1]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.1)

training_dataset_x1 = training_dataset_x.iloc[:, :-1].to_numpy(dtype='float32')
training_dataset_x2 = training_dataset_x.iloc[:, -1]

test_dataset_x1 = test_dataset_x.iloc[:, :-1].to_numpy(dtype='float32')
test_dataset_x2 = test_dataset_x.iloc[:, -1]

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x1)
scaled_training_dataset_x1 = ss.transform(training_dataset_x1)
scaled_test_dataset_x1 = ss.transform(test_dataset_x1)

from tensorflow.keras import Model
from tensorflow.keras.layers import Input, TextVectorization, Dense, Concatenate

inp1 = Input(shape=(training_dataset_x1.shape[1], ), name='Numeric-Input')
inp2 = Input(shape=(1, ), dtype='string', name='Text-Input')

tv = TextVectorization(output_mode='count')
tv.adapt(training_dataset_x2)

result = tv(inp2)
result = Concatenate()([inp1, result])
result = Dense(128, activation='relu', name='Hidden-1')(result)
result = Dense(128, activation='relu', name='Hidden-2')(result)
out = Dense(1, activation='linear', name='Output')(result)

model = Model(inputs=[inp1, inp2], outputs=[out], name='MixedRandomModel')
model.summary()

model.compile('rmsprop', loss='mse', metrics=['mae'])
hist = model.fit([scaled_training_dataset_x1, training_dataset_x2], training_dataset_y, 
        batch_size=32, epochs=100, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Mean Absolute Error - Validation Mean Absolute Error Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

eval_result = model.evaluate([scaled_test_dataset_x1, test_dataset_x2], test_dataset_y)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_dataset = pd.read_csv('predict.csv')

predict_dataset_x1 = predict_dataset.iloc[:, :-1]
predict_dataset_x2 = predict_dataset.iloc[:, -1]

scaled_predict_dataset_x1 = ss.transform(predict_dataset_x1)
predict_result = model.predict([scaled_predict_dataset_x1, predict_dataset_x2])

for val in predict_result[:, 0]:
    print(val)

#----------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesinde çok çıkışlı modeller çok girişli modellere göre daha seyrek kullanılmaktadır. Ancak biz burada çok 
    çıkışlı modeller üzerinde de örnek vermek istiyoruz. Çok çıkışlı modeller de yine Sequential model ile oluşturulamamaktadır. 
    Çok çıkışlı modeller ancak fonksiyonel biçimde oluşturulabilmektedir. Örneğin bir modelin iki çıktısı olabilir. Çıktıların bir 
    tanesi "olumlu", "olumsuz" biçiminde iki sınıflı kategorik bir çıktı iken diğeri bir regresyon çıktısı olabilir. Bunu şekilsel 
    olarak şöyle gösterbiliriz:

                                    Dense (sigmoid activation, binary_crossentropy loss, binary_accuracy metrics)
    Input --> Dense  ---> Dense 
                                    Dense (linear activation, mean_squared_error loss, mean_absolte_error metrics)

    Bu modelin oluşturulması aşağıdaki gibi yapılabilir:

    inp = Input(..., name='Input')
    result = Dense(..., name='Hidden-1')(inp)
    result = Dense(..., name=''Hidden-2)(result)
    out1  = Dense(..., name='Output-1')(result)
    out2  = Dense(..., name='Output-2')(result)
    
    model = Model(inputs=inp, outputs=[out1, out2])

    Burada Model nesnesi oluşturulurken outputs parametresinin iki çıkışı içeren bir liste olduğuna (liste olmak zorunda değil)
    dikkat ediniz. Peki bu model nasıl derlenecek ve eğitilecektir? 
   
    Çok çıkışlı modellerde en küçüklenmeye çalışılan loss fonksiyonu nasıl olacaktır? Bilindiği gibi loss fonksiyonu problemin 
    türüne göre farklı seçilmektedir. Örneğin modelde çıktılardan biri ikili sınıflandırmaya ilişkinse o çıktı için 
    "binary-crossentropy" loss fonksiyonunu kullanmak gerekir. Çıktılardan diğeri regresyona ilişkinse bu çıktı için de 
    "mean_squred_error" loss fonksiyonunu kullanmak gerekir. Biz şimdiye kadar compile metodunda loss fonksiyonu olarak bir 
    tane fonksiyon belirttik. Ancak aslında çok çıkışlı modellerde compile metonda her çıkış için ayrı bir loss fonksiyonu 
    verilebilmektedir. Bunun için loss parametresinde bir liste (liste olmak zorunda değil) ya da sözlük girilebilir. Eğer 
    loss fonksiyonları liste biçimde girilecekse buradaki sıranın Model nesnesi oluşturulurken outputs parametresindeki sıraya 
    uygun olması gerekmektedir. Eğer çıktılar için loss fonksiyonları sözlük nesnesi biçiminde girilecekse bu durumda sözlüğün 
    anahtarları katman nesnelerinin isimlerinden değerleri de loss fonksiyonlarının isimlerinden (ya da loss fonksiyonlarının 
    kendisinden) oluşturulur. Örneğin:

    model.compile(optimizer='rmsprop', loss=['binary_crossentropy', 'mse'], ...)

    Ya da örneğin:

    model.compile(optimizer='rmsprop', loss={'Output-1': 'binary_corssentropy', 'Output-2': 'mse'}, ...)

    Peki iki tane loss fonksiyonu olduğuna göre hangisi minimize edilmeye çalışılacaktır? Normal olarak Keras bu iki loss 
    fonksiyonun toplamanı minimize etmeye çalışmaktadır. Ancak loss fonksiyonlarının verdiği değerlerin skalaları farklı olabilir. 
    Bu durumda çıktıların önem dereceleri (yani bir çeşit ağırlıklı ortalamaları) compile metodunun loss_weights parametresi 
    ile belirtilebilmektedir. Bu parametreye ağırlıklı ortalama için gereken ağırlık değerleri verilir. Örneğin:

    model.compile(optimizer='rmsprop', loss={'Output-1': 'binary_corssentropy, 'Output-2': 'mse'}, 
            loss_weights={'Output-1': 1, 'Output-2': 0.1}, ...)

    Bu örnekte toplam loss değeri hesaplanırken birinci çıkışa ilişkin loss değeri 1 ile ikinci çıkışa ilişkin loss değeri 0.1 
    ile çarpılarak toplanmaktadır. Default durumda sanki bu ağırlık çarpanlarının 1 olduğu düşünülebilir. 

    Çok çıktılı modellerde genellikle birden fazla metrik değer kullanılır. Çünkü metrik değerler de aslında çıktının türüne 
    göre değişmektedir. İşte her çıktının metrik değerleri yine bir sözlük nesnesi biçiminde ya da bir liste listesi (liste 
    olmak zorunda değil) biçiminde belirtilebilmektedir. Örneğin:

    model.compile(optimizer='rmsprop', loss={'Output-1': 'binary_corssentropy, 'Output-2': 'mse'},
    loss_weights={'Output-1': 1, 'Output-2': 0.1}, metrics={'Output-1': ['binary_accuracy'], 'Output-2': ['mse']}, ...)

    Ya da örneğin:

    model.compile(optimizer='rmsprop', loss={'Output-1': 'binary_corssentropy, 'Output-2': 'mse'},
    loss_weights={'Output-1': 1, 'Output-2': 0.1}, metrics=[['binary_accuracy'], ['mse']], ...)

    Modelin test edilmesinde yine çıktı olarak iki test veri kümesi kullanılır. Çıktılara ilişkin y değerleri yine bir liste 
    biçiminde (liste olmak zorunda değil) girilmelidir. Örneğin:

    eval_result = model.evaluate(test_dataset_x, [test_dataset_y1, test_dataset_y2])
    for i in range(len(eval_result)):
        print(f'{model.metrics_names[i]}: {eval_result[i]}')

    predict metodu bize bir liste vermektedir. Listenin elemanları sırasıyla çıktı değerlerinden oluşmaktadır. Tabii bu çıktı 
    değerleri de aslında iki boyutlu bir NumPy dizilerş biçimindedir. Örneğin:

    predict_result = model.predict(predict_data.reshape(1, -1))

    print(predict_result[0][0, 0], predict_result[1][0, 0])

    Aşağıda örnekte bir tane girdi ve iki tane çıktı olan bir model örneği verilmiştir.  Bu modelin eğitim ve test veri 
    kümeleri rastgele değerlerle oluşturulmuştur. 
#----------------------------------------------------------------------------------------------------------------------------

TOTAL_ITEM = 1000
NFEATURES = 10

from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Dense 

inp = Input(shape=(NFEATURES, ), name='Input')
result = Dense(64, activation='relu', name='Dense-1')(inp)
result = Dense(64, activation='relu', name='Dense-2')(result)
out1 = Dense(1, activation='sigmoid', name='Output-1')(result)
out2 = Dense(1, activation='linear', name='Output-2')(result)

model = Model(inputs=inp, outputs=[out1, out2])

model.summary()

model.compile(optimizer='rmsprop', loss={'Output-1': 'binary_crossentropy', 'Output-2': 'mse'}, 
        loss_weights={'Output-1': 800.0, 'Output-2': 1.0}, metrics={'Output-1': ['binary_accuracy'], 'Output-2': ['mse']})

# generate random data

import numpy as np

dataset_x = np.random.random((TOTAL_ITEM, NFEATURES))
dataset_y1 = np.random.randint(0, 2, TOTAL_ITEM)
dataset_y2 = np.random.randint(0, 100, TOTAL_ITEM).astype('float32')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y1, test_dataset_y1, training_dataset_y2, test_dataset_y2 = \
        train_test_split(dataset_x, dataset_y1, dataset_y2, test_size = 0.2)

hist = model.fit(training_dataset_x, [training_dataset_y1, training_dataset_y2], batch_size=32, epochs=100, validation_split=0.2)

eval_result = model.evaluate(test_dataset_x, [test_dataset_y1, test_dataset_y2])
print(eval_result)

# generate random data for prediction 

predict_data = np.random.random(NFEATURES)
predict_result = model.predict(predict_data.reshape(1, -1))

print(predict_result[0][0, 0], predict_result[1][0, 0])

#----------------------------------------------------------------------------------------------------------------------------
                                                64. Ders - 21/09/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yazıların sınıflandırılması ve onlardan anlam çıkartılması için kullanılan önemli tekniklerden biri de sözcük gömme ("word 
    embedding") denilen tekniktir. Biz şimdiye kadar IMDB ve Reuters gibi örneklerde yazıları sütun sayısı tüm sözcük haznesi 
    (vocabulary) kadar olan vektörlerle temsil ettik. Bu yöntemin en önemli dezavantajları her yazının büyük bir vektörle ifade 
    edilmesi ve sözcükler arasında bağlamsal bir ilişkinin kurulamamasıydı. Biz bu tekniğe "vektörizasyon yöntemi" demiştik. 
    İşte sözcük gömme bu teknikten daha ileri bir tekniktir. Sözcük gömme yukarıda belirttiğimiz iki dezavantajı azaltmaktadır. 
    Yani bu teknikle hem sözcükler arasında anlamsal bir ilişki kurulur hem de yazılar daha kısa vektörlerle temsil edilir.   

    Sözcük gömme yönteminde yazı içerisindeki sözcüklerin (atomların) biri eşit uzunlukta gerçek değerlere sahip vektörlerle 
    ifade edilmektedir. Örneğin yazının içerisinde 100 tane sözcük olsun. Vektör uzunluğunun da 32 olduğunu varsayalım. Bu 
    durumda bu yazı 100x32 boyutunda bir matrisle temsil edilecektir. Peki her sözcüğün bir vektörle ifade edilmesinin anlamı 
    nedir? Bu vektör nasıl oluşturulmaktadır?
    
    Sözcük gömme yönteminde sözcüklere ilişkin vektörler oluşturulduktan sonra bunların arasında Öklit uzaklıkları (Eucledian 
    distances) birbirine yakın sözcüklerin daha az birbirine uzak sözcüklerin daha fazla olacağı biçimdedir. Öklit uzaklığı iki
    nokta arasındaki en kısa yola ilişkin uzaklıktır. Örneğin iki boyutlu uzayda (x1, y1) ve (x2, y2) noktaları arasındaki Öklit
    uzaklığı sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2) biçimindedir. Üç boyutlu uzayda (x1, yy1, z1) ve (x2, y2, z2) noktaları 
    arasındaki Öklit uzaklıkları ise sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2) biçiminde hesaplanmaktadır. Tabii n 
    boyutlu uzay için bu formül genelleştirilebilir. Örneğin sözcükleri iki elemanlı vektörlerle temsil edelim. Bu iki elemanlı 
    vektörler düzlemde (iki boyutlu uzayda) birer nokta belirtirler. Bu durumda birbirlerine yakın anlamdaki sözcükler düzlemde 
    birbirlerine daha yakın, uzak anlamdaki sözcükler birbirlerine daha uzak olacaktır. Tabii buradaki vektörler örneğin 32 eleman 
    uzunluğunda olursa aslında 32 boyutlu bir uzaydaki nokta gibi ele alınmaktadır. 
    
    Peki sözcük gömme işlemlerinde bu vektörler nasıl oluşturulmaktadır? Bu konuda çeşitli algoritmalar önerilmiştir. Örneğin 
    Google'ın "Word2Vec" algoritması Stanford'un "GloVe" algoritması Facebook'un "fastText" algoritması en fazla kullanılanlardandır. 
    Word2Vec algoritması 2013 yılında tasarlanmıştır. Metin anlamlandırmalarında önemli bir ilerleme sağlamıştır. Ancak Keras'ın 
    Embedding katmanı doğrudan bu algoritmaları kullanmaz. Sözcükler arasında eğitimde kullanılan metinlere dayalı bir ilişki 
    oluşturmaya çalışır. Biz burada "Word2Vec", "Glove" ve "fasText" algoritmaları üzerinde durmayacağız. Bu algoritmalar 
    Derneğimizin açmış olduğu "Üretici Ağlar, Doğal Dil İşleme ve Büyük Dil Modelleri" kursunda ayrıntılarıyla ele alınmaktadır.

    Sözcük gömme işlemleri aslında sözcüklerden anlam çıkartmaya ve onları bir bağlama oturtmaya çalışmaktadır. Bu nedenle 
    sözcük gömme işlemlerinin uygulanması yazılardan anlam çıkartılması biçimindeki faaliyetlerde önemli iyileşmeler sağlamaktadır. 
    Tabii aslında istenirse daha önce oluşturulmuş olan hazır vektörler de doğrudan kullanılabilir. Bu sayede eğitim çok daha 
    verimli biçimde yapılabilmektedir.  

    Yukarıda da belirttiğimiz gibi Keras'ta Sözcük gömme işlemleri Embedding isimli katmanla yapılmaktadır. Bu katman eğitim 
    sırasında sözcükleri eğitilen veri kümesisindeki yazılar bağlamında ele alıp ilişkilendirmektedir. Uygulamacı tipik olarak 
    ağın girdi katmanını Embedding katmanına, bu katmanın çıktılarını diğer ara katmanlara bağlamaktadır. Embedding sınıfının 
    __init__ metodunun parametreleri şöyledir:

    tf.keras.layers.Embedding(
        input_dim,
        output_dim,
        embeddings_initializer='uniform',
        embeddings_regularizer=None,
        embeddings_constraint=None,
        mask_zero=False,
        weights=None,
        lora_rank=None,
        **kwargs
    )

    Embedding katmanının ilk iki parametresi zorunlu parametrelerdir. Birinci parametre tüm yazılardaki tüm sözcüklerin (vocabulary) 
    sayısını belirtmektedir. İkinci parametre ise her sözcük için oluşturulacak vektörün uzunluğunu belirtmektedir. Genellekle 
    bu değerler 8, 16, 32, 64 biçiminde alınmaktadır. input_length parametresi yazıların sözcük uzunluğunu belirtmektedir. Burada 
    tüm yazıların aynı  miktarda sözcüklerden oluşması gerekir. (Vektörizasyon işleminde zaten yazılar farklı miktarda sözcüklerden 
    oluşsa bile girdi vektörleri vocabulary kadar olduğu için girdiler doğal olarak aynı boyutta olmaktadır.) Oysa gerçekte her 
    yazı (örneğin yorum) farklı miktarda sözcükten oluşabilmektedir. O halde uygulamacının her yazıyı sanki eşit miktarda sözcükten 
    oluşuyormuş gibi bir biçime dönüştürmesi gerekmektedir. Bunun için genellikle "padding" yöntemi kullanılmaktadır. Padding 
    eğer yazı küçükse yazının başının ya da sonunun boş sözcüklerle doldurulması işlemidir. Tabii yazı büyükse tam ters olarak 
    yazının başından ya da sonundan sözük atılmalıdır. weights parametresi ağırlık değerleri zaten bir biçimde uygulamacının 
    elinde bulunuyorsa o ağırlık değerleriyle katmanın set edilmesini sağlamaktadır. önceden belirlendiği biçimde verilmesini 
    Embedding katmanının kullanımına şöyle bir örnek verilebilir:

    ...
    model.add(Embedding(30000, 32, input_length=100))
    ...

    Burada tüm yazılardaki tüm sözcükler 30000 tanedir. Yazıdaki her sözcük 32 elemanlı bir vektörle temsil edilecektir. Her 
    yazı ise 100 sözcükten oluşacaktır. Eskiden Embedded katmanı aynı zamanda bir girdi katmanı gibi de kullanılmaktaydı. Ancak
    TensorFlow'un ileri sürümlerinde artık girdi katmanının her zaman Input katmanıyla oluşturulması yöntemi benimsenmiştir. 
    Bu nedenle artık Embedding katmanındaki input_length parametresi "deprecated" yapılmıştır. Yani artık girdi büyüklüğünün 
    Input katmanıyla verilmesi yönteminin kullanılması önerilmektedir. Bu durumda Embedding katmanı aşağıdaki gibi oluşturulabilir:

    ...    
    model.add(Input((100, )))
    model.add(Embedding(30000, 32))
    ...

    Burada tüm yazılardaki tüm sözcüklerin sayısı 30000 tanedir. Her sözük 32 eleman uzunluğundaki vektörle temsil edilmektedir. 
    Yazılar da 100 sözcük içermektedir. 

    Embedding katmanı sözcükleri vektörlere dönüştürmektedir. Peki Embedding katmanının girdisi nasıl olmalıdır? Embedding 
    katmanının girdisi (yani modelin girdi katmanı) yazıdaki sözcük indekslerinin numaralarından oluşmalıdır. Bu durumda 
    uygulamacının önce yine vocabulary'deki her sözcüğe birer numara vermesi sonra da yazıları bu numaralardan oluşan birer 
    dizi haline getirmesi gerekir. 

    Embedding katmanındaki eğitilebilir parametrelerin sayısı "vocabulary'deki sözcük sayısı * vektör uzunluğu" kadardır. Yani 
    yukarıdaki örnekte Embedding katmanındaki eğitilebilir parametrelerin sayısı 30000 * 32 tane olacaktır. 

    pad_sequences fonksiyonun parametrik yapısı şöyledir:

    tf.keras.utils.pad_sequences(
        sequences,
        maxlen=None,
        dtype="int32",
        padding="pre",
        truncating="pre",
        value=0.0,
    ):

    pad_sequences fonksiyonu her biri dolaşılabilir nesnelerden oluşan dolaşılabilir nesneleri parametre olarak almaktadır. 
    (Örneğin argüman NumPy dizilerinden oluşan listeler olabilir ya da listelerden oluşan listeler olabilir.) İkinci parametre 
    hedeflenen sütun uzunluğunu belirtir. (Yani bu parametre her yazının kaç sözcükle ifade edileceğini belirtmektedir.) dtype 
    parametresi hedef matristeki elemanların dtype türünü belirtmektedir. padding ve trucanting parametreleri padding ve kırpma 
    işleminin baş taraftan mı son taraftan mı yapılacağını belirtmektedir. Burada 'pre' baş tarafı 'post' son tarafı belirtir. 
    value parametresi ise padding yapılacak değeri belirtmektedir. Bu değerin default olarak 0 biçiminde olduğuna dikat ediniz. 
    Bu durumda sözcük numaralarını 1'den aşlatabilirsiniz. pad_sequences işleminin sonucunda iki boyutlu bir NumPy dizisi elde 
    edilmektedir. Örneğin:

    from tensorflow.keras.utils import pad_sequences

    a = [[1, 2, 3], [3, 4, 5, 6, 7], [10], [11, 12]]

    result = pad_sequences(a, 3, padding='post')
    print(result)

    Buradan şöyle bir çıktı elde edilecektir:

    [[ 1  2  3]
    [ 5  6  7]
    [10  0  0]
    [11 12  0]]
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Embedding katmanının çıktısı her bir yazı için iki boyutludur. Çıktı yazıdaki sözcük sayısı kadar satırdan, her sözcük için 
    de belirlenen vektör uzunluğu kadar sütundan oluşmaktadır. Burada bir noktaya dikkat ediniz: Embedding katmanı aslında tüm 
    vocabulary için vektörler oluşturmaktadır. Ancak çıktı olarak yazılardaki sözcüklere ilişkin vektörleri vermektedir. Anımsanacağı
    gibi Dense katmanlarının girdilerinin tek boyutlu olması gerekiyordu. O halde bizim Embedding katmanının çıktısını Flatten 
    ya da Reshape katmanına sokarak onu tek boyutlu hale getirmemiz sonra Dense katmanlara vermemiz gerekir.
    
    Örneğin:

    Input --> Embedding --> Flatten/Reshape --> Dense --> Dense --> Dense (Çıktı katmanı)

    Ancak maalesef kursun yapıldığı Keras versiyonunda Embedding katmanından sonra Flatten katmanının kullanılması bazı durumlarda
    sorunlara yol açabilmektedir. Bunun bir böcek olduğunu düşünüyoruz. Bu nedenle biz örneklerimizde Embedding katmanının çıktısını
    Reshape katmanına sokarak onu tek boyuta indirgeyeceğiz.

    Şimdi daha önce vektörizasyon yöntemiyle yapmış olduğumuz IMDB örneğini sözcük gömme işlemiyle yeniden yapalım. Örneğimizdeki 
    her yorumun 250 sözcükten oluştuğunu varsayalım. Bunu TEXT_SIZE değişkeni ile temsil edelim:

    TEXT_SIZE = 250

    Bizim ilk olarak tüm yazılardaki tüm sözcüklerden bir "sözcük haznesi (vocabulary)" elde etmemiz ve her sözcüğe bir indeks 
    numarası vermemiz gerekir. Biz daha önce işlemi birkaç kere yapmıştık. Anımsanacağı gibi CountVectorizer sınıfı zaten fit 
    işleminden sonra böyle bir sözlüğü bizim için oluşturuyordu. Örneğin:

    import pandas as pd

    df = pd.read_csv('IMDB Dataset.csv')

    from sklearn.feature_extraction.text import CountVectorizer

    cv = CountVectorizer()
    cv.fit(df['review'])

    Bu işlemden sonra artık cv nesnesinin vocabulary_ özniteliğinde vocabulary için bir sözlük oluşturulmuş durumdadır. Şimdi 
    bizim tüm yorumları sözcük indekslerinden oluşan liste listesi biçiminde ifade etmemiz gerekir. Bunun için yorumları tek tek 
    sözcüklere ayıracağız onlar yerine onların indekslerini atayacağız. Padding işlemleri için 0'ıncı indeksi boş bırakabiliriz.
     Örneğin:

    text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

    Ancak burada her yazının indeks dizisi farklı uzunluktadır. İşte bizim pad_sequences fonksiyonu ile bunları eşit uzunluğa
    getirmemiz gerekir:

    from tensorflow.keras.utils import pad_sequences

    dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')

    IMDB örneğinde anımsanacağı gibi her yazının pozitif ya da negatif yargı içerdiği tahmin edilmeye çalışılıyordu. O halde
    dataset_y de şöyle oluşturulabilir:

    dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

    Artık veri kümelerini eğitim ve test biçiminde iki kısma ayırabiliriz:

    from sklearn.model_selection import train_test_split

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

    Şimde de modelimizi oluşturalım:
  
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, Embedding, Reshape, Dense

    model = Sequential(name='IMBD-WordEmbedding')
    model.add(Input((TEXT_SIZE, ), name='Input'))
    model.add(Embedding(len(cv.vocabulary_), WORD_VECT_SIZE, name='Embedding'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Modelin summary metodu ile elde edilen özet bilgileri şöyledir:

    Model: "IMBD-WordEmbedding"
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
    ┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
    │ Embedding (Embedding)           │ (None, 250, 64)        │     6,521,344 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 16000)          │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     4,096,256 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 10,683,649 (40.75 MB)
    Trainable params: 10,683,649 (40.75 MB)
    Non-trainable params: 0 (0.00 B)

    Burada Embedding katmanının çıktısının (250, 64) boyutunda bir matris olduğu görülmektedir. Her yazının 250 sözcük uzunluğunda
    olduğunu anımsayınız. Her sözcük de 64 elemanlı bir vektörle temsil edilmektedir. Embedding katmanındaki eğitilebilir parametrelerin 
    sayısı 6,521,344 biçiminde rapor edilmiştir. Bu sayıyı anlayabilmek için Embedding katmanının içsel çalışmasını az çok bilmek 
    gerekir. Ancak bu sayı toplam vocabulary uzunluğu ile vektör uzunluğunun çarpımı kadardır. Bizim örneğimizde toplam vocabulary
    uzunluğu (len(cv.vocabulary_) + 1 ) = 1,018,96 kadardır. Bu değeri 64 ile çarptığımızda 6,521,344 değeri elde edilmektedir.
    Reshape katmanının modele ek bir eğitilebilir parametre eklemediğine dikkat ediniz. Birinci saklı katmanın girdisinde 250 * 64
    nörün vardır. Bu katmanda toplam 256 nöron bulunduğuna göre bu katmandaki eğitilebilir parametrelerin sayısı 250 * 64 * 256 
    + 256 = 4,096,256 kadardır. İkinci saklı katmanın girdisi 256 nörondur. Burada 256 nöron olduğuna göre ikinci saklı katmandaki 
    eğitilebilir parametrelerin sayısı 256 * 256 + 256 = 65,792 biçimindedir. Nihayet çıktı katmanının giridisi 256 nöron çıktısı 
    da 1 nöron olduğuna göre bu katmandaki eğitilebilir parametrelerin sayısı 256 * 1 + 1 = 257 olacaktır.

    Şimdi de compile ve fit işlemlerini yapalım:

    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

    from tensorflow.keras.callbacks import EarlyStopping

    esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
    hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

    Burada üst üste 5 kez val_loss değeri iyileştirilmezse eğitim sonlandırılmaktadır. Kestirim işleminde yine yazıların aynı 
    biçimde indekslere dönüştürülüp predict metoduna verilmesi gerekir:

    predict_df = pd.read_csv('predict-imdb.csv')

    predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] 
            for text in predict_df['review']]

    predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')
    predict_result = model.predict(predict_dataset_x)

    for presult in predict_result[:, 0]:
        if (presult > 0.5):
            print('Positive')
        else:
            print('Negative')
    
    Geri kalan işlemler benzer biçimde yürütülecektir. Aşağıda örneği bir bütün haline veriyoruz. 
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'])

import re

text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

from tensorflow.keras.utils import pad_sequences

dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Embedding, Reshape, Dense

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((TEXT_SIZE, ), name='Input'))
model.add(Embedding(len(cv.vocabulary_) + 1, WORD_VECT_SIZE, name='Embedding'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] 
        for text in predict_df['review']]

predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')

predict_result = model.predict(predict_dataset_x)
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Aslında daha önce görmüş olduğumuz TextVectorization katmanıyla bu işlemler daha kolay yapılabilmektedir. TextVectorization
    sınıfının __init__ metodunun parametrik yapısını yeniden anımsatmak istiyoruz:

    tf.keras.layers.TextVectorization(
        max_tokens=None,
        standardize='lower_and_strip_punctuation',
        split='whitespace',
        ngrams=None,
        output_mode='int',
        output_sequence_length=None,
        pad_to_max_tokens=False,
        vocabulary=None,
        idf_weights=None,
        sparse=False,
        ragged=False,
        encoding='utf-8',
        name=None,
        **kwargs
    )

    Anımsanacağı gibi burada output_mode parametresi "int" olarak geçildiğinde (default durum) aslında TextVectorization katmanı 
    vektör oluşturmak yerine onların indeks numaralarını oluşturuyordu. Bu katman pad_sequences işlemini de kendisi yapmaktadır. 
    Eğer katmanda output_sequence_length parametresi spesifik bir değer olarak girilirse padding otomatik olarak yapılmaktadır. 
    Metodun max_tokens parametresi sözcük sayısını üst bir limitte kısıtlamak için kullanılmaktadır. İşte eğer bu katman kullanılırsa 
    artık girdi katmanına doğrudan yazılar verilir. Yani bu katman zaten bizim yukarıda CountVectorizer ile yaptığımız işlemleri 
    kendisi yapmaktadır. Bu durumda TextVectorization katmanın kullanıldığı model şöyle oluşturulabilir:
    
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, TextVectorization, Embedding, Dense, Reshape

    tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
    tv.adapt(dataset_x)

    model = Sequential(name='IMBD-WordEmbedding')
    model.add(Input((1, ), dtype='string', name='Input'))
    model.add(tv)
    model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Vocabulary'nin TextVectorization nesnesi üzerinde adapt işleminden sonra oluşturulduğunu anımsayınız. Biz toplam 
    vocabulary genişliğini sınıfın vocabulary_size() metodu ile elde edip Embedding katmanına ilettik. Tabii TextVectorization
    katmanını kullandıktan sonra artık kestirim işleminde yalnızca kestirim yapılacak yazıları predict metoduna vermeliyiz:

    predict_df = pd.read_csv('predict-imdb.csv')

    predict_result = model.predict(predict_df['review'])
    for presult in predict_result[:, 0]:
        if (presult > 0.5):
            print('Positive')
        else:
            print('Negative')

    TextVectoriation sınıfı default olarak sözcükleri boşluk karakterlerinden ayırmakta ve bazı özel ayırma biçimlerini de 
    kullanmaktadır. Bu sınıfın sözcükleri ayırmasındaki default davranış bazı yazılar için uygun olmayabilir. Bu örneğimizdeki
    başarı (binary accuracy) bizim sözcükleri kendimizin ayırdığı CountVectorizer örneğine kıyasla oldukça kötü çıkmıştır. 
    Bunun nedeni muhtemelen sözcüklerin elde edilme biçimidir. 
    
    TextVectorization sınıfında __init__ metodunun split parametresine eğer bir çağrılabilir (callable) nesne girilirse bu 
    durumda uygulamacı kendi ssözcük ayırma yönteminin kullanılmasını sağlayabilmektedir. Aynı biçimde metodun standardize 
    parametresine de çağrılabilir bir nesne girilebilmektedir. Ancak bu parametreye girilen çağrılabilir nesneye tüm yazı tek 
    bir tensör biçiminde geçirilmektedir. Bu paramterler için fonksiyon yazımı TensorFlow bilgisi gerektirebilmektedir. 

    Örnek bir bütün olarak aşağıda verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Embedding, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de sözcük gömme işlemleri için görsel bir açıklama üzerinde duralım. Sözcük gömme algoritmaları semantik olarak 
    birbirine yakın sözcüklerin Öklit uzaklıklarının daha yakın, semantik olarak birbirine uzak sözcüklerin Öklit uzaklıklarının 
    daha uzak biçimde olmasını sağlamaktadır. Biz IMDB örneğinden hareketle bu durumu görsel olarak da temsil edebiliriz. Tabii
    bu işlemi yaparken sözcüklerin 2 elemanlı vektörlerle ifade edilmesini sağlamamız gerekir. Aşağıdaki örnekte önce IMDB 
    modeli eğitilmiş sonra da Embedded katmanına girdi uygulanıp çıktı elde edilmiştir. Bu çıktılar da saçılma grafiğinde 
    gösterilmiştir. Elde edilen grafik incelendiğinde birbirine yakın anlamlı sözcüklerin grafikte birbirine daha yakın 
    olduğunu, uzak anlamlı sözcüklerin biribirinden daha uzak konumlandırıldığını göreceksizniz.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'])

import re

text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

from tensorflow.keras.utils import pad_sequences

dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Embedding, Reshape, Dense

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((TEXT_SIZE, ), name='Input'))
model.add(Embedding(len(cv.vocabulary_) + 1, WORD_VECT_SIZE, name='Embedding'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] 
                        for text in predict_df['review']]

predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')

predict_result = model.predict(predict_dataset_x)
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
                                            66. Ders - 28/09/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi aslında sözcük gömme vektörlerini sıfırdan oluşturmak yerine zaten oluşturulmuş olan 
    vektörleri de kullanabiliriz. Çeşitli diller için önceden oluşturulmuş geniş kapasiteli ve büyük veri kümeleriyle eğitilmiş 
    hazır vektörler bulunmaktadır. Örneğin Facebook'un "fasttext" algoritması kullanılarak hazırlanmış vektörler aşağıdaki 
    bağlantıdan indirilebilir:

    https://fasttext.cc/docs/en/crawl-vectors.html

    Glove algoritması ile hazırlanmış olan vektörleri de aşağıdaki bağlantıdan indiribeilirsiniz:

    https://nlp.stanford.edu/projects/glove/

    Benzer çalışmalar başka kurumlar tarafından da yapılmıştır. Internet'te çeşitli alternatifleri bulabilirsiniz. 
    
    Genellikle bu sitelerden indirilen sözcük gömme vektörleri text bir formattadır. İlgili text dosyanın her satırında da 
    bir sözcük ve o sözcüğüe ilişkin vektör değerleri kodlanmıştır. Yani tipik bir dosyanın bir satırının görünümü şöyledir:
    
    <sözcük> <değer> <değer> <değer> <değer> ....

    Bu tür dosyaların başında genellikle iki elemanlı bir başlık kısmı bulunmaktadır. Burada toplam sözcük sayısı ve bir sözcüğün
    hangi uzunlukta vektörle ifade edileceği bilgisi yer almaktadır. Örneğin İngilizce için fasttext'ten indirdiğimiz hazır 
    sözcük gömme vektör dosyasının başlık kısmı şöyledir:

    2000000 300

    Burada toplam 2,000,000 sözcük için  veektörler bulunmaktadır. (Yani dosya toplam 2,000,000 satır büyüklüğündedir.) Her 
    sözcük 300 eleman uzunluğunda vektörden oluşmaktadır. İngilizce'de yaklaşık 800,000 sözcük vardır. Ancak bu vektörlerde 
    yalnızca sözcükler değil özel isimler, tireli sözcükler, kısaltmalar da bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hazır sözcük gömme vektörlerini kullanmak için yapılacak ilk işlem vektörlerin bulunduğu dosyayı okuyup onu bir Python 
    sözlüğü haline getirmektir. Burada sözlüğün anahtarları sözcüklerden değerleri de o sözcüğün hazır vektör değerlerinden
    oluşturulabilir. Bu işlemi şöyle yapabiliriz:

    FASTTEXT_WORD_EMBEDDING_FILE = R'C:\Users\aslan\Downloads\cc.en.300.vec'

    import numpy as np

    we_dict = {}
    with  open(FASTTEXT_WORD_EMBEDDING_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            tokens = line.rstrip().split(' ')
            we_dict[tokens[0]] = np.array([float(vecdata) for vecdata in tokens[1:]], dtype='float32')

    Burada FASTTEXT_WORD_EMBEDDING_FILE bizim aşağıdaki adresten indirip açtığımız text dosyanın yol ifadesini belirtmektedir. 
    (Bu dosya çok uzun olduğu için dosyayı kursumuzun ilgili klasörüne dahil etmedik.):

    https://fasttext.cc/docs/en/crawl-vectors.html

    Dosya buradaki liste içerisindeki "English: bin, text" bağlantısından indirilmiştir. Bu dosyada her bir İngilizce sözcük
    300'lük bir vektörle oluşturulmuştur.

    Peki biz neden bu dosyayı doğrudan Pandas'la okuyup DataFrame nesnesi yapmadık da onu satır satır okuyup bir sözlük 
    nesnesi haline getirdik? İşte aslında izleyen paragraflarda da açıklayacağımız gibi biz bu hazır vektör dosyasının hepsini 
    değil gerekli satırlarını alıp kullanacağız. Gerekli satırları bulmak için böylesi büyük bir dosyadan elde edilen DataFrame 
    nesnesi üzerinde sıralı arama uygulamak çok yavaş bir yöntemdir. Hızlı arama için sözlük nesneleri kullanılmalıdır. (Tabii
    dosyayı önce DataFrame haline getirip sonra bundan bir sözlük oluşturmak iyi bir fikir değildir. Çünkü bu durumda DataFrame 
    nesnesi de bellekte çok yer kaplayacaktır.)

    Peki bundan sonra ne yapacağız? Anımsanacağı gibi Embedding katmanının girdisi aslında sözük numaralarından oluşmaktadır. 
    Biz bu sözcük numaralarını ya manuel olarak CountVectorizer sınıfını kullanarak oluşturduk ya da hazır TextVecorization 
    katmanının oluşturmasını sağladık. İşte Embedding katmanında weights isimli parametre önceden hazırlanmış olan vektörlerin 
    kullanılmasını sağlamak için bulundurulmuştur. Eğer biz bu parametreye önceden hazırlanmış vektör matrisini girersek bu 
    katman doğrudan bu matristeki vektörleri kullanacaktır. Örneğin:

    model.add(Embedding(VOCAB_LEN, WORD_VECT_SIZE, weights = [pretrained_matrix],  name='Embedding'))

    Buradaki pretrained_matrix hazır vektörlerden elde edilmiş matrisi temsil etmektedir. Bu tür durumlarda uygulamacı artık 
    Embedding katmanını eğitminden çıkartmak da isteyebilir. Ne de olsa zaten veektörler hazır bir biçimde verilmiştir. İşte 
    katman nesnelerinde trainable isimli bir parametre ve bu parametreyi temsil eden bir öznitelik vardır. Eğer bu parametre
    ya da öznitelik False biçimde geçilirse ilgili katman "eğitimde yokmuş gibi ancak kestirim ve test işlemlerinde 
    varmış gibi" ele alınmaktadır. O halde Embedding katmanı hazır vektörlerle şöyle kullanılabilir:

    model.add(Embedding(VOCAB_LEN, WORD_VECT_SIZE, weights = [pretrained_matrix], trainable=False, name='Embedding'))

    Tabii uygulamacı hem hazır vektörleri kullanıp hem de onları eldeki veri kümesine göre iyileştirmek de isteyebilir. Bu 
    durumda trainable patametresi False geçilmez. Bu parametrenin default durumu zaten True biçimdedir.

    Fakat burada dikkat edilmesi gereken başka bir nokta daha vardır. Bizim weights parametresiyle girdiğimiz önceden eğitilmiş
    vektörlerin matristeki satır numaralarıyla sözüklerin numaralarının örtüşmesi gerekir. Yani örneğin IMDB veri kümesinde 
    "fine" sözcüğünün numarası 1172 ise bizim pretained_matrix ismiyle oluşturduğumuz matrisin 1172'inci satırı "fine" sözcüğüne 
    ilişkin vektör olmalıdır. O halde bizim dosyadan hareketle elde ettiğimiz vektörlerden kendi veri kümemizdeki sözüklere 
    karşı gelen sayılarla uyumlu bir matris elde etmemiz gerekir. Eğer biz katman olarak TextVectorization katmanını kullanıyorsak
    bu katman nesnesindeki get_vocabulary metodu bize zaten numaralarla uyumlu sözcük listesini vermektedir. O halde biz bu 
    listeden hareketle bir döngü içerisinde weights parametresi için gereken matrisi (pretrained_matrix) aşağıdaki gibi oluştuabiliriz:

    pretrained_matrix = np.zeros((len(vocab_list), WORD_VECT_SIZE), dtype='float32')

    for index, word in enumerate(vocab_list):
        vect = we_dict.get(word)
        if vect is None:
            vect = np.zeros(WORD_VECT_SIZE)
        pretrained_matrix[index] = vect
            
    Burada önce IMDB'deki sözcüklerin sayısı kadar satıra sahip ve önceden eğitilmiş sözcük gömme vektörlerinin uzunluğu
    kadar (örneğimizde 300) sütuna sahip içi sıfırlarla dolu bir matris oluşturulmuştur. Sonra IMDB'deki sözcükler önceden 
    eğitilmiş vektörlerin bulunduğu sözlükte aranmış ve oradan alınarak aynı sırada matrisin satırlarına yerleştirilmiştir.
    
    Örneğin bütün olarak kodları aşağıda verilmiştir. Ancak burada TextVectorization katmanınındaki sözcük ayırmanın yeteri 
    kadar iyi olmamasından kaynaklanan bir performan düşümü söz konusu olabilecektir. 
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 300

FASTTEXT_WORD_EMBEDDING_FILE = R'C:\Users\aslan\Downloads\cc.en.300.vec'

import numpy as np

we_dict = {}
with  open(FASTTEXT_WORD_EMBEDDING_FILE, 'r', encoding='utf-8') as f:
    for line in f:
        tokens = line.rstrip().split(' ')
        we_dict[tokens[0]] = np.array([float(vecdata) for vecdata in tokens[1:]], dtype='float32')
        
import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'])

import re

text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

from tensorflow.keras.utils import pad_sequences

dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Embedding, Reshape, Dense

pretrained_matrix = np.zeros((len(cv.vocabulary_), WORD_VECT_SIZE), dtype='float32')

for index, word in enumerate(cv.vocabulary_):
    vect = we_dict.get(word)
    if vect is None:
        vect = np.zeros(WORD_VECT_SIZE)
    pretrained_matrix[index] = vect

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((TEXT_SIZE, ), name='Input'))
model.add(Embedding(len(cv.vocabulary_), WORD_VECT_SIZE, weights=[pretrained_matrix], name='Embedding'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] 
                        for text in predict_df['review']]

predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')

predict_result = model.predict(predict_dataset_x)
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
                                            67. Ders - 29/09/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıdaki TextVectorization sınıfı kullanılarak yapılan örneğin performansı sözcük ayırmalardaki sorun yüzünden düşük 
    kalmıştır. Aşağıda aynı örnek bu katman kullanılmadan yapılmıştır. 
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 300

FASTTEXT_WORD_EMBEDDING_FILE = R'C:\Users\aslan\Downloads\cc.en.300.vec'

import numpy as np

we_dict = {}
with  open(FASTTEXT_WORD_EMBEDDING_FILE, 'r', encoding='utf-8') as f:
    for line in f:
        tokens = line.rstrip().split(' ')
        we_dict[tokens[0]] = np.array([float(vecdata) for vecdata in tokens[1:]], dtype='float32')
        
import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'])

import re

text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

from tensorflow.keras.utils import pad_sequences

dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Embedding, Reshape, Dense

pretrained_matrix = np.zeros((len(cv.vocabulary_), WORD_VECT_SIZE), dtype='float32')

for index, word in enumerate(cv.vocabulary_):
    vect = we_dict.get(word)
    if vect is None:
        vect = np.zeros(WORD_VECT_SIZE)
    pretrained_matrix[index] = vect

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((TEXT_SIZE, ), name='Input'))
model.add(Embedding(len(cv.vocabulary_), WORD_VECT_SIZE, weights=[pretrained_matrix], name='Embedding'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] 
                        for text in predict_df['review']]

predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')

predict_result = model.predict(predict_dataset_x)
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')
        
#----------------------------------------------------------------------------------------------------------------------------
    İçerisinde zamana dayalı bilgilerin bulunduğu veri kümelerine zamansal veri kümeleri (temporal data set) denilmektedir. 
    Eğer bir zamansal veri kümesindeki her satırın zamansal verisi bir sıra izliyorsa bu tür veri kümeleri de genellikle
    "zaman serileri (time series)" biçiminde isimlendirilmektedir. Örneğin yağmurun yağıp yağmayacağını tahmin etmek için her 
    10 dakikada bir hava durumuna ilişkin ölçüm alındığını düşünelim. Bu ölçüm verileri zamansal verilerdir ve bunlara "zaman 
    serileri" de denilmektedir. Çünkü bu ölçümler birbirinden kopuk değil zaman içerisinde birbirlerini izlemektedir. Yağmur 
    bir anda yağmamaktadır. Bir süreç içerisinde yağmaktadır. Belli bir andaki ölçüm değerlerinden yağmurun yağıp yağmayacağı 
    anlaşılamayabilir. Ancak geriye doğru bir grup ölçüm bize gidişat hakkında daha iyi bilgiler verebilecektir. İşte bu tür 
    durumlarda eğitim sırasında verilerin kopuk kopuk değil peşi sıra bir bağlam içerisinde değerlendirilmesi gerekir. Biz 
    daha önce resimsel veriler üzerinde resmin pixel'lerini ilişkilendirebilmek için "evrişim (convoluiton)" uygulamıştık. 
    İşte zaman serisi verileri için de benzer biçimde evrişim uygulanabilmektedir. Böylesi bir evrişim işlemi zaman serisi 
    verilerinin tek tek değil birbiriyle ilişkili biçimde ele alınmasını sağlamaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında zamansal veriler geniş bir tanımla "yağmurun yağıp yağmayacağına ilişkin 10'ar dakikalık ölçümler" gibi olmak 
    zorunda değildir. Yazılardaki sözcükler de bu bağlamda zamansal verilere benzemektedir. Yazıdaki sözcükler ondan önce 
    gelen ve ondan sonra gelen sözcüklerle ilişkilendirilirse daha iyi anlamlandırılabilir. O halde yazıların anlamlandırılmasında 
    da evrişim işlemi uygulanabilir. Biz daha önce resimler üzerinde evrişim uygulamıştık. Oradaki evrişim işlemine "iki 
    boyutlu evrişim işlemi" denilmektedir. Bunun nedeni o örneklerde alınan filtrenin (kernel) iki yönlü (sağa ve aşağıya) 
    kaydırılmasıdır. İşte zamansal verilerde uygulanan evrişim tek boyutludur. Tek boyutlu evrişim demek filtrenin tek boyutta 
    kaydırılması demektir. Metin anlamlandırma işlemlerinde de tek boyutlu evrişim uygulanmaktadır. 

    Tek boyutlu evrişim işleminde filtre büyüklüğü tek boyutludur (yani tek bir sayıdan oluşur). Bu sayı evrişime sokulacak 
    satırların sayısını belirtmektedir. Filtrenin genişliği evrişime sokulacak verilerin sütun sayısı kadardır. Örneğin:

    x x x x x x x x 
    x x x x x x x x 
    x x x x x x x x
    x x x x x x x x
    x x x x x x x x
    x x x x x x x x
    x x x x x x x x
    ...............

    Bunlar evrişime sokulacak verileri temsil ediyor olsun. Filteyi (kernel) 3 olarak olarak almış olalım. Bu durumda filtre 
    aşağıdaki gibi bir yapıya sahip olacaktır:

    F F F F F F F F  
    F F F F F F F F
    F F F F F F F F

    Buradaki filte ilk üç satır ile çakıştırılır, dot-product yapılır ve bir değer elde edilir. Sonra filtre aşağıya doğru 
    kaydırılır ve aynı işlem yinelenir. Asıl matrisin satır sayısının N olduğunu filtrenin (kernel) satır sayısının da K 
    olduğunu varsayalım. Bu durumda "padding uygulandığında" elde edilecek matris (N, 1) boyutunda, "padding uygulanmmadığında" 
    ise (N - K + 1, 1) boyutunda olacaktır. Örneğin biz biz 6 sözcük uzunluğundaki yazıların sözcüklerini sözcük gömme yöntemi 
    ile 8 elemanlı vektörle ifade etmiş olalım. Bu durumda yazımız aşağıdaki gibi bir görüntüye sahip olacaktır:

    XXXXXXXX  -> sözcük
    XXXXXXXX  -> sözcük
    XXXXXXXX  -> sözcük
    XXXXXXXX  -> sözcük
    XXXXXXXX  -> sözcük
    XXXXXXXX  -> sözcük

    Şimdi biz 2 uzunlukta bir filtre ile tek boyutlu evrişim uygulamak isteyelim. Bu durumda filtenin yapısı şöyle olacaktır:

    FFFFFFFF
    FFFFFFFF

    Biz bu filtreyi "padding uygulamadan" yukarıdan aşağıya doğru gezdirirsek aşağıdaki gibi bir vektör elde ederiz:

    R
    R
    R
    R
    R
   
    Burada R değerleri filtre matrisi ile sözcüklere ilişkin sözcük gömme matrisinin çakıştırılması ile uygulanan "dot-product"
    ve sonrasında uygulanan aktivasyon fonksiyonunun çıktısını temsil etmektedir. Biz böylece (6, 8)'lik matris yerine (5, 1)'lik
    bir matris elde etmiş olduk. Tabii biz birden fazla filtre de uygulayabiliriz. Örneğin toplamda 16 filtre uygularsak elde 
    edeceğimiz matris (16, 5, 1) boyutunda olacaktır. Biz bu katmanda birden fazla filtre uyguladığımız zaman çıktı da yine aşağdaki 
    gibi çok boyutlu bir yapıda olacaktır:

    R R R R R R R .... R R R 
    R R R R R R R .... R R R 
    R R R R R R R .... R R R 
    R R R R R R R .... R R R 
    .....

    Burada çıktının sütun sayısı uygulanan filtrenin sayısı kadardır. 

    Keras'ta tek boyutlu evirişim işlemi için Conv1D katman sınıfı bulundurulmuştur. Conv1D sınıfının __init__  metodunun parametrik 
    yapısı şöyledir:
    
    tf.keras.layers.Conv1D(
        filters,
        kernel_size,
        strides=1,
        padding='valid',
        data_format=None,
        dilation_rate=1,
        groups=1,
        activation=None,
        use_bias=True,
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        kernel_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        bias_constraint=None,
        **kwargs
    )
    
    Metodun ilk parametresi filtre sayısını, ikinci parametresi filtrenin (kernel) boyutunu belirtmektedir. Tabii burada boyut 
    tek bir sayıdan oluşur (yani yukarıdaki örnekte filtrenin satır uzunluğu). Yine metodun strides ve padding parametreleri 
    vardır. Bu padding parametresi "valid" ise padding uygulanmaz, "same" ise padding uygulanır. stride değeri yukarıdan aşağıya 
    kaydırmanın kaçar kaçar yapılacağını belirtmektedir. Bu parametrenin default değeri 1'dir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            68. Ders - 05/10/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda IMDB örneğinde sözcük gömme yapıldıktan sonra bir kez tek boyutlu evrişim işlemi uygulanmıştır. Modelin katmanları 
    şöyledir:

    TextVectorization --> Embedding --> Conv1D --> Flatten/Reshape --> Dense --> Dense --> Dense (Output)

    Model Keras'ta aşağıdaki gibi oluşturulmuştur:

    tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
    tv.adapt(dataset_x)

    model = Sequential(name='IMBD-WordEmbedding')
    model.add(Input((1, ), dtype='string', name='Input'))
    model.add(tv)
    model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
    model.add(Conv1D(128, 3, activation='relu', padding='same', name='Conv1D'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Burada görüldüğü gibi Embedding katmanından sonra Conv1D katmanı uygulanmıştır. Modelin özet (summary) görünümü şöyledir:

    Model: "IMBD-WordEmbedding"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ text_vectorization_2            │ (None, 250)            │             0 │
    │ (TextVectorization)             │                        │               │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Embedding (Embedding)           │ (None, 250, 64)        │    11,630,208 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv1D (Conv1D)                 │ (None, 250, 128)       │        24,704 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 32000)          │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     8,192,256 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 19,913,217 (75.96 MB)
    Trainable params: 19,913,217 (75.96 MB)
    Non-trainable params: 0 (0.00 B)
        
    Burada TextVectorization katmanının girdisi yazılardan, çıktısı ise bu yazıların sözcük numaralarından oluşmaktadır. 
    Bu sözcük numaraları Embedding katmanına girdi olarak verilmektedir. Embedding katmanının da çıktısının (250, 64) boyutunda
    vektörler olduğunu görüyorsunuz. Anımsanacağı gibi bu katmandaki eğitilebilir parametrelerin sayısı vocabulary uzunluğu ile 
    vektör uzunluğunun çarpımı kadardır. Burada bu katman için rapor edilen eğitilebilir parametrelerin sayısının 11,639,208 
    olduğu görülmektedir. Modelimizdeki sözcük haznesi (vocabulary) uzunluğu TextVectorization get_vocabulary() metodu ile elde 
    edildiğinde 181722 olduğu görülmektedir. O halde bu katmandaki eğitilebilir parametrelerin sayısı 181722 * 64 = 11,630,208 
    değeri elde edilmektedir. Conv1D katmanının çıktısının (250, 128) boyutlarında olduğunu görüyorsunuz. Eğer bu katmanda tek 
    bir filtre uygulanmış olsaydı çıktı (250, 1) boyutunda olacaktı. 128 filte uygulandığı için boyut (250, 128) olmuştur. 
    Şimdi bu katmandaki eğitilebilir parametrelerin sayısını hesaplayalım. Tek boyutlu evrişim işleminde içsel olarak dot-product 
    işlemindeki eleman sayısı kadar nöron bulunacaktır. Örneğimizde bir filtre için 64 * 3 tane nöron söz konusudur. Tabii 
    dot-product yapıldıktan sonra bu bir bias değerle toplanacağından bir filtre için eğitilebilir parametrelerin sayısı 64 * 3 + 1 
    tane olacaktır. Evirişim işleminde hep aynı filtre matrisinin kaydırıldığını anımsayınız. Örneğimizde toplam 128 farklı filtre
    vardır. O halde burada toplam eğitilebilir parametrelerin sayısı (64 * 3 + 1) * 128 = 24704 olacaktır. Conv1D katmanının 
    çıktısının (250, 128) boyutunda bir matris olduğunu belirtmiştik. Bu çıktı Reshape katmanı ile (Flatten katmanının bilinmeyen 
    bir problem yarattığından bahsetmiştik) tek boyutlu hale getirilmiştir. O halde Reshape katmanının çıktısı 150 * 128 = 32000 
    biçimindedir. Tabii Reshape katmanında herhangi bir eğitilebilir parametre yoktur. Sonra birinci Dense katmandaki eğitilabilir 
    parametrelerin sayısı 32000 * 256 + 256 = 8,192,256 tanedir. Bu katmanın 256'lık bir bir nörondan oluşmaktadır. Bu 266 nöron 
    sonraki Dense katmana sokulduğunda buradaki eğitilebilir parametrelerin sayısı 256 * 256 + 256 = 65,792 kadardır. Bu Dense 
    katmanın çıktısında 256 nöron bulunmaktadır. Nihayet çıktı katmanına 256 nöron girip 1 nöron çıkmaktadır. Buradaki eğitilebilir
    parametrelerin sayısı 256 * 1 + 1 = 257 tabedir. 

    Uygulamanın tüm kodları aşağıda verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Conv1D, Embedding, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Aslında tıpkı resimlerde olduğu gibi metinsel ve zamansal verilerde de evrişim işlemi sonrasında eğitilebilir parametreleri 
    azaltmak ve bazı nitelikleri belirgin hale getirmek için "pooling" işlemleri uygulanabilmektedir. Tabii buradaki pooling 
    işlemleri iki boyutlu değil tek boyutludur. Tek boyutlu "pooling" işlemleri için MaxPooling1D ve AveragePooling1D sınıfları
    bulundurulmuştur. 
        
    Tek boyutlu pooling işlemlerinde pool_size parametresi için tek bir sayı girilmektedir. Bu sayı satır sayısıdır. Pooling 
    işlemi burada belirtilen satır sayısı kadar satır üzerinde ve onların her sütununda yani sütunsal olarak uygulanmaktadır. 
    Örneğin pooling işlemine sokacağımız veriler şöyle olsun:

    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    x x x x x x x x x x
    
    Burada MaxPooling1D sınıfını kullanıp pool_size parametresini 3 girmiş olalım. Bu durumda ilk üç satır ele alınıp onların 
    sütunlarının en büyük elemanları elde edilecektir. Sonra default durumda pencere üç aşağıya kaydırılıp aynı işlem o üçlü 
    için de yapılacaktır. Bu işlemin sonucunda aynı sütun sayısına sahip ancak satır sayısı üç kat daha az olan bir matris elde 
    edilecektir. Yukarıdaki verilerin pool_size 3 alınarak tek boyutlu "pooling" işlemine sokulmasıyla elde edilen matris şöyle 
    olacaktır:

    P P P P P P P P P P   ==> ilk üç satırın sütunlarının pooling değerleri
    P P P P P P P P P P   ==> sonraki üç satırın sütunlarının pooling değerleri
    P P P P P P P P P P   ==> sonraki üç satırın sütunlarının pooling değerleri

    Tıpkı resimsel uygulamalarda olduğu gibi metinsel uygulamalarda ve zamansal uygulamalarda da evrişim ve pooling 
    işlemleri bir kez değil üst üste birden fazla kez uygulanmaktadır. 

    Peki metinsel işlemlerde MaxPooling1D katmanı mı yoksa AveragePooling1D katmanı mı tercih edilmelidir? Aslında hedefe 
    bağlı olarak bu tercih değişebilir. Ancak genel olarak metinsel uygulamalarda MaxPooling1D katmanı tercih edilmektedir. 
    Max pooling işlemi o bölgedeki en önemli sözcüklere dikkat edilmesini sağlamaktadır. MaxPooling1D ve AveragePooling1D 
    sınıflarının __init__metotlarının parametrik yapısı şöyledir:

    tf.keras.layers.MaxPool1D(
        pool_size=2,
        strides=None,
        padding='valid',
        data_format=None,
        name=None,
        **kwargs
    )

    ttf.keras.layers.AveragePooling1D(
        pool_size,
        strides=None,
        padding='valid',
        data_format=None,
        name=None,
        **kwargs
    )

    Metotlardaki pool_size parametresi pooling uygulanacak satır uzunluğunu, strides parametresi kaydırma miktarını belirtmektedir. 
    Bu parametrelerin default değerleri None biçimindedir. Bu durumda kaydırma pool_size parametresinde belirtilen değer kadar
    yapılmaktadır. padding parametreleri yine "same" ya da "valid" biçiminde girilebilmektedir. 

    Aşağıdaki örnekte IMDB veri kümesi üzerinde yine önce sözcük gömmesi sonra evrişim ve pooling işlemleri art arda uygulanmıştır. 
    Modelin katmanları şöyledir:

    TextVectorization --> Embedding --> Conv1D --> MaxPooling1D --> Conv1D --> MaxPooling1D -->  Conv1D --> MaxPooling1D
    Flatten/Reshape --> Dense --> Dense --> Dense (Output)

    Model Keras'ta şöyle kurulmuştur:

    model = Sequential(name='IMBD-WordEmbedding')
    model.add(Input((1, ), dtype='string', name='Input'))
    model.add(tv)
    model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
    model.add(Conv1D(128, 3, padding='same', name='Conv1D-1'))
    model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-1'))
    model.add(Conv1D(128, 3, padding='same', name='Conv1D-2'))
    model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-2'))
    model.add(Conv1D(128, 3, padding='same', name='Conv1D-3'))
    model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-3'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Modelden elde edilen özet görüntü (summary) şöyledir:

    Model: "IMBD-WordEmbedding"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ text_vectorization              │ (None, 250)            │             0 │
    │ (TextVectorization)             │                        │               │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Embedding (Embedding)           │ (None, 250, 64)        │    11,630,208 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv1D-1 (Conv1D)               │ (None, 250, 128)       │        24,704 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ MaxPooling1D-1 (MaxPooling1D)   │ (None, 125, 128)       │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv1D-2 (Conv1D)               │ (None, 125, 128)       │        49,280 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ MaxPooling1D-2 (MaxPooling1D)   │ (None, 63, 128)        │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Conv1D-3 (Conv1D)               │ (None, 63, 128)        │        49,280 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ MaxPooling1D-3 (MaxPooling1D)   │ (None, 32, 128)        │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 4096)           │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     1,048,832 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 12,868,353 (49.09 MB)
    Trainable params: 12,868,353 (49.09 MB)
    Non-trainable params: 0 (0.00 B)

    Burada MaxPooling1D katmanlarının boyutu iki kat azalttığına dikkat ediniz. 

    Örnek bir bütün olarak aşağıda verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Conv1D, MaxPooling1D, Embedding, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-1'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-1'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-2'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-2'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-3'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-3'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AveragePooling1D ve MaxPooling1D katmanlarının global biçimleri de vardır. Bu global pooling katmanları GlobalAveragePooling1D
    ve GlobalMaxPooling1D isimleriyle bulundurulmuştur. Tıpkı iki boyutlu evrişim işlemlerinde olduğu gibi tek boyutlu evrişim
    işlemlerinde de bu katmanlar tek bir çıktı üretmektedir. Örneğin GlobalAveragePooling1D katmanı toplamda tek bir satır 
    üretir. Örneğin bu katmanın girdisi (250, 128) boyunda bir matris ise bu durumda bu katman her sütun için o sütunun toplamdaki
    en büyük değerini elde edecektir. Bu değerler de toplamda 128 tane olacaktır. Bu katmanlar da bunların iki boyutlularında 
    olduğu gibi evrişim katmanlarının en sonunda yani Dense katmanlardan hemen önce bulunudurulmalıdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        69. Ders - 06/10/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda daha önce yapmış olduğumuz "fasttext"örneğinin bir benzeri verilmiştir. Burada hem önceden eğitilmiş "fasttext" 
    vektörleri kullanılmıştır hem de evrişim ve pooling katmanları modele eklenmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 300

FASTTEXT_WORD_EMBEDDING_FILE = R'C:\Users\aslan\Downloads\cc.en.300.vec'

import numpy as np

we_dict = {}
with  open(FASTTEXT_WORD_EMBEDDING_FILE, 'r', encoding='utf-8') as f:
    for line in f:
        tokens = line.rstrip().split(' ')
        we_dict[tokens[0]] = np.array([float(vecdata) for vecdata in tokens[1:]], dtype='float32')

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Embedding, Conv1D, MaxPooling1D, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)
vocab_list = tv.get_vocabulary()

pretrained_matrix = np.zeros((len(vocab_list), WORD_VECT_SIZE), dtype='float32')

for index, word in enumerate(vocab_list):
    vect = we_dict.get(word)
    if vect is None:
        vect = np.zeros(WORD_VECT_SIZE)
    pretrained_matrix[index] = vect
        
model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(len(vocab_list), WORD_VECT_SIZE, weights=[pretrained_matrix], name='Embedding'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-1'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-1'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-2'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-2'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-3'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-3'))
model.add(Reshape((-1, )))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Tek boyutlu evrişim ve pooling işlemleri yalnızca metinsel veri kümelerinde değil aynı zamanda zamansal (temporal) 
    veri kümelerinde de uygulabilmektedir. Gerçi izleyen paragraflarda biz zamansal veriler için daha iyi performans gösteren 
    geri beslemeli (recurrent) ağları kullanacağız. Ancak burada zamansal veriler üzerinde de tek boyutlu evirişim işlemlerine
    bir örnek vermek istiyoruz. 
    
    Zamansal verilerle (temporal data) ilgili klasik bir örnek "hava durumunun tahmin edilmesi" olabilir. Örneğin hava sıcaklığı
    birtakım faktörlere bağlıdır. Belli veriler eşliğinde o andaki ya da belli zaman sonrasındaki hava sıcaklığı tahmin edilmeye 
    çalışabilir. Örneğin biz birtakım ölçümleri alıp yarınki hava sıcaklığını bugünden tahmin etmeye çalışabiliriz. Tabii hava 
    sıcaklığı bir anda değişmemektedir. Birbirini izleyen birtakım olaylar bunda etkilidir. Biz önceki durumu bilmeden de o andaki 
    verileri göz önüne alarak hava sıcaklığını tahmin etmeye çalışabiliriz. Ancak önceki verilerin de dikkate alınması 
    tahminin daha isabetli yapılmasına yol açmaktadır. Aynı durum örneğin borsalarda da benzerdir. Bir kağıdın ya da kripto paranın 
    fiyatı birtakım olaylar sonucunda bir bağlam içerisinde değişmektedir. Yani birtakım kestirimlerde yalnızca o andaki duruma 
    değil geçmişe de bakıp bağlamı da dikkate almak kestirimi güçlendirmektedir. Finansal piyasalar bunlara tipik bir örnek 
    oluşturmaktadır. Finansal piyasalarda ilgili finansal durum bir bağlam çerçevesinde gelişip bir noktaya gelmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hava durumu tahminine örnek için kullanılan veri kümelerinden biri "Jena Climate" ("yena klaymit" biçiminde okunuyor) isimli 
    bir veri kümesidir . Bu veri kümesi aşağıdaki bağlantıdan indirilebilir:

    https://www.kaggle.com/datasets/mnassrib/jena-climate?resource=download

    Bu siteden veri kümesi indirilip açıldığında "jena_climate_2009_2016.csv" isimli bir dosya edilecektir. Veri kümesinin 
    görünümü aşağıdaki gibidir:

    "Date Time","p (mbar)","T (degC)","Tpot (K)","Tdew (degC)","rh (%)","VPmax (mbar)","VPact (mbar)","VPdef (mbar)","sh 
    (g/kg)","H2OC (mmol/mol)","rho (g/m**3)","wv (m/s)","max. wv (m/s)","wd (deg)"
    01.01.2009 00:10:00,996.52,-8.02,265.40,-8.90,93.30,3.33,3.11,0.22,1.94,3.12,1307.75,1.03,1.75,152.30
    01.01.2009 00:20:00,996.57,-8.41,265.01,-9.28,93.40,3.23,3.02,0.21,1.89,3.03,1309.80,0.72,1.50,136.10
    01.01.2009 00:30:00,996.53,-8.51,264.91,-9.31,93.90,3.21,3.01,0.20,1.88,3.02,1310.24,0.19,0.63,171.60
    01.01.2009 00:40:00,996.51,-8.31,265.12,-9.07,94.20,3.26,3.07,0.19,1.92,3.08,1309.19,0.34,0.50,198.00
    01.01.2009 00:50:00,996.51,-8.27,265.15,-9.04,94.10,3.27,3.08,0.19,1.92,3.09,1309.00,0.32,0.63,214.30
    01.01.2009 01:00:00,996.50,-8.05,265.38,-8.78,94.40,3.33,3.14,0.19,1.96,3.15,1307.86,0.21,0.63,192.70
    ...............................
    
    Dosyada bir başlık kısmı olduğunu görüyorsunuz. Bu veri kümesi 10'ar dakikalık periyotlarla havaya ilişkin birtakım değerlerin 
    ölçülerek saklanmasıyla oluşturulmuştur. Sütunlardan biri (üçüncü sütun) derece cinsinden hava sıcaklığını belirtmektedir.  
    Veri kümesinde eksik veri bulunmamaktadır. 
    
    Jena Climate örneğinde bizim amacımız belli bir zamandaki ölçüm değerinden hareketle bir gün sonraki hava ısısını tahmim 
    etmek olsun. Böyle bir modelin eğitimi için bizim bazı düzenlemeler yapmamız geekir. Burada eğitimde kullanılacak x değerlerine 
    karşı gelen y değerleri (havanın ısısı) bir gün sonraki değerler olmalıdır. Veri kümesinde bir gün sonraki değerler 24 * 60 // 10 
    = 144 satır ilerideki değerlerdir. O halde bizim eğitim verilerini oluştururken her x ile 144 ilerideki satırın y değerini 
    eşleştirmemiz gerekir. Bu işlemler çeşitli biçimlerde yapılabilir. Ayrıca veri kümesinde ölçümün yapıldığı tarih ve zaman 
    bilgisi de vardır. Peki zamansal veri hangi ölçek türündendir? İşte tarih ve zaman bilgileri uğraşılan konuya değişik 
    biçimlerde ele alınabilmektedir. Sürekli artan bir tarih-zaman bilgisinin kestirim modellerinde hiçbir kullanım gerekçesi 
    yoktur. Tarih-zaman bilgileri genellikle "özellik mÜhendisliği (feature engineering)" teknikleriyle bileşene ayrılır ve bu 
    bileşenler ayrı sütunlar biçiminde veri kümesine eklenir. Tarih bilgisinin aylara, günlere ya da haftanın günlerine ayrılması 
    ve bunların da kategorik bir bilgiler gibi ele alınması yaygındır. Yıl bilgisi de yine kategorik bir bilgi olarak ele alınabilir. 
    Buradaki "Jena Climate" veri kümesinde tarih bilgisinin ay ve gün bileşenlerinden faydalanılabilir. Ölçümün günün hangi 
    10 dakikasına ilişkin olduğu da kestirimde önemli bir bilgi oluşturabilmektedir. Gerçi zaman serisi tarzındaki veri kümelerinde 
    zaten biz ağın bu örüntüyü kendisinin yakalamasını isteriz. Bu nedenle ağın mimarisine göre bu tür bilgilerin önemi değişebilmektedir. 
    Veri kümesinin diğer sütunları zaten nümerik sütunalardır. Orada bir dönüştürmenin yapılmasına gerek yoktur. Tabii özellik 
    ölçeklemesi uygulamak gerekir. Buradaki sütunların anlamlandırılması için meteorolojiye ilişkin bazı özel bilgilere gereksinim 
    vardır. Biz bu sütunların anlamları üzerinde burada durmayacağız. 

    Veri kümesini aşağıdaki gibi okumuş olalım:

    import pandas as pd

    df = pd.read_csv('jena_climate_2009_2016.csv')

    Biz tarih ve zaman bilgisi sütununu Pandas'ın datetime türüne dönüştürebiliriz:

    df['Date Time'] = pd.to_datetime(df['Date Time'])

    Artık biz bu sütunun bileşenlerini elde edebiliriz. Ancak bu yöntem aslında bizim için daha zahmetlidir. Doğrudan biz 
    yazının içerisindeki ilgili kısımları yine yazı olarak alıp one-hot-encoding uygulayabiliriz:

    df = pd.read_csv('jena_climate_2009_2016.csv')

    df['Month'] = df['Date Time'].str[3:5]
    df['Hour-Minute'] = df['Date Time'].str[11:16]

    df.drop(['Date Time'], axis=1, inplace=True)
    from sklearn.preprocessing import OneHotEncoder

    ohe = OneHotEncoder(sparse_output=False)

    ohe.fit(df[['Month', 'Hour-Minute']])
    ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

    df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
    df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

    x = df.to_numpy('float32') 
    y = df['T (degC)'].to_numpy('float32')

    Bir günün kaç 10 dakikadan oluştuğunu aşağıdaki gibi bir değişkenle ifade edebiliriz:

    PREDICTION_INTERVAL = 24 * 60 // 10         # 144

    Biz burada dataset_x ve dataset_y verilerini aşağıdaki ayırabiliriz:

    dataset_x = x[:-PREDICTION_INTERVAL]
    dataset_y = y[PREDICTION_INTERVAL:]

    Burada her satır PREDICTION_INTERVAL kadar ileridki satırda bulunan y değeri ile eşleştirilmiştir. Veri kümesi ayrıştırıldıktan
    sonra artık özellik ölçeklemesi yapabiliriz:

    from sklearn.model_selection import train_test_split

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = \
            train_test_split(dataset_x, dataset_y, test_size=0.2, shuffle=False)

    from sklearn.preprocessing import StandardScaler

    ss = StandardScaler()
    ss.fit(training_dataset_x)
    scaled_training_dataset_x = ss.transform(training_dataset_x)
    scaled_test_dataset_x = ss.transform(test_dataset_x)

    Modelimiz aşağıdaki gibi olabilir:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input,  Dense

    model = Sequential(name='Jena-Climate')
    model.add(Input((scaled_training_dataset_x.shape[1], ),  name='Input'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Örnek bir bütün olarak aşağıda verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

PREDICTION_INTERVAL = 24 * 60 // 10         # 144

df = pd.read_csv('jena_climate_2009_2016.csv')

df['Month'] = df['Date Time'].str[3:5]
df['Hour-Minute'] = df['Date Time'].str[11:16]

df.drop(['Date Time'], axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)

ohe.fit(df[['Month', 'Hour-Minute']])
ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

x = df.to_numpy('float32') 
y = df['T (degC)'].to_numpy('float32')

dataset_x = x[:-PREDICTION_INTERVAL]
dataset_y = y[PREDICTION_INTERVAL:]

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2, shuffle=False)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(training_dataset_x)
scaled_training_dataset_x = ss.transform(training_dataset_x)
scaled_test_dataset_x = ss.transform(test_dataset_x)
   
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input,  Dense

model = Sequential(name='Jena-Climate')
model.add(Input((scaled_training_dataset_x.shape[1], ),  name='Input'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(scaled_training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Squared Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mean_absolute_error'])
plt.plot(hist.epoch, hist.history['val_mean_absolute_error'])
plt.legend(['MSE', 'Validation MSE'])
plt.show()

# evaluation

eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_df = pd.read_csv('predict.csv')

predict_df['Month'] = predict_df['Date Time'].str[3:5]
predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

predict_df.drop(['Date Time'], axis=1, inplace=True)

ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

predict_dataset = predict_df.to_numpy('float32')
scaled_predict_dataset = ss.transform(predict_dataset)

predict_result = model.predict(scaled_predict_dataset)

for presult in predict_result[:, 0]:
    print(presult)

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Jena Climate örneğini tek boyutlu evrişim katmanı ile gerçekleştirelim. Ancak bu katman bizden girdiyi iki boyutlu 
    matrisler biçiminde istemektedir. Yani bizim sinir ağına girdileri 144'lük (PREDICTION_INTERVAL) matrisler biçiminde vermemiz
    gerekir. dataset_x ve dataset_y veri kümelerini hazırlarken bizim 144'lük peşi sıra giden kaydırmalı bir veri kümesi oluşturmamız
    gerekir. Tabii burada kaydırma miktarını istediğimiz gibi alabilir. dataset_x veri kümesinin aşağıdaki gibi bir yapıya sahip
    olması gerekir:

    <ilk 144'lük satır>
    <Sonraki 144'lük satır>
    <Sonraki 144'lük satır>
    <Sonraki 144'lük satır>
    ....

    Tabii burada oluşturulacak matris çok büyük olabilir. Bunun için kaydırmayı birer değil daha daha geniş uygulayabiliriz. 
    Ya da bu tür durumlarda parçalı eğitim yoluna gidebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        70. Ders - 12/10/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi Jena Climate örneğini yukarıda açıklandığı gibi gerçekleştirelim. Programımızda üç önemli değer kullanacğız:

    PREDICTION_INTERVAL = 24 * 60 // 10         # 144
    WINDOW_SIZE = 24 * 60 // 10                 # 144
    SLIDING_SIZE = 10

    PREDICTION_ITERVAL bizim kaç 10 dakika sonraki hava ısısını tahmin edeceğimizi, WINDOW_SIZE son kaç 10 dakikalık ölçümlerden 
    kestirim yapacağımızı, SLIDING_SIZE ise kaydırma miktarını belirtmektedir. Bu kaydırma miktarı 1 olarak alınırsa veri kümesi 
    çok büyümektedir. Bu nedenle biz örneğimizde kaydırmayı 10'arlı yapacağız. Biizm öncelikle veriler üzerinde önişlemleri 
    yapmamız gerekir. Veri kümesindeki tarih ve zaman bilgisi kategorik bir bilgi olarak ele alınabilir. Burada makul kategori
    sayısı ile bu sütunu sayısallaştırabiliriz:
    
    df = pd.read_csv('jena_climate_2009_2016.csv')

    df['Month'] = df['Date Time'].str[3:5]
    df['Hour-Minute'] = df['Date Time'].str[11:16]

    df.drop(['Date Time'], axis=1, inplace=True)

    from sklearn.preprocessing import OneHotEncoder
    ohe = OneHotEncoder(sparse_output=False)

    ohe.fit(df[['Month', 'Hour-Minute']])
    ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

    df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
    df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

    Burada önce tarih ve zaman sütununu parse ettik. Sonra ay bilgisini ve saat ile dakika bilgisini one-hot-encoding uyglayarak
    sayısallaştırdık. Bunun sonucunda aşağıdaki gibi x ve y veri kümelerini elde ettik:

    raw_dataset_x = df.to_numpy('float32')
    raw_dataset_y = df['T (degC)'].to_numpy('float32')

    Burada ölçeklemenin ne zaman yapılacağı konusunda iki seçenek olabilir. Birinci seçenek ölçeklemenin henüz eğitim verileri 
    fit için uygun hale getirilmeden yapılması olabilir. İkinci seçenek ise eğitim verilerinin fit işlemi için uygun hale 
    getirilmesimden sonra ölçeklemein yapılmasıdır. Ancak scikit-learn kütüphanesindeli ölçekleme sınıfları iki boyutlu veri 
    kümsini girdi olarak almaktadır. Veriler üç boyutlu hale getirildikten sonra ölçekleme yapılacaksa bu durumda onların önce
    yeniden ikiboyuta indirgenip işlemden sonra yeniden üç boyuta yükseltilmesi gerekir. Biz burada önce ölçekleme uygulayıp 
    sonra verileri fit işlemine uygun hale getireceğiz. Verileri eğitim ve test biçiminde ikiye ayırırken train_test_split
    fonksiyonunun shuffle parametresini False yapmayı unutmayınız. 

    from sklearn.model_selection import train_test_split

    raw_training_dataset_x, raw_test_dataset_x, raw_training_dataset_y, raw_test_dataset_y =  train_test_split(raw_dataset_x, 
            raw_dataset_y, test_size=0.2, shuffle=False)
    
    Artık ölçekleme uygulayabiliriz:
    
    from sklearn.preprocessing import StandardScaler

    ss = StandardScaler()
    ss.fit(raw_training_dataset_x)
    raw_scaled_training_dataset_x = ss.transform(raw_training_dataset_x)
    raw_scaled_test_dataset_x = ss.transform(raw_test_dataset_x)

    Verilerin fit işlemi için üç boyutlu hale getirilmesi eğitim ve test veri kümelerinde ayrı ayrı yapılmalıdır. Bu nedenle
    bu işlemi bir fonksiyuna devrettik:

    def create_ts_dataset(dataset_x, dataset_y, pi, ws, ss):
        x = []
        y = []
        for i in range(0, len(dataset_x) - ws - pi, ss):
            x.append(dataset_x[i:i + ws])
            y.append(dataset_y[i + ws + pi - 1])
        return np.array(x), np.array(y)

    Artık eğitim ve test verilerini bu fonksiyon yoluyla üç boyutlu hale getirebiliriz:

    scaled_training_dataset_x, training_dataset_y = create_ts_dataset(raw_scaled_training_dataset_x, raw_training_dataset_y, 
            PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE)
    scaled_test_dataset_x, test_dataset_y = create_ts_dataset(raw_scaled_test_dataset_x, raw_test_dataset_y, 
            PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE)

    Şimdi modeli oluşturalım:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Reshape, Dense

    model = Sequential(name='Jena-Climate')
    model.add(Input((scaled_training_dataset_x.shape[1], scaled_training_dataset_x.shape[2]),  name='Input'))

    model.add(Conv1D(128, 3, padding='same', name='Conv1D-1'))
    model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-1'))
    model.add(Conv1D(128, 3, padding='same', name='Conv1D-2'))
    model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-2'))
    model.add(Conv1D(128, 3, padding='same', name='Conv1D-3'))
    model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-3'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

    Eğitim şöyle yapılabilir:

    from tensorflow.keras.callbacks import EarlyStopping

    esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
    hist = model.fit(scaled_training_dataset_x, training_dataset_y, epochs=100, batch_size=32, 
            validation_split=0.2, callbacks=[esc])

    Kesirim işlemi de şöyle yapılabilir:

    eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y)
    for i in range(len(eval_result)):
        print(f'{model.metrics_names[i]}: {eval_result[i]}')

    Kestirim işleminde bir günlük veri (WINDOW_SIZE kadar veri) "predict.csv" dosyasında bulundurulmuştur. Bu dosyadan verileri 
    okuyup yine aynı sayıllaştırmaları ve ölçeklemeyi uygulamamaız gereir:

    predict_df = pd.read_csv('predict.csv')

    predict_df['Month'] = predict_df['Date Time'].str[3:5]
    predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

    predict_df.drop(['Date Time'], axis=1, inplace=True)
    ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

    predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
    predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

    predict_dataset = predict_df.to_numpy('float32')
    scaled_predict_dataset = ss.transform(predict_dataset)

    predict_result = model.predict(scaled_predict_dataset.reshape(1, predict_dataset.shape[0], predict_dataset.shape[1]))

    for presult in predict_result[:, 0]:
        print(presult)

    Aşağıda örnek bir bütün olarak verilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

PREDICTION_INTERVAL = 24 * 60 // 10         # 144
WINDOW_SIZE = 24 * 60 // 10                 # 144
SLIDING_SIZE = 5

df = pd.read_csv('jena_climate_2009_2016.csv')

df['Month'] = df['Date Time'].str[3:5]
df['Hour-Minute'] = df['Date Time'].str[11:16]

df.drop(['Date Time'], axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)

ohe.fit(df[['Month', 'Hour-Minute']])
ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

raw_dataset_x = df.to_numpy('float32')
raw_dataset_y = df['T (degC)'].to_numpy('float32')

from sklearn.model_selection import train_test_split

raw_training_dataset_x, raw_test_dataset_x, raw_training_dataset_y, raw_test_dataset_y = \
        train_test_split(raw_dataset_x, raw_dataset_y, test_size=0.2, shuffle=False)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(raw_training_dataset_x)
raw_scaled_training_dataset_x = ss.transform(raw_training_dataset_x)
raw_scaled_test_dataset_x = ss.transform(raw_test_dataset_x)

def create_ts_dataset(dataset_x, dataset_y, pi, ws, ss):
    x = []
    y = []
    for i in range(0, len(dataset_x) - ws - pi, ss):
        x.append(dataset_x[i:i + ws])
        y.append(dataset_y[i + ws + pi - 1])
    return np.array(x), np.array(y)

scaled_training_dataset_x, training_dataset_y = \
        create_ts_dataset(raw_scaled_training_dataset_x, raw_training_dataset_y, PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE)

scaled_test_dataset_x, test_dataset_y = \
        create_ts_dataset(raw_scaled_test_dataset_x, raw_test_dataset_y, PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Reshape, Dense

model = Sequential(name='Jena-Climate')
model.add(Input((scaled_training_dataset_x.shape[1], scaled_training_dataset_x.shape[2]),  name='Input'))

model.add(Conv1D(128, 3, padding='same', name='Conv1D-1'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-1'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-2'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-2'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-3'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-3'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(scaled_training_dataset_x, training_dataset_y, epochs=100, 
        batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Squared Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mean_absolute_error'])
plt.plot(hist.epoch, hist.history['val_mean_absolute_error'])
plt.legend(['MSE', 'Validation MSE'])
plt.show()

# evaluation

eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_df = pd.read_csv('predict.csv')

predict_df['Month'] = predict_df['Date Time'].str[3:5]
predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

predict_df.drop(['Date Time'], axis=1, inplace=True)

ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

predict_dataset = predict_df.to_numpy('float32')
scaled_predict_dataset = ss.transform(predict_dataset)

predict_result = model.predict(scaled_predict_dataset.reshape(1, predict_dataset.shape[0], predict_dataset.shape[1]))

for presult in predict_result[:, 0]:
    print(presult)
    
#----------------------------------------------------------------------------------------------------------------------------
    Daha önceden de belirttiğimiz gibi bu tür yoğun verilerin kullanıldığı durumlarda eğer mümkünse eğitim, test ve kestirim 
    işlemlerinin parçalı bir biçimde yapılması daha uygundur. Biz yukarıdaki örnekte tüm eğitim verilerini tek hamlede oluşturduk.
    Bu veriler de çok yer kaplıyordu. Şimdi aynı örneği daha önce görmüş olduğumuz parçalı eğitim tekniği ile gerçekleşirelim. 
    Parçalı eğitimde dikkat edilecek anahtar noktalar şunlardır:

    - Bizin parçalı eğitim sınıfına (DataGenerator sınıfına) bazı bilgileri geçirmemiz gerekir. Sınıfın __init__ metodu şöyle 
    olabilir:

    def __init__(self, raw_x, raw_y, batch_size, pi, ws, ss, *, shuffle=True):
        super().__init__() 
        self.raw_x = raw_x
        self.raw_y = raw_y
        self.batch_size = batch_size
        self.pi = pi
        self.ws = ws
        self.ss = ss
        self.shuffle = shuffle
        self.nbatches = (len(raw_x) - pi - ws) // batch_size // ss
        self.index_list = list(range((len(raw_x) - pi - ws) // ss))  

    - Sınıfın __len__ metodu bir epoch'un kaç batch'ten oluşacağı bilgisiyle geri döndürülmelidir. Bu hesap şöyle yapılmıştır:

    self.nbatches = (len(raw_x) - pi - ws) // batch_size // ss

    - Sınıfın __getitem__ metodu model sınıfının fit, evaluate gibi metotları tarafından köşeli parantez içerisine batch 
    numarası verilerek çağrılmaktadır. 

    - Epoch'lar arasında hiç karıştırma yapmayabiliriz. Ancak eğer karıştırma yapacaksak asıl veri kümesini karıştırmak 
    iyi bir fikir değildir. Biz örneğimizde bir batch'i oluşturacak olan her eleman için bir index numarası oluşturup 
    bu index dizini karıştırdık. __getitem__ metodu şöyle yazılmıştır:

    def __getitem__(self, batch_no):               
        x = np.zeros((self.batch_size, self.ws, self.raw_x.shape[1]))
        y = np.zeros(self.batch_size)
        for i in range(self.batch_size):
            offset = self.index_list[batch_no * self.batch_size + i] * self.ss  
            x[i] = self.raw_x[offset:offset + self.ws]
            y[i] = self.raw_y[offset + self.ws + self.pi - 1]
        return tf.convert_to_tensor(x), tf.convert_to_tensor(y)

    Burada baştan x ve y için içi sıfırlarla dolu NumPy dizileri yaratılmıştır. Sonra batch'in uzunluğu kadar bir döngü
    oluşturulmuştur. Karıştırılmış index listesindeki ilgi yer batch_no * self.batch_size ile elde edilmektedir. Bu index'ten
    itibaren bu dizide self.batch_size kadar ilerlenip oradaki index'ler kullanılırsa aslında asıl dizinin farklı yerlerine 
    erişilmiş olacaktır. Tabii diziden ilgili index çekildiğinde bunun asıl dizinin hangi offseti olacağı bu değerin self.ss
    ile çarpımıyla elde edilmiştir. 

    - Her epoch bittiğinde çağrılan on_epoch_end işleminde karıştırma yapılmaktadır:

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.index_list)   

    Aşağıda örnek bir bütün olarak verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

PREDICTION_INTERVAL = 24 * 60 // 10         # 144
WINDOW_SIZE = 24 * 60 // 10                 # 144
SLIDING_SIZE = 5

BATCH_SIZE = 32
EPOCHS = 200

df = pd.read_csv('jena_climate_2009_2016.csv')

df['Month'] = df['Date Time'].str[3:5]
df['Hour-Minute'] = df['Date Time'].str[11:16]

df.drop(['Date Time'], axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)

ohe.fit(df[['Month', 'Hour-Minute']])
ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

raw_dataset_x = df.to_numpy('float32') 
raw_dataset_y = df['T (degC)'].to_numpy('float32')

from sklearn.model_selection import train_test_split

raw_temp_dataset_x, raw_test_dataset_x, raw_temp_dataset_y, raw_test_dataset_y =  train_test_split(raw_dataset_x, 
        raw_dataset_y, test_size=0.1, shuffle=False)

raw_training_dataset_x, raw_validation_dataset_x, raw_training_dataset_y, raw_validation_dataset_y =  \
        train_test_split(raw_temp_dataset_x, raw_temp_dataset_y, test_size=0.1, shuffle=False)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(raw_training_dataset_x)
raw_scaled_training_dataset_x = ss.transform(raw_training_dataset_x)
raw_scaled_validation_dataset_x = ss.transform(raw_validation_dataset_x)
raw_scaled_test_dataset_x = ss.transform(raw_test_dataset_x)

import tensorflow as tf
from tensorflow.keras.utils import PyDataset

class DataGenerator(PyDataset):
    def __init__(self, raw_x, raw_y, batch_size, pi, ws, ss, *, shuffle=True):
        super().__init__() 
        self.raw_x = raw_x
        self.raw_y = raw_y
        self.batch_size = batch_size
        self.pi = pi
        self.ws = ws
        self.ss = ss
        self.shuffle = shuffle
        self.nbatches = (len(raw_x) - pi - ws) // batch_size // ss
        self.index_list = list(range((len(raw_x) - pi - ws) // ss))  
        
    def __len__(self):
        return self.nbatches
    
    def __getitem__(self, batch_no):               
        x = np.zeros((self.batch_size, self.ws, self.raw_x.shape[1]))
        y = np.zeros(self.batch_size)
        for i in range(self.batch_size):
            offset = self.index_list[batch_no * self.batch_size + i] * self.ss     
            x[i] = self.raw_x[offset:offset + self.ws]
            y[i] = self.raw_y[offset + self.ws + self.pi - 1]
        return tf.convert_to_tensor(x), tf.convert_to_tensor(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.index_list)      
   
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Reshape, Dense

model = Sequential(name='Jena-Climate')
model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))

model.add(Conv1D(128, 3, padding='same', name='Conv1D-1'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-1'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-2'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-2'))
model.add(Conv1D(128, 3, padding='same', name='Conv1D-3'))
model.add(MaxPooling1D(2, padding='same', name='MaxPooling1D-3'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

dg_training = DataGenerator(raw_scaled_training_dataset_x, raw_training_dataset_y, BATCH_SIZE, PREDICTION_INTERVAL, 
            WINDOW_SIZE, SLIDING_SIZE, shuffle=False)
dg_validation = DataGenerator(raw_scaled_validation_dataset_x, raw_validation_dataset_y, BATCH_SIZE, PREDICTION_INTERVAL, 
            WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

hist = model.fit(dg_training, validation_data = dg_validation, epochs=EPOCHS, verbose=1, callbacks=[esc]) 
                 
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Squared Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mean_absolute_error'])
plt.plot(hist.epoch, hist.history['val_mean_absolute_error'])
plt.legend(['MSE', 'Validation MSE'])
plt.show()

# evaluation

dg_test = DataGenerator(raw_scaled_test_dataset_x, raw_test_dataset_y, BATCH_SIZE, PREDICTION_INTERVAL, 
        WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

eval_result = model.evaluate(dg_test)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_df = pd.read_csv('predict.csv')

predict_df['Month'] = predict_df['Date Time'].str[3:5]
predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

predict_df.drop(['Date Time'], axis=1, inplace=True)

ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

predict_dataset = predict_df.to_numpy('float32')
scaled_predict_dataset = ss.transform(predict_dataset)

predict_result = model.predict(scaled_predict_dataset.reshape(1, predict_dataset.shape[0], predict_dataset.shape[1]))

for presult in predict_result[:, 0]:
    print(presult)

#----------------------------------------------------------------------------------------------------------------------------
    Biz yazısal örneklerde ve zaman serilerinde tek boyutlu evrişim işlemlerini gördük. Evrişim işlemi resimsel uygulamalarda
    pixel'leri birbirleriyle ilişkilendirmek için en önemli ve etkin işlemlerden biridir. Ancak metinsel uygulamalarda ve zaman
    serilerinde evrişim işlemi bazı nedenlerden dolayı önemli faydalar sağlayamaktadır. Anımsanacağı gibi bir evrişim işleminde 
    birbirine yakın öğeleri ilişkilendirmeye çalışıyorduk. Sonra yeniden evrişim işlemleriyle bunu daha büyük öeğelere yaydırmaya
    çalışıyorduk. Ancak evrişim işlemi metinsel uygulamalarda ve zaman serilerinde iyi bir bağlamsal etki oluşturamamaktadır. 
    Bu tür uygulamalarda ağa hafıza kazandırmak gerekir. İşte ağa hafıza kazandırmak için "geri beslemeli ağlardan (recurrent 
    neural networks)" faydalanılmaktadır. 

    Geri beslemeli ağlarda temel fikir çıktının bir biçimde girdi ile ilişkilendirilip unutulmamasının sağlanmasıdır. Eğitim 
    sırasında bir önceki çıktı bir sonraki girdi ile kombine edilerek ağa verilmektedir. Örneğin ağında aşağıdaki girdileri
    teket teker aldığını varsayalım:

    xxxxxxxxxxxx
    yyyyyyyyyyyy
    zzzzzzzzzzzz
    kkkkkkkkkkkk

    Biz önce katmana x'lerin bulunduğu satırı, sonra y'lerin bulunduğu satırı ve sonra da sırasıyla diğerlerini girdi olarak 
    verdiğimizi düşünelim. İşte katmana x'lerin bulunduğu satırı verdiğimizde katmandan elde edeceğimiz çıktı değerini y'lerin 
    bulunduğu satırı verirken kombine ederek veririz. y'lerden elde edilen çıktıyı da z'leri katmana verirken kombine ederiz. 
    Yani her çıktıyı bir sonraki girdi ile kombine ederek ağa verebiliriz. Peki bu bize ne sağlayacaktır? İşte bu sayede 
    ağın eski bilgileri unutmaması onlardan elde edilen ana fikrin sürekli taze tutması sağlanmaktadır. Aslında bu yöntem 
    insanın hafıza sistemine de benzemektedir. Biz bir bilgiyi kalıcı hale getirmek için sürekli tekrarlarız. Tekrarlanmayan 
    bilgi kısa süreli hafızadan (short term memory) uçup gitmektedir. Peki bu geri besleme fikri bu haliyle ağa hafıza 
    kazandırmakta yeterli olmakta mıdır? Geri beslemeli ağlar bu anlamda ağa hafıza kazandırmaya önemli bir katkı sağlamaktadır. 
    Ancak bu haliyle ağ eski bilgileri uzun süre hafızasında tutamamaktadır. Bu probleme genel olarak "gradyan kaybolması 
    problemi (vanishing gradient problem)" denilmektedir. Son on senedir bu problem üzerinde çokça çalışılmış ve geri beslemeli 
    ağlar bu problemi tam olarak ortadan kaldırmasa da azaltacak biçimde evrimleşmiştir. Gradyan kaybolması (vanishing gradient) 
    hakkında ileride bilgiler verilecektir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Geri beslemeli ağlardaki geri besleme bir katman biçiminde oluşturulmaktadır. Bu katmanda yine n tane nörun bulunur. Ancak 
    bu katmana girdiler tek hamede değil satır satır verilir. Girdinin her satırından bir çıktı elde edilir. Sonra bu çıktı 
    girdinin sonraki satırıyla işleme sokularak yine geri besleme katmanına sokulur. Böylece katmanın her çıktısı sonraki girişle 
    işleme sokularak bir hafıza oluşturulmaya çalışılır. Tabii geri besleme katmanı genellikle tek başına kullanılmaz. Bu katmanın 
    çıktısı daha önceleri yaptığımız gibi Dense katmanlara verilir. Yani geri besleme katmanı genellikle derin ağlardaki ilk 
    katmanları oluşturmaktadır. 

    Bir Dense katmandaki bir nöronu düşünelim. Bu nörona o katmanın girdisi kadar giriş uygulanmaktadır. Aynı zamanda bu nöronda 
    bir "bias" değeri de vardır. Anımsanacağı gibi bu nöronun çıktısı şöyle oluşturulmaktadır:

    activation(dot(W, X) + b) ---> çıktı

    Bizim bu nöronda konumlandırmaya çalıştığımız değerler W ve b değerleridir. Örneğin 5 girdiye sahip olan bir Dense katmandaki
    nörounun çıktısı şöyle hesaplanmaktadır:

    activation(w0x0 + w1x1 + w2x2 + w3x3 + w4x4 + b) ---> çıktı

    Bu nöronda toplam 6 tane eğitilebilir parametre olduğuna dikkat ediniz. Eğer Dense katmanda bunun gibi N tane nöron varsa 
    bu durumda eğitilebilir parametrelerin sayısı N * 6 olacaktır. Katmanın girdi nöronlarının sayısı K tane olmak üzere Dense 
    katmandaki eğitilebilir parametrelerin sayısının K * N + N olduğunu anımsayınız. 
    
    Şimdi bir zaman serisi veri kümesindeki satırları geri besleme (recurrent) katmanına uygulayalım. Yine katmandaki belli bir 
    nörounu dikkate alalım. Katmanda toplam 10 tane nöron olsun. Katmanın girişi de (özelliklerin sayısı) 5 tane olsun. İşte geri 
    besleme katmanında her yeni girdi değerleri bu nörona aynı zamanda bir önceki çıktı değerleriyle birlikte verilir. Yani nöronun 
    girdileri yalnızca gerçek girdilerden değil aynı zamanda bir önceki çıktılardan da oluşmaktadır. Bu örneğimizde nöronun toplam 
    5 tane girdisi vardır. Katmanda da 10 nöron vardır. O halde katmandaki bir önceki çıktı da 10 tane olacaktır. İşte yeni girdi 
    (sıradaki satır) ile katmanın bir önceki çıktı nöronlarının değeri bu nörona girdi yapılmaktadır. Bu durumda katmanın girdisi 
    K tane nörondan oluşuyorsa ve katmanda N tane nöron varsa bu nöronun girdisi K + N tane olacaktır. Tabii bir de "bias" değeri 
    vardır. Bu durumda bu nörondaki eğitilebilir parametrelerin sayısı K + N + 1 tane olacaktır. Örneğimizde nöroa 5 + 10 = 15
    giriş uygulanacaktır. Bu 15 giriş ağırlık değerleriyle dot product yapılacak ve buna bir de "bias" değeri toplanacaktır. Sonra 
    da elde edilen bu değere aktivasyon fonksiyonu uygulanacaktır. 

    Geri beslemeli ağlardaki katmanlarda bulunan nöronların çıktılarını aşağıdaki gibi formülüze edebiliri:
   
    ht = activation(dot(W, xt) + dot(U, ht-1) + b)

    Burada ht nöronun çıktısını, xt uygulanan girdiyi belirtmektedir. ht-1 ise katmanın bir önceki çıktısını temsil etmektedir. 
    W değeri girdiler için konumlandırılacak ağırlık değerlerini U ise geri besleme için konumlandırılacak ağırlık değerlerini 
    belirtmektedir. 

    Geri besleme katmanı Keras'ta SimpleRNN isimli katman sınıfıyla temsil edilmektedir. Bu katman değerleri satır satır ele alıp 
    yukarıda belirttiğimiz gibi bir işlem yapmaktadır. 

    Peki geri besleme katmanındaki toplam eğitilebilir parametrelerin sayısı nasıl hesaplanmaktadır? Bu katmana K tane girdi 
    uygulandığını katmanda da N tane nöron olduğunu düşünelim. Bu durumda katmandaki her nöronda yukarıda belirttiğimiz gibi 
    K + N + 1 tane eğitilebilir parametre bulunacaktır. Bu nörondan toplam N tane olduğuna göre katmandaki toplam eğitilebilir 
    parametrelerin sayısı (K + N + 1) * N tane olacaktır. 

    SimpleRNN sınıfının __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.layers.SimpleRNN(
        units,
        activation='tanh',
        use_bias=True,
        kernel_initializer='glorot_uniform',
        recurrent_initializer='orthogonal',
        bias_initializer='zeros',
        kernel_regularizer=None,
        recurrent_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        recurrent_constraint=None,
        bias_constraint=None,
        dropout=0.0,
        recurrent_dropout=0.0,
        return_sequences=False,
        return_state=False,
        go_backwards=False,
        stateful=False,
        unroll=False,
        seed=None,
        **kwargs
    )

    Metodun ilk parametresi katmandaki nöron sayısını belirtmektedir. Aktivasyon fonksiyonunun default olarak "tanh" biçiminde 
    alındığına dikkat ediniz. Geri besleme katmanlarında ReLU fonksiyonu yerine tanh (hiperbolik tanjant) fonksiyonu tercih 
    edilmektedir. Bu fonksiyon bu katmanda "gradyen azalması (vanishing gradient)" problemine nispeten bir direnç oluşturmaktadır.

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

    model = Sequential(name='SimpleRNN-Test')
    model.add(Input((100, 10),  name='Input'))
    model.add(SimpleRNN(128,  activation='tanh', name='SimpleRNN'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Bu modeldeki özet bilgi şöyle elde edilmiştir:

    Model: "SimpleRNN-Test"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ SimpleRNN (SimpleRNN)           │ (None, 128)            │        17,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 128)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │        33,024 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 116,865 (456.50 KB)
    Trainable params: 116,865 (456.50 KB)
    Non-trainable params: 0 (0.00 B)

    Biz Dense katmanlardaki eğitilebilir parametrelerin nasıl hesaplandığını zaten görmüştük. SimpleRNN katmanınında girdi 
    olarak 10 nöron uygulanmaktadır. Katmanda toplam 128 nöron vardır. Bu durumda katmandaki her nörona aslında 10 + 128
    girdi uygulanmaktadır. Bir nörondaki toplam eğitilebilir parametrelerin sayısı 10 + 128 + 1 tane olacaktır. Katmanda toplam 
    128 nöron olduğuna göre eğitilebilir parametrelerin toplam sayısı 128 * (10 + 128 + 1) = 17792 tanedir. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

model = Sequential(name='SimpleRNN-Test')
model.add(Input((100, 10),  name='Input'))
model.add(SimpleRNN(128,  activation='tanh', name='SimpleRNN'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıdaki örnekte SimpleRNN katmanının bir simülasyonu yapılmıştır. Bu örnekte girdiler satırlardan oluşmaktadır. Her 
    satır bir önceki çıktı ile dot-product yapılmaktadır. Ayrıca örnekte her satır için elde edilen çıktıların biriktirildiğini 
    de görüyorsunuz. Bu biriktirme hakkında izleyen paragrafta açıklamalar yapacağız.
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np

TIMESPEC = 100                      # ardışıl uygulanacak eleman sayısı
INPUT_FEATURES = 32                 # girdi katmanındaki nöron sayısı
RECURRENT_LAYER_SIZE = 64           # geri besleme katmanındaki nöron sayısı

def activation(x):
    return np.maximum(0, x)

inputs = np.random.random((TIMESPEC, INPUT_FEATURES))
W = np.random.random((RECURRENT_LAYER_SIZE, INPUT_FEATURES))
U = np.random.random((RECURRENT_LAYER_SIZE, RECURRENT_LAYER_SIZE))
b = np.random.random((RECURRENT_LAYER_SIZE,))

outputs = []

output_t = np.random.random((RECURRENT_LAYER_SIZE,))
for input_t in inputs:
    output_t = activation(np.dot(W, input_t) + np.dot(U, output_t) + b)
    outputs.append(output_t)

total_outputs = np.concatenate(outputs, axis=0)

print(total_outputs)

#----------------------------------------------------------------------------------------------------------------------------
    Peki yukarıdaki gibi bir geri besleme katmanının çıktısı nasıl Dense katmanlara bağlanacaktır? Burada iki durum söz 
    konusu olabilir. Bu geri besleme katmanının nihai tek bir çıktısı sonraki Dense katmana bağlanabilir. Ya da her zamansal 
    girdinin çıktıları briktirilip düzleştirildikten sonra Dense katmana bağlanabilir. Yalnızca son parçalı girdinin Dense 
    katmana bağlanması bilgi kayıplarına yol açmaktadır. Dolayısıyla bunların biriktirilip düzleştirildikten sonra Dense katmana 
    verilmesi daha uygun bir yöntemdir. SimpleRNN katmanının parametrik yapısına bir daha dikkat ediniz:
    
    tf.keras.layers.SimpleRNN(
        units,
        activation='tanh',
        use_bias=True,
        kernel_initializer='glorot_uniform',
        recurrent_initializer='orthogonal',
        bias_initializer='zeros',
        kernel_regularizer=None,
        recurrent_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        recurrent_constraint=None,
        bias_constraint=None,
        dropout=0.0,
        recurrent_dropout=0.0,
        return_sequences=False,
        return_state=False,
        go_backwards=False,
        stateful=False,
        unroll=False,
        **kwargs
    )

    Buradaki nunits parametresi geri besleme katmanındaki nöron sayısını belirtmektedir. Geri besleme katmanlarının default 
    aktivasyon fonksiyonun "tanh" biçiminde olduğunu belirtmiştik. Hiperbolik tanjant fonksiyonu bu katmanlarda en çok tercih 
    edilen aktivasyon fonksiyonudur. Fonksiyonun diğer önemli parametresi return_sequences isimli parametredir. Bu parametre 
    True geçilirse (default durum False biçimdedir) bu durumda katman her zamansal girişin (satırların) çıktılarını biriktirir. 
    Eğer bu parametre True geçilmezse bu biriktirme yapılmaz. Dolayısıyla sonraki katmana yalnızca son zamansal verinin çıktısı 
    sokulur. 

    Peki SimpleRNN katmanının girdisi nasıl olmalıdır? İşte bu katmanın girdisi bir matris olmalıdır. Matrisin her satırı 
    zamansal veriyi belirtmektedir. Keras bu durumda bu matrisin her bir satırını zamansal veri biçiminde ele alır ve önceki 
    çıktıyla işleme sokar. (Tabii Keras paralel programlama teknikleri ile daha karmaşık bir gerçekleştirime sahiptir. Ancak 
    SimpleRNN katmanı satırları tek tek ele alıp kendi içerisinde yukarıda belirttiğimiz gibi işleme sokmaktadır.)

    Şimdi SimpleRNN katmanındaki return_sequences parametresinin True geçilmesi durumundaki çıktının nasıl olacağı üzerinde 
    duralım. Normalde bu katmanın çıktısı katmandaki nöron sayısı kadardır. Ancak return_sequences parametresi True yapıldığı 
    zaman her bir satırın çıktısı biriktirilecektir. Katmandaki nöron sayısı N ve zamansal verilerdeki satır sayısı R olmak 
    üzere çıktı da RxN'lik bir matris haline gelecektir. 
    
    Şimdi de daha önce yapmış olduğumuz örnek modeldeki return_sequences parametresini True yaparak elde edilen modelin örnek 
    çıktısını inceleyelim. Anımsanacağı gibi bu modelde her satırda toplam 10 tane özellik, SimpleRNN kkatmanında da 128 nöron 
    bulunuyordu:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

    model = Sequential(name='SimpleRNN-Test')
    model.add(Input((100, 10),  name='Input'))
    model.add(SimpleRNN(128,  activation='tanh', return_sequences=True, name='SimpleRNN'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Modelin özet çıktısı şöyledir:

    Model: "SimpleRNN-Test"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ SimpleRNN (SimpleRNN)           │ (None, 100, 128)       │        17,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 12800)          │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     3,277,056 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 3,360,897 (12.82 MB)
    Trainable params: 3,360,897 (12.82 MB)
    Non-trainable params: 0 (0.00 B)

    Burada her veri 100 satır 10 sütundan oluşmaktadır. Bu 100 satır SimpleRNN katmanına yukarıda açıkladığımız gibi peşi sıra 
    uygulanacaktır. Ancak toplamda bu 100 satır uygulandığında tek bir çıkış elde edilmeyecek bu çıkışlar biriktirilecektir. 
    Bu durumda SimpleRNN katmanının çıktısı 128 değil 100 * 128 = 12800 tane olacaktır. Biz bu matrisi reshape yaparak sonraki 
    Dense katmana uygulayacağız. Örneğimizde birinci saklı katmandaki eğitilebilir parametrelerin sayısının da arttığını 
    görüyorsunuz. Bu katmandaki eğitilebilir parametrelerin sayısı artık 12800 * 256 + 256 = 3277056 tane olacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi Jena Climate örneğini SimpleRNN katmanını kullanarak yeniden yapalım. Modelimiz şöyle olabilir:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

    model = Sequential(name='Jena-Climate')
    model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))

    model.add(SimpleRNN(128, activation='tanh', name='SimpleRNN'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Daha önce yapmış olduğumuz modeldeki Conv1D ve MaxPooling1D katmanlarını kaldırarak onların yerine SimpleRNN katmanını 
    yerleştirdik. Modlein özet çıktısı da şöyledir:

        Model: "Jena-Climate"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ SimpleRNN (SimpleRNN)           │ (None, 128)            │        38,272 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 128)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │        33,024 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 137,345 (536.50 KB)
    Trainable params: 137,345 (536.50 KB)
    Non-trainable params: 0 (0.00 B)
    
    Aşağıda örneği bir bütün olarak veriyoruz.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

PREDICTION_INTERVAL = 24 * 60 // 10         # 144
WINDOW_SIZE = 24 * 60 // 10                 # 144
SLIDING_SIZE = 5

BATCH_SIZE = 32
EPOCHS = 200

df = pd.read_csv('jena_climate_2009_2016.csv')

df['Month'] = df['Date Time'].str[3:5]
df['Hour-Minute'] = df['Date Time'].str[11:16]

df.drop(['Date Time'], axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)

ohe.fit(df[['Month', 'Hour-Minute']])
ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

raw_dataset_x = df.to_numpy('float32') 
raw_dataset_y = df['T (degC)'].to_numpy('float32')

from sklearn.model_selection import train_test_split

raw_temp_dataset_x, raw_test_dataset_x, raw_temp_dataset_y, raw_test_dataset_y =  \
        train_test_split(raw_dataset_x, raw_dataset_y, test_size=0.1, shuffle=False)

raw_training_dataset_x, raw_validation_dataset_x, raw_training_dataset_y, raw_validation_dataset_y =  \
        train_test_split(raw_temp_dataset_x, raw_temp_dataset_y, test_size=0.1, shuffle=False)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(raw_training_dataset_x)
raw_scaled_training_dataset_x = ss.transform(raw_training_dataset_x)
raw_scaled_validation_dataset_x = ss.transform(raw_validation_dataset_x)
raw_scaled_test_dataset_x = ss.transform(raw_test_dataset_x)

import tensorflow as tf
from tensorflow.keras.utils import PyDataset

class DataGenerator(PyDataset):
    def __init__(self, raw_x, raw_y, batch_size, pi, ws, ss, *, shuffle=True):
        super().__init__() 
        self.raw_x = raw_x
        self.raw_y = raw_y
        self.batch_size = batch_size
        self.pi = pi
        self.ws = ws
        self.ss = ss
        self.shuffle = shuffle
        self.nbatches = (len(raw_x) - pi - ws) // batch_size // ss
        self.index_list = list(range((len(raw_x) - pi - ws) // ss))  
        
    def __len__(self):
        return self.nbatches
    
    def __getitem__(self, batch_no):               
        x = np.zeros((self.batch_size, self.ws, self.raw_x.shape[1]))
        y = np.zeros(self.batch_size)
        for i in range(self.batch_size):
            offset = self.index_list[batch_no * self.batch_size + i] * self.ss 
            x[i] = self.raw_x[offset:offset + self.ws]
            y[i] = self.raw_y[offset + self.ws + self.pi - 1]
        return tf.convert_to_tensor(x), tf.convert_to_tensor(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.index_list)      
   
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

model = Sequential(name='Jena-Climate')
model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))

model.add(SimpleRNN(128, activation='tanh', name='SimpleRNN'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

dg_training = DataGenerator(raw_scaled_training_dataset_x, raw_training_dataset_y, 
        BATCH_SIZE, PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

dg_validation = DataGenerator(raw_scaled_validation_dataset_x, raw_validation_dataset_y, 
        BATCH_SIZE, PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

hist = model.fit(dg_training, validation_data = dg_validation, epochs=EPOCHS, verbose=1, callbacks=[esc]) 
                 
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Squared Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mean_absolute_error'])
plt.plot(hist.epoch, hist.history['val_mean_absolute_error'])
plt.legend(['MSE', 'Validation MSE'])
plt.show()

# evaluation

dg_test = DataGenerator(raw_scaled_test_dataset_x, raw_test_dataset_y, BATCH_SIZE, 
        PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

eval_result = model.evaluate(dg_test)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_df = pd.read_csv('predict.csv')

predict_df['Month'] = predict_df['Date Time'].str[3:5]
predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

predict_df.drop(['Date Time'], axis=1, inplace=True)

ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

predict_dataset = predict_df.to_numpy('float32')
scaled_predict_dataset = ss.transform(predict_dataset)

predict_result = model.predict(scaled_predict_dataset.reshape(1, predict_dataset.shape[0], predict_dataset.shape[1]))

for presult in predict_result[:, 0]:
    print(presult)

#----------------------------------------------------------------------------------------------------------------------------
                                            73. Ders - 20/10/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de aynı örneği return_sequences parametresini True geçerek yapalım:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

    model = Sequential(name='Jena-Climate')
    model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))

    model.add(SimpleRNN(128, activation='tanh', return_sequences=True, name='SimpleRNN'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Modelin özet çıktısı şöyledir:

    Aşağıdaki örnek yukarıdaki örneğin return_sequences=True dışında tamamen aynısıdır.
 #----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

PREDICTION_INTERVAL = 24 * 60 // 10         # 144
WINDOW_SIZE = 24 * 60 // 10                 # 144
SLIDING_SIZE = 5

BATCH_SIZE = 32
EPOCHS = 200

df = pd.read_csv('jena_climate_2009_2016.csv')

df['Month'] = df['Date Time'].str[3:5]
df['Hour-Minute'] = df['Date Time'].str[11:16]

df.drop(['Date Time'], axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)

ohe.fit(df[['Month', 'Hour-Minute']])
ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

raw_dataset_x = df.to_numpy('float32') 
raw_dataset_y = df['T (degC)'].to_numpy('float32')

from sklearn.model_selection import train_test_split

raw_temp_dataset_x, raw_test_dataset_x, raw_temp_dataset_y, raw_test_dataset_y =  train_test_split(raw_dataset_x, 
        raw_dataset_y, test_size=0.1, shuffle=False)

raw_training_dataset_x, raw_validation_dataset_x, raw_training_dataset_y, raw_validation_dataset_y =  \
        train_test_split(raw_temp_dataset_x, raw_temp_dataset_y, test_size=0.1, shuffle=False)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(raw_training_dataset_x)
raw_scaled_training_dataset_x = ss.transform(raw_training_dataset_x)
raw_scaled_validation_dataset_x = ss.transform(raw_validation_dataset_x)
raw_scaled_test_dataset_x = ss.transform(raw_test_dataset_x)

import tensorflow as tf
from tensorflow.keras.utils import PyDataset

class DataGenerator(PyDataset):
    def __init__(self, raw_x, raw_y, batch_size, pi, ws, ss, *, shuffle=True):
        super().__init__() 
        self.raw_x = raw_x
        self.raw_y = raw_y
        self.batch_size = batch_size
        self.pi = pi
        self.ws = ws
        self.ss = ss
        self.shuffle = shuffle
        self.nbatches = (len(raw_x) - pi - ws) // batch_size // ss
        self.index_list = list(range((len(raw_x) - pi - ws) // ss))  
        
    def __len__(self):
        return self.nbatches
    
    def __getitem__(self, batch_no):               
        x = np.zeros((self.batch_size, self.ws, self.raw_x.shape[1]))
        y = np.zeros(self.batch_size)
        for i in range(self.batch_size):
            offset = self.index_list[batch_no * self.batch_size + i] * self.ss 
            x[i] = self.raw_x[offset:offset + self.ws]
            y[i] = self.raw_y[offset + self.ws + self.pi - 1]
        return tf.convert_to_tensor(x), tf.convert_to_tensor(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.index_list)      
   
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

model = Sequential(name='Jena-Climate')
model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))
model.add(SimpleRNN(128, activation='tanh', return_sequences=True, name='SimpleRNN'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

dg_training = DataGenerator(raw_scaled_training_dataset_x, raw_training_dataset_y, BATCH_SIZE, 
        PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

dg_validation = DataGenerator(raw_scaled_validation_dataset_x, raw_validation_dataset_y, BATCH_SIZE, 
        PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

hist = model.fit(dg_training, validation_data = dg_validation, epochs=EPOCHS, verbose=1, callbacks=[esc]) 
                 
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Squared Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mean_absolute_error'])
plt.plot(hist.epoch, hist.history['val_mean_absolute_error'])
plt.legend(['MSE', 'Validation MSE'])
plt.show()

# evaluation

dg_test = DataGenerator(raw_scaled_test_dataset_x, raw_test_dataset_y, BATCH_SIZE, PREDICTION_INTERVAL, 
        WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

eval_result = model.evaluate(dg_test)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_df = pd.read_csv('predict.csv')

predict_df['Month'] = predict_df['Date Time'].str[3:5]
predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

predict_df.drop(['Date Time'], axis=1, inplace=True)

ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

predict_dataset = predict_df.to_numpy('float32')
scaled_predict_dataset = ss.transform(predict_dataset)

predict_result = model.predict(scaled_predict_dataset.reshape(1, predict_dataset.shape[0], predict_dataset.shape[1]))

for presult in predict_result[:, 0]:
    print(presult)

#----------------------------------------------------------------------------------------------------------------------------
    Aslında geri besleme katmanları genellikle bir kez değil üst üste birkaç kez uygulanmaktadır. Tıpkı üst üste evrişim 
    uygulamak gibi üst üste geri besleme uygulamak hafızanın güçlendirilmesine fayda sağlamaktadır. Tabii SimpleRNN katmanını
    birden fazla kez uygulayacaksak bir önceki katmanın çıktısının bir matris olması gerekir. Bu da önceki SimpleRNN katmanının 
    return_sequences parametresinin True geçilmesiyle sağlanabilir. Aşağıda birden fazla kez SimpleRNN katmanının kullanıldığı 
    bir model örneğini görüyorsunuz:

      from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

    model = Sequential(name='Jena-Climate')
    model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))

    model.add(SimpleRNN(128, activation='tanh', return_sequences=True, name='SimpleRNN-1'))
    model.add(SimpleRNN(128, activation='tanh', return_sequences=True, name='SimpleRNN-2'))
    model.add(SimpleRNN(128, activation='tanh', return_sequences=True, name='SimpleRNN-3'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Burada birinci SimpleRNN katmanının çıktısı artık bir matristir. Bu matris ikinci SimpleRNN katmanına sanki zaman serisi 
    gibi uygulanmıştır. İkinci SimpleRNN katmanının çıktısı da üçüncü SimpleRNN katmanına zaman serisi gibi uygulanmıştır.
    Buradaki SimpleRNN katmanlarının return_sequences parametrelerinin True geçildiğine dikkat ediniz. 

    Üst üste birden fazla kez geri besleme katmanı kullanıldığında modeldeki eğitilebilir parametrelerin sayısı artmaktadır. 
    Eğitilebilir parametrelerin sayısının artması "overfitting" ya da "underfitting" oluşturabilmektedir. Yukarıdaki modelin özet 
    çıktısı şöyledir:

    Model: "Jena-Climate"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ SimpleRNN-1 (SimpleRNN)         │ (None, 144, 128)       │        38,272 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ SimpleRNN-2 (SimpleRNN)         │ (None, 144, 128)       │        32,896 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ SimpleRNN-3 (SimpleRNN)         │ (None, 144, 128)       │        32,896 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 18432)          │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     4,718,848 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 4,888,961 (18.65 MB)
    Trainable params: 4,888,961 (18.65 MB)
    Non-trainable params: 0 (0.00 B)

    Burada toplam eğitilebilir parametrelerin sayısının 5 milyona yakın olduğu görülmektedir. 

    Aşağıda Jena Climate örneğinin üst üste üç kez SimpleRNN katmanının uygulandığı biçimi verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

PREDICTION_INTERVAL = 24 * 60 // 10         # 144
WINDOW_SIZE = 24 * 60 // 10                 # 144
SLIDING_SIZE = 5

BATCH_SIZE = 32
EPOCHS = 200

df = pd.read_csv('jena_climate_2009_2016.csv')

df['Month'] = df['Date Time'].str[3:5]
df['Hour-Minute'] = df['Date Time'].str[11:16]

df.drop(['Date Time'], axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)

ohe.fit(df[['Month', 'Hour-Minute']])
ohe_result = ohe.transform(df[['Month', 'Hour-Minute']])

df = pd.concat([df, pd.DataFrame(ohe_result)], axis=1)
df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

raw_dataset_x = df.to_numpy('float32') 
raw_dataset_y = df['T (degC)'].to_numpy('float32')

from sklearn.model_selection import train_test_split

raw_temp_dataset_x, raw_test_dataset_x, raw_temp_dataset_y, raw_test_dataset_y =  train_test_split(raw_dataset_x, 
        raw_dataset_y, test_size=0.1, shuffle=False)

raw_training_dataset_x, raw_validation_dataset_x, raw_training_dataset_y, raw_validation_dataset_y =  \
        train_test_split(raw_temp_dataset_x, raw_temp_dataset_y, test_size=0.1, shuffle=False)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(raw_training_dataset_x)
raw_scaled_training_dataset_x = ss.transform(raw_training_dataset_x)
raw_scaled_validation_dataset_x = ss.transform(raw_validation_dataset_x)
raw_scaled_test_dataset_x = ss.transform(raw_test_dataset_x)

import tensorflow as tf
from tensorflow.keras.utils import PyDataset

class DataGenerator(PyDataset):
    def __init__(self, raw_x, raw_y, batch_size, pi, ws, ss, *, shuffle=True):
        super().__init__() 
        self.raw_x = raw_x
        self.raw_y = raw_y
        self.batch_size = batch_size
        self.pi = pi
        self.ws = ws
        self.ss = ss
        self.shuffle = shuffle
        self.nbatches = (len(raw_x) - pi - ws) // batch_size // ss
        self.index_list = list(range((len(raw_x) - pi - ws) // ss))  
        
    def __len__(self):
        return self.nbatches
    
    def __getitem__(self, batch_no):               
        x = np.zeros((self.batch_size, self.ws, self.raw_x.shape[1]))
        y = np.zeros(self.batch_size)
        for i in range(self.batch_size):
            offset = self.index_list[batch_no * self.batch_size + i] * self.ss      
            x[i] = self.raw_x[offset:offset + self.ws]
            y[i] = self.raw_y[offset + self.ws + self.pi - 1]
        return tf.convert_to_tensor(x), tf.convert_to_tensor(y)
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.index_list)      
   
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, SimpleRNN, Reshape, Dense

model = Sequential(name='Jena-Climate')
model.add(Input((WINDOW_SIZE, raw_training_dataset_x.shape[1]),  name='Input'))

model.add(SimpleRNN(128, activation='tanh', return_sequences=True, name='SimpleRNN'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='linear', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

dg_training = DataGenerator(raw_scaled_training_dataset_x, raw_training_dataset_y, BATCH_SIZE, 
        PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

dg_validation = DataGenerator(raw_scaled_validation_dataset_x, raw_validation_dataset_y, BATCH_SIZE, 
        PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

hist = model.fit(dg_training, validation_data = dg_validation, epochs=EPOCHS, verbose=1, callbacks=[esc]) 
                 
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Mean Squared Error', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['mean_absolute_error'])
plt.plot(hist.epoch, hist.history['val_mean_absolute_error'])
plt.legend(['MSE', 'Validation MSE'])
plt.show()

# evaluation

dg_test = DataGenerator(raw_scaled_test_dataset_x, raw_test_dataset_y, BATCH_SIZE, 
        PREDICTION_INTERVAL, WINDOW_SIZE, SLIDING_SIZE, shuffle=False)

eval_result = model.evaluate(dg_test)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

predict_df = pd.read_csv('predict.csv')

predict_df['Month'] = predict_df['Date Time'].str[3:5]
predict_df['Hour-Minute'] = predict_df['Date Time'].str[11:16]

predict_df.drop(['Date Time'], axis=1, inplace=True)

ohe_result = ohe.transform(predict_df[['Month', 'Hour-Minute']])

predict_df = pd.concat([predict_df, pd.DataFrame(ohe_result)], axis=1)
predict_df.drop(['Month', 'Hour-Minute'], axis=1, inplace=True)

predict_dataset = predict_df.to_numpy('float32')
scaled_predict_dataset = ss.transform(predict_dataset)

predict_result = model.predict(scaled_predict_dataset.reshape(1, predict_dataset.shape[0], predict_dataset.shape[1]))

for presult in predict_result[:, 0]:
    print(presult)

#----------------------------------------------------------------------------------------------------------------------------
    Yazısal verilerin zaman serilerine benzediğinden bahsetmiştik. Her ne kadar yazılarda sözcüklerin bir zaman bilgisi (time 
    stamp) yoksa da sözcüklerin peşi sıra birbirini izelemesi onların zaman serilerine benzemesine yol açmaktadır. İşte bu 
    nedenle geri beslemeli ağlar yalnızca zaman serilerinde değil aynı zamanda metinlerin anlamlandırılmasında da kullanılmaktadır. 
    
    Biz metinler üzerinde işlemler yaparken Embedding katmanıyla sözcükleri vektörlerle ifade etmiştik. Her sözcük bir vektör
    (bir satır olarak düşünebiliriz) ile ifade edildiğine göre yazı da aslında vektörlerden oluşan bir matris biçiminde ele 
    alınabilir. O halde biz yazılar üzerinde işlemler yapan sinir ağlarında önce yazıları Embedding katmanına sokup bu katmanın 
    çıktısını da geri besleme katmanlarına verebiliriz. Böylece modelimizin katman yapısı aşağıdaki gibi olabilir:

    Yazı ---> Embedding ---> SimpleRNN ---> Dense ---> Dense ---> Çıktı
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi daha önce üzerinde çalıştığımız IMDB örneğini SimpleRNN katmanını kullanarak yeniden tasarlayalım. Modelin katman 
    yapısı şöyle olabilir:

    TEXT_SIZE = 250
    WORD_VECT_SIZE = 64
    # ....

    model = Sequential(name='IMBD-WordEmbedding')
    model.add(Input((TEXT_SIZE, ), name='Input'))
    model.add(Embedding(len(cv.vocabulary_) + 1, WORD_VECT_SIZE, name='Embedding'))
    model.add(SimpleRNN(64, activation='tanh', return_sequences=True, name='SimpleRNN-1'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Burada önce bir Embedding katman kullanılmıştır. Bu katmandan çıktı olarak her biri 64 sütundan, TEXT_SIZE kadar satırdan 
    oluşan 64'lü sırasal değerler elde edilmiştir. Bu 64'lü girişler 64 nörondan oluşan SimpleRNN katmanına girdi yapılmıştır. 
    SimpleRNN katmanında return sequences=True parametresinin girildiğine dikkat ediniz. Bu durumda her bir sözcüğün çıktısı 
    olan 64'lük vektörler bir matris biçiminde biriktirilmektedir. Sonra bunlar  Reshape katmanı ile düzleştirilip Dense katmanlara 
    verilmiştir. Bu örnekte biz yalnızca tek bir SimpleRNN katmanı kullandık. Burada birden fazla SimpleRNN katmanın kullanılması 
    parametre sayısının aşırı artması nedeniyle bir "underfitting" olgusuna yol açabilmektedir. İzleyen paragraflarda eğitilebilir
    parametrelerin sayısının azaltılması için Dropbox regülasyonu ele alınmaktadır.

    Aşağıda bu uygulamanın tüm kodları verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'])

import re

text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

from tensorflow.keras.utils import pad_sequences

dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Reshape, Dense

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((TEXT_SIZE, ), name='Input'))
model.add(Embedding(len(cv.vocabulary_) + 1, WORD_VECT_SIZE, name='Embedding'))
model.add(SimpleRNN(64, activation='tanh', return_sequences=True, name='SimpleRNN-1'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] 
                        for text in predict_df['review']]

predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')

predict_result = model.predict(predict_dataset_x)
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıdaki örneklerden elde edilen sonuçlar aslında bu hali ile SimpleRNN katmanının model üzerinde ciddi bir iyileşme 
    sağlamadığı yönündedir. Daha önce yapmış olduğumuz evrişim işlemi daha iyi bir sonucun elde edilmesine yol açmıştır. Peki 
    bu durumda geri besleme IMDB örneğinde fayda sağlamayacak mıdır? Aslında geri besleme bir hazfıza oluşturmaktadır. Ancak 
    SimpleRNN tek başına bu hafıza oluşumu için yeterli değildir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağlarında "overfitting" ve "underfitting" durumunu azaltmak için kullanılan teniklere "düzenleme (regularization)" 
    teknikleri denilmektedir. Bu bağlamda çeşitli düzenleme teknikleri geliştirilmiştir. Bunalardan önemli olanları şunlardır:

    - L1 (Lasso) ve L2 (Ridge) Düzenlemeleri
    - Dropout Düzenlemesi
    - Batch Normalization Düzenlemesi
    - Erken Sonlandırma (Early Stopping) Düzenlemesi
    - Verilen Çoğaltılması (Data Augmentation) Yoluyla Yapılan Düzenlemeler
    - Model Karmaşıklığını Azaltma Yoluyla Yapılan Düzenlemeler

    Biz bu yöntemlerden "erken sonlandırma (early stopping)" ve verilerin çolğaltılması (data augmentation)" konularını görmüştük. 
    Anımsanacağı gibi erken sonlandırma eğitimdeki metrik değerlerle sınama değerlerinin birbirinden kopması durumunda epoch 
    kaynaklı overfitting durumunu engellemek için kullanılıyordu. Verilerin çoğaltılması veri kümesinin büyütülmesi yoluyla 
    "overfitting" ve "undefitting" olgusunun azaltılmasına katkı sağlıyordu. Biz L1 ve L2 düzenlemelerini daha sonra göreceğiz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------  
    Dropout düzenlemesi 2014 yılında bazı deneysel çalışmalar eşliğinde bulunmuştur. Bu teknikte bir katmandaki nöronların 
    bazıları rastgele biçimde katmandan atılmaktadır. Böylece ağın ezberlediği yanlış şeylerin unutturulması sağlanmaktadır. 
    Dropout uygularken belli bir olasılık belirtilir. Bu olasılık o katmandaki nöronların atılma olasılığıdır. Tipik olarak 
    0.2 ile 0.5 arasındaki değerler çok kullanılmaktadır. Bazı uygulamacılar girdi katmanlarında 0.8'e varan daha yüksek 
    olasılıkları kullanmaktadır. Buradaki atılma olasılığı bir yüzde belirtmemektedir. Buradaki olasılık katmandaki her nöron 
    için ayrı ayrı uygulanan olasılıktır. Yani örneğin biz dropout olasılığını 0.1 yaptığımızda bu katmanın öncesindeki katmanda 
    100 nöron varsa bu durum bu 100 nöronun kesinlikle 10 tanesinin atılacağı anlamına gelmemektedir. Dropout düzenlemesi 
    çıktı katmanı dışındaki tüm katmanlara uygulanabilmektedir. 

    Keras'ta dropout işlemi Dropout isimli bir katman ile temsil edilmiştir. Dropout sınıfının __init__ metdounun parametrik 
    yapısı şöyledir:

    tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)

    Metodun birinci parametresi droput olasılığını belirtmektedir. Droput katmanı ondan önceki katmanın nöronlarını atmaktadır. 
    Eğitim sırasında her batch işleminde katmandaki aynı nöronlar atılmamaktadır. Eğitim sırasında katmanın farklı nöronları 
    atılarak işlemler yapılmaktadır. Yani Keras'ın Dropout katmanı nöron atmayı epoch temelinde değil batch temelinde yapar. 
    Tabii aslında nöronlar gerçek anlamda modelden atılmamaktadır. Yalnızca onların çıktıları 0'a çekilmektedir. Böylece dot-product
    işleminde işlemden 0 elde edilmektedir. Bu da nöron atılmış gibi bir etki oluşturmaktadır. 

    Katmandaki nöronların bir kısmı dropout işlemiyle atıldığında dot product sonucunda elde edilen değer toplamı azalır. Bu 
    durumu ortadan kaldırmak için genellikle uygulamacılar dropout işleminde atılmayan nöronların çıktılarını atılan nöronların 
    oranı kadar artırırlar. Yani örneğin bir katmandaki dropout olasılığı 0.20 ise bu katmanda atılmayan nöronların çıktıları 
    da o oranda artırılmaktadır. rate atılma oranını belirtmek üzere matematiksel olarak bu artırma 1 / (1 - rate)  işleminden 
    elde edilen değer kullanılarak yapılmaktadır. Tabii Keras'ta biz bu işlemi manuel olarak yapmayız. Zaten Keras'ın Dropout 
    katmanı böyle davranmaktadır. 

    Dropout işlemi yalnızca eğitimde uygulanan bir işlemdir. Ağ eğitildikten sonra test ve kestirim işlemlerinde dropout uygulanmaz.
    Burada dikkat edilmesi gereken bir nokta şudur: Dropout işlemi nöronun çıktısını sıfırlamamaktadır. Bir batch'lik işlemde 
    0 gibi göstermektedir. (Zaten Dropout katmanı önceki katmanın çıkışına uygulandığına göre önceki katmandaki nöronların 
    ağırlıkları üzerinde bir değişiklik yapamamaktadır.)

    Aşağıda dropout işleminin etkisine yönelik bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.layers import Dropout
import numpy as np

data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype='float')

dropout_layer = Dropout(0.8)

result = dropout_layer(data, training=True)
print(result)

result = dropout_layer(data, training=True)
print(result)

#----------------------------------------------------------------------------------------------------------------------------
                                            74. Ders - 26/10/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de IMDB örneğinde Dropout düzenlemesinden faydalanalım. Modelimiz şöyle olabilir:

    Input --> Embedding --> Dropout (0.3) --> SimpleRNN --> Reshape --> Dropout(0.3) --> Dense ---> Dropout (0.3) --> 
    Dropout (0.3) --> Dense (output)

    Model aşağıda verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 250
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'])

import re

text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in df['review']]

from tensorflow.keras.utils import pad_sequences

dataset_x = pad_sequences(text_vectors, TEXT_SIZE, dtype='float32')
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Embedding, Dropout, SimpleRNN, Reshape, Dense

model = Sequential(name='IMBD-WordEmbedding')
model.add(Input((TEXT_SIZE, ), name='Input'))
model.add(Embedding(len(cv.vocabulary_) + 1, WORD_VECT_SIZE, name='Embedding'))
model.add(Dropout(0.3, name='Dropout-1'))
model.add(SimpleRNN(64, activation='tanh', return_sequences=True, name='SimpleRNN-1'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dropout(0.3, name='Dropout-2'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dropout(0.3, name='Dropout-3'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dropout(0.3, name='Dropout-4'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_text_vectors = [[cv.vocabulary_[word] + 1  for word in re.findall(r'(?u)\b\w\w+\b', text.lower())] for text in predict_df['review']]

predict_dataset_x = pad_sequences(predict_text_vectors, TEXT_SIZE, dtype='float32')

predict_result = model.predict(predict_dataset_x)
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Geri beslemeli ağlarda (Recurrent Neural Networks) çıktının bir sonraki girdi ile işlemi sokulması ağa belli bir hafıza 
    kazandırmaktadır. Ancak bu hafıza "gradyen kaybolması (vanishing gradient)" denilen problem yüzünden yüzeyselleşmektedir. 
    Başka bir deyişle ağ eğitim sırasında öncekileri unutup son veriler üzerinde hafıza oluşturmaktadır. Ya da başka bir deyişle 
    oluşturulan hafıza "kısa süreli (short term)" hale gelmektedir. Çıktının sürekli girdiye verilmesi ilk girdilerin belli bir 
    zaman sonra unutulmasına yol açmaktadır. Daha önce de bahsettiğimiz gibi "gradyen kaybolması" ağın derinleşmesi sonucunda 
    oluşan genel bir problemdir. Geri beslemeli ağlar aslında ağı derinleştirmektedir. Bunun sonucu olarak da bu ağlarda gradyen 
    kaybolması daha açık bir biçimde kendini göstermektedir. İşte geçmişin ağda daha iyi hatırlanması için ve "gradyen kaybolması" 
    denilen problemi azaltmak için bazı modeller önerilmiştir. Bunlardan en önemlilerinden biri LSTM (Long Short Term Memory) 
    denilen modeldir. 

    LSTM modelinde yine SimpleRNN modelinde olduğu gibi bir önceki çıktı bir sonraki girdi ile işleme sokulmaktadır. Ancak ağa 
    başka bir girdi bileşeni daha eklenmiştir. Bu girdi bileşeni ağın geçmiş bilgileri unutmasını engellemesini (gradyen kaybolmasını 
    azaltmayı) hedeflemektedir. LSTM katmanındaki ağa eklenen bu ilave "carry" girişinin nasıl olup da "gradyen kaybolmasını"
    azalttığı konusu biraz karmaşıktır.    

    LSTM katmanında her zamansal girdi önceki çıktı ve önceki "carry" değeri ile işleme sokulmaktadır. Dolayısıyla katmanın bir 
    hücresindeki dot product işlemi şöyle yapılmaktadır:

    output_t = activation(np.dot(input_new, W) + np.dot(output_prev, U) + np.dot(carry_prev, V) + b)

    Buradaki dot product işleminin SimpleRNN'deki işlemden farkı np.dot(carry_prev, V) terimiyle temsil ettiğimiz "carry" girişidir. 
    Bu "carry" girişi aslında üç farklı terimin birleşimiyle oluşturulmaktadır:

    carry_next = = i * k + carry_prev * f

    Burada aslında bizim carry ile belirttiğimiz girdi bir tane girdinin değil üç tane girdinin birleşimidir. Bu girdileri biz 
    i, k ve f ile gösterdik. şte bu i, k ve f girdileri de şöyle oluşturulmaktadır:

    i = activation(np.dot(output_prev, Ui) + np.dot(input_prev, Wi) + bi)
    f = activation(np.dot(output_prev, Uf) + np.dot(input_prev, Wf) + bf)
    k = activation(np.dot(output_prev, Uk) + np.dot(input_prev, Wk) + bk)

    Görüldüğü gibi aslında LSTM katmanınında i, j ve k biçiminde temsil ettiğimiz üç giriş vardır. Bu i, k ve f girişleri modele 
    Ui, Uf, Uk, Wi, Wf ve Wk ağırlık değerlerini eklemektedir. Peki bu durumda böyle bir LSTM katmanında eğitilebilir kaç 
    parametre bulunacaktır? n LSTM katmanına giren nöron sayısı, m de LSTM katmanındaki nöron sayısı olsun. Bu durumda:

    - W matrisinin eleman sayısı n * m tanedir. 
    - U matrisinin eleman sayısı m * m tanedir 
    - Ui, Uf ve Uk matrislerinin eleman sayıları ise m * m tanedir. 
    - Wi, Wf ve Wk matrislerinin eleman sayıları ise n * m tanedir.
    - Bu katmandaki toplam "bias" değerlerinin sayısı da m * b + m * bi + m * bf + m * bk tanedir. 
    
    Yukarıdaki tüm değerler toplandığında ve 4 parantezine alındığında LSTM katmanındaki eğitilebilir parametrelerin sayısı da 
    4 * (n * m + m * m + m) olacaktır.
    
    Keras'taki LSTM sınıfının __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.layers.LSTM(
        units,
        activation='tanh',
        recurrent_activation='sigmoid',
        use_bias=True,
        kernel_initializer='glorot_uniform',
        recurrent_initializer='orthogonal',
        bias_initializer='zeros',
        unit_forget_bias=True,
        kernel_regularizer=None,
        recurrent_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        recurrent_constraint=None,
        bias_constraint=None,
        dropout=0.0,
        recurrent_dropout=0.0,
        seed=None,
        return_sequences=False,
        return_state=False,
        go_backwards=False,
        stateful=False,
        unroll=False,
        use_cudnn='auto',
        **kwargs
    )

    Burada yine ilk parametre katmandaki nöron sayısını belirtmektedir. Aktivasyon fonksiyonu yine default olarak "tanh" 
    biçiminde alınmıştır. Yine katmandaki çıktıların biriktirilmesi için kullanılan return_sequences parametresi vardır. 
    Yani katmanın kullanımı SimpleRNN katmanına oldukça benzemektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi LSTM katmanındaki toplam eğitilebilir parametrelerin sayısına basit bir modelle bakalım. Modelimiz şöyle olsun:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, LSTM, Reshape, Dense

    model = Sequential(name='SimpleRNN-Test')
    model.add(Input((100, 10),  name='Input'))
    model.add(LSTM(128,  return_sequences=True, name='LSTM'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dense(1, activation='linear', name='Output'))
    model.summary()

    Model elde edilen özet çıktı şöyledir:

        Model: "SimpleRNN-Test"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ LSTM (LSTM)                     │ (None, 100, 128)       │        71,168 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 12800)          │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     3,277,056 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 3,414,273 (13.02 MB)
    Trainable params: 3,414,273 (13.02 MB)
    Non-trainable params: 0 (0.00 B)

    Biz yukarıda LSTM katmanındaki toplam eğitilebilir parametrelerin sayısının n katmana giren nöron sayısı, m de katmandan 
    çıkan nöron sayısı olmak üzere  4 * (n * m + m * m + m) biçiminde olduğunu belirtmiştik. Bu örnekte n = 10, m = 128'dir. 
    O halde 4 * (10 * 128 + 128 * 128 + 128) = 71168'dir. Diğer katmalardaki eğitilebilir parametrelerin sayısı daha önceleri 
    yaptığımız biçimde hesaplanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda daha önce yapmış olduğumuz IMDB örneğinin LSTM ile yeniden gerçekleştirimi verilmiştir. Modeli şöyle oluşturduk:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, TextVectorization, Embedding, LSTM, Dropout, Dense, Reshape

    tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
    tv.adapt(dataset_x)

    model = Sequential(name='IMBD-LSTM')
    model.add(Input((1, ), dtype='string', name='Input'))
    model.add(tv)
    model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
    model.add(Dropout(0.3, name='Dropout-1'))
    model.add(LSTM(64, return_sequences=True, name='LSTM'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dropout(0.3, name='Dropout-2'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dropout(0.3, name='Dropout-3'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dropout(0.3, name='Dropout-4'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Burada vektörüzasyon işlemini TextVectorization ile yaptık . LSTM katmanında toplam 128 nöron kullandık. Overfitting problemini 
    azaltmak için de katmanların aralarına Dropout katmanlarını da yerleştirdik. Modelin özet çıktısı şöyledir:

   Model: "IMBD-LSTM"
    ┌─────────────────────────────────┬────────────────────────┬───────────────┐
    │ Layer (type)                    │ Output Shape           │       Param # │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ text_vectorization_4            │ (None, 150)            │             0 │
    │ (TextVectorization)             │                        │               │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Embedding (Embedding)           │ (None, 150, 64)        │    11,630,208 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Dropout-1 (Dropout)             │ (None, 150, 64)        │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ LSTM (LSTM)                     │ (None, 150, 64)        │        33,024 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Reshape (Reshape)               │ (None, 9600)           │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Dropout-2 (Dropout)             │ (None, 9600)           │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-1 (Dense)                │ (None, 256)            │     2,457,856 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Dropout-3 (Dropout)             │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Hidden-2 (Dense)                │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Dropout-4 (Dropout)             │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ Output (Dense)                  │ (None, 1)              │           257 │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 14,187,137 (54.12 MB)
    Trainable params: 14,187,137 (54.12 MB)
        
    Program bir bütün olarak aşağıda verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 150
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Embedding, LSTM, Dropout, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)

model = Sequential(name='IMBD-LSTM')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
model.add(Dropout(0.3, name='Dropout-1'))
model.add(LSTM(64, return_sequences=True, name='LSTM'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dropout(0.3, name='Dropout-2'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dropout(0.3, name='Dropout-3'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dropout(0.3, name='Dropout-4'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
                                            75. Ders - 27/10/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Ağa uzun dönem hafıza kazandırmaya çalışan diğer bir yöntem de GRU (Gated Recurrent Unit) isimli yöntemdir. Bu yöntemde de 
    yine ağa üçüncü bir giriş uygulanmaktadır. GRU yöntemi de Keras'ta tensorflow.keras.layers modülündeki GRU katman sınıfıyla 
    temsil edilmiştir. Dolayısıyla uygulamacı LSTM yerine GRU katmanını kullandığında yöntemi değiştirmiş olur. GRU sınıfının 
    __init__ metodunun parametrik yapısı da LSTM sınıfına benzemektedir:

    tf.keras.layers.GRU(
        units,
        activation='tanh',
        recurrent_activation='sigmoid',
        use_bias=True,
        kernel_initializer='glorot_uniform',
        recurrent_initializer='orthogonal',
        bias_initializer='zeros',
        kernel_regularizer=None,
        recurrent_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        recurrent_constraint=None,
        bias_constraint=None,
        dropout=0.0,
        recurrent_dropout=0.0,
        seed=None,
        return_sequences=False,
        return_state=False,
        go_backwards=False,
        stateful=False,
        unroll=False,
        reset_after=True,
        use_cudnn='auto',
        **kwargs
    )
    
    Yine metodun birinci parametresi katmandaki nöron sayısını, ikinci parametresi ise aktivasyon fonksiyonunu belirtmektedir. 
    
    LSTM ile GRU arasında şu farklılıklar söz konusudur:

    - LSTM'de ağa uzun dönem hafıza kazandırmak için uygulanan giriş üç bileşene sahipken GRU katmanında iki bileşene sahiptir.  
    Dolayısıyla GRU katmanı LSTM katmanına göre daha az eğitilebilir parametreye sahiptir. Genel GRU olarak katmanı LSTM 
    katmanından daha yalın görünümdedir.

    - GRU katmanında daha az bileşen olduğu için bu katmanın eğitimi LSTM katmanına göre daha hızlı yapılabilmektedir. Ayrıca 
    GRU katmanı daha düşük miktardaki eğitim verileri için bu nedenden dolayı daha uygun olabilmektedir. 

    - LSTM katmanı GRU katmanına göre daha iyi performans gösterme eğilimindedir. Yani ikisi arasındaki tercih hız ve duyarlılık 
    ihtiyacına göre değişebilmektedir. 

    Yukarıdaki IMDB veri kümesi için verdiğimiz LSTM örneğini GRU için aşağıdaki gibi yeniden düzenleyebiliriz. Buradaki tek
    yaptığımız şey LSTM yerine GRU katmanını kullanmaktır. Bu iki katmanın içsel çalışması farklı olsa da arayüz olarak aynıdır:

    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Input, TextVectorization, Embedding, GRU, Dropout, Dense, Reshape

    tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
    tv.adapt(dataset_x)

    model = Sequential(name='IMBD-LSTM')
    model.add(Input((1, ), dtype='string', name='Input'))
    model.add(tv)
    model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
    model.add(Dropout(0.3, name='Dropout-1'))
    model.add(GRU(64, return_sequences=True, name='GRU'))
    model.add(Reshape((-1, ), name='Reshape'))
    model.add(Dropout(0.3, name='Dropout-2'))
    model.add(Dense(256, activation='relu', name='Hidden-1'))
    model.add(Dropout(0.3, name='Dropout-3'))
    model.add(Dense(256, activation='relu', name='Hidden-2'))
    model.add(Dropout(0.3, name='Dropout-4'))
    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Aşağıda örnek bir bütün olarak verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 150
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Embedding, GRU, Dropout, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)

model = Sequential(name='IMBD-LSTM')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
model.add(Dropout(0.3, name='Dropout-1'))
model.add(GRU(64, return_sequences=True, name='GRU'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dropout(0.3, name='Dropout-2'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dropout(0.3, name='Dropout-3'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dropout(0.3, name='Dropout-4'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')

#----------------------------------------------------------------------------------------------------------------------------
    Geri beslemeli ağlarda biz önceki çıktıyı sonraki girdi ile ilişkilendiriyorduk. SimpleRNN katmanında "gradyen kaybolması 
    (vanishing gradient)" denilen problem yüzünden geçmişe ilişkin hafıza iyi bir biçimde tutulamıyordu. Bunun için LSTM ve 
    GRU geri beslemesi kullanılıyordu. Bu geri besleme modellerinde geçmişin daha iyi anımsanması sağlanıyordu. Ancak önceki 
    çıktının sonraki girdiyle işleme sokulması bazı durumlarda yeterli olamamaktadır. Örneğin metinlerin anlamlandırılmasında 
    önce bir şeyden bahsedilip sonra o şey hakkında bilgi verildiğinde önce bahsedilen şeyin ne olduğu ancak sonradan anlaşılmaktadır. 
    Ya da örneğin bir kişinin bir dükkana girdiği belirtilmiş olabilir. Sonra da bu dükkanın eczane olduğu söylenmiş olabilir. 
    Bu durumda eğer o dükanın baştan eczane olduğu bilinse daha iyi bir çıkarım yapılabilir. (Bazı doğal dillerin dillerin 
    gramerleri de bu biçimde gelişmiştir. Örneğin İngilizce'de "the book on the table ..." biçimindeki bir cümlede kitabın 
    masanın üzerinde olduğu sonradan anlaşılmaktadır. Ancak "masanın üzerindeki kitap ..." cümlesinde kitabın masanın üzerinde 
    olduğu baştan anlaşılmaktadır. Ancak İngilizce'de de yüklem hemen özneden sonra gelir. Böylece bir kişinin ne yaptığı baştan 
    anlaşılmaktadır.)

    İşte geri beslemede önceki çıkışın sonraki girişle ilişkilendirilmesinin yanı sıra bunun tersinin de yapılması yani sonraki 
    çıkışın önceki girişle ilişkilendirilmesinin de sağlanması daha iyi bir öğrenmeye yol açabilektedir. Bu tür geri beslemeli 
    ağlara "çift yönlü (bidriectional)" geri beslemeli ağlar denilmektedir. Mimari olarak çift yönlü geri beslemeli ağlar tek 
    yönlü ağlara geri doğru aynı biçimde bir besleme eklenmesiyle oluşturulmaktadır. Ancak çift yönlü geri besleme ağı daha 
    karmaşık bir hale getirmektedir. Dolayısıyla eğitilebilir parametrelerin sayısını da artırmaktadır. Ancak bazı uygulamalarda 
    daha iyi bir sonucun elde edilmesine olanak sağlamaktadır. Tabii zamansal verilerin söz konusu olduğu bazı uygulamalarda 
    çift yönlü geri besleme bir fayda sağlamadığı gibi modelin başarısını bile düşürmektedir. Örneğin Jena Cliamate veri kümesinde 
    çift yönlü bir geri beslemenin açık bir faydası olmayacaktır. Çünkü Jena Climate veri kümesinde gelecekteki bilginin geçmiş 
    ile yeniden ilişkilendirilmesinin açık bir yaydası yoktur. Dolayısıyla çift yönlü geri beslemenin her zaman tek yönlü geri 
    beslemeden daha iyi sonuç vereceği söylenemez. Uygulamacının gerektiğinde her iki yöntemi de denemesi tavsiye edilmektedir. 
    Yukarıda da belirttiğimiz gibi Jena Climate örneğinde olduğu gibi pek çok zaman serisi tarzındaki veri kümelerinde çift 
    yönlü geri besleme açık bir fayda sağlamamaktadır. Çift yönlü geri beslemenin en sık kullanıldığı alan "makine çevirisi", 
    "metinden anlam çıkartma" gibi metinsel işlemlerdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Keras'ta çift yönlü geri besleme işlemi tensorflow.keras.layers modülündeki Bidirectional sınıfı ile yapılmaktadır. Bu sınıf 
    dekoratör kalıbı (decorator pattern) biçiminde oluşturulmuştur. Biz bu sınıfa SimpleRNN, LSTM ya da GRU katman nesnelerini 
    veririz. Sınıf da onu çift yönlü hale getirir. Sınıfın __init__ metodunun parametrik yapısı şöyledir:

    tf.keras.layers.Bidirectional(
        layer,
        merge_mode='concat',
        weights=None,
        backward_layer=None,
        **kwargs
    )

    Metodun birinci parametresi kullanılacak geri tek yönlü geri besleme katman nesnesini almaktadır. Aslında Keras'ın içsel 
    tasarımında SimpleRNN, LSTM ve GRU katmanları RNN isimli bir sınıftan türetilmiştir. Dolayısıyla bu parametre için RNN 
    sınıfından türetilmiş bir katmana ilişkin katman nesnesi girilmelidir. Örneğin:

    model.add(Bidirectional(LSTM(64, name='LSTM', return_sequences=True), name='Bidirectional'))

    Burada Bidirectional fonksiyonun birinci parametresi LSTM nesnesi olarak girilmiştir. Yani Bidirectional sınıfı tek yönlü 
    geri beslemeli sınıfların çift yönlü çalışmasını sağlamaktadır. 

    Bu durumda bizim ağı çift yönlü yapmak için tek yapacağımız şey SimpleRNN, LSTM ya da GRU katman nesnesini Bidirectional 
    katmanına vermektir.
    
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Flatten, Dense

    model = Sequential(name='IMDB-LSTM-Bidirectional')
    model.add(Embedding(len(cv.vocabulary_), 64, input_length=TExT_SIZE, name='Embedding'))
    model.add(Bidirectional(LSTM(64, name='LSTM', return_sequences=True), name='Bidirectional'))
    model.add(Dropout(0.2, name='Dropout-1'))
    model.add(Flatten(name='Flatten'))
    model.add(Dense(64, activation='relu', name='Dense-1'))
    model.add(Dropout(0.2, name='Dropout-2'))

    model.add(Dense(1, activation='sigmoid', name='Output'))
    model.summary()

    Aşağıda daha önce yapmış olduğumuz LSTM geri beslemeli IMDB örneğinin çift yönlü hale getirilmiş biçimi verilmiştir. Aynı 
    örneği ilgili satırı aşağıdaki hale getirip GRU için de kullanabilirsiniz:

    model.add(Bidirectional(GRU(64, return_sequences=True, name='LSTM'), name='Bidrectional'))
#----------------------------------------------------------------------------------------------------------------------------

TEXT_SIZE = 150
WORD_VECT_SIZE = 64

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review']
dataset_y = (df['sentiment'] == 'positive').to_numpy(dtype='uint8')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, TextVectorization, Embedding, Bidirectional, LSTM, Dropout, Dense, Reshape

tv = TextVectorization(output_sequence_length=TEXT_SIZE, output_mode='int')
tv.adapt(dataset_x)

model = Sequential(name='IMBD-LSTM')
model.add(Input((1, ), dtype='string', name='Input'))
model.add(tv)
model.add(Embedding(tv.vocabulary_size(), WORD_VECT_SIZE, name='Embedding'))
model.add(Dropout(0.3, name='Dropout-1'))
model.add(Bidirectional(LSTM(64, return_sequences=True, name='LSTM'), name='Bidrectional'))
model.add(Reshape((-1, ), name='Reshape'))
model.add(Dropout(0.3, name='Dropout-2'))
model.add(Dense(256, activation='relu', name='Hidden-1'))
model.add(Dropout(0.3, name='Dropout-3'))
model.add(Dense(256, activation='relu', name='Hidden-2'))
model.add(Dropout(0.3, name='Dropout-4'))
model.add(Dense(1, activation='sigmoid', name='Output'))
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

hist = model.fit(training_dataset_x, training_dataset_y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)
plt.xticks(range(0, 300, 10))
plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Accuracy', 'Validation Accuracy'])
plt.show()

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

predict_df = pd.read_csv('predict-imdb.csv')

predict_result = model.predict(predict_df['review'])
for presult in predict_result[:, 0]:
    if (presult > 0.5):
        print('Positive')
    else:
        print('Negative')
 
#----------------------------------------------------------------------------------------------------------------------------
    Biz kursumuzda daha önce "aktarımsal öğrenme (transfer learning)" konusuna bir giriş yapmıştık. Aktarımsal öğrenme "daha 
    önceden eğitilmiş olan modellerden faydalanılması" anlamına geliyordu. İşte makine öğrenmesi alanında framewrok'ler ve 
    birtakım topluluklar önceden hazırlanmış ve eğitilmiş çeşitli modelleri bulundurabilmektedir. Böylece uygulamacı başarısı 
    kanıtlanmış olan modelleri sıfırdan oluşturmak yerine zaten oluşturulmuş ve eğitilmiş bu modellerden faydalanabilmektedir. 
    
    Başkaları tarafından oluşturulmuş modeller yalnızca model olarak (yani eğitilmemiş bir biçimde) karşımıza çıkabileceği gibi 
    eğitilmiş bir biçimde de karşımıza çıkabilir. Yani biz başkaları tarafından hazırlanmış modelleri yalnızca model olarak 
    kullanabiliriz ya da onlar zaten eğitilmişse onların eğitim sonucunda oluşturulmuş olan ağırlıklarını da kullanabiliriz. 

    Başkaları tarafından hazırlanmış olan modelleri kullanırken modellerin sunum biçimine dikkat edilmesi gerekir. Modeller 
    genellikle framework'e özgü bir biçimde oluşturulmaktadır. Örneğin biz PyTorch için oluşturulmuş bir modeli TensorFlow'da
    kullanamayız. Dış dünyaya sunulmuş modellerde dosya formatlarına da dikkat etmek gerekir. Farklı framework'ler farklı dosya 
    formatlarını kullanmaktadır. Dolayısıyla bu dosyaların Python'da kullanıma hazır hale getirilmesinde de framework'e özgü 
    sınıflardan ve fonksiyonlardan faydalanılmaktadır. Biz kursumuzda şimdiye kadar Keras kullandık. Keras eskiden birden fazla 
    backend ile çalışabiliyordu. Ancak daha sonra tamamen TensorFlow ile entegre edildi. Fakat Keras aynı zamanda bağımsız bir 
    proje olarak da devam ettirilmektedir. Maalesef bağımsız Keras projesi ile TensorFlow içerisine entegre edilmiş olan Keras 
    projesi arasında uyumsuzluklar da oluşmaya başlamıştır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki biz Keras'ta çalışırken hazır modelleri nasıl elde edebiliriz? Hazır modellerin bazıları zaten framework'ün içerisinde 
    sınıflar biçiminde bulundurulmuştur. En kolay yöntem bu sınıfların kullanılmasıdır. Çünkü bu sınıflar framework ile tam bir 
    uyum içerisinde çalışmaktadır. Keras'ın hazır modelleri "tensorflow.keras.applications" paketi içerisindedir. (Eskiden bu 
    paket TensorFlow'a resmi olarak dahil değildi. Sonra resmi olarak dahil edildi.) Ancak Keras'ın bu paketleri ağırlıklı olarak 
    resimsel uygulamalara yöneliktir. 

    Framework'lerin kendi hazır model sınıflarının yanı sıra çeşitli toplulukların bünyesinde oluşturulmuş olan hazır modeller 
    de bulunmaktadır. Örneğin TensorFlow (dolayısıyla Google) tarafından oluşturulmuş olan "TensorFlow Hub" denilen bir topluluk 
    vardır. Bu topluluğun üyeleri kendi hazırladıkları modellerini dış dünyaya açmaktadır. Geçen yıl bu topluluk Kaggle'ın 
    bünyesine dahil edilmiştir.  
    
    Kaggle zaman içerisinde TensorFlow dışında pek çok framework'e ilişkin modelleri barındıran vir topluluk haline gelmiştir. 
    Dolayısıyla Kaggle bu tür hazır modeller için iyi bir kaynak oluşturmaktadır. Kaggle'ın modelleri barındıran bağlantısı 
    aşağıda verilmiştir:

    https://www.kaggle.com/models
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi bir framework için oluşturulmuş model başka bir framework'te kullanılamamaktadır. Bu nedenle 
    Kaggle gibi topluluk sitelerinde model araştırırken modelin oluşturulduğu framework'e dikkat ediniz. Bu tür topluluk sitelerinde 
    modeller framework temelinde filtrelenerek aranabilmektedir. 

    Model barındıran topluluk sitelerinde modellerin bir dosya biçiminde indirilip kullanılmasına olanak sağlamaktadır. Ancak 
    kulanım kolaylığı oluşturabilmek bu tür sitelerimn bazıları URL temelli yüklemelere de izin verilebilmektedir. Böylece modelin 
    indirilip yüklenmesi tek hamlede ilgili URL belirtilerek de yapılabilmektedir. 
    
    Kaggle topluluğu modelleri ve dosyaları uzaktan indirmek için ayrı bir kütüphane de sunmaktadır. Bu kütüphaneye "kagglehup" 
    kütüphanesi denilmektedir. Ancak bu kütüphanenin kullanılabilmesi için "API Key" oluşturulması gerekmektedir. kagglehup 
    kütüophanesinin genel kullanımı aaşağıdaki bağlantıda açıklanmaktadır:
    
    https://github.com/Kaggle/kagglehub
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bir topluluktan (örneğin Kaggle) bir model indirecekseniz yalnızca onun oluşturulduğu framework'e değil aynı zamanda indirilecek
    model dosyasının dosya formatına ve içeriğinde de dikkat etmeniz gerekir. Bazı dosya formatları genel amaçlıdır, içerisine 
    değişik formatta bilgiler yerleştirilebilmektedir. Örneğin H5 formatı (Hierarchical Data Format)" genel amaçlı bir formattır. 
    ".h5" uzantılı bir dosyanın içerisinde Keras uyumlu bir model bilgisinin bulunma zorunluluğu yoktur. Örneğin "protobuf 
    formatı (Protocol Buffer Format)" da genel amaçlı bir formattır. ".pb" uzantılı bir dosyanın içinde ne olduğunun ayrıca 
    biliniyor olması gerekir. 

    TensorFlow dünyasında çok karşılaşılan dosyalar ve formatlar şunlardır:

    - H5 Dosyaları: Genellikle  bu dosyalar içerisinde TensorFlow modelinin kendisi ve ağırlıkları ya da yalnızca ağırlıkları 
    bulunabilmektedir. Bu dosyalar genellikle ".h5" ya da ".hdf5" uzantısıyla bulunur.

    - Keras Dosyaları: Eskiden Keras'ta model saklamak için H5 formatı yoğun kullanılıyordu. Sonra Keras ekibi Keras'a özgü 
    bir biçimde ".keras" uzantılı dosya formatını kullanmaya başladı. Aslınde ".keras" uzantılı dosyalar özel bir formata 
    sahip değildir. Bu dosyalar bir dizin'in zip'lenmesinden oluşmaktadır. ".keras" dosyasının belirttiği dizin içerisinde 
    H5 dosyasının yanı sıra bazı JSON dosyaları (ve duruma göre bazı grafik dosyalar vs.) da bulunur. Yani aslında model 
    bilgilerini yine H5 dosyasında tutulmaktadır.
    
    - SavedModel Formatı: Bu format TensorFlow kütüphanesi tarafından kullanılan diğer bir model sakalama formatıdır. 
    Genellikle ".pb  uzantılı (Protobcol Buffer Format) dosyaların içerisine yerleştirilmektedir. 

    - TFlite Formatı: Bu format ve dosya uzantısı TensorFlow modellerini ve ağırlık değerlerini saklamak için tasarlanmış 
    diğer bir formattır.  Özellikle düşük kapasiteli mobil aygıtlarda ve gömülü sistemlerde tercih edilmektedir. Bu format 
    yalnızca makine öğrenmesinde değil özellikle mobil aygıtlarda da başka amaçlarla kullanılabilmektedir. Format diskte daha 
    az yer kaplayacak biçimde tasarlanmıştır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        76. Ders - 02/11/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Peki yukarıdaki paragrafta açıkladığımız dosyalar ve formatlar Keras içerisinden nasıl kullanılmaktadır?

    - Biz daha önce ".h5" dosyalarını ve ".keras" uzantılı dosyaları Sequential model sınıfının save metodu ile oluşturmuştuk. 
    İşte bu dosyalar ve içerisindeki modeller tensorflow.keras.models modülündeki load_model fonksiyonuyla geri yüklenebilmektedir. 
    Zaten biz bu fonksiyonu daha önce de kullanmıştık. O halde elimizde ".h5" uzantılı ya da ".keras" uzantılı bir model varsa
    biz bunu bu load_model fonksiyonuyla yükeleyebiliriz. Bu fonksiyon bize bir Model nesnesi vermektedir. Örneğin:

    model = load_model('iris.keras')

    - <pb içerisindeki SavedModel formatındaki bilgilerin Keras'tan kullanılması eklenecek>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Örneğin kaggle.com sitesindeki Models sekmesine girip "framework" için Keras seçildiğinde karşımıza çeşitli amaçlarla 
    başkaları tarafından oluşturulmuş olan eğitilmiş ya da eğitilmemiş modeller çıkacaktır. Biz framework olarak Keras'ı 
    seçtiğimiz için genellikle buradaki model dosyaları ".keras" uzantılı biçimde karşımıza çıkacaktır. Biz de bu dosyaları 
    indirip yukarıda belirttiğimiz gibi load_model fonksiyonuyla yükleyebiliriz. Örneğin aşağıdaki siteden modeli indirdiğimizde
    "ResNet50.keras" isminde bir dosya elde etmiş olacağız:

    https://www.kaggle.com/models/paripatel2709/resnet

    Bu dosyayı da yularıda belirttiğimiz gibi load_model fonksiyonuyla yükleyebiliriz:

    from tensorflow.keras.models import load_model

    model = load_model('ResNet50.keras')
    model.summary()
#----------------------------------------------------------------------------------------------------------------------------

    from tensorflow.keras.models import load_model

    model = load_model('ResNet50.keras')
    model.summary()

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi Keras içerisinde tensorflow.keras.applications paketinde hazır bir biçimde bulunan modellerin nasıl yüklenerek 
    kendi amaçlarımız doğrultusunda kullanılabileceğine bazı örnekler verelim.

    Resim sınıflandırma ve anlamlandırmada kabul görmüş olan en önemli modellerden ikisi ResNet ve VGG modelleridir. Bu modeller 
    onlarca katmana sahip olan çok ayrıntılı modellerdir. Biz burada bu modellerin iç yapısı üzerinde durmayacağız. Ancak bu 
    modelleri açıklayan pek çok kaynak bulunmaktadır. 

    Keras içeisindeki ResNet modellerinin yanında bazı sayılar bulunmaktadır. Örneğin ResNet50, ResNet101, ResNet152 gibi. Bu 
    sayılar modelin katman sayısı ile ilgilidir. Yüksek sayılarda daha fazla katman vardır. Dolayısıyla daha fazla eğitilebilir 
    parametre bulunmaktadır. Yukarıda sözünü ettiğimiz bu hazır modeller oldukça derin bir mimariye sahiptir. Dolayısıyla bu 
    modellerin eğitilmesi daha önce yaptığımız modellere göre daha fazla zaman almaktadır. Eğitim için saatlerce zaman 
    gerekebilmektedir. 

    Örneğin biz popüler bir mimari olan DenseNet121'i kullanmak isteyelim. Bunun için önce bir DenseNet121 nesnesi yaratılır. 
    DenseNet121 sınıfının __init__ metodunun parametrik yapısı şöyledri:

    tf.keras.applications.DenseNet121(
        include_top=True,
        weights='imagenet',
        input_tensor=None,
        input_shape=None,
        pooling=None,
        classes=1000,
        classifier_activation='softmax'
    )

    Metodun birinci parametresi True geçilirse model girdi ve çıktı katmanıyla bitlikte bir bütün olarak kullanılır. Genellikle 
    bu parametre  False biçimde geçilir. Çünkü genellikle uygulamacılar modeli bir bütün olarak kullanmak yerine modeli kendi 
    amaçları doğrultusunda kullanırlar ve ince ayar (fine-tuning) yapmak isterler. Metodun weights parametresi önceden eğitimle 
    elde edilmiş olan ağırlıkların kullanılıp kullanılmayacağını belirtmektedir. Burada biz nereden elde edilen ağırlıkların 
    kullanılacağını belirtiriz. Bu parametre default olarak "imagenet" biçiminde girilmiştir. ImageNet resimlerden oluşan dev 
    bir veritabanıdır. Bu veritabanı özellikle makine öğrenmesinde resimlerle ilgili işlemler yapan modellerin eğitilmesinde 
    yaygın biçimde kullanılmaktadır. Burada weights parametresi None geçilirse model eğitilmemiş bir biçimde kullanılır. Yani 
    bu durumda tüm eğitimi uygulamacının kendisi yapmak zorundadır. Bu parametreye ağırlıkların bulunduğu desteklenen bir formattaki 
    dosyanın yol ifadesi de geçirilebilmektedir. Metodun input_shape parametresi girdi resimlerinin boyutunu belirtmektedir. 
    Burada önemli bir noktayı da belirtmek istiyoruz. Biz "ImageNet" veritabanındaki resimlerden elde edilen ağırlıkları kullanmak 
    istediğimizi düşünelim. Bu veritabanındaki eğitimler (224, 224, 3) boyutundaki resimlerle yapılmıştır. Eğer bizim resimlerimiz 
    bu boyuttan büyük ise ya da küçük ise dönüştürme sırasında performans kayıpları oluşabilecektir. Bu nedenle bu sınıfları 
    kullanıyorsanız eğitimin yapıldığı orijinal resim boyutuna ne kadar yakın bir boyut seçerseniz performans daha daha iyi 
    olacaktır. Örneğin biz CIFAR-100 örneği için ResNet121 modelini kullanmak isteyelim. Ancak önceden eğitilmiş ağırlık değerleri 
    yerine modelimizi biz kendi verilerimizle eğitmek isteyelim. Bu durumda ResNet121 nesnesi aşağıdaki gibi yaratılabilir. 
    Eğer bu katmanın önünde bir girdi katmanı bulundurulacaksa bu durumda image_shape parametresi hiç girilmeyebilir. 
    
    from tensorflow.keras.applications.densenet import DenseNet121
    
    dn121 = DenseNet121(include_top=False, weights=None, input_shape=(32, 32, 3))

    Burada include_top parametresi False geçildiği için modelin çıktı katmanını bizim oluşturmamız gerekir. 

    Peki biz bu hazır modeli nasıl CIFAR-100 örneğinde kullanabiliriz? Daha önceden de belirttiğimiz gibi bu tür hazır modellerin
    Keras'ta fonksiyonel bir biçimde kullanılması tavsiye edilmektedir. Ancak biz burada önce klasik Sequential modeli kullanacağız 
    sonra fonksiyonel model ile örnek vereceğiz. 

    Önceden oluşturulmuş Keras modeli adeta bir katman gibi Sequential modele eklenmelidir. Zaten Model  sınıflarının kendisi 
    de aynı zamanda bir katman gibi kullanılabilmektedir. (Model sınıfın da aynı zamanda çoklu bir biçimde Layer sınıfından 
    türetilmiş olduğunu anımsayınız.)
    
    model = Sequential(name='ResNet121-Cifar-100')
    model.add(Input((32, 32, 3), name='Input'))
    model.add(DenseNet121(include_top=False, weights=None, input_shape=(32, 32, 3), name='DenseModelTest'))
    model.add(Reshape((-1, )))
    model.add(Dense(128, activation='relu', name='Dense-1'))
    model.add(Dense(128, activation='relu', name='Dense-2'))
    model.add(Dense(100, activation='softmax', name='Output'))
    model.summary()

    Burada önce modele bir Input katmanı eklenmiştir. Sonra da Dense121 modelinin tamamı adeta bir katman gibi modele eklenmiştir. 
    Biz ayrıca bu hazır modelin ucuna iki Dense katman ve bir de çıktı katmanı ekledik. Artık modeli compile edip fit işlemi 
    uygulayabiliriz. Bu örnekte önceden eğitilmiş modelin ağırlıklarını kullanmadığımıza dikkat ediniz. Burada aslında biz Dense121 
    nesnesi yaratılırken input_shape parametresini girmeyebilirdik. Çünkü modelimizin bir girdi katmanı olduğu için bu sınıf 
    bu girdi katmanından hareketle zaten input_shape parametresini belirleyebilmektedir. Eğer biz hem girdi katmanı kullanıyorsak 
    hem de bu input_shape parametresine argüman giriyorsak bu durumda bu iki resim boyutunun aynı olması gerekmektedir. 

    Biz yukarıdaki örnekte yalnızca modelin kendisinden faydalanmak istedik. Tabii önceden eğitilmiş modelin ağırlıklaırnı da 
    kullanabiliriz. Bunun için Dense121 nesnesinde weights parametresi 'imagenet" biçiminde geçilmelidir: 

    model.add(DenseNet121(include_top=False, weights='imagenet', input_shape=(32, 32, 3), name='DenseModelTest'))

    Biz DenseNet121 katmanında (bu aslında aynı zamanda katmanlardan oluşan model nesnesidir) önceden elde edilmiş ağırlıkları 
    kullandığımızda artık kendi verilerimizle yaptığımız eğitimde bu katmanların eğitime dahil edilmemesini isteyebiliriz. 
    Çünkü "imagenet" o kadar büyük bir veritabanıdır ki bizim kendi resimlerimizin bu ağırlıkları fayda sağlayacak değiştirmesi 
    de oldukça güçtür. Bu nedenle biz bu katmanı eğitim işleminden muaf hale getirmek için katman sınıflarının trainable 
    parametresinden faydalanabiliriz. Örneğin:

    model.add(DenseNet121(include_top=False, weights='imagenet', input_shape=(32, 32, 3), trainable=False, name='DenseModelTest'))

    Burada artık trainable=False argümanı kullanıldığı için kendi verilerimiz ile eğitim yapılırken bu katmandaki tüm katmanlar
    eğitimden muaf tutulacaktır. Tabii test ve kestirim işlemlerinde kullanılacaktır. Bu hazır katmanı eğitimden muaf tutmanın 
    bir avantajı da eğitim sırasında geçen zamanın kısaltılmasıdır. Tabii buradaki DenseNet121 katmanın içerisinde katmanların da
    yalnızca bir bölümü için trainable=False işlemi de yapılabilir. 

    Daha öncedne de belirttiğimiz gibi hazır ağılıkların kullanılmasından sonra ayrıca modeli birkaç katman ekleyerek kendi 
    veri kümemiz için eğitmeye "ince ayar (fine-tuning)" yapılması denilmektedir. 

    Dense121 sınıfının ğırlıkları toplam 1000 tane sınıf için uygulanan eğitimle oluşturulmuştur. Bu 1000 tane sınıf içerisinde 
    pek çok farklı temadan resimler bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda DenseNet121 sınıfının CIFAR-100 örneğinde Sequantial modeli ile kullanımına bir örnek verilmiştir. Bu örnekte 
    önceden eğitilmiş modelin ağırlıkları kullanılmıştır. Burada verdiğimiz örnekten elde edilen başarı %65 civarındadır. 
    Buradaki önemli bir handikap CIFAR-100 resimlerinin gerçek eğitimde kullanılan 224x224x3'lük resimlere göre oldukça küçük
    olmasıdır. Resim büyütülüp orijinal boyuta yaklaştırıldıkça performans da artacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Reshape, Dense
    
EPOCHS = 10

from tensorflow.keras.datasets import cifar100

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar100.load_data()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
    ]

model = Sequential(name='ResNet121-Cifar-100')
model.add(Input((32, 32, 3), name='Input'))
model.add(DenseNet121(include_top=False, weights='imagenet', input_shape=(32, 32, 3), name='DenseModelTest'))
model.add(Reshape((-1, )))
model.add(Dense(128, activation='relu', name='Dense-1'))
model.add(Dense(128, activation='relu', name='Dense-2'))
model.add(Dense(100, activation='softmax', name='Output'))

model.summary()

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])

hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

import matplotlib.pyplot as plt 

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import glob
import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de DenseNet121 sınıfının fonksiyonel bir modelde nasıl kullanılacağı üzerinde duralım. Önceki konularda da bellirttiğimiz
    gibi uygulamacılar aslında öncedne hazırlanmış bu modelleri genellikle fonksiyonel model içerisinde kullanmaktadır. Fonksiyonel 
    modelde anımsayacağınız gibi sürekli çıktı girdiye verilerek katmanlar oluşturuluyordu:

    inputs = Input((32, 32, 3), name='Input')
    x = DenseNet121(include_top=False, weights='imagenet', input_shape=(32, 32, 3), name='DenseModelTest')(inputs)
    x = Reshape((-1, ))(x)
    x = Dense(128, activation='relu', name='Dense-1')(x)
    x = Dense(128, activation='relu', name='Dense-2')(x)
    outputs = Dense(100, activation='softmax', name='Output')(x)
    
    model = Model(inputs, outputs)

    Burada Model nesnesinin kullanıldığına dikkat ediniz. Anımsanacağı gibi Model nesnesi oluşturulurken ona girdi ve çıktı 
    katmanlarının verilmesi gerekiyordu. Geri kalan işlemler artık daha önce yaptığımız gibi devam ettirilebilir. 

    Aşağıda aynı ResNet121 CIFAR örneğinin fonksiyonel modelle gerçekleştirime ilişkin örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Reshape, Dense
    
EPOCHS = 10

from tensorflow.keras.datasets import cifar100

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar100.load_data()

scaled_training_dataset_x = training_dataset_x / 255
scaled_test_dataset_x = test_dataset_x / 255

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
    ]

inputs = Input((32, 32, 3), name='Input')
x = DenseNet121(include_top=False, weights='imagenet', input_shape=(32, 32, 3), name='DenseModelTest')(inputs)
x = Reshape((-1, ))(x)
x = Dense(128, activation='relu', name='Dense-1')(x)
x = Dense(128, activation='relu', name='Dense-2')(x)
outputs = Dense(100, activation='softmax', name='Output')(x)

model = Model(inputs, outputs)

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])

hist = model.fit(scaled_training_dataset_x, ohe_training_dataset_y, batch_size=32, epochs=EPOCHS, validation_split=0.2)

import matplotlib.pyplot as plt 

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = model.evaluate(scaled_test_dataset_x , ohe_test_dataset_y, batch_size=32)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

# prediction

import glob
import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de başkaları tarafından hazırlanmış ve eğitilmiş olan modellerin kulanılmasına ilişkin örnekler verelim. Biz daha 
    önce de bu amaçla çeşitli toplulukların oluşturuldupundan bahsetmiştik. Bunların en bilineni Kaggle isimli topluluktur. 
    Ancak çeşitli framework'ler de "hub" adı altında Kaggle benzeri topluluklar oluşturmuştur. Örneğin TensorFlow için kişilerin
    modellerini paylaşabileceği "tensorflow hub" denilen bir topluluk vardır. Fakat daha önceden de belirttiğimiz gibi bu 
    topluluk kendi sitesini kapatıp Kaggle'a geçmiştir. Bu tür topluluklardaki kişiler modellerini daha önce ele aldığımız 
    formatlarda sunmaktadır. Ancak bu formattaki modellerde bazı sorunlar da ortaya çıkabilmektedir. Keras için önemli sorunalardan 
    biri zamanında save edilmiş modellerin güncel versiyonlarla uyuşmamasından kaynaklanan sorunlardır. 

    Keras işin başında TensorFlow'dan bağımsız ve birden fazla backend'i destekleyecek biçimde tasarlanmıştı. Sonraları 
    TensorFlow ekibi bu Keras arayüzünü bünyesine kattı ve yeniden TensorFlow ile bağlantılı bir biçimde Keras'ı yazdı. Böylece
    biribirine benzeyen iki farklı Keras gerçekleştirimi ortaya çıktı. Ancak daha sonraları eski Keras ekibi TensorFlow desteğini
    kuvvetlendirdi. TensorFlow da bu yeni Keras ile (Keras'ın 3'lü versiyonları) kendisini senkronize etmeye başladı. Yani 
    son yıllarda TensorFlow'daki Keras kütüphanesi orijinal Keras kütüphanesi ile aynı duruma gelmiştir. Ancak bir süre önce durum 
    böyle değildi. İşte bu süreç içerisinde TensorFlow Keras ile oluşturulup save edilmiş modeller yeni TensorFlow Keras'ta çalışmaz
    hale gelebilmektedir. Ancak TensorFlow'daki orijinal Keras ile senkronize edilmiş yeni Keras'ın yanı sıra eski Keras'ı da 
    muhafaza etmektedir. Bu eski Keras'a tf_keras ismiyle erişilebilmektedir. Bu eski Keras paketini aşağıdaki gibi yükleyebilirsiniz:

    pip install tf_keras

    Bu eski Keras'la yeni Keras'ın kullanımı büyük ölçüde aynıdır. Biz eski Keras'ı kullanacaksak tf_keras ismiyle, yeni 
    Keras'ı kullanacaksak tensorflow.keras ismiyle import uygulamalıyız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    TensorFlow Hub içerisine yerleştirilmiş olan modeller için bir URL'de oluşturulmaktadır. Bu modellerin doğrudan URL eşliğinde
    yüklenenip bir katman nesnesi haline getirilmesi için KerasLayer isimli bir sınıf bulundurulmuştur. Bu sınıfın __init__ 
    metodunun parametrik yapısı şöyledir:

    hub.KerasLayer(
        handle,
        trainable=False,
        arguments=None,
        _sentinel=None,
        tags=None,
        signature=None,
        signature_outputs_as_dict=None,
        output_key=None,
        output_shape=None,
        load_options=None,
        **kwargs
    )

    Buradaki en önemli iki parametre handle ve trainable parametreleridir. handle parametresinde modelin URL'si girilmektedir. 
    trainable parametresi de modelin katmanlarının eğitime dahil edilip edilmeyeceğini belirtmektedir. Tabii yukarıda da 
    belirttiğimiz gibi TensorFlow Hub artık tümden Kaggle'a taşınmış durumdadır. Biz söz konusu modelleri bu yöntemle kullanmak 
    yerine model dosyasını Kaggle'dan indirip load_model fonksiyonuyla da yükleyebiliriz. 

    TensorFlow Hub'ı modelleri kolay yüklemek amacıyla kullanabilmek için ona özgü olan kütüphaneyi de yüklememiz gerekir. 
    Yukarıda açıkladığımız KerasLayer sınıfı da aslında bu kütüphane içerisindedir. Kütüphaneyi şöyle yükleyebiliriz:

    pip install tensorflow_hub
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                        78. Ders - 10/11/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Dersin başında geçmişe dönülerek Dataset nesnesi veren bazı hazır fonksiyonlar gözden geçirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda TensorFlow Hub'ta bulunan bir modelin kullanımına örnek verilmiştir. Ancak bu model Kesnorflow Keras'ın 2'li 
    versiyonları için hazırlanmıştır. Bu nedenle bu model için biz tensorflow.keras yerine tf_keras kullandık.
#----------------------------------------------------------------------------------------------------------------------------

from tf_keras.preprocessing import image_dataset_from_directory

IMAGE_SIZE = (250, 250)

training_dataset = image_dataset_from_directory(r'C:\Users\aslan\Downloads\flower_photos', label_mode='categorical',
      subset='training', seed=123, validation_split=0.2, image_size=IMAGE_SIZE,  batch_size=32)

validation_dataset = image_dataset_from_directory(r'C:\Users\aslan\Downloads\flower_photos', label_mode='categorical',
      subset='validation', seed=123, validation_split=0.2, image_size=IMAGE_SIZE,  batch_size=32)

from tensorflow_hub import KerasLayer

keras_layer = KerasLayer('https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2')

from tf_keras import Sequential
from tf_keras.layers import Input, Rescaling, Dense

model = Sequential(name='TensorFlow-Hub-EfficentNet')
model.add(Input(IMAGE_SIZE + (3, ), name='Input'))
model.add(Rescaling(1. / 255, name='Rescaling'))
model.add(keras_layer)
model.add(Dense(5, activation='softmax', name='Output'))

model.compile('rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
hist = model.fit(training_dataset, validation_data=validation_dataset, epochs=100)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesi uygulamalarında kullanılan en önemli araçlardan biri de "Otomatik Makine Öğrenmesi (Auto ML)" araçlarıdır. 
    Bu araçlar pek çok yükü uygulamacının üzerinden alarak kendileri yapmaktadır. Auto ML araçlarının uygulamacı için yaptığı 
    tipik işlemler şunlardır:

    - Özellik seçimi (feature selection)
    - Özelliklerin indirgenmesi (dimensionality feature reduction)
    - Verilerin ölçeklendirilmesi (feature scaling)
    - Kategorik verilerin sayısallaştırılması (label encoding, one-hot-encoding, ...)
    - Verilerin kullanıma hazır hale getirilmesi için gereken diğer işlemler
    - Model seçimi (model selection)
    - Modelin çeşitli parametrelerinin (hyperparameters) uygun biçimde ayarlanması (hyperparameter tuning)
    - Modelin kullanıma hazır hale getirilmesi (model deployment)

    Yukarıdaki işlemlerin hepsini tüm Auto ML araçları yapamamaktadır. Bu konuda araçlar arasında farklılıklar bulunmaktadır. 
    Bir ML probleminde karşılaşılan en önemli aşamalardan biri model seçimi ve modelin çeşitli üst (hyper) parametrelerinin 
    uygun biçimde konumlandırılmasıdır. Örneğin bir resim tanıma işleminde bizim problemimize özgü hangi mimarinin daha iyi 
    olduğu ve bu mimarideki katmanlardaki nöron sayılarının ne olması gerektiği, optimizasyon algoritmasındaki parametrelerin 
    nasıl ayarlanacağı uygulamacı tarafından deneme yanılma yöntemleriyle tespit edilmektedir. Bu tür araçlar bu sıkıcı deneme 
    yanılma yöntemlerini bizim için kendileri uygulamaktadır. 

    Auto ML araçlarından bazıları yapay sinir ağları için oluşturulmuştur. Bazıları ise kursumuzun sonraki bölümlerinde ele 
    alacağımız istatistiksel makine öğrenmesi yöntemlerini uygulamak için oluşturulmuştur. Bazı Auto ML araçları ise kurumuzun 
    son bölümünde ele alacağımız "pekiştirmeli öğrenme (reinforcement learning)" uygulamaları için tasarlanmıştır. 

    Otomatik makine öğrenmesi araçları süreci çok kolaylaştırmaktadır. Ancak her türlü yüksek düzeydeki araçlarda olduğu gibi 
    ince işlemler bu araçların çoğuyla yapılamamaktadır. Tabii uygulamacı bu araçları kullanıp buradan elde edilen modellerden 
    faydalanabilir. Yani bu araçları model keşfetmek için de kullanabilir. Auto ML araçlarından bazıları tüm süreci otomatize 
    ederken bazı unsurların kullanıcı tarafından belirlenmesine de olanak sağlamaktadır. Ancak bu konuda Auto ML araçları 
    arasında önemli farklılıklar bulunabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağları için kullanılan Auto ML araçlarının en yaygın kullanılanı "AutoKeras" isimli araçtır. Bunun yanı sıra 
    "H2O" ve Cloud sistemlerine entegre edilmiş araçlar da sık kullanılanlar arasındadır. Biz bu bölümde AutoKeras aracının 
    kullanımını ele alacağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            79. Ders - 16/11/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AutoKeras yapay sinir ağlarını kullanarak kestirim işlemlerini otomatize eden bir araçtır. İsminden de anlaşılacağı gibi 
    bu araç neticede bir Keras modeli oluşturmaktadır. AutoKeras aracının resmi sitesi şöyledir:

    https://autokeras.com/

    Bu sitede oldukça basit örneklerle aracın nasıl kullanılacağı açıklanmıştır. Bu konuda yazılmış olan "Automated Machine 
    Learning with AutoKeras (Luis Sobrecueva)" isimli bir kitap da bulunmaktadır 
    
    AutoKeras'ı kurmak için aşağıdaki komut uygulanabilir:

    pip install autokeras

    Biz kütüphaneyi aşağıdaki gibi import ederek kullanacağız:

    import autokeras as ak

    Ancak AutoKeras'ın install edilen tensorflow versiyonu ile uyumlu olması gerekmektedir. Kursun yapıldığı sırada AutoKeras'ın
    son versiyonu 2.0.0 versiyonudur. Ancak bu versiyon maalesef Windows'ta install edilememektedir. Fakat macOS ve Linux 
    sistemlerinde sorunsuz bir biçimde install edilebilmeektedir. Windows'ta autokeras install edilmeye çalışıldığında 2.0.0
    versiyonu değil 1.0.20 versiyonu install edilebilmektedir. Bu versiyon da maalesef tensorflow'un eski versiyonları kullanılarak 
    yazılmıştır. Bu nedenle Windows sistemlerinde tensorflow kütüphanesinin de "downgrade" edilmesi gerekir. AutoKeras'ın 1.0.20
    versiyonunun çalışabilmesi için gereksinim duyulan kütüphanelerin versiyon numaraları şöyledir:

    python==3.8.15
    tensorflow==2.10.0    
    numpy==1.24.24

    Windows'ta bu çalışma ortamını kolay hazırlamak için Anaconda'da "Envirionments/Create" yapıp Python versiyonunu 3.8.X
    olarak ayarlayıp sanal bir ortam oluşturabilirsiniz. Sonra bu sanal ortamda "Open Terminal" yapıp "conda-forge" 
    kullanarak aşağıdaki gibi kurulumu yapabilirsiniz:

    conda install autokeras --channel conda-forge
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AutoKeras kütüphanesinde yüksek seviyeli 6 temel sınıf vardır:

    ImageClassifier
    ImageRegressor
    TextClassifier
    TextRegressor
    StructuredDataClassifier (AutoKeras 2'de kaldırıldı)
    StructuredDataRegressor (AutoKeras 2'de kaldırıldı)

    ImageClassifier sınıfı resimleri sınıflandırmak için, ImageRegressor sınıfı resimlerden sınıf değil sayısal değer elde 
    etmek için (örneğin resimdeki kişinin yaşını tespit etme problemi gibi), TextClassifier sınıfı yazıları sınıflandırmak için, 
    TextRegressor sınıfı yazılardan sayısal değer elde etmek için (örneğin yazının konu ile alaka dercesini tespit etmeye çalışma 
    gibi), StructuredDataClassifier sınıfı resim ve yazı dışındaki farklı türlere ilişkin sütunlara sahip sınıflandırma modelleri 
    için ve StructuredDataRegressor sınıfı da farklı türlere ilişkin sütunlara sahip regresyon problemleri için kullanılmaktadır. 
    Ancak bu sınıflar AutoKeras 2'de kaldırılmıştır. 

    Bu sınıflar kullanılırken uygulamacı özellik ölçeklemesi, değerlerin sayısal hale dönüştürülmesi, one-hot-encoding gibi 
    işlemleri kendisi yapmaz. Bu işlemleri zaten bu sınıfların kendisi yapmaktadır.

    AutoKeras 2 ile birlikte kütüphane üzerinde önemli değişiklikler yapılmıştır. Kütüphaneye pek çok Block sınıfı ve daha genel 
    Input sınıfları eklenmiştir. Biz önce bu temel sınıfları göreceğiz sonra AutoKeras 2 ile birlikte eklenen bu yeniş sınıfları
    göreceğiz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    ImageClassifier sınıfının tipik kullanımı şöyledir:

    1) Önce ImageClassifer sınıfı türünden bir nesne yaratılır. Sınıfın __init__ metodunun parametrik yapısı şöyledir:

    autokeras.ImageClassifier(
        num_classes=None,
        multi_label=False,
        loss=None,
        metrics=None,
        project_name="image_classifier",
        max_trials=100,
        directory=None,
        objective="val_loss",
        tuner=None,
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs
    )

    Görüldüğü gibi parametreler default değerler almaktadır. num_classes parametresi çıktının kaç sınıflı olduğunu belirtmektedir. 
    Default durumda sınıf sayısı otomatik olarak belirlenmektedir. Bu belirleme training_dataset_y içerisindeki farklı değerlerin 
    sayısı ile yapılmaktadır. max_trials parametresi en fazla kaç modelin deneneceğini belirtmektedir. Tabii bu değer ne kadar 
    yüksek tutulursa o kadar iyi sonuç elde edilir. Ancak en iyi modelin bulnması süreci uzayacaktır. Burada max_trials parametresi 
    ile denenecek model sayısı demekle yalnızca katmansal farklılık kastedilmemektedir. Örneğin katmansal yapı aynı olsa bile 
    hyper parametre farklılıkları da farklı bir model olarak değerlendirilmektedir. Dolayısıyla programcının iyi bir sonuç elde 
    etmek için max_trials parametresini yüksek bir değerde tutması uygun olur. Yüksek değerler fit işleminin birkaç gün sürmesine 
    yol açabilmektedir. Bunun için uygulamacı cloud sistemlerini kullanabilir. objective parametresi model karşılaştırılırken 
    neye göre karşılaştırılacağını belirtmektedir. AutoKeras her model işlemi için bir proje dizini oluşturmaktadır. Bu dizin'in 
    ismi directory parametresi ile ayarlanmaktadır. Bu parametre girilmezse dizin'in ismi project_name parametresi ile belirtilen 
    modelin ismi biçiminde alınır. Metodun overwrite parametresi default durumda False biçimdedir. False değeri metodun daha önce 
    oluşturulnuş olan bilgileri kullanacağını belirtmektedir. True değeri ise her defasında eski proje bilgileri var olsa bile yeni 
    değerleri onun üzerine yazazağı anlamına gelmektedir. overwrite paramtresi True geçildiğinde daha önce denenmiş ve saklanmış 
    olan modeller doğrudan kullanılmaktadır. Metrik değerler metrics parametresiyle verilebilmektedir. Örneğin:

    import autokeras as ak

    ic = ak.ImageClassifier(max_trials=5, overwrite=True)

    ImageClassifier sınıfının kendi içerisinde pretrained verileri kullanıp kullanmadığı konusunda dokğmanlarda bir bilgi yoktur. 
    Bazı kaynaklar ImageClassifer 

    2) Modelin derlenmesi işlemi uygulamacı tarafından yapılmaz. Dolayısıyla uygulamacı doğrudan fit işlemi yapar. Buradaki fit 
    metodunun kullanımı tamamen Keras'taki fit metodu gibidir. fit metoduna uygulamacı training_dataset_x ve training_dataset_y 
    verilerini verir. Metodun parametreleri Keras'ta gördüğümüz gibidir. epochs parametresi her denenecek model için ne kadar 
    epoch uygulanacağını belirtir. batch_size parametresi default durumda 32'dir. fit metoduna vereceğimiz resimlerin üç boyutlu 
    bir matris biçiminde olması gerekir. Yani RGB resimler için matris boyutu (width, height, 3) biçiminde gri tonlamalı resimler 
    için (width, height, 1) biçiminde olmalıdır. Tabii fit metodu parçalı eğtim de yapabilmektedir. Yani biz bu metodun birinci 
    parametresine üretici fonksiyonları ya da Dataset nesnelerini geçirebiliriz. fit işlemi sonucunda Keras'ta olduğu gibi bir 
    History callback nesnesi elde edilmektedir. Tabii programcı fit metoduna istediği callback nesnelerini callbacks parametresi 
    yoluyla geçirebilir. fit metodununb parametrik yapısı şöyledir:

    ImageClassifier.fit(x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs)
    
    Örneğin:

   ic = ak.ImageClassifier(max_trials=5, metrics=['categorical_accuracy'], overwrite=True)

#----------------------------------------------------------------------------------------------------------------------------
                                        80. Ders - 17/11/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------
    3) En iyi model AutoKeras tarafından bulunduktan sonra model test edilmelidir. Yine modelin testi için ImageClassifier 
    sınıfının  evaluate metodu kullanılmaktadır. evaluate metodu Keras'taki Sequential sınıfının evaluate metodu gibi 
    kullanılmaktadır. Metodun parametrik yapısı şöyledir:

    ImageClassifier.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)

    Örneğin:

    eval_result = ic.evaluate(test_dataset_x, test_dataset_y)

    for i in range(len(eval_result)):
        print(f'{model.metrics_names[i]}: {eval_result[i]}')

    4) Seçilen en iyi modelin test işleminden sonra artık kestirim işlemleri yapılabilir. Bunun için ImageClassifier sınıfının 
    predict metodu kullanılmaktadır. predict metodu da tamamen Sequential sınıfının predict metodu gibidir. Parametrik yapısı 
    şöyledir:

    ImageClassifier.predict(x, batch_size=32, verbose=1, **kwargs)

    Örneğin:

    predcit_result = ic.tredict(predict_dataset_x, predict_dataset_y)

    5) Elde edilen en iyi model istenirse Keras'ın Model sınıfına dönüştürülebilir. Bunun için ImageClassifier sınıfının 
    export_model metodu kullanılmalıdır. Programcı artık bu işlemden sonra modelini save edebilir. Daha önce görmüş olduğumuz 
    işlemleri bu model nesnesi üzerinde uygulayabilir. 

    model = ic.export_model()

    Yukarıda da belirttiğimiz gibi eğer biz ImageClassifier nesnesini yaratırken overwrite parametresini True geçmezsek aslında 
    aynı proje bir daha çalıştırıldığında eski kalınan yerden devam edilir. Çünkü proje için açılan dizinde tüm deneme bilgileri, 
    model bilgileri ve kalınan kalınan  yer not alınmaktadır.

    AutoKeras modellerinde yine callback nesneleri kullanılabilmektedir. Örneğin eğer biz AutoKeras sınıflarının fit metotlarının 
    callbacks parametresine EarlyStopping callback nesnesi yerleştirirsek bu durumda denenen model belirlediğimiz patience değerine 
    bağlı olarak erken sonlandırılabilecektir. 

    Aşağıdaki örnekte CIFAR-100 veri kümesi için AutoKeras'ın ImageClassifier sınıfı kullanılmıştır. Burada max_tralis değerini 
    5'te tuttuk. 
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.datasets import cifar100

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',
    'worm']

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar100.load_data()

import autokeras as ak

ic = ak.ImageClassifier(max_trials=5, overwrite=True)

hist = ic.fit(training_dataset_x, training_dataset_y, epochs=5)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Categorical Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

model = ic.export_model()

eval_result = ic.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

model.summary()
model.save('image-classifier-best-model.h5')

import numpy as np
import glob

for path in glob.glob('test-images/*.jpg'):
    image_data = plt.imread(path)
    image_data = image_data / 255
    predict_result = model.predict(image_data.reshape(1, 32, 32, 3))
    result = np.argmax(predict_result)
    print(f'{path}: {class_names[result]}')

#----------------------------------------------------------------------------------------------------------------------------
    ImageRegressor sınıfı bir resimden hareketle bir değerin tahmin edilmesi tarzı problemlerde kullanılmaktadır. Sınıfın 
    kullanım biçimi tamamen ImageClassifier sınıfında olduğu gibidir. __init__ metodunun parametrik yapısı şöyledir:

    autokeras.ImageRegressor(
        output_dim=None,
        loss="mean_squared_error",
        metrics=None,
        project_name="image_regressor",
        max_trials=100,
        directory=None,
        objective="val_loss",
        tuner=None,
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs
    )

    Metodun output_dim parametresi çıktı katmanının kaç değişkenden oluşacağını belirtir. Bu parametre için argüman girilemzse
    çıktı katmanındaki değişken sayısı y verilerinden otomatik olarak elde edilmektedir. Yine bu sınıf da özellik seçimi, 
    özellik ölçeklemesi, one-hot-encoding gibi ön işlemleri kendisi yapmaktadır.

    Bu sınıfın kullanımına örnek için MNIST veri kümesinden faydalanacağız. Aslında MNIST veri kümesinde çıktı 0, 1, 2, ..., 9
    biçiminde kategorik bir veridir. Ancak biz bunu sanki sayısal bir veri gibi örneğimizde ele alacağız.
#----------------------------------------------------------------------------------------------------------------------------

from tensorflow.keras.datasets import mnist

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = mnist.load_data()

import autokeras as ak

ir = ak.ImageRegressor(max_trials=5)

hist = ir.fit(training_dataset_x, training_dataset_y, epochs=5)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Mean Absolute Error Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['mae'])
plt.plot(hist.epoch, hist.history['val_mae'])
plt.legend(['Mean Absolute Error', 'Validation Mean Absolute Error'])
plt.show()

model = ir.export_model()
model.summary()
model.save('image-classifier-best-model.h5')

eval_result = model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
    AutoKeras'ın TextClassifer sınıfı yazıları sınıflandırmak için kullanılmaktadır. Öneğin daha önce yapmış olduğumuz "sentiment 
    analysis" örnekleri TextClassifier sınıfıyla yapılabilir. Sınıfın __init__ metodunun parametrik yapısı şöyledir:

    autokeras.TextClassifier(
        num_classes=None,
        multi_label=False,
        loss=None,
        metrics=None,
        project_name="text_classifier",
        max_trials=100,
        directory=None,
        objective="val_loss",
        tuner=None,
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs
    )

    Metodun parametrik yapısı ImageClassifier sınıfının __init__ metoduna çok benzemektedir. Kullanımı da benzerdir.

    TextClassifier sınıfının fit metodunda training_dataset_x yazılardan oluşan bir NumPy dizisi ya da Dataset nesnesi olabilir. 
    training_dataset_y de kategorik değerlere ilişkin bir NumPy dizisi olabilir ya da sayısallaştırılmış kategorik değerlerden 
    oluşabilir. AutoKeras yazının parse edilmesi, vektörel hale getirilmesi, sözcük gömme gibi işlemleri kendisi yapmaktadır.
    Yani uygulamacının yalnızca yazıları fit metoduna vermesi yeterlidir. 
    
    Aşağıda IMDB örneği TextClassifier sınıfıyla yapılmıştır.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('IMDB Dataset.csv')

dataset_x = df['review'].to_numpy()   
dataset_y = df['sentiment'].to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

import autokeras as ak

tc = ak.TextClassifier(max_trials=3)
hist = tc.fit(training_dataset_x, training_dataset_y, epochs=10)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Binary Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Binary Accuracy', 'Validation Binary Accuracy'])
plt.show()

model = tc.export_model()
model.summary()
model.save('text-classifier-best-model.h5')

eval_result = tc.evaluate(test_dataset_x, test_dataset_y)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

texts = ['the movie was very good. The actors played perfectly. I would recommend it to everyone.', 
        'this film is awful. The worst film i have ever seen']

for predict_text in texts:
    predict_result = tc.predict(texts)  
    if predict_result[0, 0] > 0.5:
        print('Positive')
    else:
        print('Negative')
        
model.save('imdb.h5')

#----------------------------------------------------------------------------------------------------------------------------
    Şimdiye kadar kullanmadığımız bir veri kümesi de "Spam Mails" veri kümesidir. Bu veri kümesinde birtakım e-postalar ve onların 
    spam olup olmadığı bilgileri vardır. Böylece e-postalara ilişkin spam filtreleri oluşturulabilemktedir. Bugün kullanılan 
    spam filtrelerinin çoğu çeşitli makine öğrenmesi teknikleriyle oluşturulmuştur. Veri kümesi "spam_ham_dataset.csv" ismiyle 
    aşağıdaki bağlantıdan indirilebilir:

    https://www.kaggle.com/datasets/venky73/spam-mails-dataset?resource=download

    Burada Pandas'ın read_csv fonksiyonuyla okuma yapabiliriz. "label" isimli sütun "spam" ya da "ham" değerlerinden oluşmaktadır.
    label_num aynı değerlerin 0 ve 1 ile sayısallaştırılmış halini içermektedir. E-postalatın içeriği ise "text" isimli sütunda 
    bulunmaktadır.

    AutoKeras'ın TextClassifier sınıfıyla yukarıdakine benzer biçimde bu veri kümesi için model oluşturulabilir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('spam_ham_dataset.csv')

dataset_x = df['text'].to_numpy()   
dataset_y = df['label_num'].to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y)

import autokeras as ak

tc = ak.TextClassifier(max_trials=3, metrics=['binary_accuracy'])
hist = tc.fit(training_dataset_x, training_dataset_y, epochs=10)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Binary Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['binary_accuracy'])
plt.plot(hist.epoch, hist.history['val_binary_accuracy'])
plt.legend(['Binary Accuracy', 'Validation Binary Accuracy'])
plt.show()

model = tc.export_model()
model.summary()

eval_result = tc.evaluate(test_dataset_x, test_dataset_y)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
    TextRegressor sınıfı bir yazdıdan sayısal bir değer kstirmek için kullanılmaktadır. Sınıfın __init__ metodunun parametrik 
    yapısı şöyledir:

    autokeras.TextRegressor(
        output_dim=None,
        loss="mean_squared_error",
        metrics=None,
        project_name="text_regressor",
        max_trials=100,
        directory=None,
        objective="val_loss",
        tuner=None,
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs

    Sınıfın kullanımı TextClassifier sınıfına oldukça benzemektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                81. Ders 23/11/2024 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Resim ve yazı dışındaki sınıflandırma problemleri için AutoKeras'ta StructuredDataClassifier sınıfı kullanılmaktadır. 
    Sının __init__ metodunun parametrik yapısı benzerdir:

    autokeras.StructuredDataClassifier(
        column_names=None,
        column_types=None,
        num_classes=None,
        multi_label=False,
        loss=None,
        metrics=None,
        project_name="structured_data_classifier",
        max_trials=100,
        directory=None,
        objective="val_accuracy",
        tuner=None,
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs
    )

    StructuredDataClassifer sınıfında girdi olarak iki boyutlu NumPy matrisi verilir. Özellik ölçeklemesi ve kategorik verilerin 
    sayısal biçime dönüştürülmesi gibi işlemler sınıf tarafından yapılmaktadır. y verileri yine yazı içeren bir NumPy dizisi 
    olarak ya da bunların sayısallaştırılmış haliyle girilebilmektedir. 

    AutoKeras'ın 2'li versiyonlarıyla birlikte izleyen paragraflarda ele alacağımız yeni birtakım sınıflar eklenmiştir. Proje 
    ekibi bu yeni sınıfların kullanılmasını teşvik etmek amacıyla bu sınıfı tamamen AutoKeras'tan kaldırmıştır. Yani eğer siz 
    kütüphanenin 2'li versiyonlarını kullanıyorsanız bu sınıf kütüphanenizde bulunmayacaktır. Ancak biz burada yine sınıf
    hakkında bilgiler vereceğiz.

    Bu sınıfın bir uygulaması olarak Titanik veri kümesini kullanacağız. Titanik veri kümesi Titanik'te yolcu olanların hayatta 
    kalıp kalmayacağına yönelik hazırlanmış bir veri kümesidir. Böylece veri kümesindeki çeşitli özellekler bilindikten sonra 
    kişinin o faciada hayatta kalıp kalamayacağı tahmin edilmeye çalışılmaktadır. 

    Titanik veri kümesi aşağıdaki bağlantıdan indirilebilir:

    https://www.kaggle.com/datasets/yasserh/titanic-dataset

    Veri kümesinin görünümü aşağıdaki gibidir:

    PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
    1,0,3,"Braund, Mr. Owen Harris",male,22,1,0,A/5 21171,7.25,,S
    2,1,1,"Cumings, Mrs. John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
    3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
    4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
    5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
    6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
    7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
    8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S
    9,1,3,"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)",female,27,0,2,347742,11.1333,,S
    10,1,2,"Nasser, Mrs. Nicholas (Adele Achem)",female,14,1,0,237736,30.0708,,C
    ..........

    Modelden elde edilen history verilerinin içerisinde val_xxx verileri bulunmayabilir. Çünkü model denemesi yapılırken bazı 
    modellerde veriler az ise biz validation_split ile belirtsek bile validation verileri kullanılmayabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('titanic.csv')

dataset_x = df.drop(columns=['Survived'], axis=1)
dataset_y = df['Survived'].to_numpy()

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

import autokeras as ak

sdc = ak.StructuredDataClassifier(max_trials=10, overwrite=True)
hist = sdc.fit(training_dataset_x, training_dataset_y, epochs=100, validation_split=0.2)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Binary Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['accuracy'])
plt.legend(['Binary Accuracy', 'Validation Binary Accuracy'])
plt.show()

model = sdc.export_model()
model.summary()
model.save('text-classifier-best-model.tf', save_format='tf')

eval_result = sdc.evaluate(test_dataset_x, test_dataset_y)
for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')

#----------------------------------------------------------------------------------------------------------------------------
    StructuredDataRegressor yine resimsel ve metinsel olmayan regresyon problemleri için kullanılmaktadır. Sınıfın __init__ 
    metdounun parametrik yapısı yine diğer sınıflardakine oldukça benzerdir:

    autokeras.StructuredDataRegressor(
        column_names=None,
        column_types=None,
        output_dim=None,
        loss="mean_squared_error",
        metrics=None,
        project_name="structured_data_regressor",
        max_trials=100,
        directory=None,
        objective="val_loss",
        tuner=None,
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs
    )

    Ancak yukarıda da belirttiğimiz gibi kütüphanenin 2'li versyionlarıyla birlikte bu sınıf da kütüphaneden kaldırılmıştır. 

    Aşağıda "Boston Housing Prices" veri kümesi üzerinde StructuredDataRegresoor sınıfının kullanımına bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('housing.csv', delimiter=r'\s+', header=None)

dataset_x = df.iloc[:, :-1].to_numpy(dtype='float32')
dataset_y = df.iloc[:, -1].to_numpy(dtype='float32')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

import autokeras as ak

sdr = ak.StructuredDataRegressor(max_trials=10, overwrite=True, metrics=['mae'])

from tensorflow.keras.callbacks import EarlyStopping

esc = EarlyStopping(patience=5, restore_best_weights=True)

hist = sdr.fit(training_dataset_x, training_dataset_y, epochs=100, callbacks=[esc])

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

model = sdr.export_model()

eval_result = sdr.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{model.metrics_names[i]}: {eval_result[i]}')
    
import numpy as np

predict_data = np.array([[0.11747, 12.50, 7.870, 0, 0.5240, 6.0090, 82.90, 6.2267, 5, 311.0, 15.20, 396.90, 13.27]])

predict_result = model.predict(predict_data)

for val in predict_result[:, 0]:
    print(val)
#----------------------------------------------------------------------------------------------------------------------------  

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi AutoKeras 2 ile birlikte kütüphane üzerinde önemli değişiklikler yapılmıştır. Örneğin 
    kütüphaneden StructuredDataClassifier ve StructuredDataRegressor sınıfları tamamen kaldırılmıştır ve kütüphaneye pek çok 
    Block sınıfı eklenmiştir. AutoKeras 2'de üç çeşit Input sınıfı bulunmaktadır:

    Input
    TextInput
    ImageInput

    Input sınıfı sütunlara sahip klasik "tablo biçimindeki (tabular)" veri kümeleri için kullanılmaktadır. TextInput sınıfı 
    ismi üzerinde metinsel girdiler için ImageInput sınıfı ise resimsel girdiler için bulundurulmuştur. Eğer girdi için Input 
    sınıfı kullanılacaksa fit işlemi sırasında fit metoduna verilen x verilerinin hepsinin nümerik olması gerekmektedir. 
    Önişlemler AutoModel sınıfı tarafından yapılmaktadır. Dolayısıyla bizim kategorik veriler için one-hot-encoding yapmamıza 
    gerek yoktur. Ancak kategorik verileri bizim sayısal hal getirmemiz gerekir. 
    
    Bu sınıflar AutoModel sınıfına girdi yapılmaktadır. AutoModel sınıfının __init__ metodunun parametrik yapısı şöyledir:

    autokeras.AutoModel(
        inputs,
        outputs,
        project_name="auto_model",
        max_trials=100,
        directory=None,
        objective="val_loss",
        tuner="greedy",
        overwrite=False,
        seed=None,
        max_model_size=None,
        **kwargs
    )

    Metodun inputs parametresine yukarıdaki Input sınıfları türünden nesneler argüman olarak verilmektedir. outputs parametresine
    ise aşağıdaki iki sınıf türünden nesneler girilmelidir.

    ClassificationHead
    RegressionHead

    Modelin metrics parametresi ClassificationHead ve RegressionHead sınıflarında belirtilmektedir. 

    Yine metodun max_trials parametresi kaç modelin deneneceğini belirtmektedir. Diğer parametreler daha önce görmüş olduğumuz 
    sınıfların parametrelerine benzerdir. 

    Örneğin:

    import autokeras as ak

    inp = ak.Input()
    out = ak.ClassificationHead()
    auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=20, overwrite=True)

    Bu biçimde AutoModel nesnesi oluşturulduktan sonra artık AutoModel sınıfının fit metodutla eğitim, evalute metoduyla test 
    işlemi ve predict metoduyla da kestieim işlemi yapılabilir. Örneğin:

    hist = auto_model.fit(training_dataset_x, training_dataset_y, validation_split=0.2, epochs=50)
    ...
    eval_result = auto_model.evaluate(test_dataset_x, test_dataset_y)
    ...
    predict_result = auto_model.predict(predict_dataset_x)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                                82. Ders 24/11/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Titanik veri kümesi üzerinde AutoKeras 2'deki AutoModel kullanımına bir örnek verelim. AutoModel kullanırken 
    resim ve yazılardan oluşmayan klasik tablo biçiminde veri kümelerinde girdi katmanı için Input sınıfı kullanılmalıdır. 
    Ancak Input sınıfı için girdiler verilirken tüm sütunların nümerik biçimde olması gerekmektedir. One-hot-encoding gibi 
    işlemleri AutoKeras kendisi yapıyor olsa da kategorik sütunlar LabelEncoder gibi bir sınıfla sayısal biçime dönüştürülmelidir. 
    Ayrıca AutoKeras 2'de Input katmanı için girdilerde hiç eksik veri ve NaN verinin olmaması gerekir. Yani Imputation 
    ugulamacı tarafından uygulanmalıdır. 

    Biz Tintaic veri kümesi için aşağıdaki hazırlıkları yapabiliriz:

    import pandas as pd

    df = pd.read_csv('Titanic-Dataset.csv')

    dataset_y = df['Survived'].to_numpy('uint8')
    df = df.drop(['Survived', 'PassengerId', 'Cabin', 'Name', 'Parch', 'Ticket'], axis=1)

    from sklearn.preprocessing import LabelEncoder

    le = LabelEncoder()
    df['Sex'] = le.fit_transform(df['Sex'])
    df['Embarked'] = le.fit_transform(df['Embarked'])

    from sklearn.impute import SimpleImputer

    si = SimpleImputer(strategy='mean')
    df['Age'] = si.fit_transform(df[['Age']])

    dataset_x = df.to_numpy('float32')

    Burada bazı gereksiz sütunlar atılmıştır. Bazı kategorik sütunlar LabelEncoder ile sayısal biçime dönüştürülmüştür. 
    Yaş belirten Age sütununda eksik veri olduğu için bu sütun üzerinde imputation uygulanmıştır. Bundan sonra veri kümesini 
    eğitim ve test biçiminde bölebiliriz:

    from sklearn.model_selection import train_test_split

    training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = \
            train_test_split(dataset_x, dataset_y, test_size=0.2)

    Artık AutoModel nesnesini oluşturabiliriz:

    inp = ak.Input()
    out = ak.ClassificationHead()
    auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=20, overwrite=True)

    Aşağıda örnek bir bütün olarak vrilmiştir. Burada elde edilen en iyi modelin Keras katman yapısı şöyledir:

        Model: "functional"
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
    ┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
    │ input_layer (InputLayer)        │ (None, 6)              │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ cast_to_float32 (CastToFloat32) │ (None, 6)              │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ dense (Dense)                   │ (None, 256)            │         1,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ batch_normalization             │ (None, 256)            │         1,024 │
    │ (BatchNormalization)            │                        │               │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ re_lu (ReLU)                    │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ dropout (Dropout)               │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ dense_1 (Dense)                 │ (None, 256)            │        65,792 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ batch_normalization_1           │ (None, 256)            │         1,024 │
    │ (BatchNormalization)            │                        │               │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ re_lu_1 (ReLU)                  │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ dropout_1 (Dropout)             │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ dropout_2 (Dropout)             │ (None, 256)            │             0 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ dense_2 (Dense)                 │ (None, 1)              │           257 │
    ├─────────────────────────────────┼────────────────────────┼───────────────┤
    │ classification_head_1           │ (None, 1)              │             0 │
    │ (Activation)                    │                        │               │
    └─────────────────────────────────┴────────────────────────┴───────────────┘
    Total params: 69,889 (273.00 KB)
    Trainable params: 68,865 (269.00 KB)
    Non-trainable params: 1,024 (4.00 KB)
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('Titanic-Dataset.csv')

dataset_y = df['Survived'].to_numpy('uint8')

df = df.drop(['Survived', 'PassengerId', 'Cabin', 'Name', 'Parch', 'Ticket'], axis=1)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Sex'] = le.fit_transform(df['Sex'])
df['Embarked'] = le.fit_transform(df['Embarked'])

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean')
df['Age'] = si.fit_transform(df[['Age']])

dataset_x = df.to_numpy('float32')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

import autokeras as ak

inp = ak.Input()
out = ak.ClassificationHead()
auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=20, overwrite=True)

hist = auto_model.fit(training_dataset_x, training_dataset_y, validation_split=0.2, epochs=50)

keras_model = auto_model.export_model()
keras_model.summary()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Binary Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['accuracy'])
plt.legend(['Binary Accuracy', 'Validation Binary Accuracy'])
plt.show()

# evaluation

eval_result = auto_model.evaluate(test_dataset_x, test_dataset_y)
for i in range(len(eval_result)):
    print(f'{keras_model.metrics_names[i]}: {eval_result[i]}')

# prediction

df_predict = pd.read_csv('predict.csv')

df_predict = df_predict.drop(['PassengerId', 'Cabin', 'Name', 'Parch', 'Ticket'], axis=1)

le = LabelEncoder()

df_predict['Sex'] = le.fit_transform(df_predict['Sex'])
df_predict['Embarked'] = le.fit_transform(df_predict['Embarked'])

predict_dataset_x = df_predict.to_numpy('float32')
predict_result = auto_model.predict(predict_dataset_x)
print(predict_result)

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki örnekte AutoModel'in girdi ve çıktı katmanlarını birbirinden bağımsız biçimde şöyle oluştuduk:

    inp = ak.Input()
    out = ak.ClassificationHead()
    auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=20, overwrite=True)
    
    Bu biçimde bir AutoModel oluşturduğumuzda tüm ara katmanlar, ara katmanlardaki nöron sayıları AutoKeras tarafından oluşturulmaktadır. 
    Ancak biz aslında AutoModel'lerde ara katmanlar üzerinde belirlemeler de de bulunabilmekteyiz. AutoKeras'ın 2'li versiyonlarıyla 
    eklenen AutoModel sistemi fonksiyonel bir kullanıma sahiptir. Yani AutoModel bizim Keras'ta gördüğümüz fonksiyonel model 
    gibi tasarlanmıştır. AutoModel'deki fonkisyonel kullanımda katman nesneleri birbirilerine verilerek eklenebilmektedir. 
    Örneğin:

    inp = ak.Input(...)
    x = ak.DenseBlock(...)(inp)
    x = ak.DenseBlock(...)(x)
    x = ak.DenseBlock(...)(x)
    output = ak.ClassificationHead(...)(x)

    auto_model = ak.AutoModel(inputs=inp, outputs=out)

    Burada önce bir Input nesnesi oluşturulmuş sonra DenseBlock nesneleri bunun üzerine eklenmiş ve nihayetinde bir çıktı 
    nesnesi elde edilmiştir. Buradaki kullanım biçimi Keras'ta görmüş olduğumuz fonksiyonel modele çok benzemektedir. 
    Bu tüm ara katmanlar ve hypper parametreler AutoKeras tarafından elde oluşturulmaktadır. 

    AutoKeras'ın fonksiyonel kullanımında uygulamacı katmanların neler olacağını ve kaç tane olacağını ana hatlarıyla 
    kendisi oluşturmaktadır. Katmanlar Block denilen sınıflarla temsil edilmiştir. Block sınıfları şunlardır:

    ConvBlock
    DenseBlock
    ResNetBlock
    RNNBlock
    XceptionBlock
    ImageBlock
    TextBlock

    Bu Block sınıflarının yanı sıra ayrıca yardımcı birkaç sınıf da vardır:

    Normalization
    Merge
    SpatialReduction
    TemporalReduction
    Normalization

    AutoKeras'ın Block sınıfları ile model oluşturulduğunda özellik ölçeklemesi otomatik yapılmamaktadır. Özellik ölçeklemesinin
    yapılması için Normailzation katmanının kullanılması gerekmektedir. 

    DenseBlock AutoKeras'a şunları söylemektedir: "Modele bir ya da birden fazla Dense katman ekleyebilirsin, bunların nöron 
    sayılarını ve aktivasyon fonksiyonlarını sen ayarlayabilirsin". Örneğin:

    inp = ak.Input(...)
    x = ak.Normaliation()(inp)
    x = ak.DenseBlock()(x)
    out = ak.ClassificationHead()(x)
    auto_model = ak.AutoModel(inputs=inp, outputs=out)

    Burada biz AutoKeras'a girdi katmanından sonra istenildiği kadar Dense katman kullabileceğini söylemekteyiz. Ancak biz 
    istersek DenseBlock sınıfında hangi sayıda Dense katmanın kullanılacağını metodun num_layers parametresiyle belirleyebiliriz. 
    Örneğin:

    inp = ak.Input(...)
    x = ak.Normaliation()(inp)
    x = ak.DenseBlock(num_layers=2)(inp)
    out = ak.ClassificationHead()(x)
    auto_model = ak.AutoModel(inputs=inp, outputs=out)

    Burada artık girdi katmanından sonra AutoKeras kesinlikle iki tane Dense katman kullanacaktır. Ancak bu katmanın nöron 
    sayılarını ve diğer özelliklerini kendisi belirleyecektir. Biz DenseBlock ile eklenecek olan Dense katmanlardaki nöron 
    sayılarını da num_units parametresiyle belirleyebilmekteyiz. Örneğin:

    inp = ak.Input(...)
    x = ak.DenseBlock(num_layers=2, num_units=32)(inp)
    out = ak.ClassificationHead()(x)
    auto_model = ak.AutoModel(inputs=inp, outputs=out)

    Artık burada iki tane Dense katman kullanılacak ve bu katmanların nöron sayıları 32 olacaktır. 

    Aşağıdaki örnekte Titanic veri kümesinde fonksiyonel DenseBlock kullanımına örnek verilmiştir. Kodun AutoKeras'ı ilgilendiren 
    kısmı şöyledir:

    import autokeras as ak

    inp = ak.Input()
    x = ak.Normaliation()(x)
    x = ak.DenseBlock()(inp)
    out = ak.ClassificationHead()(x)
    auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=100, overwrite=True)
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd

df = pd.read_csv('Titanic-Dataset.csv')

dataset_y = df['Survived'].to_numpy('uint8')

df = df.drop(['Survived', 'PassengerId', 'Cabin', 'Name', 'Parch', 'Ticket'], axis=1)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Sex'] = le.fit_transform(df['Sex'])
df['Embarked'] = le.fit_transform(df['Embarked'])

from sklearn.impute import SimpleImputer

si = SimpleImputer(strategy='mean')
df['Age'] = si.fit_transform(df[['Age']])

dataset_x = df.to_numpy('float32')

from sklearn.model_selection import train_test_split

training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)

import autokeras as ak

import autokeras as ak

inp = ak.Input()
x = ak.Normalization()(inp)
x = ak.DenseBlock()(x)
out = ak.ClassificationHead()(x)
auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=100, overwrite=True)

hist = auto_model.fit(training_dataset_x, training_dataset_y, validation_split=0.2, epochs=50)

keras_model = auto_model.export_model()
keras_model.summary()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.title('Epoch-Loss Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(15, 5))
plt.title('Epoch-Binary Accuracy Graph', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(hist.epoch, hist.history['accuracy'])
plt.legend(['Binary Accuracy', 'Validation Binary Accuracy'])
plt.show()

# evaluation

eval_result = auto_model.evaluate(test_dataset_x, test_dataset_y)
for i in range(len(eval_result)):
    print(f'{keras_model.metrics_names[i]}: {eval_result[i]}')

# prediction

df_predict = pd.read_csv('predict.csv')

df_predict = df_predict.drop(['PassengerId', 'Cabin', 'Name', 'Parch', 'Ticket'], axis=1)

le = LabelEncoder()

df_predict['Sex'] = le.fit_transform(df_predict['Sex'])
df_predict['Embarked'] = le.fit_transform(df_predict['Embarked'])

predict_dataset_x = df_predict.to_numpy('float32')
predict_result = auto_model.predict(predict_dataset_x)
print(predict_result)

#----------------------------------------------------------------------------------------------------------------------------
                                            83. Ders - 01/12/2024 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de AutoKeras 2 ile AutoModel kullanarak evrişimli bir resim sınıflandırma örneği yapalım. AutoKeras 2'deki ConvBlock
    bir ya da birden fazla evrişim katmanını temsil etmektedir. Yani biz ImageInput nesnesinden sonra fonksiyonel modele 
    ConvBlock nesnesini eklersek aslında modele bir ya da birden fazla evrişim katmanı eklemiş oluruz. Burada da eklenecek 
    evrişim katmanlarının bazı özellikleri uygulamacı tarafından belirlenebilmektedir. ImageInput sınıfında girdiler gri tonlamalı 
    resimler (samples, width, height) biçiminde de RGB resimler (samples, width, height, channels) biçiminde de  girilebilir. 
    CIFAR-100 örneği için AutoModel şöyle oluşturulabilir:

    inp = ak.ImageInput()
    x = ak.Normalization()(inp)
    x = ak.ConvBlock()(x)
    x = ak.DenseBlock(num_layers=2)(x)
    out = ak.ClassificationHead()(x)
    auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=1, overwrite=True)

    Aşağıda CIFAR-100 örneğinin AutoModel kullanılarak gerçekleştirilmesine bir örnek verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import glob

EPOCHS = 5

from tensorflow.keras.datasets import cifar100

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar100.load_data()

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
    ]

import autokeras as ak

inp = ak.ImageInput()
x = ak.Normalization()(inp)
x = ak.ConvBlock()(x)
x = ak.DenseBlock(num_layers=2)(x)
out = ak.ClassificationHead(metrics=['categorical_accuracy'])(x)
auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=1, overwrite=True)

hist = auto_model.fit(training_dataset_x, training_dataset_y, epochs=5)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = auto_model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{eval_result[i]}')

# prediction

import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures-Cifar100/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = auto_model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')

#----------------------------------------------------------------------------------------------------------------------------
    Resim sınıflandırma işlemleri için ResNet modeli daha iyi sonuçlar vermektedir. AutoKeras'ta ResNetBlock sınıfı bu 
    modeli otomatik kullanmak için bulundurulmuştur. Model içerisindeki katmanlar ve onların hyper parametreleri ResNetBlock 
    tarafından otomatik ayarlanmaktadır. ResNetBlock kullanımına tipik örnek şöyle verilebilir:

    inp = ak.ImageInput()
    x = ak.Normalization()(inp)
    x = ak.ResNetBlock()(x)
    x = ak.DenseBlock(num_layers=2)(x)
    out = ak.ClassificationHead()(x)
    auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=1, overwrite=True)

    Aşağıda CIFAR-100 örneğinin ResNetBlock kullanılarak gerçekleştirilmesine bir örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import glob

EPOCHS = 5

from tensorflow.keras.datasets import cifar100

(training_dataset_x, training_dataset_y), (test_dataset_x, test_dataset_y) = cifar100.load_data()

from tensorflow.keras.utils import to_categorical

ohe_training_dataset_y = to_categorical(training_dataset_y)
ohe_test_dataset_y = to_categorical(test_dataset_y)

class_names = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
    ]

import autokeras as ak

inp = ak.ImageInput()
x = ak.Normalization()(inp)
x = ak.ResNetBlock()(x)
x = ak.DenseBlock(num_layers=2)(x)
out = ak.ClassificationHead()(x)
auto_model = ak.AutoModel(inputs=inp, outputs=out, max_trials=1, overwrite=True)

hist = auto_model.fit(training_dataset_x, training_dataset_y, epochs=5)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.title('Epoch - Loss Graph', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['loss'])
plt.plot(hist.epoch, hist.history['val_loss'])
plt.legend(['Loss', 'Validation Loss'])
plt.show()

plt.figure(figsize=(14, 6))
plt.title('Categorcal Accuracy - Validation Categorical Accuracy', fontsize=14, pad=10)
plt.plot(hist.epoch, hist.history['categorical_accuracy'])
plt.plot(hist.epoch, hist.history['val_categorical_accuracy'])
plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])
plt.show()

eval_result = auto_model.evaluate(test_dataset_x, test_dataset_y)

for i in range(len(eval_result)):
    print(f'{eval_result[i]}')

# prediction

import numpy as np
import os

count = 0
hit_count = 0
for path in glob.glob('Predict-Pictures-Cifar100/*.*'):
    image = plt.imread(path)
    scaled_image = image / 255
    model_result = auto_model.predict(scaled_image.reshape(-1, 32, 32, 3), verbose=0)
    predict_result = np.argmax(model_result)
    fname = os.path.basename(path)
    real_class = fname[:fname.index('-')]
    predict_class = class_names[predict_result]
    print(f'Real class: {real_class}, Predicted Class: {predict_class}, Path: {path}')
    
    if real_class == predict_class:
        hit_count += 1
    count += 1
    
print('-' * 20)
print(f'Prediction accuracy: {hit_count / count}')
#----------------------------------------------------------------------------------------------------------------------------
