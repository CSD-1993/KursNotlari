#--------------------------------------------------------------------------------------------------------------------------

                                               C ve Sistem Programcıları Derneği

                                    Üretici Yapay Zeka, Doğal Dil İşleme ve Büyük Dil Modelleri
                                                         1. Bölüm

                                                   Özet Notlar ve Örnekler
            
                                                    Eğitmen: Kaan ASLAN
                                                    
            Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
                Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:           

        Aslan, K. (2026), "Üretici Yapay Zeka, Doğal Dil İşleme ve Büyük Dil Modelleri Kursu", Özet Notlar ve Örnekler
                                        C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları sabit genişlikli font kullanan programlama editörleri ile açınız.)
                        (Editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)
                                                            
                                    Son Güncelleme Tarihi: 25/02/2026 - Salı
                                         
#---------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            1. Ders 02/02/2026 - Pazartesi
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Katılımcılarla tanışıldı.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Kursun genel işleyişi açıklandı. Kursumuzun Dropbox bağlantısı şöyledir:,

    https://www.dropbox.com/scl/fo/k35ffkp5cuml1d6640m8s/AETyCOuySFvOVcjtwWQjiG0?rlkey=domd0y77v1ka6wgagll85jgp6&dl=0

#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Derneğimiz açtığı bu kursumuzla ilişkili kurslar şunlardır:

    - Python Programlama Dili Kursu : Bu kurs Python Programlama Dilini kapsamlı bir biçimde öğretmeyi hedeflemektedir. 
    "Python Programlama Dili" kursumuzun Dropbox bağlantısı şöyledir:

    https://www.dropbox.com/sh/hez3g36x6xa97cu/AAAJzxu2Yrza9cCcO1DJWrWia?dl=0

    - Python Uygulamaları Kursu: Bu kurs Python Programlama Dilini bilenler için bir uygulama kursudur. Programlama dilinin 
    dışındaki pek çok kütüphane ve framework'ün kullanımı bu kursun konusu içerisindedir. NumPy, Pandas, Matplotlib gibi 
    yapay zeka ve makine öğrenmesinde yoğun kullanılan kütüphaneler resmi olarak bu kursta ele alınmaktadır. "Python 
    Uygulamaları" kursumuzun Dropbox bağlantısı şöyledir:

    https://www.dropbox.com/sh/vylimm3evek0nnl/AABS_KdWdRMO6Xh0Fh6HT3rFa?dl=0

    - Yapay Zeka, Makine Öğrenmesi ve Veri Bilimi Kursu: Bu kurs genel bir yapay zeka, makine öğrenmesi ve veri bilimi 
    kursudur. Kurs içerisinde sinir ağları, çeşitli istatitiksel öğrenme yöntemleri ve veri bilimine ilişkin pek çok 
    konu ele alınmaktadır. "Yapay Zeka, Makine Öğrenmesi ve Veri Bilimi" kursumuzun Dropbox bağlantısı şöyledir:

    https://www.dropbox.com/sh/xvnprjjs7w74x05/AADi9NaU4aiHYHYREwKZgIzQa?dl=0
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Kursumuzda programlama dili olarak Python kullanılacaktır. Python "genel amaçlı, yüksek seviyeli, matematiksel alana 
    yakın ve nispeten basit" bir programlama dilidir. Python "yapay zeka, makine öğrenmesi, veri bilimi ve doğal dil 
    işlemede" halen en çok kullanılan programlama dilidir. Python son 10 senedir bir atak yapmış ve dünyanın en yaygın 
    kullanılan programlama dili haline gelmiştir. Python'un neden bu kadar popüler hale geldiği ve neden yapay zeka, makine 
    öğrenmesi ve veri bilimi alanlarında yaygın biçimde kullanıldığı konusundaki tespitlerimiz şöyledir:

    - Python nispeten basit bir dildir. Python'un basitliği başka alanlardan gelip de ana uğraşı alanı programlama olmayan 
    kişiler için uygun bir seçenek oluşturmaktadır.

    - Python matematiksel ve veri işleme alanları için uygun bir tasarıma sahiptir.

    - Python çeşitli yüksek seviyeli veri yapılarını bünyesinde barındırmaktadır. Python'da az tuşa basılarak çok şey 
    yapmak mümkündür.

    - Python'da yukarıda belirttiğimiz alanlara yönelik pek çok hazır kütüphane ve framework bulunmaktadır. Bu alanlardaki 
    algoritmaları tasarlayanlar onları birincil olarak Python'da gerçekleştirmektedir.

    - Python programlama dili olarak özellikle 3'lü versiyonlarla birlikte oldukça iyileştirilmiştir. 

    - ABD'de MIT gibi üst düzey üniversiteler belli bir süredir Python dilini "Programlamaya Giriş (Introduction to 
    Programming)" gibi derslerde kullanmaya başlamıştır. Bu da dilin prestijini artırmıştır. 

    - Python'un geniş bir standart kütüphanesi vardır. Bu duruma Python dünyasında esprili olarak "bataryası içinde 
    (batteries included)" denilmektedir. Python Standart Kütüphanesi içerisinde çok çeşitli konulara yönelik büyük ölçüde 
    platform bağımsız olan hazır fonksiyonlar ve sınıflar bulunmaktadır. Bu da programlamyı oldukça kolaylaştırmaktadır. 
    Her ne kadar Python Standart Kütüpahesi içerisinde olmasa da NumPy gibi, Pandas gibi, Matplotlib gibi yardımcı 
    kütüphaneler Python'da veri analizine ilişkin işlemleri oldukça kolaylaştırmaktadır. 

    - Python bir prototip dil olarak da kullanılmaktadır. Bir algoritma ya da uygulama "acaba oluyor mu diye" hızlı bir 
    biçimde Python'da oluşturulabilmektedir. Bazı uygulamacılar ve firmalar bu prototipleri daha sonra C/C++, Rust, Java, 
    C# gibi dillerde ürüne dönüştürebilmektedir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            2. Ders - 18/01/2026 - Pazar
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Python temel olarak yorumlayıcı tabanlı (interpretive) bir propogramlama dilidir. Python kodlarını çalıştıran programlara
    Python dünyasında genel olarak "Python gerçekleştirimleri (Python Implementations)" denilmektedir. Python'un sürdürümünden 
    sorumlu olan "Python Software Foundaion" kurumunun ana Python gerçekleştirimine (Buna İngilizce "reference implementation"
    da denilmektedir) "CPython" denilmektedir. (CPython ismi bu yorumlayıcının C dilinde yazılmış olduğundan dolayı verilmiştir.)
    Python gerçekleştirimlerinin yanı sıra birtakım araçları da bünyesinde barındıran çeşitli Python dağıtımları oluşturulmuştur. 
    Yapay zeka, makine öğrenmesi ve veri bilimi alanlarında en çok Anaconda isimli dağıtım kullanılmaktadır. Biz de kurusumuzda 
    bu dağıtımı kullanacağız. Anaconda dağıtımını aşağıdaki bağlantıdan indirebilirsiniz:

    https://www.anaconda.com/download/success

    Anaconda dağıtımının temel GUI arayüzü "Anaconda Navigator" denilen programdır. Anaconda Navigator dağıtım içerisindeki 
    çeşitli araçları bünyesinde barındıran ve bazı işlemlerin yapılmasını kolaylaştıran yönetici bir program gibidir. 
    Anaconda dağıtımı birincil olarak "Spyder" isimli IDE'yi kullanmaktadır. Spyder IDE'si aslında bağımsız bir projedir. 
    Yani bu IDE'yi Anaconda olmadan da bilgisayarınıza yükleyerek kullanabilirsiniz. Anconda dağıtımı kurulduğu zaman 
    Python Standart Kütüphanesinin dışında pek çok yardımcı kütüphane de kurulmuş olmaktadır. Yani NumPy, Pandas, Matplotlib 
    gibi temel kütüphaneleri ayrıca kurmaya gerek kalmamaktadır. 

    Python diğer bazı yüksek seviyeli diller gibi komut satırı çalışmasına da izin vermektedir. Bu tür komut satırlı 
    çalışmalara son zamanlarda "REPL (Read and Evaluate and Print Loop)" da denilmektedir. Bu çalışma biçiminde bir prompt 
    çıkar. Bu prompt'ta kullanıcı bir Python deyimi yazarak ENTER tuşuna basar. Python yorumlayıcısı da o deyimi o anda 
    çalıştırır ve yeniden prompt'a düşer. 
    
    CPython dağıtımında komut satırında doğrudan "python" programı çalıştırılır ancak bir program dosyası komut satırı argümanı 
    olarak bir kaynak dosya verilmezse komut satırına düşülmektedir. "python" programı çalıştırılırken yanına komut satırı 
    argümanı olarak bir kaynak dosya ismi verilirse python yorumlayıcısı komut satırına düşülmeden o dosya çalıştırılmaktadır. 
    Aslında Python'da deyimleri tek tek çalıştıran başka REPL uygulamaları da vardır. Bunların en yaygın kullanılanılanlarından 
    biri IPython denilen uygulamadır. (Spyder IDE'si de sağ tarafta IPython konsolunu kullanmaktadır.) IPython da aslında 
    bağımsız olarak kurulabilen ayrı bir uygulamadır. IPython'daki bu komut satırı proseslerine "kernel" da denilmektedir. 
    Spyder'daki sağ bölmede birden fazla IPython konsolu açılabilmektedir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yapay zeka, makine öğrenmesi ve veri bilimi eğitiminde çok kullanılan ismine "Jupyter Notebook" denilen bir araç da 
    vardır. Jupyter Notebook açıklama yazılarıyla Python kodlarını bir arada farklı hücrelerde tutabilmektedir. Pek çok 
    bulut sistemi Jupyter Notebook hizmeti de vermektedir. Ancak biz kursumuzda çalışma hızını yavaşlattığı gerekçesiyle
    Jupyter Notebook kullanmayacağız. Anaconda Navigator içerisinde Jupyter Notebook da bulunmaktadır. Jupyter Notebook 
    artık pek çok IDE'ye de entegre edilmiştir. Örneğin VSCode IDE'sinde bir plugin olarak da yüklenebilmektedir.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yapay zeka, makine öğrenmesi ve veri biliminde Python'un standart kütüphanesinden ziyade birtakım üçüncü patyi 
    kütüphaneler çok daha yoğun bir biçimde kullanılmaktadır. Bu bölümde bu kütüphaneler hakkında bazı temel bilgiler 
    vereceğiz. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    NumPy, Python'da vektörel işlemlerin yapılabilmesine olanak sağlayan en temel kütüphanelerden biridir. NumPy sayesinde 
    örneğin bir dizideki elemanların hepsi tek hamlede işlemlere sokulabilmekte, NumPy dizisi çarpıldığında dizinin 
    karşılıklı elemanları çarpılabilmektedir. Ya da örneğin bir NumPy Dizisinin sinüsü alındığında dizinin tüm elemanlarının 
    sinüsü elde edilebilmektedir. Bu tür özelliklere sahip olan dillere "dizisel diller (array languages)" de denilmektedir. 
    Matlab gibi R gibi matematiksel alana yakın olan dillerin bu biçimde vektörel işlemler yapabilme yeteneği vardır. 
    İşte NumPy kütüphanesi Python'a bu yeteneği kazandırmaktadır. 
    
    NumPy kütüpahnesi C'de yazılmıştır. Dolayısıyla aslında bu kütüphane kullanılarak işlemler yapılırken arka planda C'de 
    yazılmış olan kodlar çalıştırılmaktadır. Bu durum NumPy işlemlerinin Python'un doğal çalışmasına göre daha hızlı 
    yapılabildiği anlamına gelmektedir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Pandas kütüphanesi NumPy kullanılarak yazılmıştır. Pandas istatistiksel veri tablolarını oluşturabilme olanağını
    sunmaktadır. İstatistikte, veri biliminde ve makine öğrenmesinde her sütunu farklı türden olabilen veri tablolarıyla 
    çok sık karşılaşılmaktadır. Bu tür veri tablolarında satırlarda varlıklar sütunlarda ise onların özellikleri bulunur. 
    Örneğin:

    ┌──────┬──────┬─────────┬────────────────────┐
    │ Boy  │ Kilo │ Cinsiyet│ Tercih Edilen Renk │
    ├──────┼──────┼─────────┼────────────────────┤
    │ 170  │ 68   │ Erkek   │ Mavi               │
    │ 165  │ 55   │ Kadın   │ Yeşil              │
    │ 180  │ 82   │ Erkek   │ Siyah              │
    │ 158  │ 52   │ Kadın   │ Mor                │
    │ 175  │ 74   │ Erkek   │ Gri                │
    │ 162  │ 58   │ Kadın   │ Kırmızı            │
    │ 185  │ 90   │ Erkek   │ Lacivert           │
    │ 168  │ 60   │ Kadın   │ Turuncu            │
    │ 172  │ 70   │ Erkek   │ Beyaz              │
    │ 160  │ 54   │ Kadın   │ Pembe              │
    └──────┴──────┴─────────┴────────────────────┘

    Burada sütunlar farklı türlerdendir. İşte NumPy ile böyle bir temsil oluşturulamamaktadır. Çünkü NumPy'daki iki boyutlu 
    dizilerin her elemanı aynı türden olmak zorundadır. İşte Pandas bizim yukarıdaki gibi veri tablolarını oluşturmamıza 
    olanak sağlamaktadır. Pandas'taki bu biçimde veri tablosu temsilinin yapılmasını sağlayan sınıfa DataFrame denilmektedir. 
    Pandas denilince akla DataFrame sınıfı gelir. DataFrame nesnelerinin sütunlarına da Pandas da Series denilmektedir. 
    Yukarıdaki veri tablosu Pandas kütüphanesi ile bir DataFrame olarak şöyle oluşturulabilmektedir.

    import pandas as pd

    data = {
        "Boy": [170, 165, 180, 158, 175, 162, 185, 168, 172, 160],
        "Kilo": [68, 55, 82, 52, 74, 58, 90, 60, 70, 54],
        "Cinsiyet": [
            "Erkek", "Kadın", "Erkek", "Kadın", "Erkek",
            "Kadın", "Erkek", "Kadın", "Erkek", "Kadın"
        ],
        "Tercih Edilen Renk": [
            "Mavi", "Yeşil", "Siyah", "Mor", "Gri",
            "Kırmızı", "Lacivert", "Turuncu", "Beyaz", "Pembe"
        ]
    }
    df = pd.DataFrame(data)
    print(df)
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yapay zeka, mekine öğrenmesi ve veri bilimi alanında görselleştirme için en yaygın kullanılan kütüphane "Matplotlib" 
    denilen kütüphanedir. Bu kütüphanenin yanı sıra "Seaborn" ve "Plotly" kütüphaneleri de yaygın kullanılmaktadır. Bu 
    alanlardaki uygulamalarda bir biçimde görselleştirmenin yapılmasına gereksinim duyulmaktadır. Kursumuzda ağırlıklı 
    olarak bu tür görselleştirmeler için Matplotlib kütüphanesini kullanacağız. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            3. Ders - 24/01/2026 - Cumartesi
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesinde en çok kullanılan kütüphanelerden biri de "scikit-learn" denilen kütüphanedir. Bu kütüphane 
    makine öğrenmesinin "istatistiksel, olasılıksal ve matematiksel" yöntemlerine ilişkin işlemler yapan sınıfları ve 
    fonksiyonları barındırmaktadır. Bugün en yaygın kullanılan makine öğrenmesi teknikleri yapay sinir ağlarını kullanmaktadır. 
    sckit-learn ise yapay sinir ağlarına ilişkin yöntemleri içermemektedir. (scikit-learn içerisinde ilkel düzeyde yapay 
    sinir ağlarına yönelik birkaç sınıf bulunuyorsa da bunlar son derece yüzeyseldir.) 

    scikit-learn kütüphanesi Python'da yazılmıştır ve NumPy kütüphanesini taban kütüphane olarak kullanmaktadır. Bu 
    kütüphanein diğer programlama dillerinden kullanılması doğrudan mümkün değildir. Ancak dolaylı biçimlerde -verimli 
    olmasa da- kullanım sağlanabilmektedir. Eskiden TensorFlow gibi, PyTorch gibi yapay sinir ağlarına ilişkin kütüphaneler 
    (bunlara "framework" de diyebliriz) bazı temel işlemleri bünyesinde barındırmıyordu. Bu nedenle scikit-learn kullanılması 
    neredeyse zorunlu hale geliyordu. Ancak daha sonraları TensorFlow gibi PyTorch gibi yapay sinir ağı framework'leri de 
    scikit-learn tarafından sınulan bazı işlemleri bünyesine katmıştır. Biz kursumuzda özellikle klasik doğal dil işleme 
    yöntemlerinde scikit-learn kütüphanesini kullanacağız. scikit-learn Anaconda dağıtımında default biçimde yüklü olarak
    bulunmaktadır. Ancak diğer ortamlar için bu kütüphaneyi şöyle kurabilirsiniz:

    pip install scikit-learn

    scikit_learn kütüphanesinde işlemler belli bir kalıba uygun bir biçimde yürütülmektedir. Buna "fit/transform/predict
    kalıbı" diyebiliriz. Kütüphane büyük ölçüde nesne yönelimli biçimde tasarlanmıştır. fit/transform/predict kalıbı 
    şöyle kullanılmaktadır:

    1) Önce ilgili sınıf türünden bir nesne yaratılır. Yaratım sirasında ilgili konuya ilişkin çeşitli argümanlar da 
    girilmektedir. Örneğin:

    cv = CountVectorizer(...)

    2) İlgili sınıf türünden nesne yaratıldıktan sonra eğitimin yapılması gerekir. Burada "eğitim (training)" demekle 
    kullanılacak algoritmanın işletilmesi ve kestirim ya da sonuç için gereken bilgilerin elde edilerek nesnenin 
    özniteliklerinde (attributes) saklanması kastedilmektedir. Örneğin doğrusal regresyon uygulamak için regresyon 
    doğrusuna ilişkin (genel olarak "hyper-plane" denilmektedir) katsayıların elde edilmesi gerekir. İşte LinearRegression 
    sınıfının fit  metodu bu katsayıları elde edip nesnenin özniteliklerinde  sağlamaktadır. fit metoduna tipik olarak 
    bir NumPy dizisi verilir. Ancak bu metotlar Python listeleriyle Pandas'ın series nesneleriyle de çalışabilmektedir. 
    Örneğin:

    cv.fit(dataset)

    fit işlemiyle eğitim sonucunda elde edilen bilgiler nesnenin özniteliklerinde (attributes) saklanmaktadır. 

    3) Artık sıra sonucu elde etmeye gelmiştir. Bunun için transform  metotları kullanılmaktadır. transform metotları 
    programcıdan dönüştürülecek verileri bir NumPy disizi olarak (Python listeleri ve Pandas Series nesneleri de 
    kullanılabilmektedir) alır ve kestirimi yaparak sonucu bize verir. Sınıfların pek çoğunda (ama hepsinde değil) 
    fit ve transform işlemlerini birlikte yapan fit_transform metotları da bulunmaktadır. (Bazı sınıflarda  inverse_transform 
    metotları da vardır. inverse_transform metotları ters işlemi yapmaktadır.) Örneğin:

    transformed_dataset = cv.transform(target_dataset)

    4) Scikit-learn içerisindeki sınıfların bazıları dönüştürme, bazıları ise kestirim yapmaktadır. Dönüştürme yapan 
    sınıflarda dönüştürmeler genel olarak transform isimli metotlarla, kestirim yapan sınıflarda ise kestirimler predict 
    isimli metotlarla yapılmaktadır. predict metotlarına kestirilecek veriler argüman verilir, metotlar da sonucu bize verir. 
    Örneğin:

    lr = LinearRegression(...)
    lr.fit(dataset)
    predict_result = lr.predict(predict_dataset)

    Scikit-learn kütüphanesindeki fit/transform/predict kalıbı boru hattı (pipeline) mekanizmasının uygulanmasını da 
    mümkün hale getirmektedir. Makine öğrenmesinde "boru hattı (pipeline)" bir işlemin çıktısının diğer işleme girdi 
    yapılması, onun çıktısın da diğerine girdi yapılması ve böylece işlemlerin peşi sıra daha zahmetsiz bir biçimde 
    gerçekleştirilmesi anlamına gelmektedir. Örneğin önce SimpleImputer işlemini, onun çıktısı üzerinde de StandardScaler 
    işlemini yapmak isteyelim. StandardScaler işleminin çıktısını da SVM işlemine sokmak isteyelim. Boru hattı kullanmadan 
    bunları aşağıdaki gibi tek tek yapmak zorunda kalırız:

    si = SimpleImputer(...)
    si_transformed = si.fit_transform(...)

    ss = StandardScaler(...)
    ss_transfomed = ss.fit_transform(si_transformed)

    svm = SVM(...)
    result = svm.fit_transform(ss_transfomed)

    İşte boru hattı mekanizması sayesinde bu işlemler tek tek değil önceki çıktıyı sonraki girdiye vererek otomatik 
    bir biçimde de yapılabilmektedir:

    pl = PipeLine([('Imputing', SimpleImputer(...)), ('Scaling', StandardScaler(...)), ('SVM', SVM(...))])
    result = pi.fit_transform(...)
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Bilimsel uygulamalarda en fazla kullanılan kütüphanelerden biri de SciPy isimli kütüphanedir. SciPy temelde bir nümerik 
    analiz kütüphanesidir. Bu kütüphane pek çok matematiksel işlem yapan sınıflara ve fonksiyonlara sahiptir. Örneğin bir 
    fonksiyonun maksimum ve minimum noktalarının elde edilmesi gibi, bir fonksiyonun belli bir noktadaki türevinin elde 
    edilmesi gibi, bir denklemin köklerinin bulunması gibi işlemler SciPy kütüphanesiyle yapılabilmektedir. SciPy lineer 
    cebir ve istatistik paketlerine de sahiptir. Örneğin SciPy ile doğrusal denklem sistemleri çözülebilmekte, bazı 
    istatistiksel hipotez testleri gerçekleştirilebilmektedir. 
    
    Anconda dağıtımında SciPy kütüphanesi de yüklü olarak gelmektedir. Ancak diğer ortamlara kütüphaneyi şöyle kurabilirsiniz:

    pip install scipy

    SciPy taban kütüphane olarak NumPy kütüphanesini kullanmaktadır. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yapay sinir ağlarına ilişkin iki taban (base) kütüphane yaygın olarak kullanılmaktadır: TensorFlow ve PyTorch. TensorFlow 
    kütüphanesi Google kökenli, PyTorch ise Meta (Facebook) kökenlidir.  Eskiden TensorFlow en yoğun kullanılan kütüphaneydi. 
    Ancak son yıllarda PyTorch kütüphanesini tercih edenler TensorFlow kütüphanesini tercih edenlerden fazla hale gelmiştir. 
    Bunların yanı sıra eskiden Theano isimli bir kütüphane de özellikle akademik çevreler tarafından kullanılıyordu. Ancak 
    2017 yılından beri bu kütüphanenin geliştirilmesi durdurulmuştur. 

    TensorFlow ve PyTorch için "kütüphane" yerine "framework" terimi de kullanılabilmektedir. Bir yazılımsal araca 
    "framework" denilebilmesi için onun "akışı bazen programcıdan alıp, gerektiğinde programcının belirlediği fonksiyonları 
    çağırabilmesi (inversion of control)" gerekmektedir. Bu açıdan bakıldığında TensorFlow ve PyTorch için "framework" 
    nitelemesi de yapılabilir. Ancak biz kursumuzda bunlara framework yerine kütüphane demeyi tercih edeceğiz. 

    TensorFlow ve PyTorch kütüphaneleri (ya da framework'leri) eskiden yalnızca yapay sinir ağlarına ilişkin temel 
    işlemleri barındırıyordu. Yani bunlar aşağı seviyeli kütüphanelerdi. Ancak zamanla bu kütüphaneler gelişti ve yüksek 
    seviyeli katmanlara da sahip olmaya başladı. Örneğin yüksek seviyeli yapay sinir ağları işlemlerini yapmak için 
    tasarlanmış olaan Keras isimli bir kütüphane bulunmaktadır. Keras başlangıçta bağımsız bir kütüphaneydi. Ancak 
    daha sonraları TensorFlow bünyesine katıldı. Biz kurusumuzda yettiği sürece bu Keras kütüphanesini kullanacağız. 
    PyTorch'a da zamanla Keras gibi yüksek seviyeli katmanlar eklenmiştir.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yukarda açıkladığımız kütüphaneler belli bir süredir var olan temel kütüphanelerdir. Doğal dil işlemede kullanılan 
    bu kursta da kullanacağımız alçak seviyeli ve yüksek seviyeli pek çok kütüphane de bulunmaktadır. Bu kütüpahanelerin 
    bir bölümü klasik doğal dil işleme işlemlerine yöneliktir. Diğer bölümü ise "dönüştürücüler (transformer)" sonrası 
    modern doğal dil işleme süreçleriyle ilgildir. Biz zaten kursumuzda yeri geldikçe bu kütüphaneleri tanıtarak kullanacağız. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Doğal dil işlemede veriler genel olarak yazılar biçimindedir. Yazılara ilişkin veri kümeleri için CSV formatı uygun 
    bir format değildir. (Örneğin CSV formatında sütunlar virgüllerle biribirinden ayrılmaktadır. Ancak yazılarda zaten 
    virgül yazının içerisinde oldukça fazla bulunabilen bir karakterdir.) 
    
    Yazısal veri kümeleri çoğu kez doğrudan "düz metin dosyaları (pure text file)" biçiminde karşımıza çıkmaktadır. Düz 
    metin dosyası demekle "içerisinde hiçbir formatlama bilgisinin bulunmadığı, yalnızca yazıların karakterlerinin bulunduğu 
    dosyaları" kastediyoruz. Bu dosyalara genellikle ".txt" uzantısı verilmektedir. Bazı metin dosyalarının içerisinde formatlama
    biligileri de (örneğin yazıların renklerine ilişkin, bold'luk durumuna ilişkin bilgiler gibi) bulunabilmektedir. Ancak 
    bu bilgiler dosyanın içerisinde binary biçimde değil yine yazısal biçimde bulundurulmaktadır. Bu tür dosyalara genel 
    olarak "zengin metin dosyaları (rich text files)" denilmektedir. Örneğin Microsoft'un ".rtf" uzantılı dosyaları, ".md" 
    uzantılı markup dosyaları, ".xml" uzantılı XML dosyaları bu bağlamda zengin metin dosyaları durumundadır. Ancak doğal 
    işlemede zengin metin dosyaları yerine genellikle (ama her zaman değil) düz metin dosyaları kullanılmaktadır. 

    Doğal dil işlemede yazılar metin dosyalarının yanı sıra veritabanlarının içerisinde de bulunabilmektedir. Doğal dil 
    işlemede yüksek miktardaki verileri depolamak için "Parquet" ("parke:" biçiminde okunuyor) "HDF (Hierarchical File 
    Format)" formatları da kullanılabilmektedir. Bazı doğal dil işleme uygulamalarında yazısal veriler web sayfalarından 
    da çekilip kullanıma hazır hale getirilebilmektedir. Web sayfasından bilgilerin elde edilmesina İngilizce "web scraping"
    denilmektedir. ("Scrape" sözcüğü "kazımak, kazıyarak elde etmek" gibi anlamlara gelmektedir.)

    Doğal işlemede en çok kullanılan formatlardan biri de JSON (Java Script Object Notation) denilen formattır. JSON 
    formatı adeta Python'daki sözlük nesnelerinin temsiline benzemektedir. JSON düz metinsel bir biçime sahiptir. Dolayısıyla 
    ayrıca sıkıştırılmamışsa JSON dosyaları büyük yer kaplama eğilimindedir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi doğal dil işlemede Python'un standard kütüphanesinin yanı sıra pek çok özel kütüphaneden 
    de faydalanılmaktadır. Python'un standart kütüphanesinde doğal dil işlemede faydalanabileceğimiz çeşitli modüller
    bulunmaktadır. Örneğin "düzenli ifadeler (regular expressions)" doğal dil işlemede yaygın biçimde kullanılmaktadır. 
    Özellikle aşağı seviyeli önişlem faaliyetlerinde düzenli ifadelerden sıkça faydalanılmaktadır. Düzenli ifadeler 
    "Python Uygulamaları" kursumuzda ele alınan bir konudur. Ancak burada kısaca bu konunun üzerinde durmak istiyoruz. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Düzenli ifadeler (regular expressions, kısaca "regex" de denilmektedir) belli bir kalıba ilişkin yazı parçalarının 
    elde edilmesi amacıyla kullanılan mini bir dildir. Bu dilin de sentaks ve semantik kuralları vardır. Regex dilinde 
    kalıp oluşturulur. Sonra oluşturulan kalıp ismine "düzenli ifade motoru (regular expression engine)" denilen motora 
    verilir. Motor da kalıba uygun parçaları elde ederek birtakım işlemler yapar. Örneğin biz bir metin içerisindeki 
    "dd/aa/yyyy" gibi bir kalıba uygun tüm tarihleri bulmak isteyebiliriz. Ya da önneğin biz bir metindeki sözcükleri 
    bulmak isteyebiliriz. Düzenli ifadeler bu tür işlemlerin kolay yapılmasını sağlamaktadır. Pek çok programlama dilinde 
    düzenli ifadeler üzerinde işlemler yapan sınıflar ve fonksiyonlar onların standart kütüphaneleri içerisinde bulunmaktadır. 
    Hatta Ruby gibi Perl gibi dillerde düzenli ifadeler sentaks bakımından dilin kendi içerisinde built-in bir biçimde 
    desteklenmektedir. Python Standart Kütüphanesinde de düzenli ifadelerle işlemler "re" isimli modülün içerisindeki 
    fonksiyonlar ve sınıflarla yapılmaktadır. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            4. Ders - 25/01/2026 - Pazar
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Burada kısaca düzenli ifadelerle nasıl kalıp oluşturulacağından bahsedeceğiz. Konunun ayrıntıları için "Python 
    Uygulamaları" kursumuza ilişkin kurs notlarını gözden geçirebilirsiniz. Python Standart Kütüphanesindeki düzenli 
    ifadelerle ilgili re modülünün dokmantasyonuna aşağıdaki bağlantıdan erişebilirsiniz:

    https://docs.python.org/3/library/re.html

    Düzenli ifadeler ile ilgili denemeler yapmak için https://regex101.com/ sitesinden faydalanabilirsiniz. 

    Eskiden text editörlerin ve kelime işlem programlarının düzenli ifadelerle işlem yapma özellikleri yoktu. Zamanla bu 
    özellikler pek çok text editöre ve kelime işlem programına eklendi. Bugün Microsoft Word gibi kelime işlemciler, 
    Visual Studio, VSCode gibi IDE'ler ve editörler düzenli ifadelerle işlemlerin yapılmasına olanak sağlamaktadır. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Düzenli ifadeler bir metin içerisindeki belli bir kalıba uygun yazı parçalarını elde etmek ve onlar üzerinde işlemler
    yapabilmek için tasarlanmış olan mini bir dildir. Metin içerisindeki yazı parçaları kalıplar oluşturularak elde 
    edilmektedir. Düzenli ifadelerdeki kalıplarda iki tür karakter kullanılmaktadır: Normal karakterler ve meta karakterler. 
    Normal karakterler kalıpta bulunması istenen gerçek karakterlerdir. Meta karakterler ise "özel anlam ifade eden" 
    karakterlerdir. Örneğin '.' karakteri bir meta karakterdir. Bu karakteri biz bir kalıp içerisinde kullandığımızda 
    bu nokta karakteri anlamına gelmez, '\n' dışındaki herhangi bir karakter" anlamına gelir. Eğer meta karakterlerin 
    normal karakterler olarak ele alınması isteniyorsa onun soluna yapışık bir ters bölü karakteri getirilmelidir. Örneğin 
    ".abc" kalıbındaki '.' "burada herhangi bir karakter bulunabilir" anlamına gelmektdir. Eğer biz bu kalıpta gerçekten 
    nokta karakterini belirtmek istiyorsak kalıbı "\.abc" biçiminde yazmalıyız. 

    Düzenli ifadeler dili maalesef standart hale getirilememiştir. Dolayısıyla ana hatları aynı olmak üzere düzenli ifadeler
    üzerinde işlem yapan motorlar arasında da küçük farklılıklar söz konusu olabilmektedir. 

    Düzenli ifadelere ilişkin kalıp oluşturma kuralları özet olarak şöyledir: (Örneklerimizde iki tırnak içerisindeki 
    kalıplar bir Python string'i olarak değerlendirilmemelidir. Yani örneğin biz "ka\." biçiminde bir kalıp yazdığımızda 
    buradaki ters bölü gerçekten ters bölü karakteridir. Bilindiği gibi Python'da ters bölü karakterlerinin gerçek ters 
    bölü karakteri olarak ele alınması için string'e yapışık r ya da R harfi getirilmektedir.)

     - Nokta bir meta karakterdir, bu karakter "\n (new line) dışındaki herhangi bir karakter" anlamına gelir. Örneğin 
     "al." kalıbı "al" ile başlayan ve üçüncü karakteri herhangi bir karakter olan üç karakteri belirtir. Dolayısıyla "al."
     kalıbı "ali" ile de uyuşabilir "alm" ile de uyuşabilir, "alr" ile de uyuşabilir. (Burada uyuşmak İnglizce "match" 
     sözcüğünün karşılığı olarak kullanılmaktadır.)

    - * meta karakteri "solundaki karakterden sıfır tane ya da daha fazla" anlamına gelmektedir. Bu durumda örneğin 
    "ka*" kalıbı "k" ile, "ka" ile, "kaa" ile, "kaaa" ile, "kaaaaa" ile uyuşur. Örneğin "k.*" kalıbı başı "k" ile başlayan 
    satır sonuna kadar tüm karakterle uyuşur. Nokta meta karakterinin '\n' karakterini içermediğine dikkat ediniz. Bu 
    durumda "k.*" kalıbı satır sonunda etkisini kaybedecektir. 

    - + meta karakteri "solundaki karakterden bir ya da daha fazlasını" belirtir. * ile + meta karakterlerini birbirine 
    karıştırmayınız. * solundaki karakterden sıfır tane ya da çok tane ile uyuşurken, + solundaki karakterden bir tane 
    ya da çok taneyle uyuşmaktadır. Örneğin "ka+" kalıbı "k" ile uyuşmaz. Ancak "ka" ile "kaaaa" ile uyuşur. Fakat "ka*" 
    kalıbı "k" ile "ka" ile "kaaaa" ile uyuşur.

    - ? meta karakteri "solundaki karakterden 0 tane ya da 1 tane" anlamına gelmektedir. Örneğin "ab?c" kalıbı "ac" ile 
    uyuşur "abc" ile de uyuşur ancak örneğin "abbc" ile uyuşmaz. 

    - Yukarıda da belirttiğimiz gibi bir meta karakter normal karakter olarak kullanılacaksa ters bölülenmelidir. Örneğin 
    "ka\." gibi bir kalıp "ka." ile uyuşur. "kal" ile ve "kar" ile uyuşmaz. Tabii ters bölünün kendisi için de '\\' 
    kullanılmalıdır. Eğer '\' karakterinin sağındaki karakter özel bir ters bölü karekteri değilse ve bir meta karakter 
    de değilse ifade geçersizdir.

    - Köşeli parantezler birer meta karakterdir. "İçerisindeki karakterlerin herhangi biri" anlamına gelir. Örneğin 
    "[abc]+" kalıbı "abcaaaabc" ile ya da "aaaabbbbbcccbcbcbca" ile uyşur. "a[bcd]?" kalıbı "a" ile "ab" ile "ac" ile 
    "ad" ile uyuşur.

    - Köşeli parantez içerisinde '-' karakteri kullanılırsa bu "solundaki karakterden sağındaki karaktere kadar herhangi 
    biri" anlamına gelir. Örneğin "[a-z]+" kalıbı "ali" ile "veli" ile "selami" ile uyuşur. Örneğin "[a-zA-Z]+" kalıbı 
    sözcükleri bulmak için kullanılabilir. Ancak default durumda Türkçe karakter bulunamayacaktır. Python'un re modülündeki
    lokal spesifik davranış fonksiyonların flags parametresi ile ayarlanabilmektedir. Normal olarak "-" karakteri bir 
    meta karakter değildir. Ancak köşeli parantezler içerisindeki '-' karakteri bir meta karakter olarak ele alınmaktadır. 
    (Bir istisna olarak köşeli parantezler içerisinde '-' karakteri son karakter olarak bulunuyorsa meta karakter olarak 
    ele alınmamaktadır.) Tabii köşeli parantezler meta karakter olduğu için gerçekten köşeli parantezleri arayacaksak 
    ters bölülemek gerekir. Örneğin amacımız köşeli parantezlerin kendisini aramak ise "\[.*\]" kalıbını kullanabiliriz. 

    - Köşeli parantezin başında '^' karakteri varsa bu durum "köşeli parantezin içerisindeki karakterlerden biri olmayan 
    herhangi bir karakter anlamına gelir. Örneğin "[^abc]" kalıbı 'a' ay da 'b' ya da 'c' olmayan herhangi bir karakter 
    anlamına gelmektedir. 

    - Düzenli ifadelerde uyum sağlayan en uzun karakter kümesi elde edilmektedir. Örneğin "abcabxyz" gibi bir yazıda 
    "[abc]+" kalıbı "abc" ve "ab" ile uyuşacaktır.  

    - Küme parantez kalıbı dört biçimde kullanılabilir: {n}, {n,k} ve {n,}, {, n}. {n} kalıbı "solundaki karakterden tam 
    olarak n tane" anlamına gelmektedir. Örneğin "a{3}" kalıbı "aaa" ile uyuşur. Eğer yazıda "aaaaaaaa" biçiminde a varsa 
    bu kalıp bunun ilk 3 tanesi ile uyuşum sağlayacaktır. {n,k} kalıbı "solundaki karakterden n ile k arasında herhangi 
    tane (n ve k dahil)" anlamına gelmektedir. Örneğin "a{3,5}" kalıbı "aaa" ile uyuşur, "aaaa" ile uyuşur, "aaaaa" ile 
    uyuşur. Ancak örneğin "aa" ile uyuşmaz. "aaaaaaaaaa" daki ilk 5 a ile uyuşur. {n,} kalıbı "solundaki karakterden en 
    az n tane" anlamına gelmektedir. {,n} kalıbı ise "solundaki karakterden en fazla n tane" anlamına gelmektedir. Örneğin 
    "a{,5}" kalıbı "en fazla 5 tane a ile uyuşum sağlar. Bu durumda 0 tane a ile de uyuşum sağlanır. Normal olarak küme 
    parantezlerinin içinde boşluk karakterleri bir soruna yol açmamaktadır. Yani örneğin "a{3,5}" kalıbını "a{3, 5}" 
    biçiminde yazsak da bir sorun oluşmaz. Ancak siz küme parantezlerinin içerisinde SPACE karakteri kullanmayınız. 

    - Köşeli parantezlerin dışındaki '^' meta karakteri "yalnızca satırın başını" dikkate alır. Örneğin "^a+" kalıbı yazının 
    başındaki a'larla uyuşur. Örneğin "^[0-9]+" kalıbı yazının başındaki sayıları bulmaktadır. 

    - '$' meta karakteri yazının sonunu temsil etmektedir. Örneğin "abc$" kalıbı satırın sonundaki "abc" ile uyuşur. 
    Örneğin "[0-9]+$" kalıbı yazının sonundaki sayıları bulur. 

    - \w meta karakterleri "herhangi bir alfabetik, nümerik ya da '_' karakteri" anlamına gelmektedir. Default durumda bu 
    kalıp "[a-zA-Z_0-9]" ile eşdeğerdir. Yani Türkçe karakterler kalıba dahil değildir. Ancak pek çok regex kütüphanesinde 
    bir lokal ayarı yapılabilmektedir. Eğer regex ortamı izin veriyorsa bu ayar yapılarak bu kalıba Türkçe karakterler de 
    dahil edilebilir. Tabii Türkçe karakterleri dahil edecek biçimde kalıp manuel olarak [\wşçğüöıŞÇĞÜÖİ] biçiminde de 
    oluşturulabilir. Düzenli ifade motorlarının "UNICODE" seçeneği de bulunabilmektedir. Bu seçenek aktif hale getirildiğinde 
    artık \w kalıbı UNICODE tablodaki tüm alfabetik karakterleri kapsar hale gelmektedir. Yani UNICODE ayarı adeta "tüm 
    lokalleri içerir" anlamına gelmektedir. "\w+" kalıbı sözcükleri bulmak için sık sık kullanılan bir kalıptır. 
    
    \W meta karakterleri ise "herhangi bir alfabetik, nümerik karakter ya da '_' karakterinin dışındaki karakter" anlamına 
    gelir. Yani \w meta karakterinin tersini belirtir. 

    - \s meta karakterleri "herhangi bir boşluk karakteri (white space)" anlamına gelmektedir. Benzer biçimde \S meta 
    karakterleri ise "herhangi bir boşluk karakteri dışındaki karakter" anlamına gelir. 

    - \d kalıbı "sayısal olan karakterlerden herhangi biri", \D kalıbı ise "sayısal olmayan karakterlerden herhangi biri" 
    anlamına gelmektedir. Örneğin "\d+" kalıbı ile sayıları bulabiliriz. O halde örneğin "[\-+]?\d+" kalıbı tamsayıları 
    bulabilir. Noktalı sayıları da bulabilen kalıp "[\-+]?\d*\.?\d+" bçiminde olabilir. 

    - \b meta karakteri "sözcük başları" anlamına gelmektedir. Örneğin "\ba+" gibi bir kalıp "a" ile başlayan sözcüklerle 
    uyuşur.

    - Parantezler gruplama için kullanılmaktadır. Örneğin "(abc)+" kalıbı ile "abc" uyuşur, "abcabc" uyuşur, "abcabcabc" 
    uyuşur. Ancak örneğin "abab" uyuşmaz.

    - '|' karakteri "veya" anlamına gelmektedir. Örneğin "ali|veli" kalıbı "ali" ya da "veli" ile uyuşabilmektedir. Buradaki
    '|' karakterinin düşük öncelikli olduğuna dikkat ediniz. Örneğin "a+|b+" kalıbı bir ya da daha fazla a ile bir ya da 
    daha fazla b ile uyuşur. 

    Çok kullanılanm bazı kalıpları aşağıda veriyoruz:

    E-Posta kalıbı: [A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}
    Noktalı sayı kalıbı: [+-]?[0-9]+\.[0-9]+|\.[0-9]+|[0-9]+\.
    Tamsayı kalıbı: [+-]?[0-9]+
    Değişken atom kalıbı: [_a-z-A-Z][_a-zA-Z-0-9]+
    day/month/year tarih kalıbı: (0?[1-9]|1[0-2])\/(0?[1-9]|1\d|2\d|3[01])\/(19|20)\d{2}
    dd/mm/yyyy tarih kalıbı: (0[1-9]|1\d|2\d|3[01])\/(0[1-9]|1[0-2])\/(19|20)\d{2}
    IPV4 adresi kalıbı: ([0-9]{1,3}\.){3}[0-9]{1,3}
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Her türlü ayrıştırma işlemi düzenli ifadelerle yapılamayabilmektedir. Bu tür durumlarda belli noktaya kadar düzenli 
    ifadeleri kullanıp sonra manuel kontrollerle hedefinize ulaşabilirsiniz. Örneğin bir programlama dilindeki programı 
    atomlarına ayırmak için düzenli ifadeler yeterli olmayabilir. Yani düzenli ifadeler bizim ayrıştırma ve bulma 
    işlemlerimizde tüm isteklerimizi yerine getiremeyebilmektedir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Python'da düzenli ifadelerle işlemler re modülündeki fonksiyonlar ve sınıflarla yapılmaktadır. re modülündeki fonksiyonlar 
    genel olarak önce kalıp yazısını sonra da asıl yazıyı parametre olarak alırlar. Yine modüldeki fonksiyonların son 
    paramatreleri düzenli ifadeler üzerindeki bazı seçenekleri belirtmektedir. Bu paramtre flags olarak isimlendirilmiştir. 
    Programcının kalıp yazısını oluşturken string'i "r" ya da "R" öneki ile oluşturması "\" karakterlerinin gerçekten ters 
    bölü karakteri olarak ele alınmasını garanti edecektir.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    re modülündeki findall isimli fonksiyon yazıdaki regex kalıbına uyan tüm parçaları bir liste olarak bize vermektedir. 
    findall fonksiyonun parametrik yapısı şöyledir:

    findall(pattern, string, flags=0)

    Örneğin:

    text = '17 asal bir sayıdır. 101 de asaldır. Ancak 115 asal sayı değildir.'
    pattern = r'\d+'
    result = re.findall(pattern, text)
    print(result)

    Burada görüldüğü gibi findall fonksiyonuna önce kalıp yazısı sonra da asıl yazı argüman olarak geçirilmiştir. flags 
    parametresi default değer aldığı için hiç kullanılmamıştır.

    findall fonksiyonu parantezli grupları da ayırıp onları birer demet olarak da vermektedir. İzleyen paragraflarda 
    gruplama konusu üzerinde de duracağız. 
#------------------------------------------------------------------------------------------------------------------------

import re

text = 'ali veli 123 selami 628 ayşe fatma 876'
pattern = r'\d+'

result = re.findall(pattern, text)      
print(result)           # ['123', '628', '876']

numbers = list(map(int, result))
print(numbers)          # [123, 628, 876]

#------------------------------------------------------------------------------------------------------------------------
    Aşağıda findall fonksiyonu ile tarihlerin elde edilmesine yönelik bir örnek verilmiştir. 
#------------------------------------------------------------------------------------------------------------------------

import re

text = 'ali veli 10/12/2009 selami 05/07/1998 ayşe fatma 23/11/2014'
pattern = r'\d\d/\d\d/\d\d\d\d'

result = re.findall(pattern, text)
print(result)       # ['10/12/2009', '05/07/1998', '23/11/2014']

#------------------------------------------------------------------------------------------------------------------------
    Aşağıda findall fonksiyonu ile bir yazıdaki e-posta adreslerinin bulunmasına örnek verilmiştir. 
#------------------------------------------------------------------------------------------------------------------------

import re

text = """bana e-posta atabilrsin. E-posta adresim aslank@csystem.org. 
Eğer bana ulaşamazsan info@csystem.org'ye de e-posta gönderebilirsin.'
"""
pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}'

result = re.findall(pattern, text)
print(result)

#------------------------------------------------------------------------------------------------------------------------
    re modülündeki split fonksiyonu str sınıfının split metodundan çok daha güçlü bir biçimde ayrıştırma yapabilmektedir. 
    Bu fonksiyon bir regex kalıbını ayıraç olarak kabul ederek yazıyı parçalarına ayırır. split fonksiyonunda verilen 
    kalıp ayıraçları belirtmektedir. Fonksiyon bu ayıraçları diğer yazının parçalarını ayırmak için kullanacaktır. 
    Fonksiyonun parametrik yapısı şöyledir:

    split(pattern, string, maxsplit=0, flags=0)

    split fonksiyonu her kalıba uygun ayıracı bulduğunda onun solundaki ve sağındaki yazı parçasını elde eder. Eğer 
    yazının başında ve sonunda ayıraç kalıbı varsa split bu ayıraç kalıbının solunda ve sağında bir yazı olmadığı için 
    boş string oluturmaktadır. Örneğin:

    text = '   ,,ali,,,  veli,,   selami   '
    pattern = "[ ,]+"
    result = re.split(pattern, text)
    print(result)

    Burada yazının başı ayırçlarla başlamıştır. O halde yazının başında bir tane boş string bulunacaktır. Yazının sonu da 
    ayırçlarla bitmiştir o halde yazının sonunda da boş string bulunacaktır. Bu kod parçası çalıştırıldığında ekrana 
    şunlar basılacaktır:

    ['', 'ali', 'veli', 'selami', '']
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    re modülündeki search fonksiyonu yazı içerisinde bir regex kalıbını arar. Eğer bulursa ilk bulduğu kalıba ilişkin re 
    modülündeki Match türünden bir sınıf nesnesiyle geri döner. Fonksiyonun parametrik yapısı şöyledir:

    re.search(pattern, string, flags=0)

    Yine fonksiyonun birinci parametresi aranacak kalıbı, ikinci parametresi aramanın yapılacağı yazıyı belirtmektedir. 
    Fonksiyon başarı durumunda Match nesnesine başarısızlık durumunda None değerine geri dönmektedir. 

    Match sınıfının start netodu kalıbın bulunduğu karakterin başlangıç index numarasını, end metodu ise bitiş index 
    numarasının bir fazlasını vermektedir. Böylece biz dilimleme yoluyla bulunan kalıbı elde edebiliriz. Örneğin:

    text = ',,,ali, veli, selami'
    pattern = r'\w+'
    m = re.search(pattern, text)

    Burada ilk uyuşumu yazı içerisindeki "ali" karakterleri sağlamaktadır. Ancak search bize "ali" yazısını değil bir 
    Match türündne bir nesne vermektedir. Bu match nesnesinin start metodu kalıbın yazı içerisindeki başlangıç index 
    numarasını, end metodu ise bitiş indeks numarasından bir sonrak indeks numarasını verir. Dolayısıyla biz bu aralığı 
    dilimlemede kullanabiliriz. Örneğin:
    
    if m:
        print(m.start(), m.end())
        print(text[m.start():m.end()])
#------------------------------------------------------------------------------------------------------------------------

import re

text = 'my email addres is aslank@csystem.org but your email address is serce@csystem.com'
pattern = r'[a-zA-Z0–9+_.-]+@[a-zA-Z0–9.-]+'

m = re.search(pattern, text)
if m:
    result = text[m.start():m.end()]
    print(result)
else:
    print('cannot find match!..')

#------------------------------------------------------------------------------------------------------------------------
    Bir kalıp parantezler kullanılarak oluşturulmuşsa kalıbın içerisindeki parantezli kısımlara "grup (group)" denilmektedir. 
    search fonksiyonu yalnızca ana kalıbı değil parantezler içerisindeki grup'ları da bulabilmektdir. Örneğin şöyle bir 
    kalıp olsun: "(\d+)@(\d+)". 123@456 gibi karakter öbeği bu kalıp ile uyuşmaktadır. İşte biz Match nesnesi ile uyuşan 
    kısmı bir bütün olarak elde edebilceğimiz gibi bunun 123 ve 456'dan oluşan gruplarını da elde edebilmekteyiz. 
    
    Match sınıfının group metodu regex kalıbındaki grupları bize vermektedir. group metodu grubun numarasını parametre 
    olarak alır. Group numaraları 1'den başlamaktadır. 0'ıncı grup numarası kalıbın tamamını belirtir. group metodu ile 
    Match sınıfının [] operatör metodu aynı işlemi yapmaktadır. Örneğin:

    text = 'ali 123@567 veli 135@854 selami'
    pattern = r'(\d+)@(\d+)'
    m = re.search(pattern, text)

    Buradaki kalıp sayı@sayı biçimindeki karakterle uyuşur. Ancak bu kalıpta @ karakterinin iki yanı birer grup biçiminde 
    oluşturulmuştur. Bu sayede biz hem uyuşan kalıbın tamamını elde edebiliriz hem de onun parçalarını elde edebiliriz. 
    Buradaki örnekte search ilk uyuşan kalıbı yani 123@567 karakterini bulacak ve bize bunu bir Match nesnesi biçiminde 
    verecektir. Bu Match nesnesinin 1'inci grubu "123" yazısından, 2'inci grubu "567" yazısından oluşacaktır. İşte biz 
    m[1] ya da m.group(1) ifadeseiyle 1'inci grubu m[2] ya da m.group(2) ifadesiyle de 2'inci grubu elde edebiliriz. 
    m[0] ya da m.group(0) ifadesi ile de biz tüm kalıba erişebiliriz. İster gruplu bir kalıp olsun isterse grupsuz bir 
    kalıp olsun m[0] her zaman uyuşan kalıbı vermektedir. Match sınıfının string örnek özniteliği bize search fonksiyonun 
    çağrılmasında kullanılan asıl yazıyı da vermektedir.
#------------------------------------------------------------------------------------------------------------------------

import re

text = 'ali veli selami 123@789 ayşe fatma'
m = r'(\d+)@(\d+)'

m = re.search(pattern, text)
if m:
    print(m[0])
    print(m[1])
    print(m[2])
else:
    print('kalıp bulunamadı!..')

#------------------------------------------------------------------------------------------------------------------------
    Aslında daha önce görmüş olduğumuz findall metodu da grupla çalışmaktadır. Eğer kalıpta parantezler varsa findall bu 
    grupları bir demet listesi olarak bize vermektedir. Örneğin:

    text = 'ali veli selami 123@789 ayşe fatma 478@456'
    pattern = r'(\d+)@(\d+)'

    result = re.findall(pattern, text)
    print(result)       # [('123', '789'), ('478', '456')]

    Ancak kalıpta tek bir grup varsa findall bize bunları tek elemanlı demet listesi olarak değil düz bir liste olarak 
    verir.

    Gruplama hem bir kalıbın bulunmasına hem de o kalıp içerisindeki bir parçanın elde edilmesine olanak sağlamaktadır.
    Örneğin birisi bize yazıdaki tek tırnak içerisinde bulunan isimleri elde etmemizi istesin. Yazı aşağıdaki gibi olsun:

    text = "ali veli 'selami' ayşe 'fatma' hasan"

    Burada bizden tüm isimlerin değil 'selami' ve 'fatma' isimlerinin bulunması istenmektedir. Kalıbı aşağıdaki oluşturmuş 
    olalım:

    pattern = r"'\w+'"

    Bu kalıpla biz tek tırnak içerisindeki isimleri tek tırnaklarıyla buluruz. Halbuki bizden tek tırnak içerisindeki isimlerin 
    tırnaksız bir biçimde bulunması istenmiştir. Tabii biz bu isimleri tırnaklı bulduktan sonra bu tırnakları atabiliriz. 
    Ancak bu da ek bir çaba gerektirir. İşte bu tür durumlarda gruplama pratik çözüm oluşturmaktadır. Örneğin:

    pattern = r"'(\w+)'"
    result = re.findall(pattern, text)
    print(result)       # ['selami', 'fatma']
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    re modülündeki match fonksiyonu search fonksiyonu gibidir. Yani yine aranan kalıba ilişkin Match nesnesini verir. 
    Ancak kalıbın yazının başında olması gerekir. Yani bu fonksiyon her zaman kalıp sanki yazının başındaymış gibi arama 
    yapmaktadır. Aşağıdaki örnekte uyuşum sağlanamayacaktır.

    text = 'ali veli selami 123@789 ayşe fatma 478@456'
    pattern = r'(\d+)@(\d+)'

    m = re.match(pattern, text)
    print(m)    # None

    Tabii aslında yazının başından arama yapmak için kalıbın başına ^ meta karakteri de getirilebilir. ^ meta karakterinin 
    yazının başından itibaren uyuşama bakacağını daha önce belirtmiştik.
#------------------------------------------------------------------------------------------------------------------------

import re

text = 'ali veli selami 123@789 ayşe fatma 478@456'
pattern = r'(\d+)@(\d+)'

result = re.match(pattern, text)
if result:
    print(result[0])
else:
    print('kalıp bulunamadı!..')

#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    re modülündeki fullmatch fonksiyonu yazının tamamının kalıba uygun olmadığını belirlemekte kullanılır. Bu fonksiyon 
    da başarı durumunda Match nesnesine, başarısızlık durumunda None değerine geri dönmektedir. Fonksiyonun parametrik 
    yapısı diğerleriyle aynıdır:

    re.fullmatch(pattern, string, flags=0)
#------------------------------------------------------------------------------------------------------------------------

import re

text = '10/12/1997'
pattern = r'\d\d/\d\d/\d\d\d\d'

a = re.fullmatch(pattern, text)
if a:
    print('Uyuşum var')
else:
    print('Uyuşum yok')

text = '10/12/1997   '

a = re.fullmatch(pattern, text)
if a:
    print('Kalıba uygun')
else:
    print('Uyuşum yok')

#------------------------------------------------------------------------------------------------------------------------
    re modlündeki sub fonksiyonu belli bir kalıbın yerine başka bir yazı yerleştirmek için kullanılmaktadır. Bu fonksiyon 
    str sınıfının replace metodunun düzenli ifade alan versiyonu gibi düşünülebilir. Fonksiyonun parametrik yapısı şöyledir:

    re.sub(pattern, repl, string, count=0, flags=0)

    Buradaki count parametresi kaç uyuşumun değiştirileceğini belirtmektedir. 0 değeri hepsinin değiştirileceği anlamına 
    gelmektedir. Tabii fonksiyon asıl yazıda bir değişiklik yapmaz, bize değiştirilmiş yeni bir yazıyı verir. Örneğin:

    text = 'ali -veli-, selami -ayşe- fatma -hayri- sibel -hasan-'
    pattern = r'-(\w+)-'

    result = re.sub(pattern, 'xxx', text)
    print(result)               # ali xxx, selami xxx fatma xxx sibel xxx

    Burada tireler arasındaki isimler xxx karakterleri ile yer değiştirilmiştir. 

    sub fonksiyonunun ikinci parametresinde değiştirilecek yazıda \1, \2, \3 gibi \n biçiminde grup belirten meta karakterler 
    kullanılabilmektedir. Bu sayede biz bir uyuşumun belli kısımlarını da değiştirebiliriz. Örneğin ondalıklı sayıların 
    ondalık kısmı '.' karakteri ile biribirinden ayrılmaktadır. Ancak bazı yerel dillerde nokta yerine virgül de kullanılmaktadır. 
    Biz bir metindeki noktalı sayılardaki noktaları virgül ile yer değiştirmek isteyelim. Bunu sağlamak için aşağıdaki gibi 
    bir kalıbı kullanabiliriz:

    text = 'Aynanın eni 18.50 santim. Boyu da 21.50 santim. Ayrıca pi sayısı da kısa bir biçimde 3.14'

    result = re.sub(r'(\d+)\.(\d+)', r'\1,\2', text)
    print(result)   # Aynanın eni 18,50 santim. Boyu da 21,50 santim. Ayrıca pi sayısı da kısa bir biçimde 3,14
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                        5. Ders - 31/01/2026 - Cumartesi
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de karakter tabloları ve karakter kodlamaları üzerinde temel bilgiler vereceğiz. Doğal dil işleme etkinlikleri 
    yazılar üzerinde yapıldığı için bu alanda çalışacak kişilerin karakter tabloları ve karakter kodlamaları konusunda 
    temel bilgilere sahip olması gerekmektedir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yazıda yer kaplayan en küçük birime karakter (character) denilmektedir. Yazılar karakterleden oluşan bir dizi gibi 
    düşünülebilir. Karakterler ise sayılarla temsil edilmektedir. Örneğin "ankara" yazısı karakterlerden oluşmaktadır. 
    Karakterler de sayılarla temsil edildiği için bu yazı aslında sayılardan oluşan bir dizi gibi düşünülebilir. Text 
    editörler bir dosyayı görüntülerken onun içerisindeki sayılara karşı gelen karakter temsillerini bize göstermektedir. 
    İşte hangi karakterlerin hangi sayılarla temsil edildiğini tanımlayan "karakter tabloları" oluşturulmuştur. 
    
    Dünyada bilişim alanında kullanılan ilk karakter tablosu "ASCII (American Standard Code Information Interchange)" 
    denilen tablodur. Orijinal ASCII tablosu 7 bitlikti, 128 farklı karaktere birer numara karşılık düşürülmüştü. (Örneğin 
    'a' karakteri tablonun 97'inci karakteridir, 'b' karakteri 98'inci karakteridir.) ASCII tablosunun yanı sıra daha 
    sonraları "EBCDIC (Extended Binary Coded Decimal Interchange Code)" gibi, "WISCII" gibi başka karakter tabloları da 
    geliştirilmiştir. IBM sistemeri uzun süre EBCDIC tablosunu kullanmıştır. Bu tablo hala IBM'in bazı sistemlerinde 
    geçmişe uyumluluğun sağlanması amacıyla kullanılmaktadır.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Bir karakter tablosunda üç önemli kavram vardır:

    1) Glyph
    2) Kod numarası (Code Point)
    3) Karakter Kodlaması (Character Encoding)

    Karakter tablosunun desteklediği karakterlerin görsel temsiline "glyph ("glif" biçiminde okunuyor)" denilmektedir. 
    Karakter tablosu içerisindeki her glyph'e 0'dan itibaren bir numara karşılık düşürülmüştür. Buna ilgili karakterin 
    "kod numarası (code point)" denilmektedir. Örneğin ASCII tablosunda 'a' karakterinin kopd numarası 97'dir. Bir glyph'e 
    ilişkin kod numarası doğrudan 2'lik sistemde bir sayı biçiminde kodlanarak dosyalarda saklanabilir. Ancak karakter 
    tabloları kod numaralarını dosyalarda saklamak için onlar üzerinde bazı dönüşümler de yapabilmektedir. İşte yapılan 
    bu dönüştürmeye "karakter kodlaması (character encoding)" denilmektedir. Bazı tablolarda kod numaraları birden fazla 
    karakter kodlaması ile sayısal biçime dönüştürülebilmektedir.  
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Karakter tabloları kabaca "bir byte'lık karakter tabloları" ve "geniş karakter tabloları" olmak üzere ikiye ayrılmaktadır. 
    Bir byte'lık karakter tablolarında karakterlere ilişkin kod numaraları bir byte ile ifade edilebilmektedir. Dolayısıyla 
    bu tablolarda en fazla 256 tane karakter (glyph) tanımlanabilmektedir. Geniş karakter tablolarında kod numaraları bir 
    byte'tan daha fazla byte ile (örneğin 2 byte ile, 4 byte ile) oluşturulmaktadır. Geniş karakter tablolarında yazılar 
    bellekte daha fazla yer kaplıyor olsa da bu karakter tablolarında çok fazla sayıda karakterin temsili yapılabilmektedir. 
    ASCIIi, EBCDIC genel olarak 1 byte'lık karakter tablolarıdır. Belli bir süredir bir byte'lık karakter tabloları artık 
    yetersiz kalmıştır.

    Bir byte'lık karakter tablolarının yetersiz kalmasından dolayı ismine "Unicode Karakter Tablosu" denilen geniş bir 
    karakter tablosu zaman içerisinde yaygınlaşmış ve yeni programlama dillerinde neredeyse default karakter tablosu haline 
    gelmiştir. Unicode karakter tablosu özünde iki byte'lık (16 bitlik) bir karakter tablosudur. Ancak zaman içerisinde 
    yapılan eklemelerle 21 bitlik bir tablo haline gelmiştir. Bu tablo neredeyse dünyanın bütün dillerindeki karakterler
    için, pek çok işaret için glyph bulundurmaktadır. Unicode karakter tablosunun temel kısmına 2^16 = 65536 farklı karakter 
    tanımlabnmıştır.  Böylece aynı dosya içerisinde dünyanın bütün dillerine ilişkin yazılar bir arada bulunabilmektedir. 
    Unicode tablonun sürüdürümü "Unicde Consortium (www.unixode.org)" tarafından yapılmaktadır. Unicode tablo bazı 
    farklılıklarla ISO tarafından da "ISO/IEC 10646" koduyla standardize edilmiştir. 
    
    Unicode tabloda eskiden (1996'ya kadarki zaman diliminde) her glyph için 16 bitlik bir kod numarası karşılık getirilmişti. 
    Sonra tabloya yeni glyp'ler eklendi. Bugün her karakterin kod numarası 21 bitle ifade edilmektedir. Yani yukarıda biz 
    Unicode karalter tablosunun tipik olarak 2 byte'lık (16 bitlik) bir karakter tablosu olduunu söylemiş olsak da aslında 
    güncel durumda 21 bitlik bir karakter tablosudur. 

    Unicode tablonun ilk 128 karakteri standart ASCII tablosu ile aynıdır. Örneğin 'a' karakterinin ASCII tablosundaki 
    kod numarası da Unicode tablodaki kod numarası da 97'dir. Unicode tablonun [128-255] arasındaki karakterleri ASCII 
    Latin-1 kod sayfası ile aynıdır. Türkçe'ye özgü karakterlerin bazılarının (Öneğin 'ş' gibi, 'ğ' gibi) kod numaraları
    256'dan büyüktür. Aşağıda bu karakterlerin Unicode tablodaki kod numaralarını veriyoruz:

    ┌───────────────────────────────────────────────────────────────┐
    │      TÜRKÇE ÖZEL KARAKTERLERİN UNICODE CODE POINT'LERİ        │
    ├───────────────────────────────────────────────────────────────┤
    │                        BÜYÜK HARFLER                          │
    ├──────────────────────────────┬────────────────────────────────┤
    │          Karakter            │      Unicode Kod Numarası      │
    ├──────────────────────────────┼────────────────────────────────┤
    │              Ç               │   U+00C7 (ondalık: 199)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              Ğ               │   U+011E (ondalık: 286)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              İ               │   U+0130 (ondalık: 304)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              Ö               │   U+00D6 (ondalık: 214)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              Ş               │   U+015E (ondalık: 350)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              Ü               │   U+00DC (ondalık: 220)        │
    ├──────────────────────────────┴────────────────────────────────┤
    │                        KÜÇÜK HARFLER                          │
    ├──────────────────────────────┬────────────────────────────────┤
    │          Karakter            │      Unicode Kod Numarası      │
    ├──────────────────────────────┼────────────────────────────────┤
    │              ç               │   U+00E7 (ondalık: 231)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              ğ               │   U+011F (ondalık: 287)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              ı               │   U+0131 (ondalık: 305)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              i               │   U+0069 (ondalık: 105)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              ö               │   U+00F6 (ondalık: 246)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              ş               │   U+015F (ondalık: 351)        │
    ├──────────────────────────────┼────────────────────────────────┤
    │              ü               │   U+00FC (ondalık: 252)        │
    └──────────────────────────────┴────────────────────────────────┘
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Peki Unocode öncesi devirlerde Türkçe gibi dillerin karakterleri nasıl ifade ediliyordu? İşte zaman içerisinde standart 
    ASCII tablosu diğer dillerin karakterlerini (glyph'lerini) de içerecek biçimde genişletildi. Bu genişletmede ASCII 
    tablosunun [0-127] arası karakterlerine ilişkin kod numaraları sabit bırakıldı. [128-255] arasındaki kod numaralarına 
    latin dillerinin çeşitli karakterlerleri yerleştirildi. Ancak bu konuda bir standardizasyon yapılmamıştı. ASCII tablosunun 
    genişletilmiş bu halleri için sıklıkla "kod sayfası (code page)" terimi kullanılıyordu. Türkçe karakterler için zaman 
    içerisinde değişik kod sayfaları (code pages) kullanılmıştır. DOS zamanlarında IBM'in 754 kod sayfası kullanılıyordu. 
    Sonra Microsoft Türkçe karakterler için 1254 diye isimlendirdiği kod sayfasını tasarladı. Nihayet 1999 yılına gelindiğinde 
    ISO durumdan vazife çıkardı ve kod sayfalarını ISO 8859-X kod numarasıyla standardize etti. Örneğin 8859-1 kod sayfasına 
    "Latin-1" kod sayfası denilmektedir. ISO Türkçe karakterler için ISO 8859-9 kod sayfasını oluşturmuştur. Bu kod sayfası 
    bugün Türkçe karakterler için en yaygın kullanılan bir byte'lık karakter tablosu durumundadır. ISO 8859-9 ile Microsoft'un 
    1254 kod sayfaları büyük ölçüde örtüşmektedir. 

    Bugün artık bir byte'lık karakter tablolarının kullanımı iyice azalmış, Unicode tablonun kullanımı ise oldukça 
    yaygınşlaşmıştır. Dolayısıyla ASCII kod sayfalarının karmaşıklığı Unicode tablo sayesinde bertaraf edilmiş durumdadır. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Bugün Unicode tablo için UTF-32, UTF-16 ve UTF-8 denilen üç farklı karakter kodlaması (character encoding) kullanılmaktadır.
    UTF-32'de Unicode karakterlerin kod numaraları 32 bitlik bir sayı biçiminde, UTF-16'da ise 16 bitlik bir sayı biçiminde 
    kodlanmaktadır. UTF-16 kodlamasında Unicode tablodaki ilk 65536 karakterin dışındaki karakterler ya hiç kodlanmamaktadır 
    ya da iki ayrı 16 bit ile kodlanmaktadır. Bu kodlamaya "surrogate çifti" denilmektedir. Ancak bugün en yaygın kullanılan 
    Unicode kodlaması UTF-8'dir. UTF-8 multibyte bir kodlamadır. Bu kodlamada bazı karakterler 1 byte ile, bazıları 2 byte 
    ile, bazıları 3 byte ile ve bazıları da 4 byte ile kodlanmaktadır. Bugün kullandığımız text editörlerin hemen hepsi 
    default durumda metin dosyalarını Unicode UTF-8 olarak açıp save etmektedir. Unicode UTF-8 kodlamasında ASCII tablosunun 
    ilk 128 karakteri 1 byte'la kodlanmaktadır. Türkçe karakterler ise 2 byte ile kodlanmaktadır, Japonca ve Çince Kanjiler 
    3 byte ile kodlanmaktadır. Tamamen İngilizce karakterle yazılmış olan metinlerin ASCII temsili ile Unicode UTF-8 
    temsilinin aynı olduğuna dikkat ediniz. 

    UTF-16 ve UTF-32 kodlamaları hem "Little Endian" hem de "Big Endian" biçiminde yaılabilmektedir. Tabii UTF-8 için 
    böyle bir Endian'lık durumu ypktur.
    
    Artık pek çok programlama dillerinin derleyicileri ya da yorumlayıcıları kaynak kodları Unicode UTF-8 formatında 
    kabul etmektedir. Tabii bu durum o dillerin resmi dokğmanlarında belirtilmiştir.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yazılarla ilgili işlem yapan sistemlerde yazıyı oluşturan tarafla yazıyı yorumlayan tarafın aynı karakter tablosu 
    ve karakter kodlaması üzerinde anlaşmış olmaları gerekir. Örneğin bir metin Unicode UTF-8 ile oluşturulmuşsa bu metni 
    diğer taraf ASCII 8859-9 olarak yorumlarsa karakterler anlamsız biçimde görüntülenecektir. Peki bizim elimizde bir 
    metin dosyası varsa biz onda kullanılan karakter tablosunu ve karakter kodlamasını nasıl tespit edebiliriz? Maalesef bu 
    tespitin yapılabilmesinin güvenilir ve sağlam bir yolu yoktur. Bazı editörler "sezgisel yöntemlerle (heuristics)" bunu 
    tespit etmeye çalışmaktadır. Ancak yukarıda da belirttiğimiz gibi bunun sağlam bir yolu yoktur. Unicode metin dosyaları 
    için "isteğe bağlı olarak (optional)" dosyanın başında "BOM (Byte of Order) marker" denilen bir belirteci bulundurulabilmektedir. 
    Eğer dosyada bu BOM belirteci varsa dosyanın Unicode karakter tablosuyla ve karakter kodlamasıyla oluşturulduğu tespit 
    edilebilmektedir. Ancak yukarıda da belirttiğimiz gibi BOM belirteci bir dosyanın başında bulunmak zoruda değildir. 
    BOM belirteçleri şunlardır:

    ┌─────────────────────────────────────────────────────┐
    │          UNICODE BOM BELİRTEÇLERİ TABLOSU           │
    ├──────────────────────────┬──────────────────────────┤
    │       Kodlama Türü       │      BOM (Hex)           │
    ├──────────────────────────┼──────────────────────────┤
    │         UTF-8            │      EF BB BF            │
    ├──────────────────────────┼──────────────────────────┤
    │       UTF-16 BE          │      FE FF               │
    ├──────────────────────────┼──────────────────────────┤
    │       UTF-16 LE          │      FF FE               │
    ├──────────────────────────┼──────────────────────────┤
    │       UTF-32 BE          │   00 00 FE FF            │
    ├──────────────────────────┼──────────────────────────┤
    │       UTF-32 LE          │   FF FE 00 00            │
    └──────────────────────────┴──────────────────────────┘ 

    İşte text editörler genellikle önce BOM belirteçlerine bakmakta, eğer dosyada BOM belirteci yoksa default olarak 
    belirledikleri bir karakter tablosu ve kodlamasıyla dosyayı açmaktadırlar. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de karakter tablolarının programlama dillerini ilgilendiren tarafları üzerinde duralım. Yukarıda da belirttiğimiz 
    gibi artık programlama dillerinin önemli bir bölümüne ilişkin derleyiciler ve yorumlayıcılae kaynak dosyaların Unicode 
    UTF-8 olmasını beklemektedir. Örneğin C#, Java ve Python dillerinde kaynak dosyalar Unicode UTF-8 biçiminde olmalıdır. 
    C Programlama Dilinde kaynak dosyalar için belli bir karakter tablosu ya da karakter kodlaması belirlenmemiştir. Ancak 
    bu dillerin standartları alfabetik ve nümerik karakterlerin kaynak kodda 1 byte yer kaplaması gerektiğini belirtmektedir. 
    Dolayısıyla C ve C++ derleyicileri ASCII kod sayfalarına ilişkin dosyaları ve Unicode UTF-8 dosyalarını kabul edebilmektedir. 
    Tabii programcının derleyiciye komut satırı argümanlarıyla bu kodlama bilgisini vermesi gerekir. Aksi takdirde derleycici 
    kendisinin belirlediği default bir kodlamayı kullanmaktadır. Java ve C# dillerinde char türü zaten 2 byte uyzunluktadır 
    ve string'ler bunların derleyicileri tarafından Unicode UTF-16 ile tutulmaktadır. 
    
    Python'da char biçiminde bir tür yoktur. Python'un str türü 3'lü versiyonlarla birlikte Unicode karakterleri tutma 
    yeteneği kazanmıştır. Yani Python'da programcı artık tüm string'lerin karakterlerinin Unicode karakterler olduğunu 
    varsaymalıdır. Ancak CPython gerçekleştirimi aslında içsel olarak kendisi string'leri mümkün olduğunca daha az yer 
    kaplayacak biçimde tutar. Tabii programcının bu içsel ayrıntıları bilmesine gerek yoktur.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Artık ana konularımıza başlamak için bazı ön bilgileri edinmiş durumdayız. Şimdi yavaş yavaş klasik doğal dil işlemenin 
    temel konularına giriş yapacağız. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Doğal dil işleme alanının tarhisel gelimini dört döneme ayırabiliriz:

    1) İlk Dönem (1950-1990): Bu döneme "sembolik dönem" de diyebiliriz. Bu dönemde "kural tabanlı (rule based)" yaklaşımlar 
    üzerinde çalışılmıştır. Burada "kural tabanlı yaklaşımlar" demekle kuralların ortaya konması ve işlemlerin bu kurallara 
    uyularak yapılmaya çalışılması kastedilmektedir. 

    2) İstatistiksel Dönem (1990-2010): Bu dönemde veri miktarının artmasıyla ve bilgisayarlar donanımlarının gelişmesiyle 
    birlikte metin yığınlarından (corpus) olasılıkların öğrenilmesi yöntemleri geliştirilmiş ve uygulamaya sokulmuştur.

    3) Derin Öğrenme Ağları Dönemi (2014-2018): Derin öğrenmenin (Deep Learning) sahneye çıkışıyla sözükler, sayılardan 
    oluşan vektörlere (Word Embeddings) dönüştürülmüş ve bu vektörler çok katmanlı sinir ağlarına girdi olarak verilmiştir. 

    4) Dönüştürücüler (transformers) ve Büyük Dil Modelleri Dönemi (Large Language Models(2018-Günümüz): "Dikkat" (Attention) 
    mekanizmasının keşfiyle doğal dil işlemede büyük bir sıçrama yaşanmıştır. Artık modeller yalnızca sözcükleri değil, 
    tüm cümlenin bağlamını aynı anda anlayabilecek hale gelmiştir.

    Biz kursumuzda ilk dönem ve istatistiksel dönem için "klasik doğal dil işleme dönemi" de diyeceğiz.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            6. Ders 01/02/2026 - Pazar
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Bu bölümde doğal dil işleme (natural language processing - NLP) faaliyetlerinin başlangıç aşaması olan "ön işlemler 
    (preprocessing)" üzerinde duracağız. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Doğal dil işlemedeki önişlem süreci birkaç adımdan oluşmaktadır. Önce metinler elde edilir. Elde edilen metinler 
    eğer gerekiyorsa temizlenir. Veriler temizlendikten sonra "normalizasyon" işlemine sokulmaktadır. Daha sonra normalize 
    edilmiş metinler atomlarına ayrılır. Buna İngilizce "tokenization" denilmektedir. Atomlara ayırma işleminden sonra da 
    genellikle atomlar üzerinde yeniden normalizasyon işlemi yapılmaktadır. Bu ikinci normalizasyona "atom normalizasyonu
    (token normalizatin)" da denilmektedir. Nihayet normalize edilmiş atomlardan "sözcük haznesi (vocabulary)" oluşturulmaktadır. 
    Doğal dil işlemedeki bu ön işlem adımlarını şekilsel olarak şöyle gösterebiliriz: 

   ┌────────────────────────────┐
   │  Verilerin Elde Edilmesi   │
   └────────────────────────────┘
                │
                │
                ▼
   ┌────────────────────────────┐
   │    Verileri Temizlenmesi   │
   └────────────────────────────┘
                │
                │
                ▼
   ┌────────────────────────────┐
   │    Metin Normalizasyonu    │
   └────────────────────────────┘
               │
               │
               ▼
   ┌────────────────────────────┐
   │     Atomlarına Ayırma      │
   │      (Tokenization)        │
   └────────────────────────────┘
               │
               │
               ▼
   ┌────────────────────────────┐
   │ Atomlarına Ayırma Sonrası  │
   │       Normalizasyon        │
   │     (Atom Normalizasyonu)  │
   └────────────────────────────┘
               │
               │
               ▼
   ┌────────────────────────────┐
   │     Sözück Haznesinin      │
   │      OLuşturulması         │
   └────────────────────────────┘
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Makine öğrenmesinin diğer alanlarında olduğu gibi doğal işlemede de birinci aşaması verilerin elde edilmesidir. Doğal 
    dil işlemede veriler yazılar biçimindedir. Yazılar da tipik olarak şu kaynaklardan elde edilmektedir:

    - Metin (text) ve ikili (binary) dosyaların içerisinden    
    - Veritabanlarından
    - Web sitelerinden 
    - Algılayıcılardan (sensörlerden)
    - Diğer başka kaynaklardan

    Python'da metin dosyalarının içerisindeki yazıları dosyayı open fonksiyonuyla açıp read fonksiyonuyla okuyarak elde
    edebilirsiniz. Ancak open fonksiyonunda kod syafasını ya da karakter kodlamasını belirtmeyi unutmayınız. Örneğin:

    f = open('test.txt', encoding='iso-8859-9')
    s = f.read()
    print(s)
    f.close()

    Default encoding genellikle "utf-8" biçimindedir. Ancak yerel makinenin ayarlarına bağlı olarak değişebilmektedir. 
    Unicode UTF-8 kodlamasıyla çalıyorsanız bunu açıkça belirtmelisiniz:

    f = open('test.txt', encoding='utf-8')
    s = f.read()
    print(s)
    f.close()
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Doğal dil işleme üzerinde çalışmalar yapabilmek için oluşturulmuş olan pek çok hazır veri kümesi bulunmaktadır. 
    Biz de kursumuzda klasik doğal dil işleme çalışmalarında hem bu hazır veri kümelerini kullanacağız hem de "terapikulubu.com" 
    sitesindeki veri kümesinden faydalanacağız. "terapikulubu.com" sitesi C ve Sistem Programcıları Derneği tarafından
    geliştirilmiş olan psikolojik bir sosyal ağdır. 
    
    Doğal dil işlemedeki veri kümeleri kullanlan doğal dile özgüdür. Aynı zamanda izleyen paragraflarda da görüleceği gibi 
    önişlem faaliyetlerinin bazıları da dile özgü bir biçimde yapılmaktadır. Biz de kursumuzda mümkün olduğunca Türkçe 
    metinler üzerinde çalışacağız. Bunun için Türkçe metinlerden oluşan veri kümlerine gereksinim duyacağız. Türkçe metinlerden
    oluşan bazı veri kümelerini aşağıda veriyoruz:

    - Turkish National Corpus (TNC)
    - BOUN Corpus
    - Turkish Wikipedia Dump
    - Turkish Product Reviews Dataset
    - Turkish Movie Reviews
    - TTK Turkish News Dataset:
    
    Kursumuzda tüm veri kümelerinin "Src" dizinin altındaki "Data" diininin içerisinde olduğunu varsayacağız ve bu dizine 
    hep göreli yol ifadeleri ile erişeceğiz. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    "beyazperde.com" sitesindeki film yorumlarından oluşan Türkçe veri kümesi aşağıdaki bağlantıdan indirilebilir:

    https://github.com/turkish-nlp-suite/BeyazPerde-Movie-Reviews/blob/main/butun-fimler/all_movies_reviews.json

    Buradan indirdiğimiz "all_movies_reviews.json" dosyasını "Src" dizinimizin altındaki "Data" dizinine yerleştiriyoruz.
    Buradaki veri kümesi JSON formatındadır.  Bu JSON dosyasının içerisindeki bilgilerin organizasyonu şöyledir:

    Veri Kümesi Alanları:

    1. url (string)
    - Filmin Beyazperde.com'daki sayfa bağlantısı
    - Örnek: "https://www.beyazperde.com/filmler/film-288667"

    2. name (string)
    - Filmin adı
    - Örnek: "Kurak Günler", "Avatar: Suyun Yolu"

    3. genre (array/dizi)
    - Film türleri listesi
    - Örnek: ["Dram", "Gerilim"], ["Bilimkurgu", "Macera", "Fantastik", "Aksiyon"]

    4. desc (string)
    - Filmin açıklaması/konusu
    - Film senaryosunun özeti

    5. directors (string)
    - Yönetmen adı
    - Örnek: "Emin Alper", "James Cameron"

    6. actors (string)
    - Başrol oyuncuları (virgülle ayrılmış)
    - Örnek: "Selahattin Paşalı, Ekin Koç, Hatice Aslan, Selin Yeninci"

    7. creators (string)
    - Yapımcı/Senarist
    - Örnek: "Emin Alper"

    8. musicBy (string)
    - Film müziği bestecisi
    - Örnek: "Stefan Will"

    9. rating (nesne/object)
    - totalRating (string): Genel ortalama puan - Örnek: "3,9"
    - ratingCount (string): Toplam oy sayısı - Örnek: "64"
    - reviewCount (string): Yorum sayısı - Örnek: "12"
    - bestRating (string): En yüksek verilebilecek puan - Örnek: "5"
    - worstRating (string): En düşük verilebilecek puan - Örnek: "0,5"

    10. reviews (array/dizi)
        Her yorum nesnesi şu alanları içerir:
        - rating (string): Kullanıcının verdiği puan - Örnek: "3,0", "4,0", "5,0", "0,5", "3,5"
        - review (string): Kullanıcının yazdığı yorum metni (uzun metin, spoiler etiketleri içerebilir: [spoiler][/spoiler])

    Önemli Noktalar:
    - Sayısal değerler string formatında
    - Ondalık ayırıcı olarak virgül (,) kullanılmakta
    - İç içe nesne yapısı mevcut (rating nesnesi)
    - reviews bir dizi/array yapısındadır
    
    Biz JSON formatindaki bu bilgileri tek hamlede Pandas'ın read_json fonksiyonu ile DataFrame nesnesi biçiminde elde 
    edebiliriz:

    import pandas as pd

    df = pd.read_json('../Data/all_movies_reviews.json')

    Pandas bu tür fonksiyonlarda default kodlama biçimini "utf-8" almaktadır. Ancak bunu açıkça da belirtebilirsiniz:

    df = pd.read_json('../Data/all_movies_reviews.json', encoding='utf-8')

    Yukarıda da belirttiğimiz gibi kursumuzda tüm veri kümelerini "Src" dizininin altındaki "Data" dizininde toplayacağız. 
    Her konu için "Src" dizininin altında yeni bir dizin yaratıp o dizini Python yorumlayıcısının "çalışma dizini (current 
    working directory)" haline getireceğiz. Dolayısıyla dosya erişimlerini de "göreli yol ifadesi (relative paths)" kullanarak 
    yapacağız.   
     
    JSON dosyalarını Pandas'ın DataFrame nesnesi biçiminde elde etmek yerine manuel biçimde de okuyabiliriz. Bunun için 
    Python'un standart kütüphanesindeki json modülü kullanılmaktadır. Bunun için dosya open fonksiyonuyla açılıp önce bir 
    dosya nesnesi, bu dosya nesnesi kullanılarak da json modülündeki load fonksiyonu ile bir json nesnesi elde edilir. B
    u json nesnesi aynı zamanda dolaşılabilir (iterable) bir nesnedir. Bu nesne dolaşıldıkça JSON dosyasındaki kayıtlar 
    elde edilmektedir. Örneğin:

    import json

    with open('../Data/all_movies_reviews.json', 'r', encoding='utf-8') as f:
        movies = json.load(f)
        for movie in movies:
            print(movie)    
    <BURADA KALDIM>
    Örneğin biz bu filmdeki tüm yorumları bir Python listesinde şağıdaki yapabiliriz:

    df = pd.read_json('../Data/all_movies_reviews.json', encoding='utf-8')
    all_reviews = []

    for review in df['reviews']:
        for d in review:
            all_reviews.append(d['review'])
    print(all_reviews)

    Veri kümesinde toplam 45280 adet yorum bulunmaktadır. Aynı işlemi şöyle de yapabilirdik:

    import json

    all_reviews = []
    with open('../Data/all_movies_reviews.json', 'r', encoding='utf-8') as f:
        movies = json.load(f)
        for movie in movies:
            for review in movie['reviews']:
                all_reviews.append(review['review'])    

    Biz burada tüm yorum yazılarını bir Python listesi olarak elde ettik. Tabii bunu yeniden Pandas DataFrame nesnesi
    haline getirebiliriz:

    df_reviews = pd.DataFrame({'review': all_reviews})

    Pandas'ın to_xxx isimli çeşitli formatlarda save işlemini yapan fonksiyonları ve metotları vardır. Daha önceden de 
    belirttiğimiz gibi yazılar için CSV formatı uygun bir format değildir. Daha önceden de belirttiğimiz gibi yazılar için 
    en uygun formatlar "parquet" formatı, "hdf5" formatı ve JSON formatıdır. Pandas'ın DataFrame sınıfının değişik formatlarda 
    save işlemi yapan metotlarının önemli olanları şunlardır:

    Pandas'ın DataFrame sınıfının değişik formatlarda save işlemi yapan metotları da vardır:

    to_csv() - CSV/TXT
    to_excel() - Excel
    to_json() - JSON
    to_html() - HTML
    to_xml() - XML
    to_latex() - LaTeX
    to_markdown() - Markdown
    to_pickle() - Pickle (.pkl)
    to_parquet() - Parquet
    to_feather() - Feather
    to_hdf() - HDF5
    to_stata() - Stata (.dta)
    to_sas() - SAS
    to_spss() - SPSS (.sav)

    Örneğin DataFrame nesnesini aşağıdaki gibi JSON dosyası olarak save edebiliriz:

    df_reviews.to_json('../Data/movie-reviews.json', orient='records', force_ascii=False)    

    Geri okumasını da şöyle yapabiliriz:

    df_reviews = pd.read_json('../Data/movie-reviews.json', encoding='utf-8')

    Biz buradaki "movie-reviews.json" dosyasını da "Data" dizininin içerisine yerleştirdik. Parquet ve HDF formatıyla 
    save ve read işlemleri de şöyle yapılabilir:

    df_reviews.to_parquet('../Data/movie-reviews.parquet')
    df_reviews = pd.read_parquet('../Data/movie-reviews.parquet')

    df_reviews.to_hdf('../Data/movie-reviews.hdf5', key='reviews')
    df_reviews = pd.read_hdf('../Data/movie-reviews.hdf5')
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Doğal dil işlemede yazısal verilerin tamamına İngilizce "corpus" denilmektedir. Yani "corpus" terimi doğal dil 
    işlemede faydalanılacak yazısal veri kümesini belirtmektedir. Yukardaki örnekte "beyazperde.com" sitesindem elde edilen 
    tüm yorum yazıları "corpus" oluşturmaktadır. Corpus yalnızca yazısal bilgilerden oluşmak zorunda değildir. Bunlara 
    iliştirilmiş diğer bilgiler de corpus'un bir parçası niteliğindedir. 
    
    Corpus sözcüğü eski Türkçeyle "külliyat" olarak ifade edilebilir. Biz "corpus" yerine kursumuzda kursumuzda "derlem" 
    terimini kullanacağız. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de doğal dil işlemede önişlem adımlarından biri olan metin normalizasyonu üzerinde duracağız. Metin normalizasyonu, 
    henüz bir işleme sokulmamış olan ham metinlerin makine öğrenmesi modelleri için standart ve tutarlı bir biçime dönüştürülmesi 
    sürecidir. Normalizasyon sırasında bilgi kayıpları kaçınılmazdır. Örneğin "harika!!!!", "harika!!", "harika!" gibi 
    yazı parçalarını biz tek ! kullanarak "harika!" biçiminde normalize edebiliriz. Ancak burada duygu yoğunluğu bakımından 
    bir kayıp oluşacaktır. Normalizasyondaki kayıplar her zaman olumsuzluk yaratmak zorunda değildir. Hatta bu kayıplar 
    bazı uygulamalarda zarardan daha çok fayda bile sağlayabilmektedir. 
    
    Normalizasyon süreci aslında hedefe uygun biçimde yapılmaktadır. Hedeflenen şey neyse kayıplar ve kazançlar ona göre 
    göz önüne alınmalıdır. Biz aşağıda normalizasyonda kullanılan temel alt yöntemleri açıklayacağız. Ancak bu alt yöntemlerin 
    hepsinin bir doğal dil işleme sürecinde uygulanması gerekmemektedir. Bunların amaca göre gerektiği kadar uygulanması
    gerekir. Doğal dil işleme uzmanı “ne kaybettiğini ve ne kazandığını bilerek" işlemleri yütürmelidir.
    
    Atomlara ayırma öncesindeki metin normalizasyon işlemini çeşitli alt başlıklara ayırabiliriz:

    - Karakter Düzeyinde Normalizasyon
        - Unicode Normalizasyonu
        - Harflendirme Normalizasyonu ya da Büyük/Küçük Harf Dönüşümü (Case Normalization) 
        - ASCII Dönüşümü (Transliteration)
        - Aksanları (Diacritics) Kaldırma

    - Yapısal Normalizasyon
        - Boşluk (Whitespace) Normalizasyonu
        - Noktalama Normalizasyonu
        - Tırnak (Apostrophe) Normalizasyonu
        - Satır Sonu ve Kontrol Karakterleri
        
    - İçerik Normalizasyonu
        - URL Normalizasyonu
        - E-posta Normalizasyonu
        - Sayı Normalizasyonu
        - Emoji ve Özel Sembollerin normalizasyonu
        - Hashtag ve Mention Normalizasyonu
        - Resmi Olmayan (Informal) Yazım Genişletme (Türkçeye Özel)
        - Yinelenen Karakterleri Azaltma       

    İzleyen paragraflarda bu alt başlıkları tek tek ele alarak Türkçe için metin normalizasyon işelmlerini gerçekleştireceğiz. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            7. Ders 07/02/2026 - Cumartesi
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Karakter düzeyinde normalizasyon "metinleri karakter temelinde ele alarak onları standart biçime dönüştürmeyi" 
    hedeflemektedir. Karakter düzeyinde normalizasyondaki ilk aşama genellikle Unicode metinlerin normalizasytonudur. 
    Bir karakter print edildiğinde görüntüsü aynı olduğu halde aslında bu görüntü farklı Unicode karakter kombinasyonlarıyla 
    oluşturulabilmektedir. Örneğin 'ğ' karakteri tek bir Unicode karakter biçiminde ya da "'g' ve üstü tırtıllı (breve) 
    karakterden oluşan iki Unicode karakter biçiminde de temsil edilebilmektedir. Doğal dil işlemeyi yapan uygulamacı için 
    bu iki temsil aslında aynıdır. Eğer bunlar aynı biçime dönüştürülmezse algoritmalar bunlarıb farklı karakter olduğunu 
    sanacaktır. 
    
    Bir harfin okunuşunu ya da anlamını değiştirmek için onun üzerine ya da altına getirilen küçük işaretlere "aksan
    karakterleri (diacritical marks)" denilmektedir. Örneğin Ö, Ü, ğ, â, ş karakterlerinin üzerindeki ve altındaki işaretler 
    aksan karakterleridir. Unicode tabloda bazı karakterler hem tek bir karakter olarak hem de aksanlı biçimde iki karakter 
    olarak temsil edilebilmektedir. 

    Unicode normalizasyonu dört biçimde yapılabilmektedir: NFC, NFD, NFKC ve NFKD.

    ┌──────┬─────────────────────────────┬───────────────────────────────────────┐
    │ Form │       Açıklama              │        Ne Zaman Kullanılır?           │
    ├──────┼─────────────────────────────┼───────────────────────────────────────┤
    │ NFC  │ Normalized Form Composed    │ Genel kullanım, insan okunabilirliği  │
    ├──────┼─────────────────────────────┼───────────────────────────────────────┤
    │ NFD  │ Normalized Form Decomposed  │ Karakter analizi, aksan kaldırma      │
    ├──────┼─────────────────────────────┼───────────────────────────────────────┤
    │ NFKC │ Compatibility Composed      │ ÖNERİLEN - Varyasyonları birleştir    │
    ├──────┼─────────────────────────────┼───────────────────────────────────────┤
    │ NFKD │ Compatibility Decomposed    │ İleri analiz için                     │
    └──────┴─────────────────────────────┴───────────────────────────────────────┘
    
    Unicode normalizasyonu Python standart kütüphanesindeki unicodedata modülünde bulunan normalize isimli fonksiyonla
    yapılabilmektedir. Bu fonksiyonun parametrik yapısı şöyledir:

    unicodedata.normalize(form, unistr)

    Fonksiyonun birinci parametresi normalizasyon türünü belirtir. Bu parametre 'NFC' gibi 'NFD' gibi yazısal biçimde 
    girilmelidir. Fonksiyonun ikinci parametresi normalize edilecek yazıyı almaktadır. Fonksiyon normalize edilmiş 
    yazıyla geri dönmektedir.

    NFC normalizasyonunda aksan karakterleriyle oluşturulmuş Unicode karakterler mümkünse tek bir karakter biçimine 
    dönüştürülmektedir. Yani örneğin eğer yazıda 'g' ve üstü tırtıl (breve) karakterleri yan yana ise NFC normaliasyonu
    bunu tek bir 'ğ' karakteri haline getirmektedir. Tabii yazıda zaten tek karakter olarak 'ğ' varsa onun üzerinde bir 
    işlem yapılmamaktadır. Yani NFC normalizasyonu aksanlı karakterleri mümkünse kompakt biçime dönüştürmektedir. (Her 
    aksanlı karakterin tek karakterlik bir karşılığının olmayabileceğine dikkatinizi çekmek istiyoruz.)

    NFD normalizasyonu NFC normalizasyonunun tam ters işlemini yapmaktadır. Yani tek bir karakter biçiminde yazılmış 
    olan Unicode karakteri iki karakter biçiminde aksanlı hale getimektedir. Örneğin:

    >>> s = 'Ülkü öğretmen'

    Buradaki s yezısının UTF-16 hex karşılığı şöyledir:

    >>> s.encode('utf-16-le').hex(' ')
    'dc 00 6c 00 6b 00 fc 00 20 00 f6 00 1f 01 72 00 65 00 74 00 6d 00 65 00 6e 00'

    Şimdi bu s yazısını NFD normalizasyonuna sokalım:

    >>> text = unicodedata.normalize('NFD', s)
    >>> text
    'Ülkü öğretmen'

    Biz burada NFD normalizasyonundan elde edilen yazıya bakarak sanki değişen bir şeyin olmadığını sanabilirsiniz. Aslında 
    glyph aynı olsa da yazısı ile text yazısı aynı Unicode kod numaralarından oluşmamaktadır: 

    >>> s == text
    False

    text değişkeninin tutttuğu yazının UTF-16 hex karşılığı şöyledir:

    >>> text.encode('utf-16-le').hex(' ')
    '55 00 08 03 6c 00 6b 00 75 00 08 03 20 00 6f 00 08 03 67 00 06 03 72 00 65 00 74 00 6d 00 65 00 6e 00'
    
    Görüldüğü gibi burada 'Ü' karakteri tek bir Unicode karakter olarak değil 'U' ve üstü iki nokta karakteri biçimine 
    dönüştürülmüştür (55 00 08 03). Şimdi biz bunu yeniden NFC dönüştürmesine sokalım:

    >>> text2 = unicodedata.normalize('NFC', text)
    >>> text2.encode('UTF-16-le').hex(' ')
    'dc 00 6c 00 6b 00 fc 00 20 00 f6 00 1f 01 72 00 65 00 74 00 6d 00 65 00 6e 00'

    Burada elde edilen bytes dizisinin ilk s yazısının bytes dizisi ile aynı olduğuna dikkat ediniz. encode metodunda 
    encoding belirtilmezse default encoding "utf-8" alınmaktadır. UTF-16 ve UTF-32 için encoding belirtilirken eğer 
    Endian'lık belirtilmezse encode metodu geri döndürdüğü bytes dizisinin başına Endian'lığı belirtmek için BOM 
    belirtecini (BOM marker) yerleştirmektir. 

    Bir karakterin üzerinde birden fazla aksan karakteri olabilir. Örneğin bir karakter hem şapkalı hem de çengelli olabilir. 
    Bu durumda biz bu karakteri NFD normalizasyonuna soktuğumuzda bu karakter üç karakter biçimine dönüştürülecektr. 
    İlki asıl karakter sonraki ikisi aksan karakterleri olacaktır.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    NFKC normalizasyonu farklı glyph'lere ilişkin ama aynı anlama gelen karakterlerin aynı kod numarasıyla temsil edilmesini 
    sağlamaktadır. Bu bakımdan daha ileri bir normalizayon yapmaktadır. Örneğin "fi" yazısı Unicode karakter karakter 
    tablosunda tek bir karakter olarak bulunmaktadır. (İki karakterin birleştirilerek sanki tek bir karakter gibi tek bir 
    glyph ile temsil edilmesine İngilizce "ligature" denilmektedir. Bu sözüğü Türkçe "bitişik harf" içiminde ifade edebiliriz.) 
    İşte bir yazıda "fi" geçtiğinde ya da bitişik harfli "fi" geçtiğinde anlamsal bir farklılık oluşmamaktadır. O halde 
    bu iki karakterin aynı karakter olarak dönüştürülmesi uygun olur. Benzer biçimde örneğin matematikte de Yunanca bir fi 
    harfi vardır. Bunların da diğer fi'lerle aynı biçimde temsil edilmesi istenebilir. NFKC dönüştürmesi bu tür karakterleri 
    aynı 
    karakterlerden oluşacak biçimde dönüştürmektedir. Örneğin:

    >>> s = "office"     # burada karakterleri ayrı ayrı kullandık
    >>> len(s)
    6
    >>> k = "oﬃce"      # burada tek karakter olan ffi karakterini kullandık
    >>> len(k)
    4

    "ffi" biçiminde bir bitişik karakter vardır. Bu nedenle yukarıdaki string'ler aslında aynı anlama geldiği halde farklı 
    uzunlukta yazılar halindedir. İşte NFKC dönüştürmesi bunları aynı karakterlerden oluşacak hale getirmektedir. Örneğin:

    >>> s_n = unicodedata.normalize('NFKC', s)
    >>> k_n = unicodedata.normalize('NFKC', k)
    >>> len(s_n)
    6
    >>> len(k_n)
    6
    >>> s == k
    False
    >>> s_n == k_n
    True

    NFKC dönüştürmesinde karakterler daha yalın, yani daha temel biçime dönüştürülmektedir. Ancak daha yalın biçim daha 
    az karakterli biçim anlamına gelmemektedir. Daha yalın biçim daha temel karakterlerle ifade edilen biçimdir. Örneğin:

    >>> unicodedata.normalize('NFKC', '⓪')
    '0'

    Unicode tablodaki yuvarlaklı rakamlar NFKC dönüştürmesinde yalnızca rakama dönüştürülmektedir. 

    NFKD dönüştürmesi de NFKC dönüştürmesi gibidir. Ancak sonuçta elde edilen karakterleri aksansal olarak ayrıştırmaktadır. 
    Yani başka bir deyişle NFC ile NFD arasındaki ilişki NFKC ile NFKD arasındaki ilişkiye benzerdir. NFKD normalizasyonu 
    da yine anlamsal olarak aynı olan farklı karakterleri aynı biçime dönüştürür ancak dönüştürülmüş biçim eğer aksansal 
    karakterler içeriyorsa bunları birden çok karakter biçiminde açar. Örneğin:

    >>> s = "Ankara'da ²⁰²⁴ yılında ①⓪⓪ bin kişi yaşıyor"
    >>> text = unicodedata.normalize('NFKC', s)
    >>> text
    "Ankara'da 2024 yılında 100 bin kişi yaşıyor"
    >>> text = unicodedata.normalize('NFKD', s)
    >>> text
    "Ankara'da 2024 yılında 100 bin kişi yaşıyor"
    >>> text = unicodedata.normalize('NFKC', s)
    >>> len(text)
    43
    >>> text = unicodedata.normalize('NFKD', s)
    >>> len(text)
    45

    Türkçe için en uygun Unicode normalizasyonu çoğu durumda NFKC normalizasyonudur. Bu normalizasyonu bir fonksiyonla 
    sarmalayabiliriz:

    def unicode_normalize(text):
        return unicodedata.normalize('NFKC', text)
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Harflendirme normalizasyonu ya da büyük harf/küçük harf normalizasyonu yazıdaki alfabetik karakterlerin küçük harfe 
    ya da büyük harfe dönüştürülmesi anlamına gelmektedir. Böylece uygulamacı aynı kavramı temsil eden büyük harfli ve 
    küçük harfli yazımları aynı biçime getirir. Bu sayede karakter sayısını ve dolayısıyla da atom (token) sayısını 
    azaltmış olur. Harflendirme normalizasyonu oldukça sık biçimde kullanılmaktadır. 
    
    Harflendirme normalizasyonunda uygulamacılar genellikle alfabetik karakterleri küçük harflere dönüştürürler. Tabii 
    yazıdaki alfabetik karakterler küçük harfe dönüştürüldüğünde anlam kayıpları da oluşabilecektir. Örneğin böyle bir 
    dönüştürme sonucunda "Apple" sözücüğü ile "apple" sözcüğü arasında fark kalmaz. Oysa "Apple" sözcüğü büyük harf 
    yazılıdğı için Apple firmasını temsil ediyor olabilir. Bu tür durumlarda eğer kayıp önemli olarak değerlendiriliyorsa
    özel isimler ve kısaltmalar büyük harfli de nırakılabilir. Konuya girişte de belirttiğimiz gibi normalizasyon
    faaliyetleri aslında amaca uygun biçimde yürütülmelidir. Bazı uygulamalarda "Apple" isminin küçük harfe dönüştürülmesinde 
    bir kayıp olmayabilir. 

    Harflendirme normalizasyonu Python'daki str sınıfının lower ya da upper metotlarıyla yapılabilir. Amcak bu metotlar 
    maalesef Türkçe karakterleri dikkate almamaktadır. Örneğin:

    >>> s = 'Izgara'
    >>> s.lower()
    'izgara'

    Burada görüldüğü gibi 'I' karakterini lower metodu 'i' karakterine dönüştürmektedir. Maalsef bu durum "locale" 
    ayarlarıyla da düzeltilememektedir. Aslında aynı dırım 'İ' dönüştürmesinde de vardır:

    >>> s = 'İzmir'
    >>> k = s.lower()
    >>> k
    'i̇zmir'
    >>> k == 'izmir'
    False
    >>> len(s)
    5
    >>> len(k)
    6

    Görüldüğü gibi 'İ' karakterini küçük harfe dönüştürdüğümüzde 'i' elde etmedik. Peki ne elde ettik? Yazının UTF-6
    kodlamasına bakalım:

    >>> k.encode('utf-16-le').hex(' ')
    '69 00 07 03 7a 00 6d 00 69 00 72 00' 

    Burada 'İ' karakteri küçük harfe dönüştürüldüğünde 'i' ve üzerinde nokta olan aksan karakteri elde edilmiştir. Ancak 
    bu karakter daha sonra NFC dönüştürmesine sokulsa bile normal 'i' karakteri haline getirilememektedir. O halde lower 
    metoduyla küçük harfe dönüştürme yapılırken iki sorunlu Türkçe karakter vardır: 'İ' ve 'I'. Diğer Türkçe karakterlerin 
    küçük harfe dönüştürülmesinde bir sorun ortaya çıkmamaktadır. Örneğin:

    >>> s = 'PİJAMALI HASTA YAĞIZ ŞOFÖRE ÇABUCAK GÜVENDİ'
    >>> k = s.lower()
    >>> k
    'pi̇jamali hasta yağiz şoföre çabucak güvendi̇'

    O halde Türkçe metinlerdeki alfabetik karakterler küçük harflere aşağıdaki gibi dönüştürülebilir:

    def case_normalize(text):
        return text.replace('İ', 'i').replace('I', 'ı').lower()

    Tabii biz bir sözlük kullanarak da bu işlemi yapabilirdik:

    turkish_special_chars = {
        'Ç': 'ç',
        'Ğ': 'ğ',
        'I': 'ı',   
        'İ': 'i',   
        'Ö': 'ö',
        'Ş': 'ş',
        'Ü': 'ü'
    }

    def case_normalize_dict(text):
        result = ''
        for c in text:
            result += turkish_special_chars.get(c, c.lower())       
        return result

    Bu fonksiyonu biraz daha hızlandırmak için şöyle bir yol izleyebiliriz: Yukarıdaki sözlüğü anahtar ve değerlerini 
    elde ederek dolaşırız. Her anahtarı str sınıfının replace metoduyla değere dönüştürürüz. Örneğin:

    def case_normalize_dict(text):
        for key, value in turkish_special_chars.items():
            text = text.replace(key, value)
        return text.lower()
            
    Aslında en hızlı yöntem muhtemelen str sınıfının translate metodunu kullanmaktır. translate metodu parametre olarak 
    bir sözlük alır ve yukarıdaki işlemin aynısını yapar. CPython gerçekleştiriminin standart kütüphanesindeki built-in 
    sınıfların kodları C'de yazılmıştır. Dolayısıyla yapılan işlem aynı olsa bile translate metodu daha hızlı çalışacaktır. 
    Bu sözlükte anahtarlar ve değerler Unicode kod numaralarından oluşmaktadır. Bu sözlüğü oluşturmak için str sınıfının 
    maketrans static metodundan faydalanılmaktadır. Eğer bu biçimdeki sözlükte değerler None ise translate onları silmektedir. 
    maketrans static metodunun üçüncü parametresi silinecek karakterleri belirtmektedir. Metot ürettiği sözlükte bu 
    karalterlerin anahtarlarını None yapmaktadır. Örneğin:
    
    Örneğin:
    
    def case_normalize_dict(text):
        return text.translate(str.maketrans('ÇĞIİÖŞÜ', 'çğıiöşü')).lower()

    >>> s = 'BUGÜNXYXY HXYAVA ÇOK XXXXYGÜZEL'
    >>> s.translate(str.maketrans('ÇĞIİÖŞÜ', 'çğıiöşü', 'XY')).lower()
    'bugün hava çok güzel'
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            8. Ders 08/02/2026 - Pazar
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Seyrek de olsa bazı uygulamalarda metinlerdeki o dile özgü karakterlerin (bizim için Türkçe) onlara en yakın ASCII 
    karakterleriyle yer değiştirmesi istenebilir. Bu dönüştürmeye "ASCII dönüştürmesi" ya da daha genel olarak İngilizce 
    "transliteration" denilmektedir. Türkçe'deki özel karakterleri onlara yakın ASCII karakterlerine dönüştürme işlemini 
    yine str sınıfının translate metodu ile aşağıdaki gibi kolayca yapabiliriz:

    def asciify_turkish_normalize(text):
        return text.translate(str.maketrans('çğıöşüÇĞİÖŞÜ', 'cgiosuCGIOSU'))

    Burdaönce str sınıfının static makestrans fonksiyonu ile dönüştürme sözlüğü elde edilmiş, o sözlükle de translate 
    metodu çağrılmıştır.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Karakter düzeyinde yapılan diğer bir normalizasyon da "aksan karakterlerinin (diacritical characters)" kaldırılmaıdır. 
    Örnneği şapkalı a'daki şapkayı kaldırmak isteyebiliriz. Türkçe'nin dışındaki diğer bazı dillerde çok fazla aksan 
    karakterleri bulunabilmektedir. Bunlardan yazının arındırılması bazı uygulamalarda fayda sağlayabilmektedir. Tabii bazı 
    uygulamalarda burada oluşacak bilgi kaybının olumsuz etkileri de olabilir. 

    Unicode karakter tablosunda pek çok dilin karakterleri bir arada bulunmaktadır. unicodedata modülündeki category isimli 
    fonksiyon bir Unicode karakteri parametre olarak alıp onun kategorisini vermektedir. Bazı Unicode kategorilerini aşağıda 
    veriyoruz:

    ┌───────────────┬──────────────────────┬────────────────────────────────────────┐
    │ Kategori Kodu │   Kategori Adı       │            Açıklama                    │
    ├───────────────┼──────────────────────┼────────────────────────────────────────┤
    │ Lu            │ Uppercase Letter     │ Büyük harfler (A, B, C, Ş, Ğ)          │
    │ Ll            │ Lowercase Letter     │ Küçük harfler (a, b, c, ş, ğ)          │
    │ Nd            │ Decimal Number       │ Ondalık sayılar (0-9)                  │
    │ Zs            │ Space Separator      │ Boşluk karakterleri                    │
    │ Po            │ Other Punctuation    │ Noktalama işaretleri (. , ! ?)         │
    │ Sm            │ Math Symbol          │ Matematiksel semboller (+, =, <, >)    │
    │ Pd            │ Dash Punctuation     │ Tire işaretleri (-, –, —)              │
    │ Ps            │ Open Punctuation     │ Açılış parantezleri ( [ {              │
    │ Pe            │ Close Punctuation    │ Kapanış parantezleri ) ] }             │
    │ So            │ Other Symbol         │ Diğer semboller (©, ®, €, ♪)           │
    │ Sc            │ Currency Symbol      │ Para birimleri (₺, $, €, £)            │
    │ Mn            │ Nonspacing Mark      │ Aksan Karakteri (ünsüz )               │
    │ Cc            │ Control              │ Kontrol karakterleri (\n, \t)          │
    │ Lo            │ Other Letter         │ Diğer dil harfleri (漢, أ, א)          │
    └───────────────┴──────────────────────┴────────────────────────────────────────┘

    Kategori listesinin tamamına aşağıdaki bağlantıdan erişebilirsiniz:

    https://www.unicode.org/reports/tr44/tr44-34.html#General_Category_Values

    İşte Unicode yazı üzerinde önce NFD normalizasyonunu uygulayıp daha sonra aksan karakterlerini atabiliriz. Ancak burada 
    bir sorun vardır. Türkçeye özgü karakterlerin bazıları NFD normalizasyonuna sokulduğunda üstteki tırtılları, noktaları 
    ve alttaki çengelleri aksan karakteri olarak ayrıştırılmaktadır. Bu dönüştürmeden sonra aksan karakterleri silindiğinde 
    bu Türkçe karakterler de değişmiş olacaktır. Örneğin dönüştürmeyi aşağıdaki gibi bir fonksiyonla yapmaya çalışırsak 
    Türkçe karakterleri de kaybederiz:

    def diacritical_normalize(text):
        ntext = unicodedata.normalize('NFD', text)
        return ''.join((c for c in ntext if unicodedata.category(c) != 'Mn'))

    Fonksiyonu şöyle düzeltebiliriz:

    def diacritical_normalize(text):
        chars = []
        for c in text:
            if c in 'çğıöşüÇĞİÖŞÜ':
                chars.append(c)
            else:
                nfd = unicodedata.normalize('NFD', c)
                for d in nfd:
                    if unicodedata.category(d) != 'Mn':
                        chars.append(d)
        return ''.join(chars)

    Burada biz yazıdaki karakterleri tek tek gözden geçirip eğer karakter özel Türkçe karakterlerinden biriyse onu 
    chars listesine ekledik. Ancak karakter özel Türkçe karakterlerinden biri değilse onu NFD normalizasyonuna sokup 
    aksan karakterlrini yok ettik. Kodun bu kısmına dikkat ediniz:

    nfd = unicodedata.normalize('NFD', c)
    for d in nfd:
        if unicodedata.category(d) != 'Mn':
            chars.append(d)

    Genellikle bir karakteri NFD normalizasyonuna soktuğumuzda karakter aksanlı ise dönüştürme sonucunda iki karakter
    elde edilmektedir: Asıl karakter ve aksan karakteri. Ancak yukarıda da belirttiğimiz gibi aksan karakterleri bir 
    tane olmak zorunda değildir ve aslında asıl karakterin başta bulunması da zorunlu değildir. Bu nedenle biz yukarıda 
    NFD normalizasyonuna soktuğumuz karakterleri bir döngü ile dolaşarak aksan karakterlerini elimine etme yoluna gittik.

    Yukarıdaki fonksiyon daha hızlı çalışacak hale de getirilebilir. Örneğin her defasında in operatörü ile sıralı 
    arama yapmak yerine bunun için bir sözlük oluşturup bir çeşit "lookup table" ile hızlandırma sağlanabilir. Ayrıca 
    NFD normaklizasyonundan sonraki döngü "liste içlemiyle" ya da "üretici ifade" ile de hızlandırılabilir. Python'daki 
    kodu hızlandırmak istediğinizde mutlaka öngürünüzü doğrulamak için timeit modülü ile test ediniz. Ya da bu konuda 
    LLM'lere başvurunuz.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            9. Ders 14/02/2026 - Cumartesi
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda karakter temelinde normalizasyon işlemlerini ele aldık. Şimdi yapısal normalizasyonlar üzerinde duralım. 
    Yapısal normalizasyonlar olarak grupladığımız normalizasyonları anımsatmak istiyoruz:

    - Yapısal Normalizasyon
        - Boşluk (Whitespace) Normalizasyonu
        - Noktalama Normalizasyonu
        - Tırnak (Apostrophe) Normalizasyonu
        - Satır Sonu ve Kontrol Karakterleri

    Metinde tutarsız biçimde bulunan boşluk karakterlerinin (white space)" aynı biçime (örneğin tek boşluk biçimine) 
    dönüştürülmesi işlemine "boşluk normalizasyonu" denilmektedir. Boşluk karakterleri atomlara ayırma (tokenization) 
    sırasında da metinden çıkartılabilmektedir. Ancak uygulamacının bu işlemi özel olarak yapması daha iyi bir kontrol 
    sağlayabilmektedir. 
    
    Boşluk karakterleri boşluk duygusu oluşturan karakterlerdir. SPACE, New Line, TAB, VTAB ve CR karakterleri ASCII 
    tablosunda da bulunan tipik boşluk karakterleridir. Ancak Unicode tabloda temel ASCII tablosunda bulunmayan boşluk 
    karakterleri de vardır. Dolayısıyla böyle bir normalizasyonun ayrı bir biçimde yapılması çoğu zaman önerilmektedir. 
    Unicode tablodaki tüm boşluk karakterlerini aşağıda tablo biçiminde veriyoruz:

    ┌─────────┬──────────────────────────────────────┐
    │ Unicode │        Karakter Adı                  │
    ├─────────┼──────────────────────────────────────┤
    │ U+0009  │ Horizontal Tab (HT)                  │
    │ U+000A  │ Line Feed (LF)                       │
    │ U+000B  │ Vertical Tab (VT)                    │
    │ U+000C  │ Form Feed (FF)                       │
    │ U+000D  │ Carriage Return (CR)                 │
    │ U+0020  │ Space                                │
    │ U+00A0  │ No-Break Space                       │
    │ U+1680  │ Ogham Space Mark                     │
    │ U+2000  │ En Quad                              │
    │ U+2001  │ Em Quad                              │
    │ U+2002  │ En Space                             │
    │ U+2003  │ Em Space                             │
    │ U+2004  │ Three-Per-Em Space                   │
    │ U+2005  │ Four-Per-Em Space                    │
    │ U+2006  │ Six-Per-Em Space                     │
    │ U+2007  │ Figure Space                         │
    │ U+2008  │ Punctuation Space                    │
    │ U+2009  │ Thin Space                           │
    │ U+200A  │ Hair Space                           │
    │ U+200B  │ Zero Width Space                     │
    │ U+200C  │ Zero Width Non-Joiner                │
    │ U+200D  │ Zero Width Joiner                    │
    │ U+202F  │ Narrow No-Break Space                │
    │ U+205F  │ Medium Mathematical Space            │  
    │ U+2060  │ Word Joiner                          │
    │ U+3000  │ Ideographic Space                    │
    │ U+FEFF  │ Zero Width No-Break Space (BOM)      │
    └─────────┴──────────────────────────────────────┘

    Daha önceden de belirttiğimiz gibi regex dili standardize edilmiş bir dil değildir. Dolayısıyla regex motorları 
    arasında farklılıklar bulunmaktadır. Regex kalıplarında kullanılan karakter tablosu Python'un regex matorunda 
    default olarak Unicode tablodur. Ancak daha önceden de belirttiğimiz gibi Python'daki regex fonksiyonlarının 
    son parametrelerine    ayarlama bayrakları girilebilmektedir. Bu bayrakların listesi şöyledir:

    ┌─────────────────┬───────────────┬─────────────────────────────────────────────────┐
    │ Bayrak          │ Kısa Kullanım │                 Açıklama                        │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.IGNORECASE   │ re.I          │ Büyük/küçük harf duyarsız eşleme yapar          │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.MULTILINE    │ re.M          │ Çok satırlı mod - ^ ve $ her satırın            │
    │                 │               │ başında/sonunda çalışır                         │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.DOTALL       │ re.S          │ Nokta (.) karakterinin yeni satır               │
    │                 │               │ karakterini de eşlemesini sağlar                │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.UNICODE      │ re.U          │ Unicode karakterler için (Python 3'te           │
    │                 │               │ varsayılan)                                     │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.ASCII        │ re.A          │ ASCII modu - \w, \b, \d gibi ifadeleri          │
    │                 │               │ ASCII ile sınırlar                              │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.VERBOSE      │ re.X          │ Regex içinde yorum ve boşluklara izin verir     │
    ├─────────────────┼───────────────┼─────────────────────────────────────────────────┤
    │ re.LOCALE       │ re.L          │ Yerel ayarlara göre eşleme yapar                │
    │                 │               │ (Tavsiye edilmez, Unicode kullanın)             │
    └─────────────────┴───────────────┴─────────────────────────────────────────────────┘

    re.UNICODE bayrağı "Unicode karakter kümesini kullan" anlamına gelmektedir. Yukarıda da belirttiğimiz gibi bu bayrak 
    Python için default durumdadır. re.ASCII bayrağı "standart ASCII karakterlerininin" kullanılmasını sağlamaktadır.
    re.LOCALE ise seçilen locale dikkate alınarak belirleme yapıacağı anlamına gemektedir. 

    Ayrıca regex motorlarında bazı bayraklar ve direktifler kalıp içerisinde de (?X...) biçiminde belirtilebilmektedir. 
    Biz bu konunun ayrıntılarına girmeyeceğiz. Ancak bayrak ve direktif belirten karakterleri bir tablo halinde aşağıda 
    vermek istiyoruz:

    ┌────────────┬──────────────────────────────────────────────────────┐
    │ Önek       │ Anlamı                                               │
    ├────────────┼──────────────────────────────────────────────────────┤
    │ (?:...)    │ Yakalamayan grup (non-capturing group)               │
    │ (?=...)    │ İleriye bakış - eşleşir ama yakalamaz (lookahead)    │
    │ (?!...)    │ Olumsuz ileriye bakış (negative lookahead)           │
    │ (?<=...)   │ Geriye bakış (lookbehind)                            │
    │ (?<!...)   │ Olumsuz geriye bakış (negative lookbehind)           │
    │ (?P<ad>...)│ İsimli yakalama grubu (named group)                  │
    │ (?i)       │ Büyük/küçük harf duyarsız (ignore case)              │
    │ (?m)       │ Çok satır modu (multiline)                           │
    │ (?s)       │ Nokta her şeyi eşleştirir (dotall)                   │
    │ (?u)       │ Unicode modu                                         │
    │ (?x)       │ Verbose mod - boşluk ve yorum satırı izin verir      │
    └────────────┴──────────────────────────────────────────────────────┘

    Regex dilinde Unicode karakter kümesindeki yukarıdaki karakterlerin çoğu '\s' ile uyuşmaktadır. Ancak istisna olarak 
    aşağıdaki karakterler uyuşum sağlamamaktadır:

    ┌─────────┬──────────────────────────────────────┐
    │ Unicode │ Karakter Adı                         │
    ├─────────┼──────────────────────────────────────┤
    │ U+200B  │ Zero Width Space                     │
    │ U+200C  │ Zero Width Non-Joiner                │
    │ U+200D  │ Zero Width Joiner                    │
    │ U+2060  │ Word Joiner                          │
    │ U+FEFF  │ Zero Width No-Break Space (BOM)      │
    └─────────┴──────────────────────────────────────┘

    Buu karakterleri de boşluk normalizasyonuna dahil edebiliriz:

    def whitespace_normalize(text):
        return re.sub(r'\s+|[\u200B\u200C\u200D\u2060\uFEFF]+', ' ', text).strip()

    Burada birkaç noktaya değinmek istiyoruz. Biz yazıdaki birden fazla boşluk karakterini tek bir SPACE karakteri 
    ile değiştirdik. Ancak bu durumda yazının başında ve sonunda bir tane SPACE karakteri kalabilmektedir. Bunları 
    da strip metouyla sildik. Python'da ve pek çok dilde \uHHHH biçimindeki karakter dizilimi HHHH hex kod numarasına 
    ilişkin Unicode karakter anlamına gelmektedir. Burada HHHH ilgili Unicode karakterin UTF-16 değeridir. UTF-32 
    için \UHHHHHHHH sentaksı kullanılmaktadır. Buradaki Unicode dönüşümü yorumlayıcı tarafından ilk aşamada yapıldığı 
    için sub fonksiyonu ters bölü karakterlerini hiç görmeyecektir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Noktalama normalizasyonu peşi sıra gelen aynı noktalama işaretlerinden yalnızca bir tanesini ya da n tanesini 
    alıp diğerlerini atarak gerçekleştirilmektedir. Tabii daha önceden de belirttiğimiz gibi bu tür normalizasyonlar duygu 
    kaybına yol açabilmektedir. Örneğin "dikkat!" ile "dikkat!!!" arasında bir vurgu farkı olabilir. Bazı alanlarda ise 
    birden fazla noktalama karakterinin peşi sıra gelmesi özel bir anlam ifade edebilmektedir. Örneğin satrançta "!!" çok 
    iyi hamle için "??" çok kötü hamle için kullanılan bir gösterimdir. Tabii daha önceden de belirttiğimiz gibi normalizasyon 
    hedefe göre gerçekeleştirilen bir süreçtir. Burada ele aldığımız türm normalizasyonların buradaki gibi uygulanması 
    zorunlu değildir.

    Önce tireleme normalizasyonu üzerinde duralım. Unicode tabloda tire görüntüsüne benzer görüntüye sahip olan birden 
    fazla karakter vardır. Onların listesini de aşağıda veriyoruz:

    ┌─────────┬───────┬──────────────────────────────────────┐
    │ Unicode │ Char  │ Karakter Adı                         │
    ├─────────┼───────┼──────────────────────────────────────┤
    │ U+002D  │ -     │ Hyphen-Minus                         │
    │ U+00AD  │ ­      │ Soft Hyphen                          │
    │ U+2010  │ ‐     │ Hyphen                               │
    │ U+2011  │ ‑     │ Non-Breaking Hyphen                  │
    │ U+2012  │ ‒     │ Figure Dash                          │
    │ U+2013  │ –     │ En Dash                              │
    │ U+2014  │ —     │ Em Dash                              │
    │ U+2015  │ ―     │ Horizontal Bar                       │
    │ U+2017  │ ‗     │ Double Low Line                      │
    │ U+2E3A  │ ⸺   │ Two-Em Dash                          │
    │ U+2E3B  │ ⸻ │ Three-Em Dash                         │
    │ U+2212  │ −     │ Minus Sign                           │
    │ U+FE58  │ ﹘    │ Small Em Dash                        │
    │ U+FE63  │ ﹣    │ Small Hyphen-Minus                   │
    │ U+FF0D  │－     │ Fullwidth Hyphen-Minus               │
    └─────────┴───────┴──────────────────────────────────────┘

    O halde bu karakterleri ASCII tablosundaki '-' karakteri ile normalize edebiliriz:

    text = re.sub(r'[\u00AD\u2010\u2011\u2012\u2013\u2014\u2015\u2017\u2E3A\u2E3B\u2212\uFE58\uFE63\uFF0D]', '-', text)

    Bilindiği gibi pek çok latin dilinde peşi sıra gelen üç nokta ayrı bir noktalama işaretdir. Buna İngilizce "ellipsis" 
    denilmektedir. Ancak Unicode tabloda yan yana üç nokta için \u2026 numaralı ayrı bir karakter de bulunduurlmuştur. 
    Bu karakteri de ... nokta ile temsil edebiliriz:

    text = re.sub(r'\u2026', '...', text)

    Ayrıca Unicode tabloda klasik noktalama işaretlerine benzeyen başka noktalama işaretleri de vardır. Bunların da 
    normalize edilmesi gerekebilir:

    text = re.sub(r'[\uFF01]', '!', text)                                 # fullwidth !
    text = re.sub(r'[\uFF1F]', '?', text)                                 # fullwidth ?
    text = re.sub(r'[\uFF0E]', '.', text)                                 # fullwidth .
    text = re.sub(r'[\uFF0C]', ',', text)                                 # fullwidth ,
    text = re.sub(r'[\uFF1B]', ';', text)                                 # fullwidth ;
    text = re.sub(r'[\uFF1A]', ':', text)                                 # fullwidth :

    Buradaki Unicode karakterler klasik ASCII  noktalama karakterlerine oldukça benzemektedir. Biz yukarıdaki işlemle 
    onların hepsini standart bir biçime dönştürmüş olduk. 

    Birden fazla noktalama karakterinin peşi sıra bulunması durumunda bunların normalizasyonu için birkaç teknik 
    kullanılabilmektedir:

    1) Peşi sıra gelen aynı noktalama karakterlerinden yalznıca birini alarak normalize etmek.

    2) Peşi sıra gelen aynı noktalama karakterlerini "bir ya da çok" biçiminde normalize etmek. Bunun için "çok" durumunu 
    aynı karakterden iki taneyle temsil edilebilir. Örneğin yazıdaki bir tane ! karakteri ! olarak bırakılır. Ancak !!!!! 
    gibi birden fazla ! karakteri !! biçiminde temsil edilebilir. Ya da çok ! ayrı bir atomla da temsil edilebilir. Bunun 
    için çok seyrek kullanılan bir Unicode karakteri seçebilirsiniz. Bazen uygulamacılar bu özel atomları açısal parantezler 
    içerisine alınmış bir sözcükle de temsil edebilmektedir. Örneğin çok sayıda ! karakteri <EXCLMS> gibi bir yazıyla 
    temsil edilebilir. Atomlarına ayırma sırasında da bu yazı tek bir atom olarak ayrıştırılabiir. Birden çok aynı nokatalama 
    karakterinin iki tane ile temsil edilmesi (collapsing) ile açısal parantez içerisinde bir sözcükle tamsil edilmesinin 
    avantajları ve dezavantajları vardır. Bu avantajları ve dezavantajları bir tablo halinde listeliyoruz:

    ┌───────────────────────────────────────────────────────────────────────────────────────────┐
    │            ÇOK SAYIDA AYNI NOKTALAMA İŞARETLERİ İÇİN STRATEJİ KARŞILAŞTIRMASI             │
    ├─────────────────────────────────────────────┬─────────────────────────────────────────────┤
    │         "collapse"  ( !!!  →  !! )          │        "token"  ( !!!  →  <EXCLMS> )        │
    ├──────────────────────┬──────────────────────┼──────────────────────┬──────────────────────┤
    │        ARTILARI      │        EKSİLERİ      │       ARTILARI       │       EKSİLERİ       │
    ├──────────────────────┼──────────────────────┼──────────────────────┼──────────────────────┤
    │ Pretrained vocab     │ Yoğunluk bilgisi     │ Atomik semantik      │ Pretrained modelde   │
    │ değişmez             │ kaybolur             │ birim oluşturur      │ embedding matrisi    │
    │                      │ (!! = !!!! = !!!!!)  │                      │ genişletilmeli       │
    │ ! → !! gradyan       │                      │ Vocab boyutu         │                      │
    │ sürekliliği sağlar   │ Karışık !?!?         │ öngörülebilir        │ Yeni embedding'ler   │
    │                      │ kural yönetimi       │                      │ sıfırdan öğrenir     │
    │ Ham metin            │ karmaşıklaşır        │ Sınıflandırmada      │                      │
    │ okunabilir kalır     │                      │ net sinyal verir     │ Ham metnin           │
    │                      │ Eşik seçimi          │                      │ okunabilirliği       │
    │ BPE / WordPiece      │ keyfi kalabilir      │ Yoğunluk             │ bozulur              │
    │ ile doğal uyum       │                      │ kategorisi korunur   │                      │
    │                      │ !! ile !             │                      │ Az veriyle eğitim    │
    │ Az miktarda ince     │ sınırı               │ Sıfırdan eğitim      │ gürültü yaratır      │
    │ ayar verisi yeterli  │ belirsizleşebilir    │ için ideal           │                      │
    │                      │                      │                      │ Eşik değeri kritik   │
    │                      │                      │                      │ hiperparametre       │
    └──────────────────────┴──────────────────────┴──────────────────────┴──────────────────────┘

    Biz birden fazla aynı noktalama karakterini iki tane olacak biçimde normalize etmek isteyelim. Bu işlemi şöyle 
    yapabiliriz:

    text = re.sub(r'\.{3,}', '...', text)           # dört ve üzeri noktayı üç noktaya indir
    text = re.sub(r'!{2,}',  '!!',   text)          # tekrarlanan ünlemi iki tane yap
    text = re.sub(r'\?{2,}', '??',   text)          # tekrarlanan soru işaretini iki tane yap
    text = re.sub(r',{2,}',  ',,',   text)          # tekrarlanan virgülü iki tane yap
    text = re.sub(r';{2,}',  ';;',   text)          # tekrarlanan noktalı virgülü iki tane yap
    text = re.sub(r':{2,}',  '::',   text)          # tekrarlanan iki noktayıiki tane yap
    text = re.sub(r'-{2,}',  '--',   text)          # tekrarlanan tireyi iki tane yap
    text = re.sub(r'(\?!)+|(!\?)+',  '!?', text)    # tekrarlanan ?! için !? yerleştir

    Yazılarda noktalama işaretlerinden önce boşluk karakteri bırakılmaz. Fakat noktalama işaretlesinden sonra bir SPACE 
    boşluk bırakılır. Ancak yazıları yazanlar bu dilbilgisi kuralına uymayabilmektedir. Dolaysıyla bu biçimde de bir 
    normalizasyonun yapılması çoğu kez uygun olmaktadır. Bu işlemi şöyle yapailiriz:

    text = re.sub(r'\s+([.,!?;:\)])', r'\1', text)                 # noktalama işaretlerinden önceki boşlukları kaldır
    text = re.sub(r'([.,!?;:])(?![.,!?;:])(?=\S)', r'\1 ', text)   # noktalama işaretlerinden sonra bir boşluk yerleştir

    ilk satırda noktalama işaretlerinin solundaki boşluk karakterleri elimine edilmiştir. ikinci satırda ise noktadan 
    sonra sıfır tane ya da daha fazla boşluk karakteri tek bir boşluk karakteriyle değiştirilmiştir. Ancak bu ikinci 
    kalıpta ? ya da ! karakterinin solunda da bunlardan biri varsa bu ikisinin arasına boşluk karakteri konulması 
    engellenmşiştir. Regex'te (?=...) biçimindeki direktife "ileriye bakış (lookahead)", (?!...) biçimindeki direktife 
    ise "olumsuz ileriye bakış (negative lookahead)" denilmektedir. (?=...) direktifinde ... ile temsil ettiğimiz kalıp 
    uyuşum sağlamak zorundadır, ancak kalıba dahil edilmez ve tüketilmez. Yukarıdaki ikinci kalıptaki (?=\S) kısmına
    dikkat ediniz. Bu kısım şu anlama gelmektedir: "Burada boşluk karakterlerinin dışında bir karakter olmak zorunda
    ama bu karakter kalıpta yer almayacak ve tüketilmeyecek". Yani sonraki arama bu karakterden başlatılacaktır. (?!...) 
    direktifinde ... temsil ettiğimiz kalıp ise uyuşum sağlamamak zorundadır, ancak kalıba dahil edilmez ve tüketilmez. 
    İkinci kalıptaki (?![.,!?;:]) kısım şu anlama gelmektedir: "Burada .,!?;: karakterinden biri olmayan bir karakter 
    bulunmak zorunda ama bu karakter kalıpta yer almayacak ve tüketilmeyecek".
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                            10. Ders 15/02/2026 - Pazar
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de noktalama normalizasyonunu yapan bir fonksiyon yazalım:

    def punctuation_normalize(text):
        text = re.sub(r'[\u00AD\u2010\u2011\u2012\u2013\u2014\u2015'
                    '\u2017\u2E3A\u2E3B\u2212\uFE58\uFE63\uFF0D]', '-', text)
        text = re.sub(r'\u2026', '...', text)
        text = re.sub(r'[\uFF01]', '!', text)           # fullwidth !
        text = re.sub(r'[\uFF1F]', '?', text)           # fullwidth ?
        text = re.sub(r'[\uFF0E]', '.', text)           # fullwidth .
        text = re.sub(r'[\uFF0C]', ',', text)           # fullwidth ,
        text = re.sub(r'[\uFF1B]', ';', text)           # fullwidth ;
        text = re.sub(r'[\uFF1A]', ':', text)           # fullwidth :
        text = re.sub(r'\.{3,}', '...', text)           # dört ve üzeri noktayı üç noktaya indir
        text = re.sub(r'!{2,}',  '!!',   text)          # tekrarlanan ünlemi iki tane yap
        text = re.sub(r'\?{2,}', '??',   text)          # tekrarlanan soru işaretini iki tane yap
        text = re.sub(r',{2,}',  ',,',   text)          # tekrarlanan virgülü iki tane yap
        text = re.sub(r';{2,}',  ';;',   text)          # tekrarlanan noktalı virgülü iki tane yap
        text = re.sub(r':{2,}',  '::',   text)          # tekrarlanan iki noktayıiki tane yap
        text = re.sub(r'-{2,}',  '--',   text)          # tekrarlanan tireyi iki tane yap
        text = re.sub(r'(\?!)+|(!\?)+',  '!?', text)    # tekrarlanan ?! için !? yerleştir
        text = re.sub(r'\s+([.,!?;:\)])', r'\1', text)
        text = re.sub(r'([.,!?;:])(?![.,!?;:])(?=\S)', r'\1 ', text) 
        return text

    Fonksiyonda yukarıda belirttiğimiz işlemler peşi sıra yapılmıştır. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de "tırnak (apostrophe) normalizasyonu" üzerinde duralım. Unicode tabloda tırnak anlamına gelen farklı karakterler 
    bulunmaktadır. Bunların da aynı biçime dönüştürülmesi gerekebilmektedir. Unicode tabloda bulunan tırnak karakterlerinin 
    listesini tabloda halinde aşağıda veriyoruz:

    ┌────────┬───────┬──────────────────────────────────────┐
    │ Unicode│ Char  │ Karakter Adı                         │
    ├────────┼───────┼──────────────────────────────────────┤
    │ U+0022 │ "     │ Quotation Mark                       │
    │ U+0027 │ '     │ Apostrophe                           │
    │ U+00AB │ «     │ Left-Pointing Double Angle Quote     │
    │ U+00BB │ »     │ Right-Pointing Double Angle Quote    │
    │ U+2018 │ '     │ Left Single Quotation Mark           │
    │ U+2019 │ '     │ Right Single Quotation Mark          │
    │ U+201A │ ‚     │ Single Low-9 Quotation Mark          │
    │ U+201B │ ‛     │ Single High-Reversed-9 Quotation Mark│
    │ U+201C │ "     │ Left Double Quotation Mark           │
    │ U+201D │ "     │ Right Double Quotation Mark          │
    │ U+201E │ „     │ Double Low-9 Quotation Mark          │
    │ U+201F │ ‟     │ Double High-Reversed-9 Quotation Mark│
    │ U+2032 │ ′     │ Prime                                │
    │ U+2033 │ ″     │ Double Prime                         │
    │ U+2034 │ ‴     │ Triple Prime                         │
    │ U+2035 │ ‵     │ Reversed Prime                       │
    │ U+2036 │ ‶     │ Reversed Double Prime                │
    │ U+2037 │ ‷     │ Reversed Triple Prime                │
    │ U+2039 │ ‹     │ Single Left-Pointing Angle Quote     │
    │ U+203A │ ›     │ Single Right-Pointing Angle Quote    │
    │ U+FF02 │ ＂    │ Fullwidth Quotation Mark             │
    │ U+FF07 │ ＇    │ Fullwidth Apostrophe                 │
    └────────┴───────┴──────────────────────────────────────┘

    Burada dış çerçevenin kusurlu olmasının nedeni VSCode editöründe bazı Unicode karakterlerin diğerlerinden farklı 
    genişlikte görüntülenmesindendir. Bu karakterleri onlara uygun tek bir karakterle temsil edebiliriz:

    text = re.sub(r'[\u201C\u201D\u201E\u201F\u2033\u2036]', '"', text)     # çift tırnak varyantları
    text = re.sub(r'[\u2018\u2019\u201A\u201B\u2032\u2035]', "'", text)     # tek tırnak varyantları

    Tırnak normalizasyonunu yapan fonksiyonu aşağıdaki gibi yazabiliriz:

    def apostrophe_normalize(text):
        text = re.sub(r'[\u201C\u201D\u201E\u201F\u2033\u2036]', '"', text)     # çift tırnak varyantları
        text = re.sub(r'[\u2018\u2019\u201A\u201B\u2032\u2035]', "'", text)     # tek tırnak varyantları
        return text

    Şimdi de yukarıda yazdığımız tüm fonksiyonları manuel bir boru hattı biçiminde test edelim:

    s = """
        “Merhaba Dünya!!! Nasılsın?İyi misin???”
    """

    text = unicode_normalize(s)
    text = case_normalize(text)
    text = diacritical_normalize(text)
    text = whitespace_normalize(text)
    text = punctuation_normalize(text)
    text = apostrophe_normalize(text)

    print(text)
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki örnekte manuel boru hattı uyguladık. Yani bir fonksiyonun çıktısını diğerinin girdisine verdik. Bu 
    boru hattını nesne yönelimli hale getirebiliriz. Boru hatları genellikle sınıflarla temsil edilmektedir. Boru hattının 
    elemanları da (bunlara adım "step" de diyebiliriz) genellikle ikili demetlerden oluşmaktadır. Demet elemanlarının 
    ilki boru hattı elemanına verilen ismi, ikincisi ise çağrılabilir (callable) nesneyi belirtmektedir. 
    
    Boru hattını temsil eden NormalizePipeline sınıfını şöyle yazabiliriz:

    class NormalizerPipeline:
        def __init__(self, steps=None):
            self.steps= steps or []
        
        def add_step(self, name, f):
            self.steps.append((name, f))
            
        def pop_step(self):
            return self.steps.pop()
        
        def insert_step(self, index, name, f):
            self.steps.insert(index, (name, f))
                
        def remove_step(self, name):
            for i in range(len(self.steps)):
                if self.steps[i][0] == name:
                    return self.steps.pop(i)
            
        def __call__(self, text):
            for _, f in self.steps:
                text = f(text)
            return text

    Sınıfın __init__ metodunda boru hattı elemanları bir liste olarak verilebilmektedir. Aynı zamanda bu elemanlar daha 
    sonra add_step metoduyla da eklenebilmektedir. pop_step metodu son elemanı, remove_step metodu ise belli bir isme 
    ilişkin elemanı boru hattından çıkarmaktadır. insert_step metodu da boru hattının arasına eleman eklemektedir. Burada 
    __call__ metodunun nasıl yazıldığına dikkat ediniz:

    def __call__(self, text):
        for _, f in self.steps:
            text = f(text)
        return text

    Metot boru hattı elemanlarındaki fonksiyonları önceki elemanın çıktısını sonraki elemanın girdisine vererek tek tek 
    çağırmaktadır. Metodun son elemanın çıktısıyla geri döndürüldüğüne dikkat ediniz.

    Yukarıdaki NormalizerPipeline sınıfını aşağıdaki gibi test edebiliriz:

    text = """
        “Merhaba Dünya!!! Nasılsın?İyi misin???”
    """

    npl = NormalizerPipeline([
        ('UN', unicode_normalize), 
        ('CN', case_normalize), 
        ('DCN', diacritical_normalize), 
        ('WSN', whitespace_normalize), 
        ('PN', punctuation_normalize),
        ])

    npl.add_step('AN', apostrophe_normalize)
    npl.pop_step()
    npl.remove_step('CN')

    normaized_text = npl(text)
    print(normaized_text)

    Buradan şöyle bir çıktı elde edilecektir:

    “Merhaba Dünya!! Nasılsın?İyi misin??”

    Bir boru hattı nesnesi boru hattı elemanı olarak da kullanılabilir. Ne de olsa o da fonksiyon gibi "çağrılabilir 
    (callable) bir nesnedir. Örneğin:

    npl1 = NormalizerPipeline([
        ('UN', unicode_normalize), 
        ('CN', case_normalize), 
        ('DCN', diacritical_normalize), 
        ])
    
    npl2 = NormalizerPipeline([
        ('NPL1', npl1), 
        ('WSN', whitespace_normalize), 
        ('PN', punctuation_normalize),
        ('AN', apostrophe_normalize),
        ])

    text = npl2(text)
    print(text)       

    Burada birincş boru hattı nesnesi fonksiyon gibi çağrıldığında o boru hattındaki fonksiyonlar çağrılacaktır.         
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki örneklerimizde Türkçeyi temel aldık. Normalizasyonların dile de bağlı olduğunu anımsayınız. Örneğin 
    İngilizce'de "isn't" gibi "aren't" gibi kısaltmaların da normalize edilmesi egrekmektedir. Ancak Türkçe'de bu biçimde 
    yaygın bir kısaltma kalıbı yoktur. Bizim farklı diller için çok sayıda normalizasyon fonksiyonu yazdığımızı düşünün. 
    İşte bu tür durumalarda uygun bir dile ilişkin boru hattı nesnesini kolay oluşturmak için yardımcı fonksiyonlardan da 
    faydalanılmaktadır. Örneğin:

    def build_turkish_normalizer():
        return NormalizerPipeline([
            ('UN', unicode_normalize), 
            ('CN', case_normalize), 
            ('DCN', diacritical_normalize), 
            ('WSN', whitespace_normalize), 
            ('PN', punctuation_normalize),
            ('AN', apostrophe_normalize)
            ])

    Bu fonksiyonu şöyle kullanabiliriz:

    tn = build_turkish_normalizer()
    normalized_text = tn(text)
    print(normalized_text)

    Benzer biçimde örnein biz build_ascii_normalizer isimli bir yardımcı fonksiyonla ASCII normalizasyonu için boru 
    hattı nesnesi oluşturabiliriz:

    def build_ascii_normalizer():
        return NormalizerPipeline([
            ('UN', unicode_normalize), 
            ('CN', case_normalize), 
            ('DCN', diacritical_normalize), 
            ('WSN', whitespace_normalize), 
            ('PN', punctuation_normalize),
            ('AN', apostrophe_normalize),
            ('ASN', asciify_turkish_normalize),
            ])

    Aşağıda şimdiye kadar yapmoş olduğumuz nromalizasyonlara ilişkin tüm kodları bir bütün oalrak veriyoruz. 
#------------------------------------------------------------------------------------------------------------------------

import unicodedata
import re

def unicode_normalize(text):
    return unicodedata.normalize('NFKC', text)

def case_normalize(text):
    return text.replace('İ', 'i').replace('I', 'ı').lower()

def asciify_turkish_normalize(text):
    return text.translate(str.maketrans('çğıöşüÇĞİÖŞÜ', 'cgiosuCGIOSU'))
        
def diacritical_normalize(text):
    chars = []
    for c in text:
        if c in 'çğıöşüÇĞİÖŞÜ':
            chars.append(c)
        else:
            nfd = unicodedata.normalize('NFD', c)
            for d in nfd:
                if unicodedata.category(d) != 'Mn':
                    chars.append(d)
    return ''.join(chars)
        
def whitespace_normalize(text):
    return re.sub(r'\s+|[\u200B\u200C\u200D\u2060\uFEFF]+', ' ', text).strip()

def punctuation_normalize(text):
    text = re.sub(r'[\u00AD\u2010\u2011\u2012\u2013\u2014\u2015'
                  '\u2017\u2E3A\u2E3B\u2212\uFE58\uFE63\uFF0D]', '-', text)
    text = re.sub(r'\u2026', '...', text)
    text = re.sub(r'[\uFF01]', '!', text)           # fullwidth !
    text = re.sub(r'[\uFF1F]', '?', text)           # fullwidth ?
    text = re.sub(r'[\uFF0E]', '.', text)           # fullwidth .
    text = re.sub(r'[\uFF0C]', ',', text)           # fullwidth ,
    text = re.sub(r'[\uFF1B]', ';', text)           # fullwidth ;
    text = re.sub(r'[\uFF1A]', ':', text)           # fullwidth :
    text = re.sub(r'\.{3,}', '...', text)           # dört ve üzeri noktayı üç noktaya indir
    text = re.sub(r'!{2,}',  '!!',   text)          # tekrarlanan ünlemi iki tane yap
    text = re.sub(r'\?{2,}', '??',   text)          # tekrarlanan soru işaretini iki tane yap
    text = re.sub(r',{2,}',  ',,',   text)          # tekrarlanan virgülü iki tane yap
    text = re.sub(r';{2,}',  ';;',   text)          # tekrarlanan noktalı virgülü iki tane yap
    text = re.sub(r':{2,}',  '::',   text)          # tekrarlanan iki noktayıiki tane yap
    text = re.sub(r'-{2,}',  '--',   text)          # tekrarlanan tireyi iki tane yap
    text = re.sub(r'(\?!)+|(!\?)+',  '!?', text)    # tekrarlanan ?! için !? yerleştir
    text = re.sub(r'\s+([.,!?;:\)])', r'\1', text)
    text = re.sub(r'([.,!?;:])(?![.,!?;:])(?=\S)', r'\1 ', text)
    return text

def apostrophe_normalize(text):
    text = re.sub(r'[\u201C\u201D\u201E\u201F\u2033\u2036]', '"', text)     # çift tırnak varyantları
    text = re.sub(r'[\u2018\u2019\u201A\u201B\u2032\u2035]', "'", text)     # tek tırnak varyantları
    return text

class NormalizerPipeline:
    def __init__(self, steps=None):
        self.steps= steps or []
     
    def add_step(self, name, f):
        self.steps.append((name, f))
        
    def pop_step(self):
        return self.steps.pop()
    
    def insert_step(self, index, name, f):
        self.steps.insert(index, (name, f))
            
    def remove_step(self, name):
        for i in range(len(self.steps)):
            if self.steps[i][0] == name:
                return self.steps.pop(i)
           
    def __call__(self, text):
        for _, f in self.steps:
            text = f(text)
        return text
            
def build_turkish_normalizer():
    return NormalizerPipeline([
        ('UN', unicode_normalize), 
        ('CN', case_normalize), 
        ('DCN', diacritical_normalize), 
        ('WSN', whitespace_normalize), 
        ('PN', punctuation_normalize),
        ('AN', apostrophe_normalize)
        ])

def build_ascii_normalizer():
    return NormalizerPipeline([
        ('UN', unicode_normalize), 
        ('CN', case_normalize), 
        ('DCN', diacritical_normalize), 
        ('WSN', whitespace_normalize), 
        ('PN', punctuation_normalize),
        ('AN', apostrophe_normalize),
        ('ASN', asciify_turkish_normalize),
        ])
            
# test 

text = """
   “Merhaba Dünya!!! Nasılsın?İyi misin???”
"""

npl = NormalizerPipeline([
    ('UN', unicode_normalize), 
    ('CN', case_normalize), 
    ('DCN', diacritical_normalize), 
    ('WSN', whitespace_normalize), 
    ('PN', punctuation_normalize),
    ])

npl.add_step('AN', apostrophe_normalize)
npl.pop_step()
npl.remove_step('CN')

normaized_text = npl(text)
print(normaized_text)

text = """
   “Merhaba Dünya!!! Nasılsın?İyi misin???”
"""

npl1 = NormalizerPipeline([
    ('UN', unicode_normalize), 
    ('CN', case_normalize), 
    ('DCN', diacritical_normalize), 
    ])
 
npl2 = NormalizerPipeline([
    ('NPL1', npl1), 
    ('WSN', whitespace_normalize), 
    ('PN', punctuation_normalize),
    ('AN', apostrophe_normalize),
    ])

text = npl2(text)
print(text)

text = """
   “Merhaba Dünya!!! Nasılsın?İyi misin???”
"""

tn = build_turkish_normalizer()
normalized_text = tn(text)
print(normalized_text)

text = """
   “Merhaba Dünya!!! Nasılsın?İyi misin???”
"""

tn = build_ascii_normalizer()
normalized_text = tn(text)
print(normalized_text)

#------------------------------------------------------------------------------------------------------------------------
                                                11. Ders 21/02/2026 - Cumartesi
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de içerik normalizasyonu üzerinde duralım. Anımsayacağınız gibi içerik normalizasyonunu aşağıdaki gibi alt 
    gruplara ayırmıştık:

    - İçerik Normalizasyonu
        - URL Normalizasyonu
        - E-posta Normalizasyonu
        - Sayı Normalizasyonu
        - Emoji ve Özel Sembollerin normalizasyonu
        - Hashtag ve Mention Normalizasyonu
        - Resmi Olmayan (Informal) Yazım Genişletme (Türkçeye Özel)
        - Yinelenen Karakterleri Azaltma       

    Bunların üzerinde duralım.
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Metinlerin içeisinde çeşitli URL'ler bulunabilmektedir. Bazı uygulamalarda bu URL'lerin bir etkisi olmayabilir. Bu 
    durumda bu URL'ler metinden tamamen atılabilir. Bazı uygulamalarda ise URL'in içeriği değil orada bulunması önemli 
    olabilir. Bazı uygulamalarda URL'lerin yalnızca ana isimleriyle yani domain isimleriyle ilgilenilebilmektedir. Bazı 
    uygulamalarda ise URL'in tamamının muhafaza edilmesi gerekebilmektedir. İşte netin içerisindeki URL'lerin aynı biçime
    dönüştürülmesine URL nomalizasyonu denilmektedir. 

    URL'leri regex kalıplarıyla elde edebiliriz. URL elde eden regex kalıplarını "Python Uygulamaları" kursunda vermiştik. 
    Siz LLM'leri kullanarak URL kalıplarını kolaylıkla elde edebilirsiniz. Aslında URL kalıplarının çok ayrıntılı olmasına 
    gerek yoktur. Ayrıntılı bir URL kalıbı şöyle oluşturulabilir:

    pattern = r'(https?:\/\/)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'

    Örneğin:

    import re

    pattern = r'(https?:\/\/)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
    text = """
    Derneğin web sitesi www.csystem.org biçimindedir. Amazon'unki ise https://amazon.com biçimindedir.
    """
    result = re.sub(pattern, '<URL>', text)    
    print(result)

    Burada aşağıdaki çıktı elde edilecektir:

    "Derneğin web sitesi <URL> biçimindedir. Amazon'unk ise <URL> biçimindedir."

    Burada biz yazı içerisindeki URL'leri "<URL>" biçiminde yer değiştirdik.

    Bir URL'deki domain ismini bulan bir regex kalıp da şöyle olabilir:

    pattern = r'(?:https?:\/\/)?(?:www\.)?([-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6})\b(?:[-a-zA-Z0-9()@:%_\+.~#?&//=]*)'

    Metindeki tüm URL'ler yerine onların domain isimlerini yerleştiren kod parçasını da şöyle yazabiliriz:

    import re

    pattern = r'(?:https?:\/\/)?(?:www\.)?([-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6})\b(?:[-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
    text = """
    Derneğin web sitesi www.csystem.org biçimindedir. Amazonunki ise https://amazon.com biçimindedir. 
    """
    result = re.sub(pattern, r'\1', text)
    print(result)

    Burada aşağıdaki çıktı elde edilecektir:

    "Derneğin web sitesi csystem.org biçimindedir. Amazonun ise amazon.com biçimindedir." 

    URL normalizasyon işlemini normalize_url isimli bir fonksiyona yaptırabiliriz. Fonksiyonun birinci parametresi 
    normalize edilecek metni, ikinci parametresi ise normalizasyon stratejisini belirtmektedir. Bu strateji şunlardan
    biri olabilir:

    'token'
    'remove'
    'domain'
    'keep'

    def url_normalize(text, strategy='token'):
        pattern_url = r'(https?:\/\/)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
        
        if strategy == 'token':
            normalized_text = re.sub(pattern_url, '<URL>', text)    
        elif strategy == 'remove':
            normalized_text = re.sub(pattern_url, '', text)    
        elif strategy == 'domain':
            pattern_domain = r'(?:https?:\/\/)?(?:www\.)?([-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6})\b(?:[-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
            normalized_text = re.sub(pattern_domain, r'\1', text)   
        elif strategy == 'keep':
            normalized_text = text
        else:
            raise ValueError('invalid startegy')   
        return normalized_text

    Bilindiği gibi URL'ler içerisinde Türkçe karakterler kullanılamamktadır. Bu da yanlış yazılmış URL'lerin normalizasyonunda
    sorun çıkarabilmektedir. eğer URL kalıplarını basitleştirirsek bu sorunu çözebiliriz. Örneğin:

    def url_normalize(text, strategy='token'):
        pattern_url = r'https?://(www\.)?[^\s]+|www\.[^\s]+'
        
        if strategy == 'token':
            normalized_text = re.sub(pattern_url, '<URL>', text)    
        elif strategy == 'remove':
            normalized_text = re.sub(pattern_url, '', text)    
        elif strategy == 'domain':
            pattern_domain1 = r'https?://([^/]+)([^\s]+)'
            pattern_domain2 = 'www\.([^/]+)([^\s]+)'          
            normalized_text = re.sub(pattern_domain1, r'\1', text)   
            normalized_text = re.sub(pattern_domain2, r'\1', normalized_text)   
        elif strategy == 'keep':
            normalized_text = text
        else:
            raise ValueError('invalid startegy')
        
        return normalized_text

    Burada artık URL geçersiz olsa bile o URL tüketilmektedir. 

    Son olarak hangi durumlarda yukarıda uyguladığımız hangi URL normalizasyonunun kullanılması gerektiğine yönelik
    bir tablo ile ipucu verelim:

    ┌─────────────────────────────┬──────────────────────┐
    │         Görev               │     Strateji         │
    ├─────────────────────────────┼──────────────────────┤
    │ Duygu analizi               │ token veya remove    │
    ├─────────────────────────────┼──────────────────────┤
    │ Konu modelleme              │ token                │
    ├─────────────────────────────┼──────────────────────┤
    │ Named Entity Recognition    │ keep                 │
    ├─────────────────────────────┼──────────────────────┤
    │ Sosyal media analizi        │ replace              │
    ├─────────────────────────────┼──────────────────────┤
    │ Web scraping analizi        │ domain               │
    └─────────────────────────────┴──────────────────────┘   
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Yukarıda URL yaptığımız işlemlerin bir benzeri e-posta adresleri için de yapılabilir. Yine e-posta adresleri yerine 
    <EMAIL> biçiminde yeni bir atom bulundurulablir, e-posta adresleri tümden silinebilir ya da olduğu gibi tutulabilir. 
    Yine biz e-posta adreslerini bir regex kalıbı ile elde edebiliriz:

    pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'

    O halde e-posta normalizasyonu yapan fonksiyonu şöyle yazabiliriz:

    def email_normalize(text, strategy='token'):
        pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        
        if strategy == 'token':
            normalized_text = re.sub(pattern, '<EMAIL>', text)
        elif strategy == 'remove':
            normalized_text = re.sub(pattern, '', text)
        elif strategy == 'keep':
            normalized_text  = text
        else:
            raise ValueError('Invalid strategy')
            
        return normalized_text
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Diğer bir içerik normalizasyonu da sayı normalizasyonudur. Sayılar yazılar içerisinde farklı biçimlerde yazılmış 
    olabilir. Örneğin bir metinde bir sayı binler basamağından virgüllerle ayrılmış biçimde (örneğin 12,500 biçiminde) 
    yazılmışken diğer bir metinde ayrılmamış biçimde (örneğin 12500 biçiminde) yazılmış olabilir. Bazı dillerde binler 
    ayıracıyla ondalık ayıraç ters biçimde kullanılmaktadır. Örneğin Türkçe'de ve Almanca'da binler ayırcı için nokta, 
    ondalık ayıraç için virgül kullanılırken İngilizce'de durum tam terstir. Ayrıca yazılar içerisinde sayılar farklı 
    biçimlerde de karşımıza çıkabilmektedir. Örneğin "bugün üç elma yedim" gibi bir yazıdaki 3 sayısı yazıyla yazılmıştır. 
    Bu yazının "bugün 3 elma yedim" yazısıyla eşdeğer hale getirilmesi de gerekebilmektedir. Bazen sayılardaki gereksiz 
    sözcüklerin atılması da fayda sağlayabilmektedir. Örneğin "bugün üç tane elma yedim" yazısındaki tanenin aslında 
    bir etkisi yoktur. Bazı metinlerde büyük sayılar bilgisayarlarda kullanılan üstel notasyonla da (örneğin 1.2e10 gibi)
    belirtilebilmektedir. Ancak yazısal sayıları sayı biçimine dönüştürürken eşanlamlı sözcükler de yine sorunlar 
    oluşabilmektedir. Örneğin "masa altı çok pisti" cümlesinin "masa 6 çok pisti" biçiminde dönüştürülmesi istenmez. 
    Yukarıda da belirttiğimiz gibi sayı normalizasyonu sanıldığında daha ayrıntılı bir konudur. 

    Sayı normalizasyonu sanıldığı kadar kolay bir süreç değildir. Örneğin metinler arasında binler ayıracıyla ondalık 
    ayıraç konusunda tem bir tutarlılık bulunmayabilir. Matematikle haşır neşir olanlar Türkçe metinlerde de binler 
    ayıracı için virgül, ondalık ayıraç için nokta kullanabilmektedir. Türkçe metinlerde her iki ifade biçimiyle de 
    karşılaşılabilmektedir. Bu durumda bu sayıları normalize edebilmek için metnin anlamlandırılması gerekebilmektedir. 
    Anlamlandırma insanlar tarafından yaplabileceği gibi makine öğrenmesi algoritmalarıyla da otomatize biçimde 
    yapılabilmektedir. Makine öğrenmesi yöntemlerini uygulamadan kural tabanlı yaklaşımlarla da tatmin edici sonuçlar 
    elde edilebilmektedir. Örneğin aslında "masa altı çok pisti" cümlesindeki altının bir sayı olmadığı kural tabanlı 
    bir biçimde kolaylıkla anlaşılabilmektedir. 

    Sayı normalizasyonunda yüzde belirten %10 gibi ifadelerde % işareti ile sayı arasındaki boşluklar da ortadan 
    kaldırılabilir. 
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
                                                12. Ders 22/02/2026 - Pazar
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Şimdi de sayı normalizasonu yapan bir fonksiyon yazalım. Ancak bu fonksiyonda ayrıntılara fazla girmeyelim. Fonksiyonu 
    şöyle yazabiliriz:

    def normalize_turkish_number(text, strategy='token'):
        text = text_to_number(text)                         # yazısal sayıları dönüştürür
        text = re.sub(r'(\d+)\.(\d{3})', r'\1\2', text)     # binler basamağını ortadan lakdırır
        text = re.sub(r'(\d+),(\d+)', r'\1.\2', text)       # Ondalık ayırcacı olan virgülü nokta yapar
        text = re.sub(r'%\s*(\d+)', r'%\1', text)           # % işareti ile sayı arasındaki boşluk karakterlerini atar
        
        match strategy:
            case 'token':
                text = re.sub(r'\d+([.,]\d+)*', '<NUM>', text)
            case 'remove':
                text = re.sub(r'\d+([.,]\d+)*', '', text)
            case 'keep':
                pass
            case _: 
                raise ValueError('invalid strategy'        
        return text

    ilk satırdaki text_to_number fonksiyonu yazı içerisindeki yazısal sayıları sayısal biçime dönüştürmektedir:

    text = text_to_number(text)  

    Örneğin yazı içerisinde "bin üç yüz altmış üç" gibi bir ifade varsa bu fonksiyonun bunun yerine yazıya "1363"
    yazısını yerleştirmektedir. Yazısal biçimdeki sayısal ifadeleri sayısal biçime (örneğin int türden bir değere) 
    dönüştümek için klasik bir algoritma kullanılmaktadır. Biz kursumuzda bu işlemi yapan text_to_number fonksiyonunu
    ayrı bir dosyaya ("text2num.py" dosyasına) yerleştirip fonksiyonu from import deyimi ile import ettik.

    İkinci kalıp binler basamağı noktalarla ayrılmış sayılardaki noktaları ortadan kaldırmaktadır:

    text = re.sub(r'(\d+)\.(\d{3})', r'\1\2', text) 

    Üçüncü kalıp ondalık ayıracı olan virgülü nokta yapmaktadır:

    text = re.sub(r'(\d+),(\d+)', r'\1.\2', text)

    Dörüdnüc kalıp  % işareti ile sayı arasındaki boşluk karakterlerini atmaktadır:

    text = re.sub(r'%\s*(\d+)', r'%\1', text)

    match deimi de parametreye bakarak uygun stratejiyi uygulamaktadır. 

    Hangi durumlarda hangi stratejilerin uygun olabileceğini aşağıda bir tablo biçiminde veriyoruz:

    ┌──────────────────────┬─────────────────┬──────────────────────────────┐
    │       Görev          │    Strateji     │          GErekçesi           │
    ├──────────────────────┼─────────────────┼──────────────────────────────┤
    │ Duygu analizi        │ token/remove    │ Sayılar duyguyla ilgisiz     │
    ├──────────────────────┼─────────────────┼──────────────────────────────┤
    │ Finansal analizler   │ keep            │ Sayılar önemli bilgi taşır   │
    ├──────────────────────┼─────────────────┼──────────────────────────────┤
    │ Haber sınıflandırma  │ token           │ Sayılar genelde gürültü      │
    ├──────────────────────┼─────────────────┼──────────────────────────────┤
    │ Soru yanıtlamas      │ keep            │ Sayısal yanıtlar için gerekli│
    └──────────────────────┴─────────────────┴──────────────────────────────┘

    Burada kullandığımız tezt2num modülünü de aşağıda veriyoruz.
#------------------------------------------------------------------------------------------------------------------------

# text2num.py 

import re 

ones =  {
    'sıfır': 0, 'bir': 1, 'iki': 2, 'üç': 3, 'dört': 4,
    'beş': 5, 'altı': 6, 'yedi': 7, 'sekiz': 8, 'dokuz': 9
}


tens = {
    'on': 10, 'yirmi': 20, 'otuz': 30, 'kırk': 40,
    'elli': 50, 'altmış': 60, 'yetmiş': 70, 'seksen': 80, 'doksan': 90

}

scales = {
    'yüz': 100, 'bin': 1000, 'milyon': 1_000_000, 'milyar': 1_000_000_000
}

all_numbers = {**ones, **tens, **scales}

def turkish_text_to_number(text: str) -> int:
    words = text.strip().lower().split()

    def parse_chunk(word_list: list) -> int:
        result = 0
        current = 0

        for word in word_list:
            if word in ones:
                current += ones[word]
            elif word in tens:
                current += tens[word]
            elif word == 'yüz':
                current = (current if current != 0 else 1) * 100
            else:
                pass
        return result + current

    result = 0
    current_words = []

    i = 0
    while i < len(words):
        word = words[i]
        if word in ('bin', 'milyon', 'milyar'):
            scale = scales[word]
            if word == 'bin' and not current_words:
                chunk_value = 1
            else:
                chunk_value = parse_chunk(current_words)
                if chunk_value == 0:
                    chunk_value = 1
            result += chunk_value * scale
            current_words = []
        else:
            current_words.append(word)
        i += 1

    result += parse_chunk(current_words)
    return result


print(turkish_text_to_number('iki milyon üç yüz kırk beş bin altı yüz yetmiş sekiz'))  # 2345678

def text_to_number(text, threshold=2):  
    def convert_match(m):
        return str(turkish_text_to_number(m[0]))
    
    pattern = r'\b(?:' + '|'.join(all_numbers.keys()) + r')(?:\s+(?:' + '|'.join(all_numbers.keys()) + r'))' + f'{{{threshold - 1},}}' + r'\b'
    text = re.sub(pattern, convert_match, text, flags=re.IGNORECASE)
    return text

#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------
    Unicode tabloda resimsel pek çok karakter vardır. Bunlara "emoji" karakterleri denilmeketdir. ("Emoji" sözcüğü Japonca'dan
    geçmiştir ve "resimsel karakter" anlamına gelmektedir. Yani kökü "emotion" sözcüğü değildir. ASCII karalterleriyle 
    oluşturulan(örneğin :-) ya da :( gibi) duygusal tepki belirten karakter kümelerine ise İngilizce "emoticon" denilmektedir. 
    Bu sözcük "emotion" sözcüğünden türetilmiştir ve bilgisayar alanında "emoji" sözcüğünden daha önce kullanılmaya 
    başlanmıştır.) İşte Unicode tablodaki emoji karakterlerini normalize edilmesi de gerekebilmektedir. Unicode tabloda 
    sanıldığından daha fazla (3700 civarı) emoji karakteri vardır. Ancak metinlerde az sayıda emoji karakteri kullanılmaktadır. 

    Python'da emojilerle çalışmak için "emoji" isimli üçüncü parti bir kütüphane yaygın olarak kullanılmaktadır. Bu 
    kütüphaneyi şöyle yükleyebiliriz:

    pip install emoji

    emoji kütüphanesi emoji normalizasyonunu yapmayı oldukça kolaylaştırmaktadır. Kütüphanedeki emojize fonksiyonu Unicode 
    metindeki emoji karakterleri yerine :emojinin_betimsel_ifadesi: yazıları yerleştirmektedir. Bu yazılar doğrudan 
    ayrı birer atom (token) olarak kullanılabilir. emojize fonksiyonun delimters parametresi default olan ':' karakterleri 
    yerine başka karakterlerin kullanılmasına olanak sağlamaktadır. Örneğin:

    text = "🌍 Merhaba dünya! 👋 Bugün harika bir gün ☀️ ve ben çok mutluyum 😄. Sabah kahvemi ☕ " \
    "içerken penceremden baktım, kuşlar 🐦 uçuşuyordu, çiçekler 🌸 açmıştı. Bahçede bir kedi 🐱 ve" \
    " köpek 🐶 birlikte oynuyordu — inanılmaz! 😲"
        
    result = emoji.demojize(text)
    print(result)

    Burada şöyle bir çıktı elde edilmektedir:

    ':globe_showing_Europe-Africa: Merhaba dünya! :waving_hand: Bugün harika bir gün :sun: ve ben çok mutluyum 
    :grinning_face_with_smiling_eyes:. Sabah kahvemi :hot_beverage: içerken penceremden baktım, kuşlar :bird: uçuşuyordu, çiçekler 
    :cherry_blossom: açmıştı. Bahçede bir kedi :cat_face: ve köpek :dog_face: birlikte oynuyordu — inanılmaz! :astonished_face:'

    Görüldüğü gibi emoji karakterleri iki nokta üst üste arasındaki yazılara dönüştürülmüştür. Bu işlemin tersi de demojize
    fonksiyonuyla yapılmaktadır. Örneğin:

    s = emoji.demojize(result)
    
    delimiters parametresiyle ':' yerine istenilen ayıraçlar kullanılabilmektedir. Örneğin:

    result = emoji.demojize(text, delimiters='<>')

    Bir Unicode karakterin emoji karakeri olup olmadığı is_emoji fonksiyonu ile kontrol edilebilmektedir. Örneğin:

    s = []
    for c in text:
        if emoji.is_emoji(c):
            s.append('<EMOJİ>')
        else:
            s.append(c)
            
    result = ''.join(s)
    print(result)

    Burada yazı içerisindeki emoji karakterleri <EMOJI> atomuna dönüştürülmüştür. Tabii bu işlem aslında emoji kütüphanesi 
    ve regex işlemleriyle de yapılabilirdi:

    demojized_text = emoji.demojize(text, delimiters='<>')
    result = re.sub(r'<[^>]*>', '<EMOJI>', demojized_text)
    print(result)

    Aslında bunu yapan replace_emoji isminde tek bir fonksiyon da vardır. Örneğin:

    result = emoji.replace_emoji(text, '<EMOJI>')    
#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------------------------

