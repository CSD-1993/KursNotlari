#----------------------------------------------------------------------------------------------------------------------------
                                    
                                     Yapay Zeka, Makine öğrenmesi ve Veri Bilimi
                                                        Kursu
            
                                          Sınıfta Yapılan Örnekler ve Özet Notlar 
                                                       
                                                       3.Bölüm

                                                 Eğitmen: Kaan ASLAN
                                        
        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:           

        Aslan, K. (2025), "Yapay Zekâ, Makine Öğrenmesi ve Veri Bilimi Kursu", Sınıfta Yapılan Örnekler ve Özet Notlar, 
            C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları okurken editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                            Son Güncelleme: 19/10/2025 - Pazar

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            162. Ders - 19/10/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu son bölümünde bazı istatistik konuları üzerinde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Belli bir hipotezin (yani varsayımın) belli bir anlam düzeyinde (significance level) test edilmesini sağlayan istatistiksel 
    yöntemlere "hipotez testleri (hypothesis testing)" denilmektedir. Hipotez testleri başta sosyal bilimleri ve sağlık bilimleri
    olmak üzere endüstri mühendisliği, ekonomi ve işletme gibi pek çok alanda yaygın olarak kullanılmaktadır. 
    
    Örneğin yeni bir ilacın kan şekerini düşürdüğü iddia edilsin. Yen, üretilecek olan bir ilacın gerçekten kan şekerini düşürüp 
    düşürmediğini nasıl anlayabiliriz? İlk akla gelecek yöntem bu ilacı çeşitli kişilere uygulayıp sonuçları karşılaştırmaktır. 
    Bunun için bir grup denek seçeriz. Deneklerin kan şekerini ilaç uygulanmadan önce ve uygulandıktan sonra ölçebiliriz. Bu 
    ölçümler arasında anlamlı bir fark olup olmadığına bakabiliriz. Ancak ne kadarlık bir fark bizim ilacın kan şekerini 
    düşürdüğünü kabul etmemiz için yeterli olacaktır? Örneğin kişinin tokluk kan şekeri 120 olsun. Bu ilaç bir kişide kan 
    şekerini 119'a diğer bir kişide 118'e düşürmüşse ancak diğer bir kişide ise hiçbir etki yaratmamışsa biz bu ilacın kan 
    şekerini düşürdüğünü söyleyebilir miyiz? Şüphesiz bizim hatırı sayılır bir düşüşü dikkate almamız gerekir. Öte yandan ilaç 
    herkeste tamamen aynı düşüşü de     sağlamayacaktır. O zaman bir biçimde bizim ortalama düşüşü dikkate almamız gerekebilir. 
    İşte önce eski durumu ölçüp sonra işlemi uygulayıp yeni durumu ölçüp aradaki farkların ortalamasına belli bir güven düzeyi 
    içerisinde bakarak sonuç çıkartan istatistiksel hipotez testleri geliştirilmiştir. 
    
    Her hipotez testi tipik bazı deney kalıplarından elde edilen sonuçları test etmek için geliştirilmiştir. (Örneğin "kan 
    şekerini düşüren ilaç" örneğimize "ön test son test (pre-post test)" deney tasarımı denilmektedir.) Bu nedenle yalnızca 
    hipotez testlerininin nasıl uygulandığını bilmek yetmez, hangi durumlarda hangi testlerin uygulanması gerektiğini bilmek 
    de gerekir. Biz de bu bölümde çok kullanılan bazı deney kalıplarına yönelik hipotezlerin testleri üzerinde duracağız ve 
    bu testlerin Python'da nasıl yapıldığını uygulamalı olarak göreceğiz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri örneklemden hareketle ana kütle hakkında çıkarımlar yapmaktadır. Yukarıdaki örnekte biz ilacın kan şekerini 
    düşürüp düşürmediğini o ilacı belli sayıda kişilere uygulayarak test ettik. Ancak ilaç aslında herkese uygulanmalıydı. 
    Fakat bunun imkanın olmadığı da açıktır. O halde bu tür durumlarda ilacı belli sayıda kişiye uygulayıp onlardan elde edilen 
    sonuca dayalı olarak herkes hakkında genel bir hüküm verme yoluna gidilmektedir. Özetle hipotez testleri örneklemler üzerinde 
    uygulanmaktadır. Ancak test sonucunda ana kütle hakkında çıkarımlarda bulunulmaktadır. Örneğin bir havuzun değişik yerlerinden 
    damlalıkla su alıp onlardan birtakım öşçüm değerleri elde etmiş olalım. Sağlıklı bir suda kabul edilen değerleri de zaten 
    biliyor olalım. Havuzdaki suyun sağlıklı olup olmadığına havuzdaki tüm suya bakarak karar vermemekteyiz. Havuzdan belli 
    sayıda örnek alarak buna karar vermekteyiz. 

    2000'li yılların başlarında bilgisayar donanımlarının ve veri aktarım teknolojisinin gelişmesiyle birlikte "örneklemden 
    hareketle anakütle hakkında kestirimde bulunmaya" alternatif olarak "tüm anakütleyi ya da onun büyük kısmını göz önüne 
    alarak kestirimde bulunmayı" hedefleyen bir yaklaşım da ortaya atılmıştır. Büyük veri (big data) olarak isimlendirilen 
    bu yaklaşım pek çok uygulamada başarıyla kullanılmıştır.  Büyük veri yaklaşımı aynı zamanda komşu disiplinleri de 
    etkilemiş makine öğrenmesi ve veri bilimi alanının gelişmesine de katkı sağlamıştır. Ancak büyük veri yaklaşımının her 
    alanda kullanılması mümkün değildir. Bu nedenle örneklemetemelli yöntemler geçerliliğini kaybetmiş değildir. Örnekleme 
    temelli yöntemlerle büyük veri yöntemlerinin hibrit bir biçimde uygulandığı yöntemler de söz konuzu olmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipoez testleri için araştırıcı öncelikle bir hipotez oluşturur. Bu hipoteze (yani uygulamacının arzu ettiği sonuca ilişkin 
    hipoteze) istatistikte genellikle H1 hipotezi ya da "alternatif hipotez" denilmektedir. Ancak bir "değişiklik olmadığına 
    yönelik", "eski duruma yönelik", "statatükoya yönelik" hipoteze de H0 hipotezi ya da "null hipotez" denilmektedir. O halde 
    hipotez testlerinde iki hipotez vardır: H0 ve H1 hipotezleri. Bizim iddia ettiğimiz hipoteze H1 hipotezi, zaten var olan 
    duruma ilişkin hipoteze ise H0 hipotezi denilmektedir. Bu hipotezlere ilişkin birkaç örnek verelim:

    - Kan şekerini düşürdüğünü iddia ettiğimiz ilaç için ilacı uygulamadan ve ilacı uguladıktan sonra ölüçümler yapalım
    ("ön test son test" deney kalıbı) Sonra bu ölçümlerin ortalamasını hesaplayalım. Buradaki H0 ve H1 hipotezleri şöyle 
    oluşturulabilir:

    H0 Hipotezi: İlaç uygulamadan önceki ortalama kan şekeri ile ilaç uygulandıktan sonra ölçülen kan şekeri arasında 
    anlamlı (belirlenen anlam düzeyi dikkate alındığında) bir farklılık yoktur. Bu hipotez matematiksel sembollerle şöyle 
    ifade edilebilir:

    H0: μ₀ = μ₁

    H1 Hiptezi: İlaç uygulamadan önceki ortalama kan şekeri ilaç uygulandıktan sonra ölçülen kan şekerinden anlamlı (belirlenen 
    anlam düzeyin dikkate alındığında) bir biçimde daha yüksektir. Bu hipotez matematiksel sembollerle şöyle ifad edilebilir:
    
    H1: μ₀ > μ₁ 
    
    Burada μ₀ ilaç uygulanmadan önceki kan şekeri ortalamasını μ₁ ise ilaç uyguladıktan sonraki kan şekeri ortalamasını 
    belirtmektedir. 

    - Bir egzersizin uykusuzluğu azalttığına yönelik bir hipotez söz konusu olsun. Burada yine rastgele kişiler seçerek 
    onların önceki uyku sürelerini ve egzersizden sonraki uyku sürelerini ölçebiliriz. Bunların ortalamalarını hipotez 
    testi ile karşılaştrmak isteyebiliriz. Ho ve H1 hipotezleri yine şöyle oluşturulabilir:

    H0: μ₀ = μ₁
    H1: μ₀ < μ₁ 

    Nurada yine μ₀ egzersiz uygulanmadan önceki ortalamayı μ₁ ise egzersiz uygulandıktan sonraki ortalamayı belirtmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri genellikle iki gruba ayrılarak ele alınmaktadır:

    1) Parametrik hipotez testleri
    2) Parametrik olmayan hipotez testleri

    Parametrik hipotez testleri oransal (ratio) ve aralık (interval) ölçeklerine ilişkin değişkenlere uygulanabilmektedir. 
    Bu testlerde ortalama, standart sapma gibi dağılım parametreleri kullanılmaktadır ve örneklemin belli bir dağılıma (tipik 
    olarak normal dağılıma) uygun olması koşulu vardır. Parametrik olmayan testler herhangi bir ölçek türündeki değişkenlere 
    uygulanabilmektedir. Bu testlerin temel özelliği, örneklemden elde edilen değerlerin belli bir dağılıma uygunluk varsayımı 
    gerektirmemesidir. Kategorik (nominal) ve sıralı (ordinal) ölçekteki veriler için parametrik testler uygulanamadığından, 
    bu tür verilerde parametrik olmayan testler kullanılır. Ancak parametrik olmayan testler, dağılım varsayımlarını karşılamayan 
    oransal ve aralık ölçekli verilere de yaygın olarak uygulanmaktadır.
    
    Oransal ve aralık ölçekli değişkenlerde, eğer dağılım varsayımları (normallik, varyans homojenliği gibi) karşılanıyorsa, 
    parametrik testler tercih edilmelidir. Çünkü parametrik testler daha yüksek istatistiksel güce (power) sahiptir ve daha 
    hassas sonuçlar verir. Ancak bu varsayımlar ihlal edildiğinde, parametrik olmayan testler daha uygun ve güvenilir sonuçlar 
    sağlar.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Parametrik istatistiksel hipotez testleri büyük ölçüde "merkezi limit teoremine (central limit theorem)" dayandırılmaktadır. 
    Anımsanacağı gibi merkezi limit teoremi, özet olarak, bir ana kütleden seçilen yeterince büyük örneklem ortalamalarının 
    normal dağılıma yakınsadığı anlamına gelmektedir. Biz kursumuzda her bir hipotez testinin dayandığı istatistiksel hesaplamalar 
    üzerinde durmayacağız. Eğer konuyu matematiksel/teorik detaylarda ele almak istiyorsanız başka kaynaklara başvurabilirsiniz.
     Kursumuzda biz daha çok uygulamaya yönelik bilgiler aktaracağız.

    Tipik parametrik hipotez testleri şunlardır:

    - Tek örneklem t-testi
    - Bağımsız örneklem t-testi
    - Eşleştirilmiş (bağımlı) örneklem t-testi
    - Tek yönlü ANOVA
    - İki yönlü ANOVA
    - Tekrarlı ölçümler için ANOVA
    - Pearson korelasyon testi
    - Doğrusal regresyon analizi
    - F-testi (varyans karşılaştırması)
    - Levene testi (varyans homojenliği)
    - Welch t-testi (eşit varyans varsayımı olmadan)

    Tipik parametrik olmayan hipotez testleeri de şunlardır:

    - Mann–Whitney U testi (Wilcoxon rank-sum testi)
    - Wilcoxon signed-rank testi
    - Kruskal–Wallis H testi
    - Friedman testi
    - Spearman sıra korelasyon testi
    - Kendall Tau korelasyon testi
    - Ki-kare (χ²) bağımsızlık testi
    - Ki-kare uygunluk testi
    - Fisher’in kesin testi
    - Sign testi
    - Median testi
    - Theil–Sen kestirimi (parametrik olmayan regresyon)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sosyal bilimlerde ve sağlık bilimlerinde hipotez testleri için genellikle paket programlar kullanılmaktadır. Bunları 
    kullanmak için programlama bilmeye gerek yoktur. Bu alanda çok kullanılan paket programlar şunlardır:

    - SPSS (PSPP iismli GNU versiyonu vardır)
    - SAS
    - Minitab
    - Matlab (Octave isimli GNU versiyonu var)

    R programlama dili oldukça basit domain specific bir matematik ve istatistik dilidir. Ancak profesyonel dünyada paket 
    programlar ve R dilinden ziyada Python kullanılmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistiksel hipotez testleri için oluşturulmuş çeşitli kütüphaneler vardır. NumPy, Pandas ve scikit-learn hipotez 
    testlerini desteklemektedir. Python'da hipotez testleri için en yaygın kullanılan kütüphane statsmodels isimli 
    kütüphanedir. Ancak genel amaçlı nümerik analiz kütüpahensi olan SciPy kütüphanesi de hipotez testlerini desteklemektedir. 
    Biz SciPy kütüphanesinden zaten daha önce bahsetmiştik. statmodels kütüphanesi şöyle kurulabilir:


#----------------------------------------------------------------------------------------------------------------------------

<BURADA KALDIK>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesi uygulamaları için çeşitli kurumlar tarafından cloud temelli hizmetler sunulmaktadır. Bu cloud 
    hizmetlerinin en yaygın kullanılanları şunlardır:

    - Goodle Cloud Platform (Vertex AI)
    - Amazon Web Services (Sage Maker)
    - Microsodt Azure
    - IBM Watson

    Bu servislerin hepsinin ortak birtakım özellikleri ve amaçları vardır:

    - Bu platformlar bize CPU ve bellek sağlamaktadır. Dolayısıyla bizim makine öğrenmesi işlemleri için ayrı bir makine tahsis 
    etmemize gerek kalmaz. Pek çok modelin eğitimi günlerce sürebilmektedir. Bunun için makinenin evde tutulması uygun olamayabilir. 
    
    - Bu platformlar "ölçeklenebilir (scalable)" çözümler sunmaktadır. Yani kiralanan birimler büyütülük küçültülebilmektedir. 

    - Bu platformlar "deployment" için kullanılabilmektedir. Yani burada eğitilen modellerle ilgili işlemler Web API'leriyle
    uzaktan yapılabilmektedir. (Örneğin biz makine öğrenmesi uygulamasını buralarda konuşlandırabiliriz. predict işlemlerini 
    cep telefonumuzdaki uygulamalardan yapabiliriz. Böylece uygulamamız mobil aygıtlardan da web tabanlı olarak kullanılabilir 
    hale gelmektedir.)

    - Bu platformlar kendi içerisinde "Automated ML" araçlarını da bulundurmaktadır. Dolayısıyla aslında konunun teorisini bilmeyen 
    kişiler de bu Automated ML araçlarını kullanarak işlemlerini yapabilmektedir. 

    Yukarıdaki platformlar (IBM Watson dışındaki) aslında çok genel amaçlı platformlardır. Yani platformlarda pek çok değişik hizmet de 
    verilmektedir. Bu platformalara "yapay ze makine öğrenmesi" unsurları son 10 senedir eklenmiş durumdadır. Yani bu platformlardaki 
    yapay zeka ve makine öğrenmesi kısımşları bu platformların birer alt sistemi gibidir. Bu platformların pek çok ayrıntısı olduğunu 
    hatta bunlar için sertifikasyon sınavlarının yapıldığını belirtmek istiyoruz. 

    Tabii yukarıdaki platformlar ticari platformlardır. Yani kullanım için ücret ödenmektedir. Ücret ödemesi "kullanım miktarı ile"
    ilişkilidir. Yani ne kadar kullanılırsa o kadar ücret ödenmektedir. (Bu bakımdan modellerin eğitimini unutursanız, bu eğitimler
    bu platformun kaynaklarını kullandığı için ücretlendirilecektir. Denemeler yaparken bu tür hesaplamaları durdurduğunuzdan emin 
    olmalısınız.) Tabii bu platformlarda da birtakım işlemler bedava yapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bütün cloud sistemlerinde makine öğrenmesi işlemleri yapılırken birbirleriyle ilişkili üç etkinlik yürütülür: 
    
    Data + Model + Hesaplama 

    Üzerinde çalışacağımız veriler genellikle bu cloud sistemlerinde onların bu iş için ayrılan bir servisi yoluyla upload 
    edilir. Model manuel ya da otomatik bir biçimde oluşturulmaktadır. Cloud sistemleri kendi içerisindeki dağıtık bilgisayar
    sistemleri yoluyla model üzerinde eğitim, kestirim gibi işlemler yapmamıza olanak vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Google Cloud Platform (kısaca GCP) Amazon AWS ve Microsoft Azure platformlarının doğrudan rekabetçisi konumundadır. 
   GCP 2008’de kurulmuştur. Aslında diğer platformlarda olan servislerin tamamen benzeri GCP’de bulunmaktadır. 
   GCP’ye erişmek için bir Google hesabının açılmış olması gerekir. 

    GCP’nin ana sayfası şöyeldir:

    https://cloud.google.com/

    GCP işlemlerini yapabilmek için kontrol panele (konsol ortamına) girmek gerekir. Kontrol panel adresi de şöyledir:

    https://console.cloud.google.com

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   GCP’de tüm işlemler bir proje eşliğinde yapılmaktadır. Çünkü işletmeler değişik projeler için değişik hizmetler alabilmektedir. 
   Projenin yaratımı hemen konsole sayfasından yapılabilmektedir. Projeyi yarattıktan sonra aktif hale getirmek (select etmek) gerekir. 
   Proje aktif hale geldiğinde proje sayfasına geçilmiş olur. Tabii proje yaratmak için bizim Google'a kredi kartımızı vermiş olmamız
   gerekir. Yukarıda da belirttiğimiz gibi biz kredi kartını vermiş olsak bile Google kullanım kadar para çekmektedir. 
   Projenin "dashboard" denilen ana bir sayfası vardır. Burada projeye ilişkin pek çok özet bilgi ve hızlı erişim bağlantıları 
   bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP’nin -tıpkı diğer platformlarda olduğu gibi- “CPU + RAM” kiralaması yapan ve “Compute Engine” denilen bir servisi vardır. 
    Benzer biçimde yine veritablarını host etmek için ve birtakım dosyaları saklamak için kullanılabilecek “Cloud Storage” hizmeti 
    bulunmaktadır. 

    GCP içerisinde birtakım servislerin erişebileceği bir storage alanına gereksim duyulmaktadır. Bunun için “Cloud Storage” 
    hizmetini seçmek gerekir. Ancak Google bu noktada sınırlı bedava bir hizmet verecek olsa da kredi kartı bilgilerini istemektedir. 
    Tıpkı AWS’de olduğu gibi burada da “bucket” kavramı kullanılmıştır. Kullanıcının önce bir “bucket yaratması” gerekmektedir.
    Bucket adeta cloud alanı için bize ayrılmış bir disk ya da klasör gibi düşünülebilir. Dosyalar bucket'lerin içerisinde bulunmaktadır.   
    Bucket yaratılması sırasında yine diğerlerinde olduğu gibi bazı sorular sorulmaktadır. Örneğin verilere hangi bölgeden erişileceği, 
    verilere hangi sıklıkta erişileceği gibi. Bucket’e verilecek isim yine AWS’de olduğu gibi GCP genelinde tek (unique) olmak zorundadır. 
    Bir bucket yaratıldıktan sonra artık biz yerel makinemizdeki dosyaları bucket'e aupload edebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında GCP üzerinde işlem yapmak için çeşitli komut satırı araçları da bulundurulmuştur. Biz bu komut satırı araçlarını 
    yerel makinemize install edip işlemleri hiç Web arayüzünü kullanmadan bu araçlarla da yapabilmekteyiz. Bu araçlar bizim 
    istediğimiz komutları bir script biçiminde de çalıştırabilmektedir. Aslında bu komut satırı araçları "Cloud Shell" ismiyle
    Web tabalı olarak uzak makinede de çalıştırılabilmektedir. 

    Yerel makinemize aşağıdkai bağlantıyı kullanarak gsutil programını kurabiliriz:

    https://cloud.google.com/storage/docs/gsutil_install

    Örneğin gsutil programı ile yerel makinemizdeki "cvid.csv" dosyasını GCP'deki bucket'imiz içerisine şöyle kopyalayabiliriz:

    gsutil cp covid.csv gs:/kaanaslan-test-bucket
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP içerisindeki Auto ML aracına "Vertex AI" denilmektedir. Vertex AI aracına erişmek için GCP kontrol panelindeki ana menüyü
    kullanabilirisniz. Vertex AI'ın ana kontrol sayfasına "Dashboard" denilmektedir. Dolayısıyla bizim Dashboard'a geçmemiz gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tipik olarak Vertex-AI'da işlem yapma aşamaları şöyledir:

    1) Veri kümesi bucket içerisine upload edilir. (Bu işlem Dataset oluşturulurken de yapılabilmektedir.)
    2) Dataset oluşturulur.
    3) Eğitim işlemi yapılır
    4) Deployment ve Test işlemleri yapılır
    5) Kestirim işlemleri yapılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Vertex AI'da ilk yapılacak şey bir "Dataset" yaratmaktır. Bunun için Vertex AI sayfasındaki "Datasets" sekmesi seçilir. 
    Buradan Create düğmesine basılır. Burada Dataset için bölge seçilir. (Bu bölgenin bucket ile aynı bölgede olması gerekmez
    ancak aynı bölgede olması daa uygundur.) Dataset'e bir isim verilir. Sonra problemin türü seçilir. Bir CSV dosyasından hareketle
    kestirim yapacaksak "Tabular" sekmesinden "Regression/Classification" seçilmelidir. Daytaset yaratıldıktan sonra artık bu dataset'in
    bir CSV dosyası ilişkilendirilmesi gerekmektedir. Ancak Vertex AI backet'teki CSV dosyalarını kullanabilmektedir. Burada üç seçenek 
    bulunmaktadır:

    * Upload CSV files from your computer
    * Select CSV files from Cloud Storage
    * Select a table or view from BigQuery

    Biz yerel bilgiyasarımızdaki bir CSV dosyasını seçersek zaten bu CSV dosyası önce bucket içerisine kopyalanmaktadır. 
    Eğer zaten CSV dosyasımız bir bucket içerisindeyse doğrudan bucket içerisindeki CSV dosyasını belirtebiliriz. BigQuery
    GCP içerisindeki veritabanı biçiminde organize edilmiş olan başka bir depolama birimidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Dataset oluşturulduktan sonra artık eğitim işlemine geçilebilir. Bunun için Vertex AI içerisindeki "Traning" sekmesi kullanılmaktadır. 
    Training sayfasına geçildiğinde "Create" düğmesi ile eğitim belirlemelerinin yapıldığı bölüme geçilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Training işleminde peşi sıra birkaç aşamadan geçilmektedir. "Traingng method" aşamasında bize hangi veri kümesi için eğitim 
    yapılacağı ve problemin bir "sınıflandırma problemi mi yoksa regresyon problemi mi" olduğu sorulur. Bundan sonra "Model details" 
    aşamasına geçilir. Bu aşamada bize veri kümesindeki kestirilecek sütunun hangisi olduğu sorulmaktadır. Bu aşamada "Advanced 
    Options" düğmesine basıldığında test ve sınama verilerinin miktarları belirlenebilmektedir. Default durumda test verileri ve 
    sınama verileri veri kümesinin %10'u biçiminde alınmaktadır. "Join featurestore" aşamasından doğrudan "Continue" ile geçilebilir. 
    Bundan sonra karşımıza "Training options" aşaması gelecektir. Burada eğitimde hangi sütunların kullanılacağı bize sorulmaktadır. 
    Yine bu aşamada da "Advanced Options" seçeneği vardır. Burada bize Loss fonksiyonu sorulmatadır. Tabii bunlar default değerlerle 
    geçilebilir. En sonunda "Compute and pricing" aşamasına gelinir. Burada dikkat etmek gerekir. Çünkü Google eğitimde harcanan
    zamanı ücretlendirmektedir. Google'ın ücretlendirme yöntemi aşağıdaki bağlantıdan imncelenebilir:

    https://cloud.google.com/vertex-ai/pricing

    Burada "Budget" eğitim için maksimum ne kadar zaman ayrılacağını belirtmektedir. Klasik tabular verilerde en az zaman 1 
    saat olarak, resim sınıflandırma gibi işlemlerde en az zaman 3 olarak girilebilmektedir. 

    En sonunda "Start Training" ile eğitim başlatılır. Eğitimler uzun sürebildiği için bitiminde e-posta ile bildirm yapılmaktadır. 

    Eğitim bittikten sonra biz eğitim hakkında bilgileri Training sekmesinden ilgili eğitimin üzerine tıklayarak görebiliriz. 
    Eğer problem regresyon problemi ise modelin başarısı çeşitli metrik değerlerle gösterilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kesitirim işlemlerinin yapılabilmesi için önce modelin "deploy edilmesi ve bir endpoint oluşturulması" gerekmektedir. 
    Modelin deploy edilmesi demek cloud sistemi içerisinde dışarıdan kullanıma hazır hale getirilmesi demektir. Böylece biz 
    kestirimi uzaktan programlama yoluyla da yapabiliriz. Deployment işlemi "Prediction" sekmesinden girilerek yapılabileceği gibi 
    "Model Registry" sekmesninden de yapılabilmektedir. EndPoint yaratımı sırasında bize Endpoint için bir isim sorulmaktadır. Sonra 
    model için bir isim verilmekte ve ona bir versiyon numarası atanmaktadır. Buradaki "Minimum number of compute nodes" ne kadar yüksek 
    tutulursa erişim o kadar hızlı yapılmaktadır. Ancak node'ların sayısı doğrudna ücretlendirmeyi etkilemektedir. Dolayısıyla burada 
    en düşük sayı olan 1 değerini girebilirsiniz. Daha sonra bize modelin deploy edileceği makinenin özellikleri sorulmaktadır. 
    Burada eğitimin başka bir makinede yapıldığına ancak sonucun kestirilmesi için başka bir makinenin kullanıldığına dikkat ediniz. 
    Modelimiz deploy edildikten sonra kullanım miktarı kadar ücretlendirme yapılmaktadır. Dolaysıyla denemelerinizden sonra 
    bu deployment işlemini silebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işleminden sonra artık predict işlemi yapılabilir. Bu işlem tamamen görsel arayüzle yapılabileceği gibi 
    Web API'leriyle ya da bunları kullanan Python koduyla da yapılabilmektedir. Eğer deploy edilmiş modelde kestirim 
    işlemini programlama yoluyla yapacaksanız bunun için öncelikle aşağıdaki paketi kurmanız gerekmektedir:

    pip install google-cloud-aiplatform

    Bundan sonra aşağıdaki import işlemini yapıp modüldeki init fonksiyonunun uygun parametrelerle çağrılması gerekmeketdir:

    from google.cloud import aiplatform

    aiplatform.init(....)

    predict işlemi için Training sekmesinden Deploy & Test sekmesini kullanmak gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Birden fazla predict işlemi "batch predict" denilen sekme ile yapılmaktadır. Uygulamacı kestirim için yine bir CSV dosyası 
    oluşturur. Bu CSV dosyasına bucket'e upload eder. Sonra "Batch predict" sekmesinden bu CSV dosyasına referans ederek 
    işlemi başlatır. Sonuçlar yine bu işlem sırasında belirlenen bucket'ler içerisinde CSV dosyaları biçiminde oluşturulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Vertex AI ile resim sınıflandırma işlkemi yapalım. Resim sınıflandırma gibi bir işlem şu aşamalardan geçilerek 
    gerçekleştirilmektedir:

    1) Resimler Google cloud'ta bir bucket'e upload edilir. 
    2) Resimler bir CSV dosyası haline getirilir. Tabii burada resmin içerisindeki data'lar değil onun bucket'teki yeri 
    kullanılmaktadır. 
    3) Bu CSV dosyasından hareketle Dataset oluşturulur. 
    4) Training işlemi yapılır.
    5) Deployment ve EndPoint ataması yapılır 
    6) Kestirim işlemi görsel atayüz yoluyla ya da WEB API'leri ya da Pythonkoduyla yapılır.

    Burada Dataset oluşturulurken bizden bir CSV dosyası istenmektedir. Bu CSV dosyası aşağıdaki gibi bir formatta oluşturulmalıdır:

    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı

    Örneğin:

    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (1).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (10).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (100).jpg,boot
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Metin sınıflandırmaları da nemzer biçmde yapılabilmektedir. Burada iki seçenek söz konusudur. Metinler ayrı dosyalarda 
    bulunudurulup dosyalar bucket içerisine upload edilebilir yine resim sınıflandırma örneğinde olduğu gibi CSV dosyası 
    metinlere ilişkin dosyalardan ve onların sınıflarından oluşturulabilir. Ya da doğrudan metinlerin kendisi ve onların sınıfları da
    CSV dosyasının içerisinde bulunabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Amazon firması Cloud paltformlarına ilk giren büyük firmalardandır. Amazon'un cloud platformuna AWS (Amazon Web Services)
    denilmektedir. AWS iki yüzün üzerinde servis barındıran dev bir platformdur. Platformun pek çok ayrıntısı vardır. Bu nedenle 
    platformun öğrenilmesi ayrı bir uzmanlık alanı haline gelmiştir. Biz kurusumuzda platformun yapay zeka ve makine öğrenmesi 
    için nasıl kullanılacağı üzerinde özet bir biçimde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS ortamında makine öğrenmesi etkinlikleri işleyiş olarak aslında daha önce görmüş olduğumuz Google Cloud Platform'a 
    oldukça benzemektedir.  Google Cloud Platform'daki "Vertex AI" servisinin Amazonda'ki mantıksal karşılığı "SageMaker"
    isimli servistir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin ana kontrol paneline aşağıdaki bağlantı ile erişilebilir:
    
    console.aws.amazon.com

    Tabii AWS hizmeti almak için yine bir kayıt aşaması gerekmektedir. AWS kaydı sırasında işlemlker için bizden kredi kartı 
    bilgileri istenmektedir. Ancak AWS diğerlerinde olduğu gibi "kullanılan kadar paranın ödendiği" bir platformdur.

    AWS'nin konsol ekranına giriş yapıldığında zaten bize son kullandığmız servisleri listelemektedir. Ancak ilk kez giriş 
    yapıyorsanız menüden "SageMaker" servisini seçmelisiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker'a geçildiğinde onun da bir "dash board" denilen kontrol paneli vardır. Burada biz notebook'lar yaratıp uzaktan 
    manuel işlemler yapabiliriz. Ancak SageMaker'ın Auto ML aracına "AutoPilot" denilmektedir. SageMaker'ı görsel olarak daha
    zahmetsiz kullanabilmek için ismine "Studio" denilen Web tabanlı bir IDE geliştirilmiştir. Son yıllarda "Google'ın collab'ına"
    benzer "Studio Lab" denilen bedava bir ortam da eklenemiştir. Kullanıcılar genellikle işlemlerini bu Studio IDE'siyle yapmaktadır. 
    SageMaker içerisinde "Studio"ya geçebilmek için en az bir "kullanıcı profilinin (user profile)" yaratılmış olması gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin veri saklamak için çeşitli servisleri vardır. Makine öğrenmesiş için en önemli depolama servisi S3 denilen servistir. 
    S3 servisinde de tıpkı GCP'de olduğu gibi "bucket" adı altında bir çeşit folder'lar oluşturulmaktadır. Sonra bu bucket'lere 
    dosyalar upload edilmektedir. Amazon "veri merkezlerini (data centers)" "bölge (zone)" denilen alnlarla bölümlere ayırmıştır. 
    Server'lar bu bölgelerin içerisindeki veri merkezlerinin içerisinde bulunmaktadır. Tıpkı GCP'de olduğu her bölgede her türlü 
    servis verilmeyebilmetedir. Kullanıcılar coğrafi bakımdan kendilerine yakın bölgeri seçerlerse erişim daha hızlı olabilmektedir. 

    Bir bucket yaratmak için ona "dünya genelinde tek olan (unique)" bir isim vermek gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker Studio'da Auto-ML etkinlikleri için Autopilot denilen uygulama kullanılmaktadır. Dolayısıyla Auto-ML işlemi için 
    AutoML seçilebilir. Autopilot'ta bir Auto-ML çalışması yapmak için bir "experiment" oluşturmak gerekir. Experiment 
    oluşturabilmek için "File/New/Create AutoML Experiment" seçilebilir ya da doğrudan Auto ML (Autopilot) penceresinde de 
    "Create Autopilot Experiment" seçilebilir. Yeni bir experiment yaratılırken bize onun ismi ve CSV dostasının bucket'teki 
    yeri sorulmaktadır. Sonra Next tuşuna basılarak bazı gerekli öğeler belirlenir. Örneğin tahmin edilecek hedef sütun ve 
    kestirimde kullanılacak sütunlar bu aşamada bize sorulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eğitim işlemi bittiğinde veri dosyanın bulunduğu bucket içerisinde bir klasör yaratılır ve bu klasör içerisinde model ile 
    ilgili çeşitli dosyalar bulundurulur. Buradaki iki dosya önemlidir:

    SageMakerAutopilotDataExplorationNotebook.ipynb
    SageMakerAutopilotCandidateDefinitionNotebook.ipynb
    
    Buradaki "SageMakerAutopilotDataExplorationNotebook.ipynb" dosyası içerisinde veriler hakkında istatistiksel bşrtakım özellikler 
    raporlanır. "SageMakerAutopilotCandidateDefinitionNotebook.ipynb" dosyasının içerisinde ise Autopilot'ın bulduğu en iyi modellerin
    nasıl işleme sokulacağına ilişkin açıklamalar bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kestirim yapabilmek için EndPoint oluşturulmuş olması gerekmektedir. Tıpkı GCP'de olduğu gibi modelin çalıştırılabilmesi için 
    bir server'a deploy edilmesi egrekmektedir. Deploy işlemi sonucunda bize bir EndPoint verilir. Biz de bu EndPoint'i kullanarak 
    Web arayüzü ile ya da Python programı ile uzaktan kestirimde bulunabiliriz. 
    
    AWS'de uzaktan Python ile işlem yapabilmek için "sagemaker" ve "boto3" gibi kütüphaneler oluşturulmuştur. Kütüphaneler şöyle yüklenebilir.  

    pip install sagemaker
    pip install boto3

    sagemaker kütüphanesi Web arayüzü ile yapılanları programlama yoluyla yapabilmekt için boto3 kütüphanesi ise uzaktan
    kesitirm (prediction) gibi işlemleri yapabilmek için kullanılmaktadır. 

    sagemaker kütüphanesi ile uzaktan işlemlerin yapılması kütüphanenin dokümantasyonlarında açıklanmıştır. Aşağıdaki 
    bağlantıyı kullanarak kodlar üzerinde değişiklikler yaparak ve kodlarda ilgili yerleri doldurarak uzaktan işlemler yapabilirsiniz:

    https://sagemaker.readthedocs.io/en/stable/overview.html
    
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   boto3 kütüphanesi ile uzaktan işlemler yapılırken önce bir Session nesnesinin yaratılması gerekmektedir. Sessin nesnesi yaratılırken 
   bizim AWS kaynaklarına ulaşabilmemiz için iki parola bilgisine sahip olmamız gerekir. Birincisi “aws_access_key_id” ve 
   ikincisi de “aws_secret_access_key”. Amazon servisleri uzaktan erişimler için “public key/private key” kriptografi uygulamaktadır. 
   Bu parola bilgileri Session nesnesi yaratılırken aşağıdaki verilebilir:

    import boto3

    session = boto3.Session(aws_access_key_id=XXXXX', aws_secret_access_key='YYYYY')

    Session nesnesi yaratıldıktan sonra hangi servisin kullanılacağını belirten bir kaynak nesnesi yaratılır. Örneğin:

    s3 = session.resource('s3')

    Aslında bu kaynak nesneleri session nesnesi yaratılmdan doğrudan da yaratılabilmektedir. Ancak parolaların bu durumda 
    “~/.aws/credentials” dosyasına aşağıdaki formatta yazılması gerekir:

    [default]
    aws_access_key_id = YOUR_ACCESS_KEY
    aws_secret_access_key = YOUR_SECRET_KEY

    Burada yukarıdaki iki anahtarı elde etme işlemi sırasıyla şu adımlarla yapılmaktadır:

    1) https://console.aws.amazon.com/iam/ Adresinden IAM işlemlerine gelinir. 
    2) Users sekmesi seçilir
    3) Kullanıcı ismi seçilir
    4) "Security credentials" sekmesi seçilir. 
    5) Buradan Create Acces Key seçilir. 

    İşlemler sırasında eğer yukarıdaki anahtarlar girilmek istenmiyorsa (bu amahtarların görülmesi istenmeyebilir) yukarıda da belirttiğimiz
    gibi bu anahtarlar özel bir dosyanın içerisine yazılabilir. Oradan otomatik alınabilir. Eğer bu anahtarlar ilgili dosyanın 
    içerisine yazılmışsa Session nesnesi yaratılırken parametre bu iki anahtarı girmemize gerek kalmaz. Örneğin:

    session = boto3.Session()

    Bu dosya bu bilgiler Amazon'un komut satırından çalışan aws programıyla da girilebilmektedir. Amazon'un komut satırından çalışan aws programını aşağıdaki 
    bağlantıdan inmdirerek kurabilirsiniz:

    https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

    Bu iki anahtarı ilgili dosyaya yazmak için aws programı şöyle kullanılabilir:

    aws configure

    Biz programlama yoluyla uzaktan bu bucket işlemlerini yapabiliriz. Örneğin tüm bucket'lerin isimleri aşağıdaki gibi elde edilebilir:

    for bucket in s3.buckets.all():
        print(bucket.name)
    
    Belli bir bucket'teki dosya aşağıdaki gibi download edilebilmektedir:

    s3 = boto3.client('s3')
    s3.download_file('kaanaslan-test-bucket', 'x.txt', 'y.txt')

    Burada söz konusu bucket içerisindeki "x.txt" dosyası "y.txt" biçiminde download edilmiştir. 

    Uzaktan predict işlemi yine boto3 kütüphanesi ile yapılabilmektedir. Aşağıda buna bir ilişkin bir örnek verilmiştir:

    import boto3

    session = boto3.Session(aws_access_key_id='AKIAWMMTXFTMCYOF352A',aws_secret_access_key='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', region_name='eu-central-1')

    predict_data = '''6,148,72,35,0,33.6,0.627,50
    1,85,66,29,0,26.6,0.351,31'
    '''

    client = session.client('runtime.sagemaker')
    response = client.invoke_endpoint(EndpointName='diabetes-test', ContentType='text/csv', Accept='text/csv', Body=predict_data)

    result = response['Body'].read().decode()
    print(result)

    Burada Session sınıfının client metodu kullanılarak bir sagemaker nesnesi elde edilmiştir. Sonra bu nesne üzerinde invoke_endpoint
    metodu çağrılmıştır. Tabii arka planda aslında işlemler Web Servisleriyle yürütülmektedir. Bu boto3 kütüphanesi bu işlemleri kendi 
    içerisinde yapmaktadır. Gelen mesajdaki Body kısmının elde edilip yazdırıldığında dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Microsoft Azure 2009 yılında kurulan bir bulut sistemidir. 2014 yılında Microsoft bu Azure sistemine yapay zeka ve makine 
   öğrenmesine ilişkin servisleri eklemiştir. Microsoft Azure daha önce görmüş olduğumuz Google Cloud Platform ve Amazon AWS 
   sistemine benzetilebilir. Benzer hizmetler Azure üzerinde de mevcuttur. Azure ML de hiç kod yazmadan fare hareketleriyle 
   ve Auto MLaraçlarıyla kullanılabilmektedir. Tıpkı Google Cloud Platform ve Amazon SageMaker’da olduğu gibi bir SDK eşliğinde 
   tüm yapılan görsel işlemler programlama yoluyla da yapılabilmektedir. Microsoft Azure ML için tıpkı GCP ve AWS’de olduğu gibi 
   sertifikasyon süreçleri oluşturmuştur. Bu konuda sertifika sınavları da yapmaktadır. Yani Azure sistemi bütün olarak bakıldığında 
   çok ayrıntılara sahip bir sistemdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure kullanımı için ilk yapılacak şey bir e-posta ile Microsoft hesabı açmaktır. Sonra bu hesap kullanılarak Azure hesabı 
    açılmalıdır. Azure hesabı açılırken "bedava" ve "ödediğn kadar kullan" biçiminde seçenekler karşımıza gelmektedir. Bedava 
    kullanımın pek çok kısıtları vardır. Bu nedenle deneme hesabınızı "ödediğin kadar kullan" seçeneği ile oluşturabilirisiniz. 
    Ancak kullanmadığınız servisleri her ihtimale karşı kapatmayı unutmayınız. Azure sistemine abona olunduktan sonra bir 
    kullajıcı için "abone ismi" oluşturulmaktadır. 
        
    Azure sistemini e-posta ve parola ile girildikten sonra ana yönetim sayfası portal sayfasıdır. Portal sayfasına 
    doğrudan aşağıdaki bağlantı ile girilebilmektedir:
    
    https://portal.azure.com

    Azure'ün ana sayfasına geçtikten sonra buradan Yapay Zeka ve Makine Öğrenmesi için "Azure Machine Leraning" seçilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure Machine Learning sayfasına geçildiğinde önce bir "Workspace" seçiminin yapılması gerekmektedir. Workspace yapılacak çalışmalar 
    için bir klasör gibi bir organizasyon oluşturmaktadır. Yeni bir Workspace oluşturabilmek için "Oluştur (Create)" düğmesine basılır. 
    Ancak bir "workspace" oluştururken bizim bir "kaynak grubuna (resource group)" ihtiyacımız vardır. Bu nedenle önceden bir 
    kaynak grubu oluşturulmuş olmalıdır. Kaynak grubu oluşturabilmek için ana menüden (hamburger menüden) "Kaynak Grupları (Resource Gropus)"
    seçilir. Kaynak grubu birtakım kaynakların oluşturduğu gruptur. Workscpace de bir kaynaktır. Dolayısıyla workspace'ler 
    kaynak gruplarının (resource groups) bulunurlar. Kaynak Grupları menüsünden "Oluştur (Create)" seçilerek kaynak grubu oluşturma
    sayfasına geçilir. Yaratılacak kaynak grubuna bir isim verilir. Bütün bu isimler dünya genelinde tek olmak zorundadır. 
    Kaynak Grupları diğer cloud sistemlerind eolduğu gibi bölgelerle ilişkilendirilmiştir. Bu nedenle kaynak grubu yaratılırken 
    o kaynak grubunun bölgesi de belirtilir. 

    Workspace oluştururken bizden bazı bilgilerin girilmesi istenmektedir. Ancak bu bilgiler default biçimde de oluşturulabilmektedir. 
    Ancak bizim workspace'e bir isim vermemiz ve onun yer alacağı kaynak grubunu (resource group) belirtmemiz gerekir. Bu adımlardan 
    sonra nihayet workspace oluşturulacaktır. Tabii bir workspace oluşturduktan saonra tekrar tekrar workspace oluşturmaya genellikle 
    gerek yoktur. Farklı çalışmaları aynı workspace içerisinde saklayabiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Microsoft da tıpkı Amazon'da olduğu gibi makine öğrenmesiişlemleri için Web tabanlı bir IDE benzeri sistem oluşturmuştur. 
    Buna "Machine Learning Studio" ya da kısaca "Studio" denilmektedir. Workspace'i seçip "Studio düğmesine basarak Studi IDE'sine 
    geçebiliriz. Auto ML işlemleri için Studio'da "Automated ML" sekmesine tıklanılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Machine Learning Studio'da Auto ML işlemlerini başlatmak için "Automated ML" sayfasında "New Automated ML Job" seçilir. 
    Daha önceden de gördüğümüz gibi bu tür Cloud Platformlarında dört ana unsur vardır:

    1) Storage (Eğitim için veri kümesisi barındırmak için ve eğitim sonucunda oluşturulacak dosyaları barındırmak için)
    2) CPU (Eğitimi yapabilmek için gereken makine)
    3) Model (Çeşitli yöntemlerle veri kümesine uygun en iyi ML modeli)
    4) Deployment ya da EndPoint (Hedef modelin konuşlandırılması ve Web Servisler yoluyla uzaktan kullanılabilir hale getirilmesi)

    Microsoft Azure sisteminde de "New Automated ML Job" işleminin ilk aşamasında bizden hangi veri kümesi üzerinde ML çalışması yapılacağı 
    sorulmaktadır. Biz bu aşamada yeni bir veri kümesini Azure'ün Storage sistemine upload edebiliriz. Ya da bu upload etme işlemi 
    daha önceden oluşturulabilir. Azure sisteminde upload edilmiş veri kümelerine "data asset" denilmektedir. "New Automated ML Job"
    işleminde toplam dört aşama bulunmaktadır:

    1) Select data asset: Bu aşamada üzerinde çalışılacak veri kümesi belirtilir. Yukarıda söz ettiğimiz gibi bu veri kümesi daha 
    önceden "data asset" biçiminde oluşturulmuş olabilir ya da bu aşamada oluşturulabilir. 

    2) Configure job: Burada işlem için önemli bazı belirlemeler yapılmaktadır. Örneğin yapılacak işleme bir isim verilmektedir. 
    Veri kümesindeki tahmin edilecek hedef sütun belirtilmektedir. Eğitim için kullanılacak makinenin türü de bu aşamada 
    sorulmaktadır. Makine türü için "Compute Instance" seçilebilir. Tabii bizim daha önceden yaratmış olduğumuz bir hesapalama 
    maknesi (compute instance) bulunmuyor olabilir. Bu durumda bir hesaplama makinesinin (yani eğitimde kullanılacak makinenin)
    yaratılması gerekecektir. Tabii aslında bir hesaplama makinesini (compute instance) daha önce de yaratmış olabiliriz. Hesaplama 
    makinesini daha önceden bu işlemden bağımsız olarak yaratmak için Manage/Compute sekmesi kullanılmaktadır. 

    3) Select task and settings: Burada bize problemin türü sorulmaaktadır. Tabii Azure hedef sütundan hareketle aslında problemin
    bir sınıflandırma (lojistik regresyon) problemi mi yoksa regresyon problemi mi olduğunu belirleyebilmektedir. 

    4) Hyperparameter Configuration: Bu aşamad bize sınama yönteminin ne olacağı ve test verilerinin nasıl oluşturulacağı sorulmaktadır.

    Bu aşamalardan geçildikten sonra Auto-ML en iyi modelleri bulmak için işlemleri başlatır. İşlemler bitince yine bize bildirimde bulunulmaktadır. 

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Auto-ML aracı işini bitirdikten sonra EndPoint oluşturulmultur. Biz Studo'da EndPoints sekmesine gelerek ilgili endpoint'in 
    üzerine tıkladığımızda yukarıdaki menüde "Details", "Test", "Consume" gibi seçenekler bulunur. Burada "Test" seçildiğinde 
    GUI'den kestirim yapılabilmektedir. "Consume" kısmında Web servisleri ile predict işlemi yapan bir Python kodu bulundurulmaktadır. 
    Ancak bu kodda api_key kısmı boş bir string'tir. Buradaki API key "Consume" sekmesinden "Primary Key" kısmından alınabilir. 

    Örneğin burada Consume sekmesindeki kod aşağıdaki gibidir. Ancak bu kodlarda dikkat edilmesi gereken nokta şudur: Bu kodlarda
    hesaba erişim için gereken "api key" boş bırakılmıştır. Bizim bu API key'i alıp buraya kopyalamamız gerekir. Daha önceden de 
    belirttiğimiz gibi bu API key Consume sekmesinden Primary Key alanından elde edilebilmektedir.  Buradaki "consume kodu" şaşağıda
    verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import urllib.request
import json
import os
import ssl

def allowSelfSignedHttps(allowed):
    # bypass the server certificate verification on client side
    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):
        ssl._create_default_https_context = ssl._create_unverified_context

allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.

# Request data goes here
# The example below assumes JSON formatting which may be updated
# depending on the format your endpoint expects.
# More information can be found here:
# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script
data =  {
  "Inputs": {
    "data": [
      {
        "age": 0,
        "sex": "example_value",
        "bmi": 0.0,
        "children": 0,
        "smoker": "example_value",
        "region": "example_value"
      }
    ]
  },
  "GlobalParameters": 0.0
}

body = str.encode(json.dumps(data))

url = 'https://kaanaslantestworkspace-wyndx.northeurope.inference.ml.azure.com/score'
# Replace this with the primary/secondary key or AMLToken for the endpoint
api_key = ''
if not api_key:
    raise Exception("A key should be provided to invoke the endpoint")

# The azureml-model-deployment header will force the request to go to a specific deployment.
# Remove this header to have the request observe the endpoint traffic rules
headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'automl457b251af41-1' }

req = urllib.request.Request(url, body, headers)

try:
    response = urllib.request.urlopen(req)

    result = response.read()
    print(result)
except urllib.error.HTTPError as error:
    print("The request failed with status code: " + str(error.code))

    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure
    print(error.info())
    print(error.read().decode("utf8", 'ignore'))

#----------------------------------------------------------------------------------------------------------------------------
    Aslında Microaoft tarafından hazırlanmış olan azureml isimli başka bir kütüphane daha bulunmaktadır. İşlemler bu kütüphane ile
    daha az kod yazarak da yapılabilmektedir. Bu kütüphane için aşağıdaki paketlerin yüklenmesi gerekmektedir:

    pip install azureml
    pip install azureml-core
    pip install azureml-data

    Aşağıda azureml kullanılarak kestirim işlemine örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import json
from pathlib import Path
from azureml.core.workspace import Workspace, Webservice
 
service_name = 'automl245eb70546-1',
ws = Workspace.get(
    name='KaanWorkspace',
    subscription_id='c06cf27d-0995-4edd-919a-47fd40f4a7ae',
    resource_group='KaanAslanResourceGroup'
)
service = Webservice(ws, service_name)
sample_file_path = '_samples.json'
 
with open(sample_file_path, 'r') as f:
    sample_data = json.load(f)
score_result = service.run(json.dumps(sample_data))
print(f'Inference result = {score_result}')

#----------------------------------------------------------------------------------------------------------------------------
    Azure'ün Machine Learning servisinde diğerlerinde henüz olmayan "designer" özelliği de bulunmaktadır. Bu designed sayesinde 
    sürükle bırak işlemleriyle hiç kod yazmadan makşne öğrenmesi modeli görsel biçimde oluşturulabilmektedir. Bunun için 
    Machine Learning Stdudio'da soldaki "Designer" sekmesi seçilir. Bu sekme seçildiğinde karşımıza bazı seçenekler çıkacaktır. 
    Biz hazır bazı şablonlar kullanarak ve şablonları değiştirerek işlemler yapabiliriz ya da sıfırdan tüm modeli kendimiz 
    oluşturabiliriz. 

    Model oluştururken sol taraftaki pencerede bulunan iki sekme kullanılmaktadır. Data sekmesi bizim Azure yükledeiğimiz veri 
    kümelerini göstermektedir. Component sekmesi ise sürüklenip bırakılacak bileşnleri belirtmektedir. Her bileşen dikdörtgensel 
    bir kutucuk ile temsil edilmiştir. Kod yazmak yerine bu bileşenler tasarım ekranına sürüklenip bırakılır. Sonra da bu bileşenler
    birbirlerine bağlanır. Sürüklenip bırakılan bileşenlerin üzerine tıklanıp farenin sağ tuşu ile bağlam menüsünden bileşene özgü 
    özellikler görüntülenebilir. Bu bağlam menüsünde pek çok bileşen için en önemli seçenek "Edit node name" seçenğidir. Bu seçenekte
    bileşene ilişkin özenmli bazı özellikler set edilmektedir. 

    Eğer model için şablon kullanmayıp sıfırdan işlemler yapmak istiyorsak işlemlere Data sekmesinde ilgili veri kümesini sürükleyip
    tasarım ekranına bırakmakla başlamalıyız. 

    Veri kümesini belirledikten sonra biz veri kümesindeki bazı sütunlar üzerinde işlem yapmak isteyebiliriz. Bunun için 
    "Select Columns in Dataset" bileşeni seçilir. Veri kümesinin çıktısı bu bileşene fare hareketi ile bağlanır. Sonra 
    "Select Columns in Dataset" bileşeninde bağlam menüsünden "Edit node name" seçilir. Buradan da "Edit columns" seçilerek 
    sütunlar belirlenir. 

    Bu işlemden sonra eksik veriler üzerinde işlemlerin yapılması isteniyorsa "Clean Missing Data" bileşni seçilerek tasarım 
    ekranına bırakılır. 

    Bundan sonra veri kümesini eğitim ve test olmak üzere ikiye ayırabiliriz. Bunun için "Split Data" bileşeni kullanılmaktadır. 
    Bu bileşende "Edit node names" yapıldığında bölmenin yüzdelik değerleri ve bölmenin nasıl yapılacağına yönelik bazı 
    belirlemeler girilebilir. 

    Bu işlemden sonra "Özellik Ölçeklemesi (Feature Scaling)" yapılabilir. Bunun için "Normalize Data" bileşeni seçilir. 
    Bu bileşende "Edit node names" seçildiğinde biz ölçekleme üzerinde belirlemeleri yapabilecek duruma geliriz. 

    Bu aşamalardan sonra artık sıra modelin eğitimine gelmiştir. Modelin eğitimi için "Train Model" bileşeni kullanılmaktadır. 
    Bu bileşenin iki girişi vardır. Girişlerden biri "Dataset" girişidir. Bu girişe veri test veri kümesi bağlanır. Bileşenin
    birinci girişi olan "Untrained model" girişine ise problemin türünü belirten bir bileşen bağlanır. Çeşitli problem türleri 
    için çeşitli bileşenler vardır. Örneğin ikili sınıflandırma problemleri için "Two class logistic regression" bileşeni kullanılır. 
    Eğitime ilişkin hyper parametreler bu bileşende belirtilmektedir. 

    Eğtim işleminden sonra sıra modelin test edilmesine gelmiştir. Modelin testi için "Score Model" bileşeni kullanılır. Score Model
    bileşeninin iki girişi vardır: "Trainded Model" ve "Dataset" girişleri. "Train Model" bleşeninin çıkışı "Trained Model" girişine, 
    "Split Data" bileşeninin ikinci çıkışı ise "Dataset" girişine bağlanır.

    Model Designer'da oluşturulduktan sonra artık sıra modelin eğitilmesine gelmiştir. "Configure & Submit" düğmesine basılır. 
    Burada artık eğitim için kullanılacak CPU kaynağı belirlenir. (Anımsayacağınız gibi Automated araçlarda bizim belirlememiz
    gereken üç unsur "Data" + "Model" + "CPU" biçimindeydi.) 

    Nihayet işlemleri başlatıp deployment işlemi için "Inference Pipeline"" seçeneği seçilmelidir. 

    Azure Designer'daki tüm bileşenler (components) aşağıdaki Microsoft bağlantısında dokümante edilmiştir:

    https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference?view=azureml-api-2


    Biz Inference Pipeline işlemini yaptığımızda Azure bize modelimizi kestirimde kullanılabecek hale getirmektedir. Bunun 
    için model bir "Web Service Output" bileşeni eklemektedir. Bu "Web Service Output" bileşeni kestirim çıktılarının 
    bir web servis biçiminde verileceği anlamına gelmektedir. Eskiden Azure aynı zamanda "Inference Pipeline" seçildiğinde
    modelimize bir "Web Service Input" bileşeni de ekliyordu. Bu bileşen de girdilerin web service tarafından alınacağını
    belirtmekteydi. Designer'ın yeni versiyonlarında "Infererence Pipeline" yapıldığında bu "Web Service Input" bileşeni artık
    eklenmemektedir. Web Service Input bileşeni eklendikten sonra artık bizim "Select Columns in Dataset" bileşeninden 
    kestirilecek sütunu çıkartmamız gerekmektedir. Ayrıca bu Web Service Input bileşeninin çıktısının artık "Select Columns in Dataset"
    bileşenine değil doğrudan ApplyTransformation bileşenine bağlanması gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Desginer'ın aytıntılı kullanımı için Microsoft dokümanlarını inceleyebilirsiniz. Örneğin aşağıdaki dokümanda lojistik olmayan 
    regresyon probleminin adım adım designer yardımıyla oluşturulması anlatılmaktadır:

    https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-train-score?view=azureml-api-1

#----------------------------------------------------------------------------------------------------------------------------
    IBM'in Cloud Platformu da en çok kullanılan platformlardan biridir. İşlevsellik olarak diğerlerine benzemektedir. IBM'de 
    cloud platformu içerisinde makine öğrenmesine ilişkin servisler bulundurmaktadır. Genel olarak IBM'in bu servislerine
    Watson denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformu için yine bir hesap açılması gerekir. Hesap açma ve sign in işlemleri cloud.ibm.com adresinden 
    yapılmaktadır. Hesap açılırken girilen e-posta adresi aynı zmaanda "IBMid" olarak kullanılmaktadır. IBMid cloud platformunda
    "user id" gibi kullanılmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformuna login olunduktan sonra karşımıza bir "Dashboard" sayfası çıkmaktadır. Burada ilk yapılacak şey 
    "Create Resource" seçilerek kaynak yaratılmasıdır. Buradan "Watson Studio" seçilir. Bedava hesp ile ancak bir tane "Watson Studio"
    kaynağı oluşturulabilmektedir. Kaynak oluşturulduktan sonra "Launch in IBM Cloud Pak for Data" düğmesine basılarak
    "IBM Watson Studio" ortamına geçilmektedir. IBM Watson Studio bir çeşit "Web Tabanlı IDE" gibi düşünülebilir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Watson Studio'ya geçildiğinde öncelikle bir projenin yaratılması gerekmektedir. Bunun için "New Project" seçilir. 
    Proje bir isim verilir. Sonra projenin yaratımı gerçekleşir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Proje yaratıldıktan sonra "New Asset" düğmesi le yeni bir proje öğesi (asset) oluşturulmalıdır. Burada Otomatik ML işlemleri 
    için "Auto AI" seçilebilir.
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Auto AI seçildiğinde yaratılacak işlem (experiment) için bir isim verilmelidir. Sonra bu işlem (experiment)" bir ML servisi 
    ile ilişkilendirilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Watson Studio'da Auto AI projesinde bizden öncelikle veri kümesinin yüklenmesi istenmektedir. Veri kümesi yüklendikten sonra 
    bu veri kümesinin ardışık "time series" verilerinden oluşup oluşmadığı bize sorulmaktadır. Bundan biz kesitim yapılacak sütun
    belirleriz. Uygulamacı problemin türüne göre problemin çeşitli meta parametrelerini kendisi set edebilmektedir. Bu işlem 
    "Experiment Settings" düğmesiyle yapılmaktadır. Nihayet uygulamacı "Run Experiment" seçeneği ile eğitimi başlatır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Modeller oluşturulduktan sonra bunlar performansa göre iyiden kötüye doğru sıralanmaktadır. Bu modeller save edilebilir. 
    Modeller save edilirken istenirse model kodları bir Jupyter Notebook olarak da elde edilebilmektedir. Bu notebook yerel makineye
    çekilip IBM Python kütüphanesi kurulduktan sonra yerel makinede de çalıştırılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işlemi için önce bir "deployment space" yaratılır. Ancak bu deployment space'te Assets kısmında deploy edilecek 
    modelin çıkması için dah önceden Model sayfasında "Promote to Deployment Space" seçilmelildir. Bundan sonra Deployment Space'te
    Assets kısmında ilgili model seçilerek "Create Deployment" düğmesi ile deployment işlemi yapılır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Deployment sonrası yine Wen Servisleri yoluyla model kullanılabilmektedir. Bu işlem Watson Web API'leriyle yapılabilecğei gibi
    diğer cloud platfotrmlarında olduğu gibi bu Web API'lerini kullanan Python kütüphaneleriyle de yapılabilmektedir. Kütüphane 
    aşağıdaki gibi install edilebilir:

    pip install ibm-watson-machine-learning   
#----------------------------------------------------------------------------------------------------------------------------



