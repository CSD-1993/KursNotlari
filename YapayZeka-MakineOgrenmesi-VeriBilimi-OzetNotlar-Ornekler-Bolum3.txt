#----------------------------------------------------------------------------------------------------------------------------
                                    
                                     Yapay Zeka, Makine öğrenmesi ve Veri Bilimi
                                                        Kursu
            
                                          Sınıfta Yapılan Örnekler ve Özet Notlar 
                                                       
                                                       3.Bölüm

                                                 Eğitmen: Kaan ASLAN
                                        
        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:           

        Aslan, K. (2025), "Yapay Zekâ, Makine Öğrenmesi ve Veri Bilimi Kursu", Sınıfta Yapılan Örnekler ve Özet Notlar, 
            C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları okurken editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                            Son Güncelleme: 02/01/2026 - Cuma

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            162. Ders - 19/10/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu son bölümünde bazı istatistik konuların üzerinde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Belli bir hipotezin (yani varsayımın) belli bir anlam düzeyinde (significance level) test edilmesini sağlayan istatistiksel 
    yöntemlere "hipotez testleri (hypothesis testing)" denilmektedir. Hipotez testleri başta sosyal bilimler ve sağlık bilimleri
    olmak üzere endüstri mühendisliği, ekonomi ve işletme gibi pek çok alanda yaygın olarak kullanılmaktadır. 
    
    Örneğin yeni bir ilacın kan şekerini düşürdüğü iddia edilsin. Üretilecek olan bu ilacın gerçekten kan şekerini düşürüp 
    düşürmediğini nasıl anlayabiliriz? İlk akla gelecek yöntem bu ilacı çeşitli kişilere uygulayıp sonuçları karşılaştırmaktır. 
    Bunun için bir grup denek seçebiliriz. Deneklerin kan şekerini ilaç uygulanmadan önce ve uygulandıktan sonra ölçebiliriz. 
    Bu ölçümler arasında anlamlı bir fark olup olmadığına bakabiliriz. Ancak ne kadarlık bir fark bizim ilacın kan şekerini 
    düşürdüğünü kabul etmemiz için yeterli olacaktır? Örneğin kişinin tokluk kan şekeri 120 olsun. Bu ilaç bir kişide kan 
    şekerini 119'a diğer bir kişide 118'e düşürmüşse ancak diğer bir kişide ise hiçbir etki yaratmamışsa biz bu ilacın kan 
    şekerini düşürdüğünü söyleyebilir miyiz? Şüphesiz bizim hatırı sayılır bir düşüşü dikkate almamız gerekir. Öte yandan 
    ilaç herkeste tamamen aynı düşüşü de sağlamayabilecektir. O zaman bir biçimde bizim ortalama düşüşü dikkate almamız 
    gerekebilir. İşte bu tür deney kalıpları için işlem uygulanmadan önceki ve işlem uygulandıktan sonraki farkların
    ortalamasına belli bir güven düzeyi içerisinde bakarak sonuç çıkartan istatistiksel hipotez testleri geliştirilmiştir. 
    Böyle bir test ile ilacın kan şekerini düşürme konusundaki etkisi herkesin kabul edebileceği biçimde nesnel olarak 
    gösterilebilmektedir. (Tabii gerçek yaşamda bir ilacın piyasaya sürülmesi için pek çok farklı testin belli prosedürlere uygun 
    olarak yapılması gerekmektedir.)
    
    Her hipotez testi tipik bazı deney kalıplarından elde edilen sonuçları test etmek için geliştirilmiştir. (Örneğin "kan 
    şekerini düşüren ilaç" örneğimize "öntest-sontest (pre-post test)" deney tasarımı denilmektedir.) Bu nedenle yalnızca 
    hipotez testlerinin nasıl uygulandığını bilmek yetmez, hangi durumlarda hangi testlerin uygulanması gerektiğini de bilmek 
    gerekir. Biz de bu bölümde çok kullanılan bazı deney kalıplarına yönelik hipotezlerin testleri üzerinde duracağız ve 
    bu testlerin Python'da programlama yoluyla nasıl yapıldığını uygulamalı olarak göreceğiz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri örneklemden hareketle ana kütle hakkında çıkarımlar yapmaktadır. Yukarıdaki örnekte biz ilacın kan şekerini 
    düşürüp düşürmediğini o ilacı belli sayıda kişilere uygulayarak test ettik. Ancak ilaç aslında herkese uygulanmalıydı. 
    Fakat bunun imkanın olmadığı da açıktır. O halde bu tür durumlarda ilacı belli sayıda kişiye uygulayıp onlardan elde edilen 
    sonuca dayalı olarak herkes hakkında genel bir hüküm verme yoluna gidilmektedir. Özetle hipotez testleri örneklemler üzerinde 
    uygulanmaktadır. Ancak test sonucunda ana kütle hakkında çıkarımlarda bulunulmaktadır. Örneğin bir havuzun değişik yerlerinden 
    damlalıkla su alıp onlardan birtakım ölçüm değerleri elde etmiş olalım. Sağlıklı bir suda kabul edilen değerleri de zaten 
    biliyor olalım. Havuzdaki suyun sağlıklı olup olmadığına havuzdaki tüm suya bakarak karar vermemekteyiz. Havuzdan belli 
    miktarda örnekler alarak buna karar vermekteyiz. 

    2000'li yılların başlarında bilgisayar donanımlarının ve veri aktarım teknolojisinin gelişmesiyle birlikte "örneklemden 
    hareketle anakütle hakkında kestirimde bulunmaya" alternatif olarak "tüm anakütleyi ya da onun büyük kısmını göz önüne 
    alarak kestirimde bulunmayı" hedefleyen bir yaklaşım da ortaya atılmıştır. Büyük veri (big data) olarak isimlendirilen 
    bu yaklaşım pek çok uygulamada başarıyla kullanılmıştır. Büyük veri yaklaşımı aynı zamanda komşu disiplinleri de etkilemiş 
    makine öğrenmesi ve veri bilimi alanının gelişmesine de katkı sağlamıştır. Ancak büyük veri yaklaşımının her alanda 
    kullanılması mümkün değildir. Bu nedenle örnekleme temelli yöntemler geçerliliğini kaybetmiş değildir. Zaman içerisinde 
    örnekleme temelli yöntemlerle büyük veri yöntemlerinin hibrit bir biçimde uygulandığı yöntemler de geliştirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri için araştırıcı öncelikle bir hipotez oluşturur. Bu hipoteze (yani uygulamacının arzu ettiği sonuca 
    ilişkin hipoteze) istatistikte genellikle H1 hipotezi ya da "alternatif hipotez" denilmektedir. Ancak "bir değişiklik 
    olmadığına yönelik", "eski duruma yönelik", "statükoya yönelik" hipoteze de H0 hipotezi ya da "null hipotez" denilmektedir. 
    O halde hipotez testlerinde iki hipotez vardır: H0 ve H1 hipotezleri. Bizim iddia ettiğimiz hipoteze H1 hipotezi, zaten 
    var olan duruma ilişkin hipoteze ise H0 hipotezi denilmektedir. Bu hipotezlere ilişkin birkaç örnek verelim:

    - Kan şekerini düşürdüğünü iddia ettiğimiz ilaç için ilacı uygulamadan ve ilacı uyguladıktan sonra ölçümler yapalım
    ("öntest-sontest" deney kalıbı). Sonra bu ölçümlerin ortalamasını hesaplayalım. Buradaki H0 ve H1 hipotezleri şöyle 
    oluşturulabilir:

    H0 Hipotezi: İlaç uygulanmadan önceki ortalama kan şekeri ile ilaç uygulandıktan sonraki ortalama kan şekeri arasında 
    anlamlı (belirlenen anlam düzeyi dikkate alındığında) bir farklılık yoktur. Bu hipotez matematiksel sembollerle şöyle 
    ifade edilebilir:

    H0: μ₀ = μ₁

    H1 Hipotezi: İlaç uygulanmadan önceki ortalama kan şekeri ilaç uygulandıktan sonraki ortalama kan şekerinden anlamlı 
    (belirlenen anlam düzeyinde dikkate alındığında) bir biçimde daha yüksektir. Bu hipotez matematiksel sembollerle şöyle 
    ifade edilebilir:
    
    H1: μ₀ > μ₁ 
    
    Burada μ₀ ilaç uygulanmadan önceki kan şekeri ortalamasını μ₁ ise ilaç uyguladıktan sonraki kan şekeri ortalamasını 
    belirtmektedir. 

    - Bir egzersizin uykusuzluğu azalttığına yönelik bir hipotez söz konusu olsun. Burada yine rastgele kişiler seçerek 
    onların egzersizden önceki uyku sürelerini ve egzersizden sonraki uyku sürelerini ölçebiliriz. Bunların ortalamalarını 
    hipotez testi ile karşılaştırmak isteyebiliriz. H0 ve H1 hipotezleri yine şöyle oluşturulabilir:

    H0: μ₀ = μ₁
    H1: μ₀ < μ₁ 

    Burada yine μ₀ egzersiz uygulanmadan önceki ortalamayı μ₁ ise egzersiz uygulandıktan sonraki ortalamayı belirtmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri genellikle iki gruba ayrılarak ele alınmaktadır:

    1) Parametrik hipotez testleri
    2) Parametrik olmayan (non-parametric) hipotez testleri

    Parametrik hipotez testleri oransal (ratio) ve aralık (interval) ölçeklerine ilişkin değişkenlere uygulanabilmektedir. 
    Bu testlerde ortalama, standart sapma gibi dağılım parametreleri kullanılmaktadır ve örneklemin belli bir dağılıma (tipik 
    olarak normal dağılıma) uygun olması koşulu vardır. Parametrik olmayan testler herhangi bir ölçek türündeki değişkenlere 
    uygulanabilmektedir. Bu testlerin temel özelliği, örneklemden elde edilen değerlerin belli bir dağılıma uygunluk varsayımı 
    gerektirmemesidir. Kategorik (nominal) ve sıralı (ordinal) ölçekteki veriler için parametrik testler uygulanamadığından, 
    bu tür verilerde parametrik olmayan testler kullanılır. Ancak parametrik olmayan testler, dağılım varsayımlarını karşılamayan 
    oransal ve aralık ölçekli verilere de yaygın olarak uygulanmaktadır.
    
    Oransal ve aralık ölçekli değişkenlerde eğer dağılım varsayımları (normallik, varyans homojenliği gibi) karşılanıyorsa, 
    parametrik testler tercih edilmelidir. Çünkü parametrik testler daha yüksek istatistiksel güce (power) sahiptir ve daha 
    hassas sonuçlar verir. Ancak bu varsayımlar ihlal edildiğinde parametrik olmayan testler daha uygun ve güvenilir sonuçlar 
    sağlar.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Parametrik hipotez testleri büyük ölçüde "merkezi limit teoremine (central limit theorem)" dayandırılmaktadır. Anımsanacağı 
    gibi merkezi limit teoremi, özet olarak, bir ana kütleden seçilen yeterince büyük örneklem ortalamalarının normal dağılıma 
    yakınsadığını belirtmektedir. Biz kursumuzda her bir hipotez testinin dayandığı istatistiksel ve matematiksel temel üzerinde 
    durmayacağız. Eğer konuyu matematiksel ve teorik ayrıntılarıyla öğrenmek istiyorsanız başka kaynaklara başvurabilirsiniz. 
    Kursumuzda biz daha çok uygulamaya yönelik bilgiler aktaracağız.

    Parametrik hipotez testlerinin en yaygın kullanılanları şunlardır:

    - Tek örneklem t-testi
    - Bağımsız örneklem t-testi
    - Eşleştirilmiş (bağımlı) örneklem t-testi
    - Tek yönlü ANOVA
    - İki yönlü ANOVA
    - Tekrarlı ölçümler için ANOVA
    - Pearson korelasyon testi
    - Doğrusal regresyon analizi
    - F-testi (varyans karşılaştırması)
    - Levene testi (varyans homojenliği)
    - Welch t-testi (eşit varyans varsayımı olmadan)

    Parametrik olmayan hipotez testlerinin ise en yaygın kullanılanları şunlardır:

    - Mann–Whitney U testi (Wilcoxon rank-sum testi)
    - Wilcoxon signed-rank testi
    - Kruskal–Wallis H testi
    - Friedman testi
    - Spearman sıra korelasyon testi
    - Kendall Tau korelasyon testi
    - Ki-kare (χ²) bağımsızlık testi
    - Ki-kare uygunluk testi
    - Fisher'in kesin testi
    - Sign testi
    - Median testi
    - Theil–Sen kestirimi (parametrik olmayan regresyon)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sosyal bilimlerde ve sağlık bilimlerinde hipotez testleri için genellikle paket programlar kullanılmaktadır. Bunları 
    kullanmak için programlama bilmeye gerek yoktur. Bu alanda yaygın kullanılan paket programlardan bazıları şunlardır:

    - SPSS (PSPP isimli GNU versiyonu var)
    - SAS
    - Minitab
    - Matlab (Octave isimli GNU versiyonu var)

    R Programlama Dili oldukça basit "domain specific", matematiksel ve istatistiksel bir dilidir. Ancak profesyonel dünyada 
    paket programlardan ve R dilinden ziyade Python Programlama Dili tercih edilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistiksel hipotez testleri için oluşturulmuş çeşitli kütüphaneler vardır. NumPy, Pandas ve scikit-learn hipotez 
    testlerini desteklemektedir. Python'da hipotez testleri için en yaygın kullanılan kütüphane statsmodels isimli kütüphanedir. 
    Ancak genel amaçlı nümerik analiz kütüphanesi olan SciPy kütüphanesi de hipotez testlerinin bazılarını bünyesinde barındırmaktadır. 
    Biz SciPy kütüphanesini zaten daha önce çeşitli konularda kullanmıştık. 
    
    statsmodels kütüphanesini şöyle kurabilirsiniz:

    pip install statsmodels
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri istatistikte uzun süredir iyi bilinen konulardandır. Bu nedenle konunun zaman içerisinde geniş ve yeterli 
    bir yabancı ve yerli literatürü oluşturulmuştur. Bu konuda  "Sosyal Bilimler İçin Veri Analizi El Kitabı (Şener Büyüköztürk)" 
    isimli kitabı kaynak kitap olarak kullanabilirsiniz. Bu kitap yöntemlerin istatistiksel ve matematiksel temeli üzerinde 
    ayrıntıya girmemekle birlikte "hangi durumlarda hangi hipotez testlerinin kullanılacağı konusunda" iyi bir bilgi sunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde iki önemli kavram vardır: α (alfa) ve p değeri. α, araştırmacı tarafından belirlenen tip I hata 
    olasılığıdır. Yani H0 doğru iken H0’ı reddetme olasılığıdır. p değeri ise test sonucunda elde edilen bir değerdir ve "H0 
    hipotezi doğru kabul edilirse, gözlenen test istatistiği kadar uç bir değer elde etme olasılığını" bileritmektedir. α değeri 
    ne kadar küçültülürse testin güvenilirliği o kadar artar. Örneğin test sonucunda p = 0.01 çıkmış olsun. Bu değer bizim 
    belirlediğimiz α değerinden küçüktür. Yani H0 hipotezinin reddedilme sınırın altına kalmaktadır. Dolayısıyla H0 hipotezi 
    reddedilir. Matematiksel olarak gösterirsek:

    - Eğer p < α ise H0 hipotezi reddedilir.
    - Eğer p ≥ α ise H0 hipotezi reddedilmez.

    Bunu bir tablo biçiminde de aşağıdaki gibi verebiliriz:

    ┌───────────────┬──────────────────┬──────────────────────────────────────────┐
    │ Karşılaştırma │      Karar       │                  Yorum                   │
    ├───────────────┼──────────────────┼──────────────────────────────────────────┤
    │  p < α        │ H₀ reddedilir    │ Sonuç istatistiksel olarak anlamlıdır    │
    ├───────────────┼──────────────────┼──────────────────────────────────────────┤
    │  p ≥ α        │ H₀ reddedilemez  │ Veri, H₀'ı reddetmek için yeterli değil  │
    └───────────────┴──────────────────┴──────────────────────────────────────────┘

    Hipotez testlerine p < α bölgesine "reddetme aralığı (rejection level)",  p ≥ α bölgesine ise kabul aralığı (acceptance 
    region) da denilmektedir.  p değerinin bir koşullu olasılık belirttiğine dikkat ediniz:

    p = P(gözlenen değer ∣ H₀ doğru)

    Başka bir deyişle p değeri "H0 doğru kabul edilirse bu veriyi elde etme olasılığımız nedir?" sorusunun yanıtını vermektedir. 

    Hipotez testleri genel olarak H0 hipotezinin reddedilip reddedilmeyeceği üzerine kurulmaktadır. H0 hipotezinin reddedilmesi, 
    H1 hipotezinin kesin olarak kabul edildiği anlamına gelmez; yalnızca verilerin H1 lehine anlamlı kanıt sunduğu anlamına 
    gelir. Ancak anlatımlarda çoğu kez H0 ya da H1 hipotezinin reddedilmesidiğerinin kabul edilmesi biçiminde ifade edilmektedir. 
    Fakat aslında doğru ifade biçiminin eğer "H0 hipotezi reddediliyorsa" "H1 lehine anlamlı kanıt vardır" biçiminde, eğer 
    "H0 hipotezi reddedilmiyorsa, "H1 hipotezi lehine anlamlı kanıt yoktur" biçiminde olması gerekmektedir. 

    Görüldüğü gibi α değeri bir eşik değerini belirtir ve "tolere edilebilir tip I hata oranı" anlamına gelmektedir. Güven 
    aralıklarında kullanılan güven düzeyi (confidence level) ise 1 − α şeklinde tanımlanır. Örneğin α = 0.05 seçildiğinde güven 
    düzeyi %95 olur ancak güven aralığı (confidence interval) bu güven düzeyine göre hesaplanan bir sayı aralığıdır ve α’nın 
    kendisiyle aynı şey değildir.

    Peki araştırmalarda α için genellikle hangi değerler kullanılmaktadır? Şüphesiz bu durum testin önemine göre değişebilir. 
    Tipik kullanılan α değeri 0.05'tir. Ancak örneğin sağlık bilimlerinde tolerans daha düşük olduğu için α değeri 0.01 
    olarak belirlenebilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde verilen karar için dört durum söz konusudur:

    1) H0 reddedilmemelidir, H0 reddedilmemiştir.
    2) H0 reddedilmemelidir, H0 reddedilmiştir.         ===> Tip 1 Hatası
    3) H0 reddedilmelidir, H0 reddedilmemiştir.         ===> Tip 2 Hatası
    4) H0 reddedilmelidir, H0 reddedilmiştir.

    İstatistiksl olarak anlatım doğru olmasa da bu durumları pratik olarak şöyle de ifade edebiliriz:

    1) H0 kabul reddedilmemelidir, H0 kabul edilmiştir.
    2) H0 kabul edilmelidir, H0 kabul edilmemiştir.     ===> Tip 1 Hatası
    3) H0 kabul edilmemelidir, H0 kabul edilmiştir.     ===> Tip 2 Hatası
    4) H0 kabul edilmemelidir, H0 kabul edilmemiştir.

    İşte H0'ın reddedilmemesi gerektiği halde H0'ın reddedildiği duruma, hatalı bir deyişle H0'ın kabul edilmesi gerektiği 
    halde H1'in kabul edilmesi durumuna "Tip 1 Hatası (Type 1 Error)", H0'ın reddedilmesi gerektiği halde H0'ın reddedilmediği 
    durumuna,  hatalı bir deyişle H1'in kabul edilmesi gerektiği halde H0'ın kabul edildiği duruma "Tip 2 Hatası (Type 2 Error)" 
    denilmektedir. "tip 1 Hatası" yerine "alfa hatası", "tip 2 hatası" yerine "beta hatası" deyimleri de kullanılmaktadır. 
    tip 1 ve tip 2 hataları bir tablo halinde aşağıdaki gibi de gösterebiliriz:

    ╔════════════╦════════════════════════════╦════════════════════════════════════╗
    ║            ║        Testin Kararı       ║            Gerçek Durum            ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║            ║ H₀ reddedilmez (kabul)     ║ H₀ reddedilir                      ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║ H₀ doğru   ║ Doğru karar                ║ Tip I hata (alfa hatası)           ║
    ║            ║ (H₀ doğru, reddedilmedi)   ║ (Yanlış alarm)                     ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║ H₀ yanlış  ║  Tip II hata (β hatası)    ║  Doğru karar                       ║
    ║            ║ (Gerçek farkı göremedik)   ║ (Fark var, doğru tespit edildi)    ║
    ╚════════════╩════════════════════════════╩════════════════════════════════════╝
 
    Tip 1 hatanın "false positive", "Tip 2 hatanın ise "false negative" anlamına da geldiğine dikkat ediniz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinden en temel olanlardan birisi "bağımsız örneklemler t-testi" denilen testtir. Bu teste İngilizce 
    "independent samples t-test" ya da "two-sample t-test" ya da "unpaired t-test" de denilmektedir. Bu test iki bağımsız 
    grubun elde ettiği puanların ortalamasının karşılaştırılması amacıyla kullanılmaktadır. Bu test için tipik deney kalıbı 
    şöyledir: İki bağımsız grup vardır. Bir deney bu iki gruba ya da gruplarından birine uygulanmaktadır. Deney sonucunda 
    "grupların elde ettiği ortalama puanlar arasında anlamlı bir fark var mı" diye bakılmaktadır. Tipik olarak gruplardan biri
     "kontrol grubu" olabilmektedir. Kontrol grubunun üzerinde bir işlem uygulanmaz. Yalnızca diğer grup üzerinde işlem uygulanır. 
     Ya da iki grup üzerinde ayrı işlemler uygulanır. Amaç iki grubun ortalama puanlarının karşılaştırılmasıdır. Bağımsız 
     örneklemler t-tesindeki "bağımsız (independent)" ya da "eşleşmemiş (unpaired)" terimi iki kümenin elemanlarının birbirinden 
     farklı olması anlamına gelmektedir. Yani söz konusu deneydeki iki kümede ortak eleman yoktur. 
    
    Aşağıda bağımsız örneklemler t-tesinin kullanılabileceği birkaç deney kalıbı veriyoruz: 

    - Bir sınıfta girilen sınavdaki matematik puanlarının kız öğrencilerle erkek erkek öğrenciler arasında farklılık gösterip
    göstermediği istatistiksel bakımdan test edilmek istenmektedir. Bunun için bağımsız örneklemler t-testi kullanılabilir. 
    Burada kız ve erkek öğrencilerin bağımsız gruplar oluşturduğuna dikkat ediniz. 

    - Bir hastalığın tedavisi için iki alternatif ilaç vardır. Amaç bu iki ilacın tedavi süresi bakımından yarattığı etkinin 
    birbirinden farklı olup olmadığını anlamaktır. İki ilaç iki farklı gruba verilir. İki farklı gruptaki kişilerin iyileşme 
    süreleri elde edilir. Bunların ortalamaları arasında anlamlı bir fark olup olmadığına bağımsız örneklemler t-testi uygulanarak 
    karar verilebilir.  

    - Sigara içenlerle içmeyenler arasında akciğer kapasitesi bakımından bir farklılık olup olmadığı araştırılmak istensin. 
    Sigara içenler ve içmeyenler iki bağımsız gruptur. Bunların akciğer kapasiteleri ölçülür. Bağımsız örneklemler t-testi 
    ile ortalamalar arasında bir farklılığın olup olmadığına karar verilir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-testinin varsayımları (uygulanabilmesi için sağlanması gereken şartları kastediyoruz) şunlardır:

    1) Gruplar birbirinden bağımsız olmalıdır.
    2) Her iki grubun ana kütlesi normal dağılmış olmalıdır.
    3) Grupların geldiği anakütlenin varyansları eşit olmalıdır (σ₁² = σ₂²)

    Bu koşulların sağlanıp sağlanmadığına başka hipotez testleriyle karar verilebilir. Kursumuz ilk konularında normal dağılım 
    testinin nasıl yapıldığını görmüştük. Bağımsız iki grubun varyanslarının eşitliği (buna varyans homojenliği de denilmektedir) 
    için çeşitli hipotez testleri kullanılmaktadır. Bu bakımdan en yaygın kullanılan hipotez testi "Levene testi" ve "F-testi" 
    denilen testlerdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-tesinin dayandığı fikir oldukça basittir. İki grup ortalamaları arasındaki fark elde edilir. Bu 
    ortalamalara ilişkin farkların normal dağıldığı kabulüyle elde edilen bu farkın uç bir fark olup olmadığına belirlenen 
    α düzeyinde bakılmaktadır. Tabii iki ortalama arasındaki fark bu ortalamaların standart sapmaları da dikkate alınarak 
    anlamlandırılmaktadır. Bağımsız örneklemler t-testi için H0 ve H1 hipotezleri şöyledir:

    H₀: μ₁ = μ₂     
    H₁: μ₁ ≠ μ₂

    Test işlemi yapıldıktan sonra elde edilen p değerinden hareketle H0 hipotezinin reddedilip reddedilmeyeceği belirlenir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-testi SciPy kütüphanesinin stats modülündeki ttest_ind fonksiyonuyla uygulanabilmektedir. Fonksiyonun 
    parametrik yapısı şöyledir:

    ttest_ind(a, b, *, axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, 
            alternative='two-sided', trim=0, method=None, keepdims=False)

    Fonksiyonun ilk iki parametresi bağımsız örneklemler t-testi uygulanacak grup değerlerini belirtmektedir. Diğer parametreler
    default değerler almıştır. Fonksiyon geri dönüş değeri olarak TTestResult sınıfı türünden bir nesne vermektedir. Aslında 
    bu sınıf tuple sınıfından türetilmiştir. Dolayısıyla isimli bir demet gibidir ve demet gibi açılabilmektedir. Bu sınıfın 
    pvalue örnek özniteliği test sonucunda elde edilen p değerini, df örnek özniteliği serbestlik derecesini ve statistic örnek 
    özniteliği ise t istatistik değerlerini vermektedir. Fonksiyonun geri dönüş değeri iki elemanlı bir demet biçiminde de 
    kullanılabilir. Bu demetin ilk elemanı t istatistiğini, ikinci elemanı p değerini vermektedir. Bu iki eleman açıma sokulabilir. 
    Örneğin:

    from scipy.stats import ttest_ind

    group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
    group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

    stat, p = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)

    Buradan şöyle bir çıktı elde edilmiştir:

    t-istatistiği: 3.8249455333812685
    p-değeri: 0.005054073879038671

    Çıktıdaki p değerine bakıldığında oldukça düşük olduğu görülmektedir. α değerinin 0.05 seçilmiş olduğunu varsayarsak 
    p <= α durumu oluşmaktadır. Bu durumda H0 hipotezi reddedilir. Bu da grup ortalamaları arasında istatistiksel olarak 
    anlamlı bir fark olduğu anlamına gelmektedir. Yani ortalamalar arasındaki bu farklılık normal olarak karşılanacak bir 
    farklılık değildir. 

    statsmodels isimli kütüphanede de bağımsız örneklemler t-testi statsmodels.stats.weightstats modülündeki ttest_ind 
    fonksiyonuyla uygulanmaktadır. Fonksiyonun parametrik yapısı şöyledir:

    statsmodels.stats.weightstats.ttest_ind(x1, x2, alternative='two-sided', usevar='pooled', 
            weights=(None, None), value=0)

    Fonksiyonun SciPy'daki ttest_ind fonksiyonuna çok benzediğine dikkat ediniz. Fonksiyonun geri dönüş değeri 3 elemanlı 
    bir demettir. Demetin elemanları sırasıyla t istatistiğini, p değerini ve serbestlik derecesini vermektedir. Örneğin:

    from statsmodels.stats.weightstats import ttest_ind

    group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
    group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

    stat, p, df = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)
    print('serbestlik derecesi:', df)

    Buradan şöyle çıktı elde edilmiştir:

    t-istatistiği: 3.8249455333812716
    p-değeri: 0.005054073879038644
    serbestlik derecesi: 8.0

    Buradaki serbestlik derecesi iki grubun elemanları toplamından 2 eksik olan değerdir. 
#----------------------------------------------------------------------------------------------------------------------------

from statsmodels.stats.weightstats import ttest_ind

group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

stat, p, df = ttest_ind(group1, group2)

print('t-istatistiği:', stat)
print('p-değeri:', p)
print('serbestlik derecesi:', df)

#----------------------------------------------------------------------------------------------------------------------------
    Daha önce üzerinde çalıştığımız zambak veri kümesini anımsayınız. Bu veri kümesinde zambaklar üç sınıfa ayrılmıştı. Veri 
    kümesi şu görünümdeydi:

    Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
    1,5.1,3.5,1.4,0.2,Iris-setosa
    2,4.9,3.0,1.4,0.2,Iris-setosa
    3,4.7,3.2,1.3,0.2,Iris-setosa
    4,4.6,3.1,1.5,0.2,Iris-setosa
    ...
    104,6.3,2.9,5.6,1.8,Iris-virginica
    105,6.5,3.0,5.8,2.2,Iris-virginica
    106,7.6,3.0,6.6,2.1,Iris-virginica
    107,4.9,2.5,4.5,1.7,Iris-virginica
    ...
    52,6.4,3.2,4.5,1.5,Iris-versicolor
    53,6.9,3.1,4.9,1.5,Iris-versicolor
    54,5.5,2.3,4.0,1.3,Iris-versicolor
    55,6.5,2.8,4.6,1.5,Iris-versicolor
    56,5.7,2.8,4.5,1.3,Iris-versicolor
    57,6.3,3.3,4.7,1.6,Iris-versicolor

    Burada araştırmacı "Iris-stosa" ile "Iris-virginica" zambak türlerinin PetalLengthCm özelliklerine ilişkin ortalamalarının 
    anlamlı bir biçimde farklı olup olmadığını test etmek istesin. Hipotezler şöyle oluşturulabilir:

    H₀: "Iris-stosa" ve "Iris-virginica" zambak türlerinin PetalLengthCm ortalamaları birbirine eşittir.
    H₁: "Iris-stosa" ve "Iris-virginica" zambak türlerinin PetalLengthCm ortalamaları birbirine eşit değildir. 

    Testi statsmodels kütüphanesini kullanarak şöyle yapabiliriz:

    df = pd.read_csv('iris.csv')

    group1 = df.loc[df['Species'] == 'Iris-setosa', 'PetalLengthCm']
    group2 = df.loc[df['Species'] == 'Iris-virginica', 'PetalLengthCm']

    stat, p, df = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)
    print('serbestlik derecesi:', df)

    Elde edilen çıktı şöyledir:

    t-istatistiği: -39.46866259397271
    p-değeri: 5.717463758170621e-62
    Serbestlik Derecesi (df): 98.0

    Görüldüğü gibi p değeri çok çok küçüktür. Bu durumda H0 hipotezi reddedilir. Yani iki zambak türünün PetalLengthCm 
    ortalamaları anlamlı bir biçimde birbirinden farklıdır. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from statsmodels.stats.weightstats import ttest_ind

df = pd.read_csv('iris.csv')

group1 = df.loc[df['Species'] == 'Iris-setosa', 'PetalLengthCm']
group2 = df.loc[df['Species'] == 'Iris-virginica', 'PetalLengthCm']

stat, p, df = ttest_ind(group1, group2)

print('t-istatistiği:', stat)
print('p-değeri:', p)
print('serbestlik derecesi:', df)

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de bağımsız örneklemler t-tesinin dayandığı istatistiksel temel hakkında bazı açıklamalar yapalım. İki örneğin 
    elde edildiği anakütlenin X ve Y olduğunu varsayalım. Bunların normal dağılmış olduğunu kabul etmiştik:

    X ~ N(μ₁, σ₁²) 
    Y ~ N(μ₂, σ₂²) 

    Örneklem ortalamalarının beklenen değerleri şöyle olacaktır (merkezi limit teoreminden):

    E(X̄) = μ₁
    E(Ȳ) = μ₂

    Örneklem ortalamalarının varyansları da şöyle olacaktır (merkezi limit teoreminden):

    Var(X̄) = σ₁²/n₁
    Var(Ȳ) = σ₂²/n₂

    Bu durumda örneklem ortalamaları arasındaki farkların beklenen değeri şöyle olur:

    E(X̄ - Ȳ) = E(X̄) - E(Ȳ) = μ₁ - μ₂

    Örneklem ortalamaları arasındaki farkların varyansı şöyle olacaktır:

    Var(X̄ - Ȳ) = Var(X̄) + Var(Ȳ) = σ₁²/n₁ + σ₂²/n₂

    Burada varyansların farklarının alınmadığına varyansların toplandığına dikkat ediniz. Örneklem ortalamaları arasındaki 
    farkların standart hatasını (yani örneklem farklarına ilişkin dağılımın standart sapmasını) şöyle ifade edebiliriz:

    SE(X̄ - Ȳ) = √(σ₁²/n₁ + σ₂²/n₂)

    Eğer anakütle varyansları biliniyorsa örneklem farklarına ilişkin standart normal dağılım şöyle elde edilebilir:

         (X̄ - Ȳ) - (μ₁ - μ₂)
    Z =  ----------------------
          √(σ₁²/n₁ + σ₂²/n₂)

    Ancak genellikle anakütlelere ilişkin varyanslar bilinmez. Bu durumda standart normal dağılım yerine t dağılımı 
    kullanılmalıdır:

    s₁² = Σ(Xᵢ - X̄)² / (n₁ - 1)
    s₂² = Σ(Yⱼ - Ȳ)² / (n₂ - 1)

    Anakütle varyanslarının eşit olduğunu kabul ettiğimizde "ağırlıklandırılmış bileşik varyans (pooled variance)"
    şöyle hesaplanmaktadır:

    s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2)

    Nihayetinde t istatistiği de şu hale gelecektir:

       (X̄ - Ȳ) - (μ₁ - μ₂)
   t = -----------------------
       sₚ√(1/n₁ + 1/n₂)

    Serbestlik derecesi de df = n₁ + n₂ - 2 olacaktır. 
    
    Eğer anakütle varyansları eşit kabul edilmezse buna "Welch t-testi" denir. Bu duurmda t istatistiği ve serbestlik derecesi 
    şu hale gelmektedir:

        (X̄ - Ȳ) - (μ₁ - μ₂)
    t = -------------------
        √(s₁²/n₁ + s₂²/n₂)

         (s₁²/n₁ + s₂²/n₂)²
    df = ---------------------------------------
         [(s₁²/n₁)²/(n₁-1)] + [(s₂²/n₂)²/(n₂-1)]
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Çok kullanılan hipotez testlerinden biri de "varyans analizi (analysis of variance)" denilen kısaca ANOVA biçiminde de
    ifade edilen hipotez testidir. Varyans analizi "tek yönlü (one-way)" ve "çift yönlü (two-way)" olmak üzere iki biçimde 
    uygulanabilmektedir. Biz burada önce tek yönlü varyans analizini sonra da çift yönlü varyans analizini ele alacağız. 

    Tek yönlü varyans analizi (one-way ANOVA) 2'den fazla bağımsız grubun ortalamaları arasında anlamlı bir farkın olup 
    olmadığını test etmek amacıyla kullanılmaktadır. Yani bu anlamda bağımsız örneklemler t-testinin çok gruplusu gibi 
    düşünülebilir. Tek yönlü ANOVA testi için sağlanması gereken koşullar bağımısız örneklemler t-tesi ile benzerdir:

    1) Gruplardaki elemanlar birbirinden bağımsız olmalıdır. (Yani bir eleman birden fazla grupta bulunmamalıdır.)
    2) Gruplara ilişkin ana kütleler normal dağılmış olmalıdır.
    3) Gruplara ilişkin anakütle varyansları eşit olmalıdır. 

    Tabii yine bu koşulların sağlanıp sağlanmadığına başka hiptez testleriyle bakılmaktadır. Anımsayacağınız gibi normallik 
    testi için "Kolmogorov-Simirnov" ya da "Shapiro Wilk" testi, varyans homojenliği için ise "Levene" testi ya da "F testi"  
    kullanılabiliyodu.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA birden fazla bağımısz grubun ortalamalarının biribirine eşit olup olmadığını istatistiksel olarak test 
    etmekte kullanılmaktadır. Yani bu bağlamda bağımızsız örneklemler t-testinin çok gruplusu gibidir. Tek yönlü ANOVA testinde
    H0 ve H1 hipotezleri şöyle oluşturulmaktadır:

    H₀: μ₁ = μ₂ = μ₃ = ... = μₖ
    H₁: En az bir grup ortalaması diğerlerinden farklıdır.

    Örneğin aynı ürünün üç farklı renkteki ambalajlarının satış miktarı üzerinde anlamlı bir farklılığa sahip olup olmadığını
    anlamaya çalışalım. Burada ambalajlar şunlar olsun:

    Grup 1: Kırmızı ambalaj
    Grup 2: Mavi ambalaj
    Grup 3: Yeşil ambalaj

    Bu ambalajlara sahip ürünlerin aynı mağazadaki günlük satış miktarlarını elde etmiş olalım. Bir haftalık süre içerisinde 
    deneyi sürdürelim. Elimizde toplam her ambalajdan 7 adet satış değeri bulunacaktır. Bunların ortalamasını elde elde edip
    tek yönlü ANOVA testi ile bunların herahngi birisinin diğerin farklı bir satış miktarına sahip olup olmadığını anlamak 
    isteyebiliriz. Buradaki hipotezlerimiz şöyle oluşturulabilir:

    H0: Kırmızı, Mavi ve Yeşil ambalajların ortalama satışları arasında bir farklılık yoktur.
    H1: Kırmızı, Mavi ve Yeşil ambalajların ortalama satışları arasında en az biri diğerinden farklıdır.

    Örneğin darklı gübre türlerinin bitki büyümesine etkisinin olup olmadığı yine tek yönlü ANOVA testi ile test edilebilir.

    Pekiyi elimizde 3 farklı grubun ortalamaları olsa biz bu ortlamaların en az birinin diğerlerinden farklı olup olmadığını 
    bağımsız örneklemler t-testi ile anlayabilir miyiz? İşte bağımısz örneklemler t-testi iki grup arasında yürütüldüğü 
    için bizim C(5, 2) kadar farklı t-testi yapmamız gerekir. Bu hem zahmetlidir hem de "tip 1 hata (type 1 error)" olasılığını 
    yükseltmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA testi SciPy kütüphanesindeki stats modülünde bulunan f_oneway fonksiyonuyla uygulanabilmektedir. Fonksiyonun
    parametrik yapısı şöyledir:

    f_oneway(*samples, axis=0, equal_var=True, nan_policy='propagate', keepdims=False)

    Burada fonksiyonun parametresinin *'lı olduğuna dikkat ediniz. Yani fonksiyonu çağırırken biz gruplara ilişkin değerleri 
    farklı argümanlar olarak girebiliriz. Diğer parametreler default değer almıştır. Fonksiyon iki elemanlı bir demete geri 
    dönmektedir. Demetin birinci elemanı F değerini, ikinci elemanı ise F değerinin F dağılımına sokularak elde edildiği 
    p değerini vermektedir. Uygulamacı bu p değerine bakar. Bu p değeri belirlediği α değerinden (tipik olarak 0.05) küüçükse 
    H0 hipotezini reddetmekte değilse reddetmemektedir. Başka bir deyişle bu p değeri α değerinden küçükse grup ortalamalarından 
    en az biri diğerinden farklıdır. Eğer bu p değeri α değerinden küçük değilse grup ortalamalarının birbirine eşit olduğu 
    kabul edilmektedir. Bu durumu özetle şöyle ifade edebiliriz:

    ┌───────────────┬─────────────────────────────────────────────────────────┐
    │ Karşılaştırma │ Yorum                                                   │
    ├───────────────┼─────────────────────────────────────────────────────────┤
    │ p ≤ α         │ H₀ reddedilir → Gruplar arasında anlamlı fark vardır.   │
    ├───────────────┼─────────────────────────────────────────────────────────┤
    │ p > α         │ H₀ reddedilemez → Gruplar arasında anlamlı fark yoktur. │
    └───────────────┴─────────────────────────────────────────────────────────┘

    F değerinin ne anlam ifade ettiğini izleyen paragraflarda ele alacağız. İlerleyen kısımlarda da F dağılımı hakkında bilgiler 
    vereceğiz. 

    Örneğin:

    grup_A = [20, 21, 19, 22, 20]
    grup_B = [30, 31, 29, 32, 30]
    grup_C = [25, 27, 26, 28, 27]

    f_val, p_val = stats.f_oneway(grup_A, grup_B, grup_C)

    print('F istatistiği:', f_val)
    print('p-değeri:', p_val)

    Burada üç ayrı gruptaki değerlerin ortalamaları arasında anlamlı bir fark olup olmadığına bakılmıştır. Elde p değeri 
    ile hipotez testi karara bağlanabilir:

    if p_val < 0.05:
        print('Sonuç: Gruplar arasında anlamlı fark vardır (H₀ reddedilir).')
    else:
        print('Sonuç: Gruplar arasında anlamlı fark yoktur (H₀ reddedilmez).')
#----------------------------------------------------------------------------------------------------------------------------

from scipy import stats

grup_A = [20, 21, 19, 22, 20]
grup_B = [30, 31, 29, 32, 30]
grup_C = [25, 27, 26, 28, 27]

f_val, p_val = stats.f_oneway(grup_A, grup_B, grup_C)

print('F istatistiği:', f_val)
print('p-değeri:', p_val)

if p_val < 0.05:
    print('Sonuç: Gruplar arasında anlamlı fark vardır (H₀ reddedilir).')
else:
    print('Sonuç: Gruplar arasında anlamlı fark yoktur (H₀ reddedilmez).')

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA testi statsmodels kütüphanesindeki stats modülünde bulunan anova_lm fonksiyonuyla da uygulanabilmektedir. 
    Fonksiyonun parametrik yapısı şöyledir.

    statsmodels.stats.anova.anova_lm(*args, **kwargs)

    Fonksiyonun birinci parametresi en küçük kareler yöntemiyle elde edilen model nesnesini belirtmektedir. Bu nesne asıl 
    işlemi yapan stats.anova modülündeki anova_lm fonksiyonuna verilmektedir. Örneğin:

    data = {
        'group': ['A'] * 5 + ['B'] * 5 + ['C'] * 5,
        'height': [20, 21, 19, 22, 20,      # Group A
                30, 31, 29, 32, 30,         # Group B
                25, 27, 26, 28, 27]         # Group C
    }

    df = pd.DataFrame(data)
    model = ols('height ~ C(group)', data=df).fit()

    Burada oluşturulan DataFrame nesnesi şöyledir:

       group  height
    0      A      20
    1      A      21
    2      A      19
    3      A      22
    4      A      20
    5      B      30
    6      B      31
    7      B      29
    8      B      32
    9      B      30
    10     C      25
    11     C      27
    12     C      26
    13     C      28
    14     C      27

    Burada ols fonksiyonu ile elde edilen nesne anova_lm fonksiyonuna verilir:

    anova_table = sm.stats.anova_lm(model, typ=2)

    Burada da ANOVA tablosu elde dilmektedir. Fonksiyondaki typ parametresi tek yönlü ANOVA için genellikle 2 olarak girilmektedir. 
    Fonksiyondan bir ANOVA tablosu elde edilir. Bu ANOVA tablosu bir DataFrame nesnesi biçimindedir. Örneğimizde şöyle bir 
    DataFrame nesnesi elde edilmiştir:

                sum_sq    df     F        PR(>F)
    C(group)    254.8      2.0  98.0       3.687291e-08
    Residual    15.6    12.0   NaN        NaN
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de tek yönlü ANOVA testinin dayandığı istatistiksel temel üzerine açıklamalar yapalım. Tek yönlü ANOVA testi için 
    iki varyans kaynağı hesaplanmaktadır: Gruplar içi varyans değeri ve gruplar arası varyans değeri. Bu iki varyans değeri 
    hesaplandıktan sonra, gruplar arası varyans değeri gruplar içi varyans değerine bölünerek bir F değeri (test istatistiği) 
    elde edilmektedir. Sonra da elde edilen hesaplanmış F değeri, serbestlik derecelerine göre F tablosundan bulunan kritik 
    değer ile karşılaştırılmaktadır:

            Gruplar Arası Varyans
    F = ─────────────────────────────
            Gruplar İçi Varyans

    Gruplar arası varyans değerini hesaplamak için önce kareler toplamının elde edilmesi gerekir. Gruplar arası kareler 
    toplamı şöyle hesaplanmaktadır:

    Gruplar arası kareler toplamı  = Σnⱼ(X̄ⱼ - X̄)²

    Burada X̄ tüm gruplardaki elemanların, X̄ⱼ ise j'inci grubun ortalamasıdır. Burada her grup ortalamasının genel ortalamadan 
    farkının karelerinin ağırlıklı biçimde toplandığını görüyorsunuz. Buradaki nⱼ, j'inci grubun eleman sayısını belirtmektedir. 
    Grup içi kareler toplamı ise şöyle hesaplanmaktadır:

    Grup içi kareler toplamı = ΣΣ(Xᵢⱼ - X̄ⱼ)²
     
    Burada X̄ⱼ j'inci grubun ortalamasını, Xᵢⱼ ise j'inci gruptaki elemanları belirtmektedir. Gruplar arası varyans şöyle 
    hesaplanmaktadır:

    Gruplar arası varyans = Gruplar arası kareler toplamı / (toplam grup sayısı - 1) 

    Burada toplam grup sayısı - 1 değeri serbestlik derecesini belirtmektedir. 

    Gruplar içi varyans da şöyle hesaplanmaktadır:

    Gruplar içi varyans = Gruplar içi kareler toplamı / (toplam eleman sayısı - grup sayısı)

    İşte bunun sonucu olarak yukarıda belirttiğimiz gibi önce bir F değeri elde edilmektedir. Bu F değeri de F dağılımına 
    sokularak p değeri elde edilecek, p değeri de belirlenen alfa değeri ile karşılaştırılacaktır. Şimdi burada yapılanları 
    adım adım örneklendirelim. Elimizde aşağıdaki gibi üç grup olsun:

    Grup A: 20, 21
    Grup B: 30, 31, 29, 32
    Grup C: 32, 33, 33 

    Öncelikle grup ortalamasını ve toplam ortalamayı bulalım.

    A ort. = (20 + 21) / 2 = 20.5
    B ort. = (30 + 31 + 29 + 32) / 4 = 30.5
    C ort. = (32 + 33 + 33) / 3 = 32.67

    Genel ortalama da şöyledir:

    (20 + 21 + 30 + 31 + 29 + 32 + 32 + 33 + 33) / 9 ​=  29

    Şimdi de gruplar arası kareler toplamını bulalım:

    A: 2 * (20.5 − 29)² = 144.50
    B: 4 * (30.5 − 29)² = 9.00
    C: 3 * (32.67 − 29)² = 40.41

    Gruplar arası kareler toplamı = 144.50 + 9.00 + 40.41 = 193.91

    Şimdi de gruplar içi kareler toplamını bulalım:

    A Grubu: (20 − 20.5)² + (21 − 20.5)² = 0.50
    B Grubu: (30 − 30.5)² + (31 − 30.5)² + (29 − 30.5)² + (32 − 30.5)² = 5.00
    C Grubu: (32 − 32.67)² + (33 − 32.67)² + (33 − 32.67)² = 0.67 

    Gruplar içi kareler toplamı = SS_W = 0.50 + 5 + 0.67 = 6.17

    Şimdi de varyansları hesaplayalım. Gruplar arası varyans hesaplarken gruplar arası kareler toplamı, (toplam eleman sayısı 
    - grup sayısı) değerine bölünür:

    Gruplar arası varyans = 193.91 / (3 - 1) =  96.96
    Gruplar içi varyans = 6.17 / (9 - 3) = 1.03

    Artık F değeri elde edilebilir:

             96.96
    F = ──────────────── = 94.14
             1.03

    İşte karar buradaki F değerine bakılarak verilmektedir. F dağılımında df1=2, df2=6 ve α = 0.05 için F değeri 5.14'tür. 
    Bizdeki F değeri ise bu değerden yüksektir (yani 5.14 < 94.14). Daha biçimsel açıklamayı şöyle yapabiliriz: Elde ettiğimiz 
    F değeri için F dağılımından elde edilen p değeri yaklaşık 0.000077'tir. Sonuç olarak 0.00007 < 0.05 olduğu için H0 hipotezi 
    reddedilmektedir. Yani ortalamalar arasında anlamlı farklılıklar vardır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde "post-hoc" testler denilen bir kavram sıkça kullanılmaktadır. "Post-hoc" terimi Latince "bundan sonra"
    ya da "bu durum oluştuktan sonra" gibi anlamlara gelmektedir. Bir hipotez testi gerçekleştirildikten sonra daha fazla 
    bilgi edinmek için yapılabilecek ilave testlere "post-hoc" testler denilmektedir. Biz kursumuzda hipotez testlerinden 
    sonra uygulanabilecek "post-hoc" testler üzerinde fazlaca durmayacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA testinde eğer H0 hipotezi reddedilirse (yani ortalamalar arasında anlamlı farklılıklar varsa) birkaç "post-hoc" 
    test önerilmektedir. Bu durumda "post-hoc" testler "hangi iki grubun birbirinden farklı" olduğu konusunda bilgiler vermektedir. 
    Aşağıda hangi durumlarda hangi "post-hoc" testin kullanılabileceğini bir tablo biçiminde veriyoruz:

    ┌─────────────────────┬───────────────────────┬─────────────────────┬───────────────────────────────┬──────────────────────────┐
    │ Post-hoc Testi      │ Varyans Homojenliği?  │ Örneklem Büyüklüğü  │ Avantajı                      │ Not                      │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Tukey HSD           │ Gerekli               │ Benzer              │ En güvenilir klasik yöntem    │ Varsayımlar sağlanmalı   │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Bonferroni / Holm   │ Gerekmez              │ Fark etmez          │ Basit ve sağlam               │ Konservatif              │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Scheffé             │ Gerekli               │ Fark etmez          │ En korumacı                   │ Gücü düşük               │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Games–Howell        │ Gerekmez              │ Fark etmez          │ Heterojen varyans için en iyi │ Modern, güçlü            │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Dunnett             │ Fark etmez            │ Fark etmez          │ Kontrole karşı en güçlü yöntem│ Sadece kontrol kıyasları │
    └─────────────────────┴───────────────────────┴─────────────────────┴───────────────────────────────┴──────────────────────────┘
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İki yönlü ANOVA, iki faktörün etkilerini ve birlikte etkilerini karşılaştırmak için kullanılmaktadır. İki yönlü ANOVA'da 
    deneyde iki faktör vardır. Faktörler kategorik bağımsız değişkenlerdir. Örneğin tipik olarak aktörlerden biri grupları, 
    diğeri ise gruplara uygulanan işlemleri belirtebilir. İki yönlü ANOVA'ya iki faktörlü ANOVA da denilmektedir. Örneğin 
    kadın ve erkekler üç farklı egzersiz deneyine sokulmuş olsun:

    Faktör 1 (Cinsiyet): Kadın, Erkek
    Faktör 2 (Egzersiz Tipi): Kardiyo, Kuvvet, Kombine 
     
    Burada toplam 6 ayrı ölçüm değeri elde edilecektir. Ölçülen değerlerin kilo kaybı olduğunu varsayalım. Deneklerden 
    aşağıdaki gibi veriler elde ettiğimizi düşünelim:

    ┌──────────┬──────────┬───────────────┬─────────────────┐
    │ Denek No │ Cinsiyet │ Egzersiz Tipi │ Kilo Kaybı (kg) │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │    1     │  Kadın   │    Kardiyo    │      3.2        │
    │    2     │  Kadın   │    Kardiyo    │      4.1        │
    │    3     │  Kadın   │    Kardiyo    │      3.8        │
    │    4     │  Kadın   │    Kardiyo    │      4.5        │
    │    5     │  Kadın   │    Kardiyo    │      3.6        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │    6     │  Kadın   │    Kuvvet     │      2.8        │
    │    7     │  Kadın   │    Kuvvet     │      3.2        │
    │    8     │  Kadın   │    Kuvvet     │      2.5        │
    │    9     │  Kadın   │    Kuvvet     │      3.0        │
    │   10     │  Kadın   │    Kuvvet     │      2.9        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   11     │  Kadın   │    Kombine    │      5.1        │
    │   12     │  Kadın   │    Kombine    │      5.8        │
    │   13     │  Kadın   │    Kombine    │      5.3        │
    │   14     │  Kadın   │    Kombine    │      6.0        │
    │   15     │  Kadın   │    Kombine    │      5.5        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   16     │  Erkek   │    Kardiyo    │      4.5        │
    │   17     │  Erkek   │    Kardiyo    │      5.2        │
    │   18     │  Erkek   │    Kardiyo    │      4.8        │
    │   19     │  Erkek   │    Kardiyo    │      5.0        │
    │   20     │  Erkek   │    Kardiyo    │      4.9        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   21     │  Erkek   │    Kuvvet     │      4.2        │
    │   22     │  Erkek   │    Kuvvet     │      4.8        │
    │   23     │  Erkek   │    Kuvvet     │      4.5        │
    │   24     │  Erkek   │    Kuvvet     │      4.6        │
    │   25     │  Erkek   │    Kuvvet     │      4.4        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   26     │  Erkek   │    Kombine    │      6.8        │
    │   27     │  Erkek   │    Kombine    │      7.2        │
    │   28     │  Erkek   │    Kombine    │      6.5        │
    │   29     │  Erkek   │    Kombine    │      7.0        │
    │   30     │  Erkek   │    Kombine    │      6.9        │
    └──────────┴──────────┴───────────────┴─────────────────┘

    Görüldüğü gibi bu deney kalıbında gruplar bulunmaktadır. Ancak bu gruplar da kendi aralarında ayrışmaktadır. Burada 
    "Kardiyo", "Kuvvet" ve "Kombine" biçiminde üç ayrı grup vardır. Ancak bu gruplar da kendi aralarında "Kadın" ve "Erkek" 
    olmak üzere ikiye ayrılmaktadır. Yukarıdaki ölçümlerin ortalamaları aşağıdaki gibi bir tabloyla ifade edilebilir:

    ┌──────────────────────────────────────────────────────────────┐
    │              GRUPLARA GÖRE ORTALAMALAR (kg)                  │
    ├──────────┬──────────┬──────────┬──────────┬──────────────────┤
    │ Cinsiyet │ Kardiyo  │  Kuvvet  │ Kombine  │  Satır Ortalaması│
    ├──────────┼──────────┼──────────┼──────────┼──────────────────┤
    │  Kadın   │   3.84   │   2.88   │   5.54   │      4.09        │
    │  Erkek   │   4.88   │   4.50   │   6.88   │      5.42        │
    ├──────────┼──────────┼──────────┼──────────┼──────────────────┤
    │  Sütun   │   4.36   │   3.69   │   6.21   │      4.75        │
    │ Ort.     │          │          │          │   (Genel Ort.)   │
    └──────────┴──────────┴──────────┴──────────┴──────────────────┘

    İki yönlü ANOVA'da üç farklı hipotez sınanmaktadır:

    1) Birinci faktörün seviyeleri arasında ortalamalar bakımından anlamlı bir farklılık var mıdır?
    2) İkinci faktörün seviyeleri arasında ortalamalar  ortalamalar arasında bir farklılık var mıdır?
    3) İki faktör arasında etkileşim etkisi var mı? Yani birinci faktörün etkisi, ikinci faktörün seviyelerine bağlı olarak 
    değişmekte midir?

    Yukarıdaki örnek bağlamında hipotezler şöyle ifade edilebilir:

    1) Kadın ve erkeklerin kilo kayıplarının ortalamaları arasında anlamlı bir farklılık var mı?
    2) Üç farklı egzersiz tipinin kilo kaybı ortalamaları arasında anlamlı bir farklılık var mı?
    3) Cinsiyet ve egzersiz tipi arasında etkileşim var mı? Yani egzersiz tipinin etkisi cinsiyete göre değişiyor mu? Bu 
    maddedeki hipotezler şöyle oluşturulabilir:

    H₀: Etkileşim yoktur. Egzersiz tipinin etkisi kadın ve erkeklerde aynıdır.
    H₁: Etkileşim vardır. Egzersiz tipinin etkisi cinsiyete göre değişir.

    ANOVA'daki üçümncü hipoteze dikkat ediniz. Burada "egzersiz tipinin cinsiyete göre değişip değişmeyeceği tek bir 
    hipotez biçiminde oluşturulmaktadır. Örneğin bazı egzersiz tipleri cinsiyete göre değişmiyor ancak bazıları değişiyor 
    olabilir. Ancak test tüm sütunları kapsayacak biçimde yapılmaktadır. Yani iki yönlü ANOVA'nın üçüncü hipotezi iki 
    faktör arasında etkileşimin nerede oluştuğu hakkında bilgi vermemektedir. Bu tespit ancak ayrıca uygulanacak post-hoc 
    testlerle açıklığa kavuşturulabilmektedir. 

    İki yönlü ANOVA uygulamak için sağlanması gereken koşullar şunlardır:

    1) Bağımsızlık. Bir katılımcının verisi diğer katılımcıların verisini etkilememeli. Başka bir deyişle yukarıdaki matrisin
    bir hücresindeki bir katılımcı başka hücresinde bulunmamalıdır. Örneğin bir kadın hem kardiyo hem de kuvvet egzersizine 
    sokulamaz. 

    2) Her bir hücredeki anakütlelerin normal dağılmış olması gerekmektedir. (Yani yukarıdaki örnekte matrisin 6 hücresi de 
    kendi içinde normal dağılmış olmalıdır.)

    3) Tüm hücrelerin anakütlelerine ilişkin varyanslar birbirine eşit olmalıdır.    

    İki yönlü ANOVA testi iki faktör olduğundan dolayı iki yönlüdür. Eğer faktör sayısı üç olursa buna üç yönlü ANOVA, faktör 
    sayısı dört ya da aha fazla olursa da buna çok yönlü ANOVA (multiway ANOVA) denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SciPy kütüphanesinde iki yönlü ANOVA testini yapan bir fonksiyon bulunmamaktadır. iki yönlü ANOVA testi için Statsmodels 
    kütüphanesinde statsmodels.stats.anova modülündeki anova_lm fonksiyonu kullanılmaktadır. İşlemler sırasıyla şöyle 
    yapılmaktadır:

    1) Veri kümesi bir Pandas DataFrame nesnesi olarak oluşturulur. Burada sütunlardan biri birinci faktörü, diğeri ikinci 
    faktörü ve diğeri de elde edilen puanları belirtmektedir. Yukarıdaki veri kümesi için DataFrame nesnesi şöyle oluşturulabilir:

    data = {
        'Denek': range(1, 31),
        'Cinsiyet': ['Kadın']*15 + ['Erkek']*15,
        'Egzersiz': ['Kardiyo']*5 + ['Kuvvet']*5 + ['Kombine']*5 + ['Kardiyo']*5 + ['Kuvvet']*5 + ['Kombine']*5,
        'Kilo_Kaybi': [3.2, 4.1, 3.8, 4.5, 3.6,  # Kadın-Kardiyo
                    2.8, 3.2, 2.5, 3.0, 2.9,  # Kadın-Kuvvet
                    5.1, 5.8, 5.3, 6.0, 5.5,  # Kadın-Kombine
                    4.5, 5.2, 4.8, 5.0, 4.9,  # Erkek-Kardiyo
                    4.2, 4.8, 4.5, 4.6, 4.4,  # Erkek-Kuvvet
                    6.8, 7.2, 6.5, 7.0, 6.9]  # Erkek-Kombine
    }

    df = pd.DataFrame(data)

    Bu DataFrame nesnesi print edildiğinde şunlar görülecektir:

        Denek Cinsiyet Egzersiz  Kilo_Kaybi
    0        1    Kadın  Kardiyo          3.2
    1        2    Kadın  Kardiyo          4.1
    2        3    Kadın  Kardiyo          3.8
    3        4    Kadın  Kardiyo          4.5
    4        5    Kadın  Kardiyo          3.6
    5        6    Kadın  Kuvvet           2.8
    6        7    Kadın  Kuvvet           3.2
    7        8    Kadın  Kuvvet           2.5
    8        9    Kadın  Kuvvet           3.0
    9       10    Kadın  Kuvvet           2.9
    10      11    Kadın  Kombine          5.1
    11      12    Kadın  Kombine          5.8
    12      13    Kadın  Kombine          5.3
    13      14    Kadın  Kombine          6.0
    14      15    Kadın  Kombine          5.5
    15      16    Erkek  Kardiyo          4.5
    16      17    Erkek  Kardiyo          5.2
    17      18    Erkek  Kardiyo          4.8
    18      19    Erkek  Kardiyo          5.0
    19      20    Erkek  Kardiyo          4.9
    20      21    Erkek  Kuvvet           4.2
    21      22    Erkek  Kuvvet           4.8
    22      23    Erkek  Kuvvet           4.5
    23      24    Erkek  Kuvvet           4.6
    24      25    Erkek  Kuvvet           4.4
    25      26    Erkek  Kombine          6.8
    26      27    Erkek  Kombine          7.2
    27      28    Erkek  Kombine          6.5
    28      29    Erkek  Kombine          7.0
    29      30    Erkek  Kombine          6.9

    2) Bir model nesnesi oluşturulur. Model nesnesi oluşturmak için statsmodels.formula.api modülündeki ols fonksiyonu 
    kullanılmaktadır. Bu fonksiyonla model nesnesi oluşturulurken Pandas dataframe nesnesinin sütunlarının ne anlam 
    ifade ettiği belli bir sentaksla belirtilmektedir. Örneğimiz için model yazısı şöyle oluşturulabilir:

    "Kilo_Kaybi ~ C(Cinsiyet) + C(Egzersiz) + C(Cinsiyet):C(Egzersiz)"

    Burada DataFrame içerisindeki Kilo_Kaybi sütununun bağımlı değişken olduğu, birinci faktörün "Cinsiyet", ikinci faktörün 
    "Egzersiz" sütunu olduğunu ve "Cinsiyet" ile "Egzersiz" sütunları arasında ilişki olduğu belirtilmektedir. ols nesnesi 
    yaratıldıktan sonra sınıfın fit metodu ile fit işlemi yapılmalıdır. fit metodu birtakım gerekli işlemleri yapıp nesnenin 
    özniteliklerinde saklamaktadır. Örneğin:

    model = ols('Kilo_Kaybi ~ C(Cinsiyet) + C(Egzersiz) + C(Cinsiyet):C(Egzersiz)', data=df).fit()

    3) Fit edilmiş model nesnesi anova_lm fonksiyonuna argüman olarak verilir. Bu fonksiyondan DataFrame biçiminde ANOVA 
    tablosu elde edilir:

    anova_table = anova_lm(model, typ=2)

    anova_lm fonksiyonunun typ parametresi 1, 2 ya da 3 girilmektedir. Yukarıdaki örnekte olduğu gibi iki yönlü ANOVA testlerinde 
    bu parametre için 2 girilmelidir. Buradan elde edilen DataFrame nesnesi şöyledir:

                                sum_sq     df           F         PR(>F)
    C(Cinsiyet)              13.333333    1.0  127.591707  4.325899e-11
    C(Egzersiz)              34.072667    2.0  163.027113  1.078726e-14
    C(Cinsiyet):C(Egzersiz)   0.420667    2.0     2.012759  1.555575e-01
    Residual                  2.508000  24.0           NaN            NaN

    Bu tablodayu yorumlamak için en önemli iki sütun F değerine ve F değerine karşı gelen p değerine ilişkin tütunlardır.
    p değerlerine baktığımızda bu testten şu sonuçları çıkartabiliriz:

    1) Cinsiyetler arasında elde edilen kilo kaybı ortalamaları bakımından anlamlı bir fark vardır (p değeri çok küçük)
    2) Egzersiz yöntemleri arasında ortalama kilo kaybı bakımından anlamlı bir fark vardır (p değeri çok küçük)
    3) Egzersiz tipinin etkisi kadınlarda ve erkeklerde benzerdir. Yani bir egzersiz tipi kadınlarda ne kadar etkiliyse 
    erkeklerde de o kadar etkilidir (p değeri yeteri kadar küçük değil).
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinden elde edilen sonuçların tez gibi, makale gibi çalışmalarda akademik bir dille ifade edilmesi 
    gerekebilmektedir. Bunun için çeşitli akademik yazım formatları kullanılmaktadır. Örneğin sosyal bilimlerde "APA (American 
    Psychological Association) formatı", fen bilimlerinde "IEEE formatı" yaygın kullanılmaktadır. Bugün artık LLM'ler sayesinde 
    bu tür raporların uygun formatlarda ifade edilmesi oldukça kolaylaşmıştır. Örneğin yukarıdaki iki yönlü ANOVA uygulanan 
    deney kalıbı için APA formatına uygun aşağıdaki gibi bir özet oluşturulabilir:
     
    *******************************

    Cinsiyet Ana Etkisi
     
    Cinsiyet değişkeninin kilo kaybı üzerinde istatistiksel olarak anlamlı bir ana etkisi bulunmuştur, F(1, 24) = 127.59, 
    p < .001, η² = .265. Erkekler (M = 5.42, SD = 1.29) kadınlardan (M = 4.09, SD = 1.20) anlamlı derecede daha fazla kilo 
    kaybetmişlerdir. Etki büyüklüğü (η² = .265) büyük düzeyde bir etki olduğunu göstermektedir (Cohen, 1988).
     
    Egzersiz Tipi Ana Etkisi

    Egzersiz tipi değişkeninin kilo kaybı üzerinde istatistiksel olarak anlamlı bir ana etkisi tespit edilmiştir, 
    F(2, 24) = 163.03, p < .001, η² = .677. Etki büyüklüğü çok yüksek düzeydedir ve egzersiz tipinin kilo kaybındaki varyansın 
    %67.7'sini açıkladığını göstermektedir. Kombine egzersiz programı (M = 6.21, SD = 0.75) en yüksek kilo kaybını sağlarken, 
    bunu kardiyo (M = 4.36, SD = 0.64) ve kuvvet antrenmanı (M = 3.69, SD = 0.76) izlemiştir.

    Etkileşim Etkisi

    Cinsiyet ve egzersiz tipi arasındaki etkileşim istatistiksel olarak anlamlı bulunmamıştır, F(2, 24) = 2.01, p = .156, 
    η² = .008. Bu bulgu, egzersiz tiplerinin kilo kaybı üzerindeki etkisinin kadınlar ve erkekler için benzer olduğunu 
    göstermektedir.

    Model Uyumu

    Genel model istatistiksel olarak anlamlı bulunmuştur ve kilo kaybındaki toplam varyansın %95.0'ını açıklamaktadır 
    (R² = .950, düzeltilmiş R² = .940).

    **********************************
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda bağımsız örneklemler için t-testini ve ANOVA testini inceledik. Eğer birden fazla deney aynı deneklere 
    uygulanıyorsa bu durumda "eşleşmiş (paired) örneklemler" testleri denilen testler kullanılmaktadır. Bu tür testlerin tipik 
    deney kalıbı "öntest-sontest" kalıbıdır. Bu deney kalıbında denekler üzerinde henüz ilgili işlem uygulanmadan bir ölçüm 
    uygulanır. İlgili işlem uygulandıktan sonra da yeniden ölçüm uygulanır. Bu iki ölçümün ortalamaları arasındaki farklara 
    bakılır. Tabii bu deney kalıbından amaç uygulanan işlemin anlamlı bir farklılığa yol açıp açmadığıdır. Örneğin kan şekerini 
    düşürdüğü iddia edilen yeni bir ilaç üretilecek olsun. Deneklerin önce kan şekerleri ölçülür (öntest), sonra bu ilaç 
    uygulanır, sonra da deneklerin yeniden şekerleri ölçülür (sontest). Eğer ilaç etkiliyse arada anlamlı bir fark olmalıdır. 
    (Tabii aslında ilaç endüstrisinde bir ilacın kullanıma girebilmesi kısa ve uzun dönemli için pek çok testlerin ve prosedürlerin 
    uygulanması gerekmektedir. Biz burada yalnızca ilacın etkisine yönelik hipotez testi örneği vermek istedik.) İşte bu 
    testlerle öntest-sontest "ortalamaları arasında anlamlı bir fark var mı" diye bakılmaktadır. öntest-sontest deney 
    kalıbı araştırmalarda oldukça sık uygulanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş t-testinin H0 ve H1 hipotezleri şöyle oluşturulmaktadır:

    H0 Hipotezi: öntest ve sontest ölçümleri birbirleriyle aynıdır. Bu durumu matematiksel olarak şöyle ifade edilebiliriz:

    H₀: μ_D = 0

    H1 Hipotezi: öntest ve sontest ölçümleri birbirinden farklıdır. Bu durum matematiksel olarak şöyle ifade edilebilir:

    H₁: μ_D ≠ 0

    Burada μ_D sontest ile öntest arasındaki farkı belirtmektedir. 

    Eşleştirilmiş örneklemler t-testinin kullanılabilmesi için şu koşulların sağlanması gerekmektedir:

    1) Test uygulanan gruplar aynı olmalıdır. Yani aynı kişilere öntest ve sontest uygulanmalıdır. 
    2) öntest sontest puan farklarının normal dağılmış olması gerekmektedir. 
    3) Ölçümlerde aşırı uç değerlerin (outliers) bulunmaması gerekir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            163. Ders - 23/11/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş örneklemler t-testi için SciPy kütüphanesindeki stats modülünde bulunan ttest_rel fonksiyonu kullanılmaktadır. 
    Bu fonksiyon bizden parametre olarak öntest ve sontest değerlerini alır (hangi parametrenin öntest hangisinin sontest 
    olduğunun bir önemi yoktur) bize iki elemanlı TestResult türünden bir demet geri döndürür. Demetin ilk elemanı t istatistiğini 
    ikinci elemanı da p değerini vermektedir. Yine bu p değeri önceden belirlenmiş alfa değerinden küçükse H0 hipotezi reddedilmekte, 
    değilse H0 hipotezi reddedilmemektedir. 
    
    Örneğin refleks geliştirme eğitiminde eğitimin bir faydasının dokunmadığını anlamak için deneklere önce uyaranlar verilip 
    onların tepki süreleri ölçülmektedir. Sonra deneklere refleks eğitimi verilip yeniden tepki süresi ölçüm yapılmaktadır. 
    Burada öntest puanlarıyla sontest puanları arasında anlamlı bir farklılığın oluşması beklenmektedir. Örneğin:

    pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
    post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

    t_stat, p_val = stats.ttest_rel(pre, post)

    print('t-istatistiği:', t_stat)
    print('p-değeri:', p_val)
    print('Ortalama iyileşme (ms):', np.mean(pre - post))

    Burada p değeri çok küçük çıkmıştır dolayısıyla Ho hipotezi reddedilir. Yani öntest sontest puanları arasında anlamlı 
    farklılıklar vardır. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy import stats

pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

t_stat, p_val = stats.ttest_rel(pre, post)

print('t-istatistiği:', t_stat)
print('p-değeri:', p_val)
print('Ortalama iyileşme (ms):', np.mean(pre - post))

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş örneklemler t-testi statsmodels kütüphanesiyle de gerçekleştirilebilmektedir. Bunun için statsmodels.stats.weightstats
    modülündeki DescrStatsW sınıfı kullanılmaktadır. Uygulamacı bu sınıf türünden bir nesne yaratır. Ancak nesneyi yaratırken 
    öntest ile sontest farkını argüman olarak verilmelidir:

    pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
    post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

    diff = pre - post
    d_stats = DescrStatsW(diff)

    Buradan elde edilen nesne ile sınıfın ttest_mean fonksiyonu 0 argümanıyla çağrılmalıdır:

    t_stat, p_val, df = d_stats.ttest_mean(0)

    ttest_mean metodundan üçlü bir demet elde edilmektedir. Demetin birinci elemanı t istatistiğini, ikinci elemanı p değerini 
    ve üçüncü elemanı da serbestlik derecesini belirtmektedir:

    print('t-istatistiği:', t_stat)
    print('p-değeri:', p_val)
    print('Serbestlik derecesi (df):', df)
    print('Ortalama iyileşme (ms):', np.mean(diff))

    Burada p değerinin çok düşük çıkması H0 hipotezinin reddedileceği anlamına gelmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy import stats

pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

t_stat, p_val = stats.ttest_rel(pre, post)

print("t-istatistiği:", t_stat)
print("p-değeri:", p_val)
print("Ortalama iyileşme (ms):", np.mean(pre - post))

from statsmodels.stats.weightstats import DescrStatsW

diff = pre - post
d_stats = DescrStatsW(diff)
t_stat, p_val, df = d_stats.ttest_mean(0)  

print('t-istatistiği:', t_stat)
print('p-değeri:', p_val)
print('Serbestlik derecesi (df):', df)
print('Ortalama iyileşme (ms):', np.mean(diff))

#----------------------------------------------------------------------------------------------------------------------------
                                            164. Ders - 30/11/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş örneklemlerde yalnızca öntest-sontest değil ikiden fazla test de uygulanabilmektedir. Bu deney kalıbında 
    aynı deneklere ikiden fazla test uygulanmaktadır. Amaç tüm bu testlerden elde edilen skorlar arasında bir farklılığın 
    olup olmadığının istatistiksel bakımdan değerlendirilmesidir. Bunun için kullanılan hipotez  testine "tekrarlı ölçümlerde 
    tek yönlü ANOVA (repeated measures one-way ANOVA)" denilmektedir. Örneğin aynı denek grubuna gün içerisinde farklı zamanlarda 
    aynı deney uygulanmış olabilir. Bunlar arasında zamana dayalı bir farklılığın oluşup oluşmadığı tespit edilmeye çalışabilir. 
    Örneğin günün farklı saatlerinde kişilerin bilişsel performnsları arasında bir farklılık olup olmadığı tespit edilmek 
    istensin. Bu amaçla aynı kişilere saat 9'da, saat 12'de, saat 16'da, saat 20'de ve saat 24'te aynı testler uygulnamış olsun. 
    Eğer testin uygulandığı zaman bilişsel başarıyı etkilemiyorsa bu testlerden alınan puan farklarının istatistiksel bakımdan 
    anlamlı olmaması gerekir. (Bu tür deneylerde eğer deneklere aynı test verilirse denekler bunları öğrenebilir ve zamanla 
    test performansı artabilir. Genellikle bu tür çalışmalarda testlerin eşdeğer zorlukta alternatif versiyonları 
    oluşturulmaktadır.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tekrarlı ölçümlerde tek yönlü ANOVA için hipotezler şöyle oluşturulmaktadır:

    H₀: μ₁ = μ₂ = μ₃ = ⋯ = μₖ
    H₁: ∃ i, j   öyle ki   μᵢ ≠ μⱼ

    H1 hipotezinin "en az bir koşul ortalaması diğerlerinden farklıdır" anlamına geldiğine dikkat ediniz. Burada μ₁, μ₂, μ₃, 
    ...μₖ seçilen örnekteki birinci ölçümün, ikinci ölçümün, üçüncü ölçümün ve n'inci ölçümün ortalmasını blirtmektedir. 

    Örneğin öğrencilerin dönem içerisindeki üç farklı sınavdan aldıkları notların ortalamaları arasında anlamlı bir farklılık 
    olup olmadığının araştırılması istensin. Bu durumda H0 ve H1 hipotezleri şöyle oluşturulur:

    H₀: μ₁ = μ₂ = μ₃
    H₁: ∃ i, j  öyle ki   μᵢ ≠ μⱼ

    Örneğin yüksek tansiyondan şikayet eden hastaların tansiyonları üç farklı zamanda ölçülmüş olsun:

    - Tedaviden önce
    - Tedaviden hemen sonra
    - Tedaviden 1 ay sonra

    Burada amaç bu üç ölçüm ortalamaları arasında anlamlı bir fark olup olmadığının belirlenmesi olabilir. Örneğin sporcuların 
    üç farklı koşu hızında nabızları ölçülüyor olsun:

    - Hız 1: Yavaş
    - Hız 2: Orta
    - Hız 3: Hızlı

    Bu nabız ortalamaları arasında anlamlı bir farklılığın olup olmadığına "tekrarlı ölçümlerde tek yönlü ANOVA testi" ile 
    karar verilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tekrarlı ölçümler için tek yönlü ANOVA testine ilişkin SciPy'da bir fonksiyon ya da sınıf bulunmamaktadır. Bu test statsmodels 
    kütüphanesindeki statsmodels.stats.anova modülünde bulunan AnovaRM sınıfıyla yapılabilmektedir. Örneğin yukarıda sözünü 
    ettiğimiz sporcuların koşu sırasındaki ortalama nabız ölçülerine ilişkin deney için aşağaıdaki bilgiler elde edilmiş olsun:

    n_athletes = 20

    df = pd.DataFrame({
        'Athlete_ID': range(1, 21),
        'Slow': [68, 72, 65, 70, 75, 69, 71, 67, 73, 68, 66, 74, 70, 72, 69, 71, 68, 73, 67, 70],
        'Medium': [85, 92, 88, 90, 95, 87, 89, 86, 93, 88, 84, 94, 91, 90, 89, 92, 87, 91, 85, 88],
        'Fast': [115, 125, 118, 120, 130, 117, 122, 116, 128, 119, 114, 127, 121, 123, 120, 126, 118, 124, 116, 119]
    })

    Buradaki DataFrame nesnesinin görünümü şöyledir:

    Athlete_ID  Slow  Medium  Fast
    1           68      85   115
    2           72      92   125
    3           65      88   118
    4           70      90   120
    5           75      95   130
    6           69      87   117
    7           71      89   122
    8           67      86   116
    9           73      93   128
    10          68      88   119
    11          66      84   114
    12          74      94   127
    13          70      91   121
    14          72      90   123
    15          69      89   120
    16          71      92   126
    17          68      87   118
    18          73      91   124
    19          67      85   116
    20          70      88   119

    Ancak AnovaRM uygulamak için oluşturulan DataFrame nesnesinin bu biçimde olmaması gerekir. Yukarıdaki biçim görsel 
    ve algısal bakımdan daha uygundur. Ancak AnovaRM sınıfı deneklere ilişkin tüm ölçümleri satırsal istememektedir. 
    AnoVRM sınıfı her ölçümün ayrı bir satırda bulunmasını istemektedir. Yukarıdaki gibi DataFrame nesnesini her satırda 
    yalnızca bir ölçümün bulunacağı biçime dönüştürmek için DataFrame sınıfının melt isimli bir metodu kullanılmaktadır. 
    Örneğin:

    melted_df = df.melt(id_vars=['Athlete_ID'], 
                  value_vars=['Slow', 'Medium', 'Fast'],
                  var_name='Speed', 
                  value_name='HeartRate')

    Burada artık DataFrame içerisindeki her bir ölçüm bilgisi ayrı bir satır haline getirilmiştir. melted_df nesnesinin 
    görünümü şöyle olacaktır:

        Athlete_ID   Speed  HeartRate
        1           Slow         68
        2           Slow         72
        3           Slow         65
        4           Slow         70
        5           Slow         75
        6           Slow         69
        7           Slow         71
        8           Slow         67
        9           Slow         73
        10          Slow         68
        11          Slow         66
        12          Slow         74
        13          Slow         70
        14          Slow         72
        15          Slow         69
        16          Slow         71
        17          Slow         68
        18          Slow         73
        19          Slow         67
        20          Slow         70
        1           Medium       85
        2           Medium       92
        3           Medium       88
        4           Medium       90
        5           Medium       95
        6           Medium       87
        7           Medium       89
        8           Medium       86
        9           Medium       93
        10          Medium       88
        11          Medium       84
        12          Medium       94
        13          Medium       91
        14          Medium       90
        15          Medium       89
        16          Medium       92
        17          Medium       87
        18          Medium       91
        19          Medium       85
        20          Medium       88
        1           Fast        115
        2           Fast        125
        3           Fast        118
        4           Fast        120
        5           Fast        130
        6           Fast        117
        7           Fast        122
        8           Fast        116
        9           Fast        128
        10          Fast        119
        11          Fast        114
        12          Fast        127
        13          Fast        121
        14          Fast        123
        15          Fast        120
        16          Fast        126
        17          Fast        118
        18          Fast        124
        19          Fast        116
        20          Fast        119

    melt metoduun nasıl çağrıldığına dikkat ediniz:

    melted_df = df.melt(id_vars=['Athlete_ID'], 
                  value_vars=['Slow', 'Medium', 'Fast'],
                  var_name='Speed', 
                  value_name='HeartRate')

    Burada value_name parametresi asıl değerin hangi sütnda olduğunu belirtmektedir. value_vars parametresi satırlara ayrıştırmada
    kullanılacak kategorik değerleri belirtmektedir. var_name bu kategorik değerlerin DataFrame'deki sütun ismini belirtmektedir. 

    DataFrame nesnesi "melted" hale getirildikten sonra artık AnovaRM nesnesi oluşturulabilir. AnovaRM sınıfının __init__ metodunda 
    data parametresine melt edilmil DataFrame nesnesi girilir. Bağımlı değişken (yani ölçüm elde ettiğimiz değişken, örneğimizdeki 
    depvar parametresiyle belirtilmelidir. Metodun within parametresi ortalamaları karşılaştırılacak sütunun ismini almaktadır 
    (örneğimizde "Speed" sğtunu). subject parametresi ise denekleri belirten sütunu belirtmektedir (örneğimizdeki "Athlete_ID").
    Örneğimiz için AnovaRM nesnesi şöyle yaratılabilir:

    anovarm = AnovaRM(data=melted_df, depvar='HeartRate',  subject='Athlete_ID', within=['Speed'])

    AnovaRM nesnesi oluşturulduktan sonra fit işlemi yapılır. Gerçek hipotez testi işlemleri bu fit işlemi sırasında yapılmaktadır. 
    Örneğin:

    anova_result = anova_model.fit()

    fit işlemi sonucunda AnovaResults isimli bir sınıf türünden bir nesne elde edilmektedir. Bu nesnenin anova_table özniteliği
    DataFrame türündendir. Bu DataFrame nesnesinin sütunları ANOVA testi sonucunda elde edilen değerleri vermektedir. Örneğin:

            F Value         Num DF  Den DF        Pr > F
    Speed  7324.536822     2.0    38.0  6.984224e-50

    Buradaki F değerinin çok yüksek olduğuna dikkat ediniz. Bu F değerine karşı gelen değeri ise çok küçüktür. Bu durumda 
    H0 hipotezi reddedilecektir. Yani sporcuların yavaşi orta ve hızlı koşarkenki nabız ortalamaları birbirine eşit değildir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            165. Ders - 06/12/2025 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aynı deneklere uygulanan iki yönlü ANOVA testi kendi içerisinde iki gruba ayrılmaktadır:

    1) Karşık ölçümlerde iki yönlü ANOVA (Two-way ANOVA for mixed measures)
    2) Tekrarlı ölçümlerde iki yönlü ANOVA (Two-way ANOVA for repeated measures)

    Karışık ölçümlerde iki yönlü ANOVA'da yine iki faktör vardır. Faktörlerden birine ilişkin denekler ile diğerine ilişkin 
    denekler birbirinden farklıdır. Yani iki grup deneğin birden fazla ölçümden elde ettiği puanlarının ortalaması 
    karşılaştırılmaktadır. Örneğin depresyon problemi için üç farklı terapi yönteminin karşılaştırılması istensin. Bunlar 
    örneğin "bilişsel davranışçı terapi", "analitik yönelimli terapi" ve "şema terapisi" olsun. Deneklere terapiden önce ve 
    terapiden sonra "Beck Depresyon Ölçeğinin" uygulandığını kabul edelim.. Buradaki araştırma kalıbını aşağıda matrisel 
    biçimde gösterebiliriz:

                                    öntest (BDÖ Puanı)        sontest (BDÖ Puanı)
    Bilişsel Davranış Terapi         X                           X   
    Analitik Yönelimli Terapi        X                           X
    Şema Terapisi                    X                           X

    Burada bir terapi yöntemine dahil olan bir kişinin başka bir terapi yöntemine dahil olmadığına dikkat ediniz. Yani burada 
    "bilişsel davranışçı terapi" alanlar, "analitik yönelimli terapi" alanlar ve "şema terapisi" alanlar tamamen farklı kişilerdir. 
    Ancak terapi alan her kişiye öntest ve sontest uygulanmıştır. İşte bu tür testlere "karşışık ölçümlerde iki yönlü ANOVA"
    denilmektedir. Örneğimizdeki karışık ölümlerde iki yönlü ANOVA testinin hipotezleri şunlardır:

    1) Terapi yöntemlerinin depresyon üzerinde bir etkisinin olup olmadığına yönelik hipotez. (Yani herhangi bir terapi 
    yöntemi için öntest-sontest puanaları arasında farklılık var mı?) Bu hipotez şöyle ifade edilebilr: 

    H₀: μ_öntest = μ_sontest
    H₁: μ_öntest ≠ μ_sontest

    Buradaki H0 hipotezi öntest ve sontest puanları arasında bir farklılığın olmadığını H1 hipotezi de bir farklılığın olduğunu
    belirtmektedir. Bu hpotezde sütunların arasındaki ortalamalara bakıldığına dikkat ediniz. 

    2) Depresyon üzerindeki tkileri bağlamında terapi yöntemleri arasında bir farklılık var mı hiptezi. (Yani bu terapilerin 
    etkisi aynı mı hipotezi.) Buradaki hipotez şöyle oluşturulabilir:

    H₀: μ_BDT = μ_AYT = μ_ŞT
    H₁: En az iki grup ortalaması birbirinden farklıdır

    H0 hipotezinde üç terapi yönteminin de öntest ve sontest puanlarının aynı olduğu yani terapi yöntemi ne olursa olsun oluşan 
    etkinin aynı olduğu iddia edilmektedir. Bu hipotezde öntest-sontest puanları karşılaştırılmamaktadır. Bunlar birleştirilerek 
    kullanılmaktadır. 

    3) Terapi türlerine göre terapi öncesi ve sonrası arasındaki değişim miktarı farklı mı? Buradaki hipotezleri de şöyle ifade 
    edebiliriz:

    H₀: (μ_sontest - μ_öntest)_BDT = (μ_sontest - μ_öntest)_AYT = (μ_sontest - μ_öntest)_ŞT
    H₁: En az bir grupta zaman etkisi diğerlerinden farklıdır

    Tabii buradaki deneyde sütunlardaki zamansal ölçümler daha fazla olabilirdi. Örneğin ölçümler terapiden önce, terapiden 
    hemen sonra ve terapiden üç ay sonra da yapılabilirdi. Bu durumda üçüncü hipotez istatistiksel olarak şöyle ifade edilirdi:

    H₀: (μ_t₁ - μ_t₀)_BDT = (μ_t₁ - μ_t₀)_AYT = (μ_t₁ - μ_t₀)_ŞT
    VE
    (μ_t₂ - μ_t₁)_BDT = (μ_t₂ - μ_t₁)_AYT = (μ_t₂ - μ_t₁)_ŞT
    VE
    (μ_t₂ - μ_t₀)_BDT = (μ_t₂ - μ_t₀)_AYT = (μ_t₂ - μ_t₀)_ŞT

    H₁: Yukarıdaki eşitliklerden en az biri sağlanmaz

    Burada ikinci hipotezde H0'ın reddedildiğini varsayalım. Bu durum bize hangi terapi yönteminin daha iyi sonuç verdiği
    hakkında bilgi vermemektedir. Biz bundan yalnızca bu terapi yöntemlerinin birbirinden farklı etkilere sahip olduğunu 
    anlarız. Daha önceden de bahsettiğimiz gibi bu tür durumlarda daha ileri tetkikler gerekmektedir. Bunlara "post-hoc testler" 
    denildiğini anımsayınız. Bu durumda uygulanacak ikili karşılaştırma yapan post-hoc testler aşağıdaki tabloda verilmiştir:

    ╔════════════════╦═══════════════════════════════════╦═══════════════════════════════╦═══════════════════════╦═══════════════════════╗
    ║                ║                                   ║                               ║                       ║                       ║
    ║   Test Adı     ║            Formül                 ║      Ne Zaman Kullanılır      ║       Avantajları     ║      Dezavantajları   ║
    ║                ║                                   ║                               ║                       ║                       ║
    ╠════════════════╬═══════════════════════════════════╬═══════════════════════════════╬═══════════════════════╬═══════════════════════╣
    ║                ║                                   ║                               ║                       ║                       ║
    ║                ║  α_düzeltilmiş = α / k            ║ • Her durumda kullanılabilir  ║ • Tip I hatayı iyi    ║ • Çok muhafazakâr     ║
    ║  Bonferroni    ║                                   ║                               ║   kontrol eder        ║                       ║
    ║                ║  k = karşılaştırma sayısı         ║ • Varyanslar eşit olmasa da   ║ • Basit ve anlaşılır  ║ • Gücü düşük          ║
    ║                ║                                   ║                               ║                       ║                       ║
    ╠════════════════╬═══════════════════════════════════╬═══════════════════════════════╬═══════════════════════╬═══════════════════════╣
    ║                ║              ________             ║                               ║                       ║                       ║
    ║                ║             ╱ MSₑᵣᵣₒᵣ             ║ • Tüm ikili karşılaştırmalar  ║ • Dengeli güç         ║ • Varyans eşitsizliği ║
    ║  Tukey HSD     ║  HSD = q × √  ───────             ║                               ║                       ║   durumunda hatalı    ║
    ║                ║                 n                 ║ • Varyanslar eşit             ║ • Yaygın kullanım     ║                       ║
    ║                ║                                   ║ • Grup büyüklükleri eşit      ║                       ║ • Dengesiz n'de       ║
    ║                ║                                   ║                               ║                       ║   sorunlu             ║
    ╠════════════════╬═══════════════════════════════════╬═══════════════════════════════╬═══════════════════════╬═══════════════════════╣
    ║                ║                                   ║                               ║                       ║                       ║
    ║                ║  F_kritik = (k-1) × F_α,k-1,N-k   ║ • Tüm olası kontrastlar       ║ • En esnek            ║ • En düşük güç        ║
    ║   Scheffé      ║                                   ║                               ║                       ║                       ║
    ║                ║                                   ║ • Karmaşık karşılaştırmalar   ║ • Her tür             ║ • Çok muhafazakâr     ║
    ║                ║                                   ║                               ║   karşılaştırma       ║                       ║
    ║                ║                                   ║                               ║                       ║                       ║
    ╠════════════════╬═══════════════════════════════════╬═══════════════════════════════╬═══════════════════════╬═══════════════════════╣
    ║                ║           _______________         ║                               ║                       ║                       ║
    ║                ║          ╱  s²ᵢ     s²ⱼ           ║ • Varyanslar eşit değil       ║ • Varyans varsayımı   ║  Karmaşık hesaplama   ║
    ║ Games-Howell   ║  t = Δμ̄ ╱  ───  +  ───            ║                               ║   gerekmez            ║                       ║
    ║                ║        √    nᵢ      nⱼ            ║ • n'ler farklı                ║                       ║                       ║
    ║                ║                                   ║                               ║ • Güvenilir           ║                       ║
    ║                ║                                   ║                               ║                       ║                       ║
    ╠════════════════╬═══════════════════════════════════╬═══════════════════════════════╬═══════════════════════╬═══════════════════════╣
    ║                ║                                   ║                               ║                       ║                       ║
    ║                ║       M̄ᵢ - M̄ₖₒₙₜᵣₒₗ                  ║ • Kontrol grubu var          ║ • Kontrol odaklı      ║ • Tüm ikilileri test  ║
    ║   Dunnett      ║  d = ───────────────              ║                               ║                       ║   etmez               ║
    ║                ║            SE                     ║ • Sadece kontrol ile          ║ • Daha güçlü          ║                       ║
    ║                ║                                   ║   karşılaştırma               ║                       ║                       ║
    ║                ║                                   ║                               ║                       ║                       ║
    ╚════════════════╩═══════════════════════════════════╩═══════════════════════════════╩═══════════════════════╩═══════════════════════╝

    Hangi post-hoc testin kullanılacağına yönelik karar ağacını da aşağıda veriyoruz:

                     Grup Ana Etkisi Anlamlı mı?
                                  │
                        ┌─────────┴─────────┐
                      Evet                 Hayır
                        │                   │
                        ▼                   ▼
            Varyanslar eşit mi?      Post-hoc gerekmez
            (Levene testi)           Rapor et ve bitir
                        │
             ┌──────────┴──────────┐
           Evet                   Hayır
            │                      │
            ▼                      ▼
        n'ler eşit mi?        Games-Howell
             │                 kullan! ⭐
        ┌────┴────┐
       Evet      Hayır
        │          │
        ▼          ▼
    Tukey     Tukey veya
    HSD      Games-Howell

    Karışık ölçümlerde iki yönlü ANOVA testini kullanabilmek için sağlanması gereken koşullar şınlardır:

    - Gözlemle bağımsız olmalıdır. (Gruplar arası ölçümler birbirinden bağımsız olacak.)

    - Tekrarlı ölçüm içeren faktör için küresellik (sphericity) varsayımı sağlanmalıdır. (Fark skorlarının varyansları
     birbirine eşit olacak.)

    - Normallik varsayımı sağlanmalıdır. (Her hücrede bağımlı değişkenin dağılımı normal olmalı.)

    - Varyans homojenliği sağlanmalıdır. (Gruplar arası faktör için varyanslar birbirine benzer olmalı.)

    - Bağımlı değişken en az aralık (interval) ölçeğinde olmalıdır.

    - Aykırı değer bulunmamalıdır veya analiz öncesi kontrol edilmelidir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Karışık ölçümlerde iki yönlü ANOVA testi için SciPy kütüphanesinde hazır bir fonksiyon ya da sınıf yoktur. Ancak statsmodels 
    kütüphanesinde statsmodels.stats.anova modülünde bulunan anova_lm fonksiyonu ile bu işlem yapılabilmektedir. Örneğin 
    yukarıdaki terapi deneyinden elde edilen Beck 
    Depresyon Ölçeği puanları şöyle olsun:

    import pandas as pd

    df = pd.DataFrame({
        'Denek': ['D1', 'D2', 'D3', 'D4', 'D5',    # BDT grubu
                'D6', 'D7', 'D8', 'D9', 'D10',   # AYT grubu
                'D11', 'D12', 'D13', 'D14', 'D15'], # ŞT grubu
        
        'Terapi': ['BDT', 'BDT', 'BDT', 'BDT', 'BDT',
                'AYT', 'AYT', 'AYT', 'AYT', 'AYT',
                'ST', 'ST', 'ST', 'ST', 'ST'],
        
        'OnTest': [32, 30, 28, 31, 29,    # BDT öntest
                31, 29, 30, 32, 28,    # AYT öntest
                30, 31, 29, 32, 28],   # ŞT öntest
        
        'SonTest': [14, 12, 10, 13, 11,   # BDT sontest (çok iyileşme!)
                    23, 21, 22, 24, 20,   # AYT sontest (az iyileşme)
                    17, 15, 16, 18, 14]   # ŞT sontest (orta iyileşme)
    })

    Bu DataFrame nesnesi aşağıdaki gibi bir görünüme sahiptir:

       Denek Terapi  OnTest  SonTest
    0     D1    BDT      32       14
    1     D2    BDT      30       12
    2     D3    BDT      28       10
    3     D4    BDT      31       13
    4     D5    BDT      29       11
    5     D6    AYT      31       23
    6     D7    AYT      29       21
    7     D8    AYT      30       22
    8     D9    AYT      32       24
    9    D10    AYT      28       20
    10   D11     ST      30       17
    11   D12     ST      31       15
    12   D13     ST      29       16
    13   D14     ST      32       18
    14   D15     ST      28       14

    Anımsanacağı gibi statsmodels kütüphanesindeki AnovaRM sınıfı bizden melt edilmiş DataFrame nesnesi istemektedir. melt 
    işlemini şöyle yapabiliriz:

    melted_df = pd.melt(
        df,
        id_vars=['Denek', 'Terapi'],
        value_vars=['OnTest', 'SonTest'],
        var_name='Zaman',
        value_name='BDI'
    )

    Buradaki melted_df DataFrame nesnesi de aşağıdaki gibi bir görünüme sahiptir:

    melted_df = pd.melt(
        df,
        id_vars=['Denek', 'Terapi'],
        value_vars=['OnTest', 'SonTest'],
        var_name='Zaman',
        value_name='BDÖ'
    )

       Denek Terapi    Zaman  BDÖ
    0     D1    BDT   OnTest   32
    1     D2    BDT   OnTest   30
    2     D3    BDT   OnTest   28
    3     D4    BDT   OnTest   31
    4     D5    BDT   OnTest   29
    5     D6    AYT   OnTest   31
    6     D7    AYT   OnTest   29
    7     D8    AYT   OnTest   30
    8     D9    AYT   OnTest   32
    9    D10    AYT   OnTest   28
    10   D11     ST   OnTest   30
    11   D12     ST   OnTest   31
    12   D13     ST   OnTest   29
    13   D14     ST   OnTest   32
    14   D15     ST   OnTest   28
    15    D1    BDT  SonTest   14
    16    D2    BDT  SonTest   12
    17    D3    BDT  SonTest   10
    18    D4    BDT  SonTest   13
    19    D5    BDT  SonTest   11
    20    D6    AYT  SonTest   23
    21    D7    AYT  SonTest   21
    22    D8    AYT  SonTest   22
    23    D9    AYT  SonTest   24
    24   D10    AYT  SonTest   20
    25   D11     ST  SonTest   17
    26   D12     ST  SonTest   15
    27   D13     ST  SonTest   16
    28   D14     ST  SonTest   18
    29   D15     ST  SonTest   14

    Artık karışık ölümlerde ANOVA testini aşağıdaki gibi uygulayabiliriz:

    from statsmodels.formula.api import ols
    from statsmodels.stats.anova import anova_lm

    model = ols('BDÖ ~ C(Terapi) + C(Zaman) + C(Terapi):C(Zaman) + C(Denek)', 
                data=melted_df).fit()
    anova_table = anova_lm(model, typ=2)

    Buradan yine bir ANOVA tablosu elde edilmektedir. Yuarıdaki çözümden elde edilen NAOVA tablosu şöyledir:

                             sum_sq    df            F        PR(>F)
    C(Terapi)             38.875502   2.0    77.751004  1.351971e-07
    C(Zaman)            1333.333333   1.0  5333.333333  2.890474e-17
    C(Denek)            1557.000000  14.0   444.857143  4.627576e-14
    C(Terapi):C(Zaman)   126.666667   2.0   253.333333  1.533760e-10
    Residual               3.000000  12.0          NaN           NaN

    Bu ANOVA tablosundaki F değerlerini çok yüksek P değerlerinin ise çok düşük olduğunu görüyorsunuz. Buradan şu sonuçlar
    çıkartılabilir:

    C(Zaman) satırına baktığımızda buradagi F değerinin çok büyük P'nin ise çok küçük olduğunu görüyoruz. Bu durum
    "hangi tedavi terapi yöntemi olursa olsun, genel olarak bunun olumlu etkisinin olduğu" anlamına geşmektedir. Burada 
    C(Terapi):C(Zaman) etkileşiminde yine F değeri çok yüksek P değeri ise çok düşüktür. Yani terapiler arasında öntest-son 
    test bağlamında farklılıklar vardır. (Başka bir deyişlem bu terapi yöntemleri aynı etkiye sahip değildir.) Tabii hangi 
    terapi yünteminin daha iyi bir etkiye sahip olduğunu anlamak için post-hoc testler kullanılmalıdır. Burada "Tukey HSD"
    testi önerbilir. 

    Bıu örnekte bir uyarıda bulunmak istiyoruz. Yukarıdaki test puanları tamamen rastgele oluşturulmuştur. Yani bu gerçek 
    bir araştırmadan elde edilen puanlar değildir. Bu nedenle bu veriler üzerinde terapi yöntemlerini kıyaslamayınız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            166. Ders - 07/12/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz iki yönlü ANOVA testlerini "karışık ölçümlerde iki yönlü ANOVA" ve "tekrarlı ölçümlerde iki yönlü ANOVA" olmak üzere
    ikiye ayırmıştık. Şimdi tekrarlı ölçümlerde iki yönlü ANOVA testini ele alacağız. Tekrarlı ölçümlerde iki yönlü ANOVA 
    testi her deneğin her ölçüme katılmasıyla oluşan nispeten az kullanılan bir testtir. Örneğin bu testte iki faktörü aşağıdaki
    gibi matrisel bir biçimde göstermiş olalım:

        B1  B2
    A1  X   X 
    A2  X   X
    A3  X   X

    Karşık ölçümlerde iki yönlü ANOVA testinde yukarıdaki matriste her denek A1, A2 ya da A3'e ilişkin satırlarda ölçülmektedir. 
    Ancak tekrarlı ölçümlerde iki yönlü ANOVA'da her denek bu hücrelerin hepsinde ölçülmektedir. Dolayısıyla bu tür deney 
    kalıpları ile seyrek bir biçimde karşılaşılmaktadır. 

    Örneğin Uyku yoksunluğu süreleri boyunca bilişsel performansın düşüşünü ve kafeinin bu düşüşü azaltıp azaltmadığını 
    incelemek isteyelim. Bunun aynı deneklere aynı dikkat testini tüm koşullarda tekrar tekrar uygulayalım. Fakötörlere ilişkin
    matrisimiz şöyle olsun:
                         Kalkar Kalkmaz Kalktıktan 6 saat sonra   Kalktıktan 12 sonra    Kalktıktan 18 saat sonra 
    0 Kafein                    X               X                       X                       X
    100 mg Kafein               X               X                       X                       X
    200 mg Kafein               X               X                       X                       X

    Burada her denek bu matrisin her hücresi için ölçülmektedir. Bunun karşık ölçümlerde iki yönlü ANOVA testinden farklılığına 
    dikkat ediniz. Karşışık ölçümlerde iki yönlü ANOVA testimde 0 kafein alanlar, 100 mg kafein alanlar, 200 mg kafein alanlar 
    farklı kişilerdir. Ancak burada deneye katılan herkese her koşul uygulanmaktadır. Burada X'lerle gösterilen değerler 
    dikkat testinden alınan puanlardır. 

    Tekrarlı ölçümlerde ANOVA testinin hipoezleri şöyle oluşturulmaktadır:

    Ana Etki Hipotezi — Faktör A (satırlar) için

    H₀A: μ_{A1} = μ_{A2} = μ_{A3}
    ⇒ Faktör A'nın (örn. kafein miktarının) ortalamalar üzerinde hiçbir etkisi yoktur.

    H₁A: En az bir μ_{Ai} farklıdır
    ⇒ Faktör A'nın istatistiksel olarak anlamlı bir etkisi vardır.

    Bu hipotezde satırlara ilişkin farklılıklar var mı diye bakılmaktadır. Ancak sütundaki değerlerin hepsi aynı kümeye 
    dahil edilmektedir. Yani örneğimizde satırların sütun farkı gözetilmeksizin ortalamaları karşılaştırılmaktadır. 

    Ana Etki Hipotezi — Faktör B (sütunlar) için

    H₀B: μ_{B1} = μ_{B2} = μ_{B3}
    ⇒ Faktör B'nin (örn. uyku yoksunluğu süresinin) ortalamalar üzerinde etkisi yoktur.

    H₁B: En az bir μ_{Bj} farklıdır
    ⇒ Faktör B’nin istatistiksel olarak anlamlı bir etkisi vardır.

    Burada da sütunlar arasında ortlama bakımından bir farklılığın olup olmadığı sorgulanmaktadır. Benzer biçimde bir sütunlardaki
    tüm satırsal değerler aynı kümenin elemanıymış gibi işleme sokulmaktadır. 

    Etkileşim (Interaction) Hipotezi — A × B

    H₀AB: A ve B faktörlerinin etkileri birbirinden bağımsızdır
    ⇒ Ortalama değişimleri paraleldir, etkileşim yoktur.

    H₁AB: A ve B faktörleri arasında etkileşim vardır
    ⇒ Bir faktörün etkisi diğer faktörün düzeyine göre değişmektedir.

    Bu etkileşim hipotezinde satırlardaki sütunsal değişimlerin hep aynı biçimde mi olup olmadığına bakılmaktadır. Örneğin
    “Kafeinin etkisi uyku yoksunluğu düzeyine göre değişiyor mu?” gibi sorunun yanıtı bu hipotezle test edilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tekrarlı ölçümlerde iki yönlü ANOVA testi statsmodels kütüphanesindeki AnovaRM sınıfı ile yapılabilmektedir. Biz daha 
    önce AnovaRM sınıfını bağımısz örneklemlerde tek yönlü ANOVA'da uygulamıştık. Kullanım benzerdir. Önce AnovARM nesnesi 
    yaratılır. Sonra fit işlemi yapılır. AnovaRM nesnesi yaratılırken within parametresine iki faktör de girilmelidir. 
    (Anımsanacağı gibi tek yönlü ANOVA'da biz buraya tek bir faktörü giriyorduk.) Yukarıdaki kafein örneğimiz için her deneğin
    matrisin her elemanından elde ettiği dikkat puanları şöyle olsun:

    df = pd.DataFrame({
        'Denek': ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10'],
        
        # 0 mg Kafein
        '0mg_Kalkar': [85, 82, 88, 79, 91, 84, 87, 83, 90, 86],
        '0mg_6saat': [78, 75, 81, 72, 84, 77, 80, 76, 83, 79],
        '0mg_12saat': [68, 65, 71, 62, 74, 67, 70, 66, 73, 69],
        '0mg_18saat': [55, 52, 58, 49, 61, 54, 57, 53, 60, 56],
        
        # 100 mg Kafein
        '100mg_Kalkar': [87, 84, 90, 81, 93, 86, 89, 85, 92, 88],
        '100mg_6saat': [83, 80, 86, 77, 89, 82, 85, 81, 88, 84],
        '100mg_12saat': [76, 73, 79, 70, 82, 75, 78, 74, 81, 77],
        '100mg_18saat': [67, 64, 70, 61, 73, 66, 69, 65, 72, 68],
        
        # 200 mg Kafein
        '200mg_Kalkar': [89, 86, 92, 83, 95, 88, 91, 87, 94, 90],
        '200mg_6saat': [87, 84, 90, 81, 93, 86, 89, 85, 92, 88],
        '200mg_12saat': [82, 79, 85, 76, 88, 81, 84, 80, 87, 83],
        '200mg_18saat': [74, 71, 77, 68, 80, 73, 76, 72, 79, 75]
    })

    Bu DataFrame nesnesini her satırda tek bir ölçüm değeri olacak biçimde melt edelim:

    melted_df = df.melt(id_vars='Denek', var_name='Kosul', value_name='Dikkat_Puani')
    melted_df[['Kafein', 'Zaman']] = melted_df['Kosul'].str.split('_', n=1, expand=True)

    Şimdi artık AnovaRM nesnesimizi oluşturabiliriz:

    result = AnovaRM(data=melted_df, depvar='Dikkat_Puani', subject='Denek', within=['Kafein', 'Zaman']).fit()

    Buradan elde edilen AnovaResults nesnesinin anova_table özniteliği bize sonuçları bir DataFrame olarak vermektedir. 
    Örneğin:

                       F Value  Num DF  Den DF  Pr > F
    Kafein       -2.266827e+29     2.0    18.0     1.0
    Zaman        -1.019083e+30     3.0    27.0     1.0
    Kafein:Zaman  8.332066e+27     6.0    54.0     0.0

    AnovaResults sınıfının summary metodu bize test sonucunu bir str nesnesi olarka vermektedir. Örneğin:

                                   Anova
    ========================================================================
                                F Value                Num DF  Den DF Pr > F
    ------------------------------------------------------------------------
    Kafein        -226682671068001679853146341376.0000 2.0000 18.0000 1.0000
    Zaman        -1019082738771153603930651361280.0000 3.0000 27.0000 1.0000
    Kafein:Zaman     8332065609378057509266259968.0000 6.0000 54.0000 0.0000
    ========================================================================

    Burada çıkan sonuçlar şöyledir:

    - Kafein dozunun (0 mg, 100 mg, 200 mg) dikkat puanları üzerinde anlamlı bir etkisi yoktur.
    - Uyanıklık süresinin (kalkar kalkmaz, 6, 12, 18 saat sonra) dikkat puanları üzerinde anlamlı bir etkisi yoktur.
    - Kafeinin dikkat puanları üzerindeki etkisi, zaman noktalarına göre değişmez (etkileşim yoktur).

    Tabii burada gerçek bir araştırma verisi üzerinde bu sonuçlar çıkartılmamıştır. Tamamen rastgele üretilen veriler üzerinde 
    bu sonuçlar çıkartılmıştır. Buradaki sonuçları yanlışlıkla bir araşırma sonucuymuş gibi yorumlamayınız. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from statsmodels.stats.anova import AnovaRM

# Veriyi açıkça oluştur
df = pd.DataFrame({
    'Denek': ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10'],
    
    # 0 mg Kafein
    '0mg_Kalkar': [85, 82, 88, 79, 91, 84, 87, 83, 90, 86],
    '0mg_6saat': [78, 75, 81, 72, 84, 77, 80, 76, 83, 79],
    '0mg_12saat': [68, 65, 71, 62, 74, 67, 70, 66, 73, 69],
    '0mg_18saat': [55, 52, 58, 49, 61, 54, 57, 53, 60, 56],
    
    # 100 mg Kafein
    '100mg_Kalkar': [87, 84, 90, 81, 93, 86, 89, 85, 92, 88],
    '100mg_6saat': [83, 80, 86, 77, 89, 82, 85, 81, 88, 84],
    '100mg_12saat': [76, 73, 79, 70, 82, 75, 78, 74, 81, 77],
    '100mg_18saat': [67, 64, 70, 61, 73, 66, 69, 65, 72, 68],
    
    # 200 mg Kafein
    '200mg_Kalkar': [89, 86, 92, 83, 95, 88, 91, 87, 94, 90],
    '200mg_6saat': [87, 84, 90, 81, 93, 86, 89, 85, 92, 88],
    '200mg_12saat': [82, 79, 85, 76, 88, 81, 84, 80, 87, 83],
    '200mg_18saat': [74, 71, 77, 68, 80, 73, 76, 72, 79, 75]
})


melted_df = df.melt(id_vars='Denek', var_name='Kosul', value_name='Dikkat_Puani')
melted_df[['Kafein', 'Zaman']] = melted_df['Kosul'].str.split('_', n=1, expand=True)

result = AnovaRM(data=melted_df, depvar='Dikkat_Puani', subject='Denek', within=['Kafein', 'Zaman']).fit()
summary = result.summary()
print(summary)

#----------------------------------------------------------------------------------------------------------------------------
                                        167. Ders - 13/12/2025 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Araştırmalarda çok kullanılan diğer bir hipotez testine "kovarsyans analizi ya da kısaca ANCOVA (Analysis of Covariance)" 
    denilmektedir. ANCOVA testinde belli bir faktörün etkisi aynı düzeye getirilerek diğer faktör üzerinde hipotez testi 
    uygulanmaktadır. Örneğin deney öncesinde ve deney sonrasında öntest-sontest ölçümleri yapılmış olsun. Deney öncesindeki 
    öntest puanları birbirinden farklı olacaktır. Biz deney öncesindeki puanları hiç göz önünde bulundurmadan deney sonrasındaki 
    puanları karşılaştıramayız. Bu tür durumlarda sontest ile öntest arasındaki farklara dayalı hipotez testi uygulamak da
    çoğu kez uygun olmamaktadır. Çünkü puanlar arasındaki farklar her zaman değişim için anlamlı olmayabilir. Örneğin bir 
    testten elde edilebilecek maksimum puan 100 olsun. Öntesti 20 olan kişinin deney sonrasındaki sıçraması öntesti 95 
    olanınkinden çok daha fazla olabilecektir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kovaryans analizinde yukarıda da belirttiğimiz gibi bir ya da birden fazla sütun değeri (örneğin öntest) sanki aynı değere 
    getirilerek ANOVA uygulanmaktadır. Bu aynı değere getirme doğrusal regresyonla yapılmaktadır. Dolayısıyla ANCOVA adeta 
    önce doğrusal regresyon sonra ANOVA uygulamaya benzemektedir. Örneğin biz sanki tüm denekler aynı öntest skoruna sahipmiş 
    gibi sontest puanını ayarlarız. Sona da sontest puan ortalamalarını karşılaştırırız. Tabii ölçüm yalnızca öntest ve sontest
    biçiminde iki tane olmak zorunda değildir. Birden fazla terapinin etkinliğinin karşılaştırılması için önce öntest uygulanıp 
    terapi sırasında, terapiden hemen sonra ve terapiden üç ay sonra öçlümler yapılabilir. ANCOVA'da etkisi eşitlenmeye çalışılan 
    özelliğe "kovaryant (covariate)" denilmektedir. Örneğin öntest-sontest deney kalıbında kovaryant öntest olabilir. Kovaryant 
    bir tane olmak zorunda da değildir. Birden fazla sütun kovaryant olabilir. Yani biz tek bir sütunun değil birden fazla sütunun 
    değeri sanki aynıymış gibi hedef sütunu da ayarlamak isteyebiliriz. Örneğin farklı eğitim yönteminin matematik skoru üzeirindeki 
    etkilerini test etmek isteyelim. Öğrencilerin test öncesinde aşağıda belirtilen birden fazla özellikleri biribirinden farklı 
    olabilecektir:

    - Öntest matematik skoru
    - Genel zeka/yetenek testi skoru (genel yetenek)
    - Matematiğe yönelik tutum skoru (motivasyon)
    - Önceki yıl matematik notu (geçmiş performans)
    - Sosyoekonomik düzey (çevresel faktörler)

    Bu faktörler aslında matematik için uygulanan sontest skorunu etkileyebilecektir. İşte biz sanki bu yukarıdaki özelliklerin 
    hepsi aynıymış gibi sontest matematik skorlarını düzeltip hipotez testine sokabiliriz. Bu durumda yukarıdaki özelliklerin 
    hepsi kovaryant durumunda olacaktır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kovaryans analizinde kovaryantı ya da kovaryantları kontrol altına alabilmek için regresyon uygulanmaktadır. Uygulanan 
    regresyonun eğim katsayıları aynı fakat bias değerleri (intercepts) farklı olmaktadır. Örneğin üç farklı matematik eğitim 
    yöntemi için öntest-sontest uygulanmış olsun. Aşağıdaki gibi bir matrisel puanlama elde edilecektir:

                    Öntest Puanı    Sontest Puanı
    A Yöntemi       X               X
    B Yöntemi       X               X
    C Yöntemi       X               X

    Buradaki X'ler grupların aldıkları puanları belirtmektedir. Bu örnekte öntest puanları kovaryant yapıldığında sontest 
    puanları buna göre ayarlanacaktır. Bunun için her yöntem için grubumda öntest puanı verildiğinde sontesti tahmin eden 
    bir regresyon modeli oluşturulur. Bu regresyonda B0 + B1X (X burada öntest puanlarını belirtiyor) doğru denkleminde B1 
    katsayısı her grup için aynıdır ancak B0 değerleri gruplarda farklı olabilmektedir. Eğer kovaryant birden fazla ise bu 
    durumda çoklu doğrusal regresyon uygulanmaktadır. Ancak uyulamada kovaryant sayısının fazla olması çoklu doğrusal 
    regresyondaki hata miktarını artırdığı için arzu edilmemektedir. Örneklem büyüklüğüne de bağlı olarak 2-5 arasındaki 
    kovaryant uygun olabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    ANCOVA hipotez testi için SciPy kütüphanesinde hazır bir sınıf ya da fonksiyon yoktur. statsmodels kütüphanesinde bu 
    test kolaylıkla yapılabilmektedir. Örneğin yukarıdaki örnekte sözünü ettiğimiz matematik eğitimine yönelil üç modeli 
    için aşağıdaki gibi öntest-sontest puanları elde edielmiş olsun:

    df = pd.DataFrame({
        'OgrenciID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 
                     23, 24, 25, 26, 27, 28, 29, 30],
        'Grup': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 
                'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',
                'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],
        'Ontest': [45.3, 52.1, 48.7, 55.2, 42.8, 50.5, 46.9, 53.4, 49.2, 51.6,
                  47.8, 51.3, 49.5, 54.1, 46.2, 52.8, 48.3, 50.7, 53.9, 45.6,
                  49.1, 52.5, 47.3, 51.8, 48.6, 53.2, 46.4, 50.9, 52.3, 48.9],
        'Sontest': [50.2, 58.5, 53.1, 61.3, 47.5, 55.8, 52.4, 59.7, 54.3, 57.1,
                   59.3, 64.2, 61.5, 67.8, 58.1, 65.3, 60.7, 63.4, 66.9, 57.4,
                   68.5, 72.3, 66.8, 71.6, 67.9, 74.1, 65.2, 70.8, 73.5, 68.1]
    })

    Bu DataFrame nesnesini melt edelim:

    melted_df = df.melt(
        id_vars=['OgrenciID', 'Grup'],
        value_vars=['Ontest', 'Sontest'],
        var_name='Test_Turu',
        value_name='Skor'
    )

    Bundan sonra statsmodels kütüphanesinin ols fonksiyonu ile bir model nesnesi oluşturulur ve fit edilir:

    model_anova = ols('Sontest ~ C(Grup)', data=df).fit()

    Buradaki ols fonksiyonunun içerisine yazdığımız model yazısı şu anlama gelmektedir: Burada Grup etiketleri dikkate 
    alındarak sontest puanları ayarlanıp ANOVA uygulanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from statsmodels.formula.api import ols

df = pd.DataFrame({
    'OgrenciID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],
    'Grup': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 
             'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',
             'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],
    'Ontest': [45.3, 52.1, 48.7, 55.2, 42.8, 50.5, 46.9, 53.4, 49.2, 51.6,
               47.8, 51.3, 49.5, 54.1, 46.2, 52.8, 48.3, 50.7, 53.9, 45.6,
               49.1, 52.5, 47.3, 51.8, 48.6, 53.2, 46.4, 50.9, 52.3, 48.9],
    'Sontest': [50.2, 58.5, 53.1, 61.3, 47.5, 55.8, 52.4, 59.7, 54.3, 57.1,
                59.3, 64.2, 61.5, 67.8, 58.1, 65.3, 60.7, 63.4, 66.9, 57.4,
                68.5, 72.3, 66.8, 71.6, 67.9, 74.1, 65.2, 70.8, 73.5, 68.1]
})

melted_df = df.melt(
    id_vars=['OgrenciID', 'Grup'],
    value_vars=['Ontest', 'Sontest'],
    var_name='Test_Turu',
    value_name='Skor'
)

model_anova = ols('Sontest ~ C(Grup)', data=df).fit()

#----------------------------------------------------------------------------------------------------------------------------
                                            168. Ders - 19/12/2025 - Cuma
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de istatistikçe çokça karşılaşılan "ki kare dağılımı (chi-square distribution) ve "ki kare hipotez testleri" üzerinde 
    duracağız. "ki (chi)" ismi Yunanca χ sembolünden gelmektedir. Bu sembol X harfine benzemekle birlikte biraz daha uzun ve 
    el yazısı gibi yazılmaktadır. Yunanca'da Latin alfabesindeki X harfi yoktur. Chi karakteri İngilizce'de genellikle "kay"
    ya da "ki" biçiminde okunmaktadır. Türkçe'de de genellikle istatistik alanından gelenler "kay" ya da "ki" biçiminde 
    okumaktadır. Biz kursumuzda bu sözcüğü "ki" olarak telaffuz edeceğiz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Ki kare dağılımı "birden fazla standart normal dağılıma sahip rassal değişkenin karelerinin toplamı" biçimindedir. 
   Matematiksel olarak dağılımı şöyle ifade edebiliriz:

   χ² = Z₁² + Z₂² + ... + Zₖ²

   Burada  Z₁, Z₂, ..., Zₖ bağımsız standart normal dağılımlara sahip rastgele değişkenlerdir. Kare toplamı söz konusu 
   olduğu için zaten dağılıma "ki kare dağılımı" denilmiştir. Tabii buradaki rassal değişkenler normal dağılmış ama standardize
   edilmemişse önce onların standardize edilmesi gerekmektedir. Örneğin standart normal dağılmış üç rassal değişkenden elde
   edilen değerler şöyle olsun:

    Z₁ = 1.2
    Z₂ = -0.5
    Z₃ = 0.8

    Burada "ki kare" değeri şöyle hesaplanmaktadır:

    Z₁² = (1.2)² = 1.44
    Z₂² = (-0.5)² = 0.25
    Z₃² = (0.8)² = 0.64

    χ² = 1.44 + 0.25 + 0.64 = 2.33

    Ki kare dağılımında kareler toplandığı için dağılımın grafiği her zaman Y eksenine soldan yaslanmış durumdadır. 
    Dağılımın grafiği normal dağılıma benzemekle birlikte sağdan çarpıktır. Ki kare dağılımında da "serbestlik derecesi
    (degrees of freedom)" vardır. Serbestlik derecesi kareleri alınan rassal değişkenlerin sayısının bir eksiğidir. 
    Yukarıdaki örnekte serbestlik derecesi 3 - 1 = 2'dir. Serbest derecesi büyüdükçe eğri daha simetrik hale gelir ve 
    standart normal dağılıma benzer. Yani tıpkı t dağılımında olduğu gibi her serbestik derecesi için ayrı bir ki kare 
    eğrisi vardır.  

    Ki kare dağılımının olasılık yoğunluk fonksiyonu şöyledir:

    f(x) = (1 / (2^(k/2) · Γ(k/2))) · x^(k/2 - 1) · e^(-x/2)

    Buradaki Γ sembolü "gama fonksiyonunu" belirtmektedir. Gama fonksiyonu şöyledir:

    Γ(z) = ∫₀^∞ t^(z-1) · e^(-t) dt

    Ancak eğer n pozitif bir sayı ise yukarıdaki integral şu hale gelmektedir:

    Γ(n) = (n - 1)!

    SciPy kütüphanesi içerisindesinde scipy.stats modülünde bulunan chi2 isimli singleton nesne ki kare dağılımı ile ilgili 
    işlemleri yapmak için bulundurulmuştur. chi2 nesnesinin kullanımı diğer dağılım nesneslerine benzemektedir. Nesneye ilişkin 
    sınıfın önemli metotları şunlardır:

    pdf: Olasılık yoğunluk fonksiyonu değerini verir. Parametrik yapısı şöyledir:

    pdf(x, df, loc=0, scale=1)

    Burada df serbestlik derecesini, loc ortalamayı ve scale de standart sapmayı belirtmektedir. 

    cdf fonksiyonu belli bir değere kadarki kümülatif olasılığı elde etmekte kullanılmaktadır. Fonksiyonun parametrik yapısı 
    şöyledir:

    cdf(x, df, loc=0, scale=1)

    ppf metodu ise cdf fonksiyonun tersini yapmaktadır:

    ppf(q, df, loc=0, scale=1)

    chi2 nesnesi ile kullanabileceğimiz diğer önemli metotlar için SciPy dokümanlarına başvurabilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aşağıda 3, 5, 7, 9, 11, 13 serbestlik dereceleri için ki kare olasılık yoğunluk fonksiyonları çizdirilmiştir. Burada 
    serbestlik derecesi yükseldikçe ortalamanın sağa doğru kaydığına ve gitgide eğrinin normal dağılım eğrisine benzediğine
    dikkat ediniz.  
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2

x = np.linspace(0, 20, 1000)

plt.title('Chi-Sequare PDF Graph (DOF = 5')
for df in range(3, 15, 2):
    y = chi2.pdf(x, df=df)
    plt.plot(x, y)
plt.legend([f'df = {str(k)}' for k in range(3, 15, 2)])
plt.show()

#----------------------------------------------------------------------------------------------------------------------------
    Biz merkezi limit teoremini ele aldığımız ilk bölümlerde bir anakütleden seçilen belli büyüklükteki örnek ortalamalarının 
    normal dağılma eğiliminde olduğunu görmüştük. İşte örneklem varyasnları da kik kare dağılma eğilimindedir. Örneğin biz 
    bir anakütleden seçilen n e elemanlı örneklerin varyanslarını hesaplayıp bunun grafiğini (histogramını) çizdiğimizde 
    ki kare dağılımına ilişkin bir grafik görürüz. Anakütleden alınan örnek ortalamaları normal dağılırken örnek varyansları 
    ki kare dağılmaktadır. 

    Biz yukarıda ki kare dağılımını standart normal dağılmış k tane değişkenin karelerinin toplamı biçiminde tanımlamıştık:

    χ² = Z₁² + Z₂² + ... + Zₖ²

    İşte aslında örneklem varyansı da aslında yukarıdaki formüle benzer bir hale gelmektedir. Örneklem varyanslarının dağılımının
    yukarıdaki biçimle nasıl ifade edilebildiğini başka kaynaklara ya da LLM'lere başvurarak öğrenebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Ki kare dağılımıyla ilgili üç temel hipotez testi bulunmaktadır:

    1) Ki Kare Uyum İyiliği Testi (Chi-Square Goodness-of-Fit Test)
    2) Ki Kare Bağımsızlık Testi (Chi-Square Test of Independence)
    3) Ki Kare Homojenlik Testi (Chi-Square Test of Homogeneity)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Önce "ki kare uyum iyiliği testini" ele almak istiyoruz. Biz yukarıda ki kare değerlerinin standart normal dağılma sahip 
    rassal değişkenlerin karelerinin toplamı biçiminde tanımlamıştık:

    χ² = Z₁² + Z₂² + ... + Zₖ²

    Standart normal dağılımda ortalamanın 0, standart sapmanın 1 olduğunu anımsayınız. 
    
    Ki kare uyum iyiliği testinde elimizde "gözlenen frekeans değerleri ve beklenen frekans değerleri" vardır. Biz de bu testte 
    gözlenen frekans değerlerin beklenen frekans değerleri gibi olup olmadığına belli bir anlamlılık düzeyinde karar veririz. 
    Ki kare uyum iyiliği testinin hipotezleri şöyledir:

    H₀ Hipotezi: Gözlenen frekanslar (Oᵢ), beklenen frekanslara (Eᵢ) uygundur. Bunu sembollerle şöyle gösterebiliriz:

    H₀: Oᵢ = Eᵢ (tüm i kategorileri için)
    H₀: pᵢ = pᵢ⁽⁰⁾

    H₁ Hipotezi: En az bir kategori için gözlenen frekanslar, beklenen frekanslardan farklıdır.

    H₁: ∃ i öyle ki Oᵢ ≠ Eᵢ
    H₁: ∃ i öyle ki pᵢ ≠ pᵢ⁽⁰⁾

    Elimizde gözlenen değerlerin frekanslarıyla bunların beklenen değerlerinin (ortlamalarının) olduğunu varsayalım. Burada 
    ilgili olayın gözlenen değerinin frekansının poission dağılımına uygun olduğunu kabul edelim. Bu durumda gözlenen frekans 
    değerlerinin standartlaştırılmış halinin karesini şöyle ifade edebiliriz. 

    Z_i² = [(O_i - E_i) / σ]² = (O_i - E_i)² /  σ²

    Poisson dağılımda "varyans = ortalama = λ" biçiminde olduğunu anımsayınız. Bu durumda eşitlik yaklaşık aşağıdaki 
    duruma gelmektedir:

    Z_i² = (O_i - E_i)² / E_i

    Burada şöyle bir duruma gelmekteyiz: Elimizde gözlenen frekans değerleri ve bunlara ilişkin beklenen değerler varsa ve bu 
    gözlenen frekans değerleri poisson dağılıyorsa χ² değeri şöyle hesaplanabilmektedir:

   χ² = Σ [(O_i - E_i)² / E_i]
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi "ki kare uyum iyiliği" hipotez testi gözlenen frekans değerlerinin beklnene frekans değerleri
    ile uyumlu olup olmadığını belirlemek için kullanılmaktadır. Tabii frekans terimini kullandığımıza göre burada frekansı 
    hesaplanacak öğelerin kategorik ölçeğe ilişkin olması gerekmektedir. Yani bu testte gözlenen değişkenler kategorik 
    değişkenlerdir. 
    
    Örneğin bir zarın adil olup olmadığını istatistiksel olarak test etmek için "ki kare uyum iyiliği" testi kullanılabilir. 
    Bu durumda deneyi yapan kişi belli sayıda (örneğin 600 kez) zar atar. Burada zarın her yüzü için beklenen değer 100'dür.
    Ancak deneyde muhtemelen zarın her yüzü için değer 100 olmayacaktır, fakat 100'ün etrafında değerlerler elde edilecektir. 
    Örneğin zar atımından elde edilen sıklık sayıları aşağıdaki gibi elde edilmiş olsun:

    ┌──────────┬────────────────┬───────────────┐
    │ Zar Yüzü │ Gözlenen (Oᵢ)  │ Beklenen (Eᵢ) │
    ├──────────┼────────────────┼───────────────┤
    │    1     │      98        │     100       │
    │    2     │     103        │     100       │
    │    3     │      97        │     100       │
    │    4     │     102        │     100       │
    │    5     │      95        │     100       │
    │    6     │     105        │     100       │
    └──────────┴────────────────┴───────────────┘

    Şimdi bu frekansların poisson dağıldığını varsayalım (doğadaki kesikli frekans dağılımları genel olarak poisson dağılımına 
    uymaktadır.) Bu durmda artık χ² değerini şöyle hesaplayabiliriz:

    χ² = Σ [(O_i - E_i)² / E_i]

    ┌──────────┬─────┬─────┬──────────────┬──────────────────┐
    │ Zar Yüzü │ Oᵢ  │ Eᵢ  │ (Oᵢ - Eᵢ)²   │ (Oᵢ - Eᵢ)²/Eᵢ    │
    ├──────────┼─────┼─────┼──────────────┼──────────────────┤
    │    1     │  98 │ 100 │      4       │    4/100 = 0.04  │
    │    2     │ 103 │ 100 │      9       │    9/100 = 0.09  │
    │    3     │  97 │ 100 │      9       │    9/100 = 0.09  │
    │    4     │ 102 │ 100 │      4       │    4/100 = 0.04  │
    │    5     │  95 │ 100 │     25       │   25/100 = 0.25  │
    │    6     │ 105 │ 100 │     25       │   25/100 = 0.25  │
    └──────────┴─────┴─────┴──────────────┴──────────────────┘

    χ² = 0.04 + 0.09 + 0.09 + 0.04 + 0.25 + 0.25 = 0.76

    Peki χ² değerini hesapladıktan sonra ne yapacağız? İşte elimizde yine bir alfa değerinin bulunuyor olması gerekir. Biz 
    α değerinin 0.05 alındığını varsayalım. 0.76'nın ki kare değeri için p olasılığı 0.98'dir. Bu durumda p > α için H0 hipotezi 
    reddedilmez. Yani sonuç olarak 0.05 düzeyinde zarın adil olmadığına yönelik bir kanıt elde edilememiştir. 

    Biz bu örnekte bir zarın adil olup olmadığını belli bir anlam düzeyi ile istatistiksel olarak test etmiş olduk. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Ki kare uyum iyiliği testi SciPy kütüphanesindeki scipy.stats modülünde bulunan chisquare fonksiyonuyla yapılabilmektedir. 
    Bu fonksiyona biz gözlenen değer frekansları ile beklenen değer frekanslarını veririz. Fonksiyon da bize ki kare istatistiği 
    ile p değerini verir. Örneğin:

    from scipy.stats import chisquare
    import numpy as np

    observed = np.array([98, 103, 97, 102, 95, 105])
    expected = np.array([100, 100, 100, 100, 100, 100])

    chi2_statistic, p_value = chisquare(f_obs=observed, f_exp=expected)
    print(chi2_statistic)
    print(p_value)

    Buradan şöyle bir çıktı elde edilmektedir:

    0.76
    0.9795142086400531

    Buradaki p değeri belirlediğimiz α = 0.05 değerinden daha büyük olduğu için H0 hipotezi reddedilmemektedir. 
#----------------------------------------------------------------------------------------------------------------------------

from scipy.stats import chisquare
import numpy as np

observed = np.array([98, 103, 97, 102, 95, 105])
expected = np.array([100, 100, 100, 100, 100, 100])

chi2_statistic, p_value = chisquare(f_obs=observed, f_exp=expected)
print(chi2_statistic)
print(p_value)

#----------------------------------------------------------------------------------------------------------------------------
                                            169. Ders - 20/12/2025 - Cumartesi
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    statsmodels kütüphanesinde ki kare uyum iyiliği testi için statsmodels.stats.gof modülündeki chisquare fonksiyonu 
    bulundurulmştur. Fonksiyonun kullanımı SciPy'dakine oldukça benzemektedir:

    import numpy as np
    from statsmodels.stats.gof import chisquare

    observed = np.array([98, 103, 97, 102, 95, 105])
    expected = np.array([100, 100, 100, 100, 100, 100])

    chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)

    print(chi2_stat)
    print(p_value)
#----------------------------------------------------------------------------------------------------------------------------

    import numpy as np
    from statsmodels.stats.gof import chisquare

    observed = np.array([98, 103, 97, 102, 95, 105])
    expected = np.array([100, 100, 100, 100, 100, 100])

    chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)

    print(chi2_stat)
    print(p_value)

#----------------------------------------------------------------------------------------------------------------------------
    Ki kare uyum iyiliği testi için başka bir örnek daha verelim. Kişilere 200 ms (saniyenin beşte biri) süresince evrensel 
    yüz ifadeleri gösteriliyor olsun. Amaç da bu kişilerin yüz ifadelerini tanıma frekanslarının aynı olup olmadığını tespit 
    etmek olsun. Yani kişiler her yüz ifadesini aynı oranda mı tanıyorlar, yoksa bazı yüz ifadeleri diğerlerinden daha mı 
    fazla tanınıyor? Deneyimizin hipotezlerini şöyle oluşturabiliriz,.

    H₀: Katılımcıların algıladığı yüz ifadeleri eşit olasılıkla dağılmıştır.
    H₁: Algılanan yüz ifadeleri eşit dağılmamıştır.

    Deneklere gösterlen yüz ifadeleri şunlardan oluşyor olsun:

    Mutlu
    Üzgün
    Öfkeli
    Korkulu
    Nötr

    Tabii biz burada gerçek bir deneyin sonuçları üzerinde hipotez testi yapmayacağız. Kendi uydurduğumuz değerleri kullanarak
    hipotez testi yapacağız. Bu nedenle burada elde edilen sonucun hiçbir anlamı yoktur. Uydurulmuş verilerden çıkarılan 
    bu sonuçları ciddiye almayınız.

    ┌───────────┬───────────────┬───────────────┐
    │ İfade     │ Gözlenen (O)  │ Beklenen (E)  │
    ├───────────┼───────────────┼───────────────┤
    │ Mutlu     │ 145           │ 100           │
    │ Üzgün     │  92           │ 100           │
    │ Öfkeli    │  88           │ 100           │
    │ Korkulu   │  75           │ 100           │
    │ Nötr      │ 100           │ 100           │
    └───────────┴───────────────┴───────────────┘

    Şimdi ki kare uyum iyiliği testini uygulayalım:

    import numpy as np
    from scipy.stats import chisquare

    observed = np.array([145, 92, 88, 75, 100])
    expected = np.array([100, 100, 100, 100, 100])

    chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)

    print(chi2_stat)
    print(p_value)

    Buradan şöyle bir sonuç elde edilmiştir:

    28.580000000000002
    9.513478455818174e-06

    Buradkai p değeri  α = 0.05 değerinden daha küçük olduğu için H0 hipotezi reddedilir. Yani "katılımcıların algıladığı yüz 
    ifadelerinin eşit olasılıkla dağıldığına yönelik" bir kanıt bulunamamıştır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Diğer çok kullanılan ki kare testine de "ki kare bağımsızlık (chi-square test of independence)" testi denilmektedir. Bu 
    testin amacı iki olayın birbirindne bağımsız olup olmadığını anlamaktır. Örneğin üç tür psikoterapi yönteminin iyileştirme 
    sıklıkları aşağıdaki gibi elde edilmiş olsun:

    ┌────────────────┬──────────┬─────────────────┬───────────┬──────────┐
    │ Terapi Yöntemi │ İyileşti │ Kısmi İyileşti  │ İyileşmedi│  Toplam  │
    ├────────────────┼──────────┼──────────────── ┼───────────┼──────────┤
    │ BDT            │    45    │      25         │    10     │    80    │
    │ PDT            │    30    │      35         │    15     │    80    │
    │ İnsancıl       │    35    │      30         │    15     │    80    │
    ├────────────────┼──────────┼─────────────────┼───────────┼──────────┤
    │ TOPLAM         │   110    │      90         │    40     │   240    │
    └────────────────┴──────────┴─────────────────┴───────────┴──────────┘

    Burada BDT "Bilişsel Davranışçı Terapi", PDT "Psikodinamik Terapi" anlamına gelmektedir. İnsancıl terpilere İngilizce 
    "Humanistic Therapy" denilmektedir. Bu üç terapiyi alanlar birbirinden farklı kişilerdir. Terapi sonucunda kişiler
    "iyileşti", "kısmi iyileşti" ya da "itileşmedi" biçiminde etiketlenmiştir. Toplam sütunlarında satırsal ve sütunsal 
    toplamlar verilmiştir. Buradaki hipotezler şöyle ifade edilebilir:

    H₀: Terapi yöntemi ile iyileşme durumu arasında ilişki yoktur.
    H₁: Terapi yöntemi ile iyileşme durumu arasında ilişki vardır.

    Tabii yukarıdaki değerler bir araştırmadan alınmamıştır, rastgele uydurulmuştur. Bu değerlerden çıkan sonuç da tamamen 
    uydurma olacaktır. Amacımız "ki kare bağımsızlık testnin" nasıl uygulandığını açıklamaktır. 

    Anımsayacağınız gibi eğer iki olay biribirinden bağımsız ise bu olayların olma olasılığı olasılıkların çaprmımıyla elde 
    edilmektedir:

    P(A ∩ B) = P(A) × P(B)

    Ki kare bağımızlık testinde de aslında bu çıkarım kullanılmaktadır. Yukarıdaki örnekte beklenen değerler şöyle elde 
    edilecektir:

    ┌────────────────┬──────────┬───────────────┬───────────┐
    │ Terapi Yöntemi │ İyileşti │ Kısmi İyileşme│ İyileşmedi│
    ├────────────────┼──────────┼───────────────┼───────────┤
    │ BDT            │  36.67   │     30.00     │  13.33    │
    │ PDT            │  36.67   │     30.00     │  13.33    │
    │ İnsancıl       │  36.67   │     30.00     │  13.33    │
    └────────────────┴──────────┴───────────────┴───────────┘ 

    Buradaki değerler şöyle edilmiştir:

                (Satır Toplamı) × (Sütun Toplamı)
    E[i,j] =  ─────────────────────────────────
                        Genel Toplam

    ┌──────────────────────┬─────────────────────┬──────────┐
    │       Hücre          │      Hesaplama      │  Sonuç   │
    ├──────────────────────┼─────────────────────┼──────────┤
    │ BDT × İyileşti       │ (80 × 110) / 240    │  36.67   │
    │ BDT × Kısmi          │ (80 × 90) / 240     │  30.00   │
    │ BDT × İyileşmedi     │ (80 × 40) / 240     │  13.33   │
    ├──────────────────────┼─────────────────────┼──────────┤
    │ PDT × İyileşti       │ (80 × 110) / 240    │  36.67   │
    │ PDT × Kısmi          │ (80 × 90) / 240     │  30.00   │
    │ PDT × İyileşmedi     │ (80 × 40) / 240     │  13.33   │
    ├──────────────────────┼─────────────────────┼──────────┤
    │ İnsancıl × İyileşti  │ (80 × 110) / 240    │  36.67   │
    │ İnsancıl × Kısmi     │ (80 × 90) / 240     │  30.00   │
    │ İnsancıl × İyileşmedi│ (80 × 40) / 240     │  13.33   │
    └──────────────────────┴─────────────────────┴──────────┘

    Burada neden satır toplamı ile sütun toplamı çarpılıp toplam eleman sayısına bölünmektedir? İşte eğer satır ve sütun 
    olasılıkları biribirinden bağımsızsa yani örneğimizdeki terapi türü ile iyileşme arasında bir ilişki yoksa bu durumda 
    yukarıda açıkladığımız çarpım kuralı devreye girecektir:

    P(Terapi ve İyileşme) = P(Terapi Türü) × P(İyileşme Türü)

    Buradaki P(Terapi Türü) o terapi türündeki kişi sayısının toplam kişi sayısına oranıdır. Benzer biçimde P(İyileşme Türü)
    de o iyileşme türündeki kişi sayısının toplam kişi sayısına oranıdır. Dolayısıyla beklenen değer zaten bu olasılığın 
    kişi sayısı ile çarpımı olduğuna göre beklenen değer eşitliği aşağıdaki duruma gelmektedir:

                  (Satır Toplamı) × (Sütun Toplamı)
    E[i,j] =  ─────────────────────────────────
                        Genel Toplam

    İşte beklenen değerler elde edildikten sonra artık ki kare değeri hesaplanmaktadır. Ki kare değerlerin gözlenen ile gerçekleşen
    değerlerin farklarının karelerinin beklenen değere oranlarının toplamıyla hesaplanıyordu:

    (Gözlenen - Beklenen)²
    χ² = Σ ─────────────────────
                Beklenen

    veya kısaca:

           (O - E)²
    χ² = Σ ────────
            E

    Elimizde örnekte ki kare değeri şöyle olacaktır:

    ┌──────────────────────┬─────┬───────┬────────┬──────────┬───────────┐
    │      Hücre           │  O  │   E   │  O-E   │  (O-E)²  │ (O-E)²/E  │
    ├──────────────────────┼─────┼───────┼────────┼──────────┼───────────┤
    │ BDT × İyileşti       │ 45  │ 36.67 │  8.33  │  69.3889 │  1.8923   │
    │ BDT × Kısmi          │ 25  │ 30.00 │ -5.00  │  25.0000 │  0.8333   │
    │ BDT × İyileşmedi     │ 10  │ 13.33 │ -3.33  │  11.0889 │  0.8318   │
    ├──────────────────────┼─────┼───────┼────────┼──────────┼───────────┤
    │ PDT × İyileşti       │ 30  │ 36.67 │ -6.67  │  44.4889 │  1.2133   │
    │ PDT × Kısmi          │ 35  │ 30.00 │  5.00  │  25.0000 │  0.8333   │
    │ PDT × İyileşmedi     │ 15  │ 13.33 │  1.67  │   2.7889 │  0.2092   │
    ├──────────────────────┼─────┼───────┼────────┼──────────┼───────────┤
    │ İnsancıl × İyileşti  │ 35  │ 36.67 │ -1.67  │   2.7889 │  0.0761   │
    │ İnsancıl × Kısmi     │ 30  │ 30.00 │  0.00  │   0.0000 │  0.0000   │
    │ İnsancıl × İyileşmedi│ 15  │ 13.33 │  1.67  │   2.7889 │  0.2092   │
    └──────────────────────┴─────┴───────┴────────┴──────────┴───────────┘

    χ² = Σ [(O - E)² / E]

    χ² = 1.8923 + 0.8333 + 0.8318 + 1.2133 + 0.8333 + 0.2092 + 0.0761 + 0.0000 + 0.2092 = 6.0985 

    Şimdi de bu değere karşı gelen ki kare dağılımındaki olasılık değerini hesaplayalım. Tabii bizim sebestlik derecesini 
    de tespit etmemiz gerekir. Bu modeldeki serbestlik derecesi "satır sayısı + sütun sayısı - 2" biçimindedir. Yani 
    örneğimizdeki serbestlik derecesi 3 + 3 - 2 = 4'tür. Şimdi 4 serbestlik derecesi için ki kare tablosunda 6.0985 değerine 
    karşı gelen olsaılık değerini elde edelim. Bu değer 0.1923'tür. Bu değer α = 0.05 değeri ile karşılaştırıldığında 
    p > α durumu oluşmaktadır. Bu durumda H0 hipotezi reddedilmez. Yani "terapi yöntemi ile iyileşme durumu arasında ilişki 
    olduğuna yönelik bir kanıt elde edilememiştir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Ki kare bağımsız testin yukarıda manuel bir biçimde gerçekleştirdik. Bu test için SciPy kütüphanesinde scipy.stats modülü 
    içerisinde chi2_contingency isimli bir fonksiyon da bulunduurlmuştur. Fonksiyonun parametrik yapısı şöyledir:

    chi2_contingency(observed, correction=True, lambda_=None, *, method=None)[

    Fonksiyonun birinci parametresi gözlenen değerleri belirtmektedir. Bu parametre için çok boyutlu biir NumPy dizisi argüman 
    olarak girilir. Fonksiyon bize ki kare değerini, p değerini, serbestlik derecesini ve beklenen değerleri dörtlü bir demet 
    biçiminde vermektedir. Örneğin yukarıda manuel olarak gerçekleştirdiğimiz hipotez testini SciPy kütüphanesindeki chi2_contingency
    fonksiyonu ile aşağıdaki gibi yapabiliriz:
    

    observed_values = np.array([
        [45, 25, 10],       # BDT (Bilişsel Davranışçı Terapi)
        [30, 35, 15],       # PDT (Psikodinamik Terapi)
        [35, 30, 15]    # İnsancıl Terapi
    ])
    chi2, p_value, dof, expected_values = chi2_contingency(observed_values)

    Buradan şöyle bir çıktı elde edilmiştir:

    Ki-kare değeri: 6.0985
    p değeri: 0.1919
    serbestlik derecesi: 4
    beklenenen değerler: [[36.66666667 30.         13.33333333]
    [36.66666667 30.         13.33333333]
    [36.66666667 30.         13.33333333]]
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz ki kare hipotez testlerini üç gruba ayırmıştık: Uyum iyiliği testi, bağımsızlık testi ve homojenlik testi. Uyum 
    iyiliği testini ve bağımsızlık testini gördük. Şimdi de homojenlik testi üzerinde açıklamalar yapalım. Aslında ki kare
    homojenlik testinin manuel yürütülmesi (yani hesaplama aşamaları) ki kare bağımsızlık testine oldukça benzemektedir. 
    Ancak ki kare homojenlik testinde odaklanılan yer farklıdır. Bu test birden fazla anaktlenim (grubun) aynı kategorik 
    değişken üzerindeki dağılımlarının homojen (benzer) olup olmadığını test etmek için kullanılmaktadır. Burada homojenlik
    demekle dağılım benzerliği kastedilmektedir. Ki kare homojenlik testinin hipotezleri şöeyledir:

    H₀: Tüm anakütle dağılımları homojendir (aynıdır)
    H₁: En az bir anakütle dağılımı diğerlerinden farklıdır.

    Aşağıda hangi durumlarda hangi ki kare testinin kullanılabileceğine yönelik bir tablo veriyoruz:

    ┌─────────────────────┬──────────────────────────┬─────────────────────────┬──────────────────────────┐
    │ Özellik             │ Uyum İyiliği             │ Bağımsızlık             │ Homojenlik               │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ Değişken sayısı     │ 1 kategorik              │ 2 kategorik             │ 1 kategorik + 1 grup     │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ Araştırma sorusu    │ Dağılım teoriye uyuyor   │ İki değişken ilişkili   │ Gruplar homojen mi?      │
    │                     │ mu?                      │ mi?                     │                          │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ H₀                  │ Gözlenen = Beklenen      │ Değişkenler bağımsız    │ Dağılımlar homojen       │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ Örnekleme           │ Tek örneklem             │ Tek örneklem            │ Grup başına örneklem     │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ Beklenen frekans    │ Teoriden/varsayımdan     │ Marjinal oranlardan     │ Havuzlanmış oranlardan   │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ df                  │ k - 1 - p                │ (r-1)(c-1)              │ (r-1)(c-1)               │
    ├─────────────────────┼──────────────────────────┼─────────────────────────┼──────────────────────────┤
    │ Örnek               │ Zar dengeli mi?          │ Cinsiyet-memnuniyet     │ Şehirlerde parti tercihi │
    └─────────────────────┴──────────────────────────┴─────────────────────────┴──────────────────────────┘

    Örneğin dört üniversitedeki öğrenciler üzerinde stres ölçümü yapılıyor olsun. Öğrenciler streas bakımından üç kategoriye 
    ayrılmış olsun. Aşağıdaki tabloda hangi üniversitenin hangi kategorisinde kaç öğrenci bulunduğu belirtilmektedir:

    ┌────────────────┬──────────────┬─────────────┬──────────────┬─────────┐
    │ Üniversite     │ Düşük Stres  │ Orta Stres  │ Yüksek Stres │ Toplam  │
    ├────────────────┼──────────────┼─────────────┼──────────────┼─────────┤
    │ Üniversite 1   │      32      │     58      │      40      │   130   │
    │ Üniversite 2   │      28      │     65      │      47      │   140   │
    │ Üniversite 3   │      35      │     55      │      35      │   125   │
    │ Üniversite 4   │      25      │     70      │      55      │   150   │
    ├────────────────┼──────────────┼─────────────┼──────────────┼─────────┤
    │ TOPLAM         │     120      │    248      │     177      │   545   │
    └────────────────┴──────────────┴─────────────┴──────────────┴─────────┘

    Burada ki kare homojenlik testi için hipotezleri şöyle oluşturabiliriz:

    H₀: Dört üniversitedeki öğrencilerin stres düzeyi dağılımları birbirinden farklı DEĞİLDİR (homojendir).
    H₁: En az bir üniversitedeki öğrencilerin stres düzeyi dağılımı diğerlerinden FARKLIDIR (homojen değildir).

    Burada üniversitelerde araştırmaya katılan farklı sayıda öğrenciler olduğuna dikkat ediniz. Test edilen şey bu dört 
    üniversitedeki öğrencilerin stres kategorilerine ilişkin dağılımının benzer olup olmadığıdır. Yani bu dört üniversitede de
    "düşük stres", "orta stres" ve "yüksek stres" oranları (dağılımları) birbiri ile aynı mıdır değil midir? Yukarıdaki 
    hipotezleri aşağıdaki gibi de daha ayrıntılı ifade edebiliriz:

    ┌─────────────────────────────────────────────────────────────────┐
    │ H₀: Homojenlik Hipotezi                                         │
    ├─────────────────────────────────────────────────────────────────┤
    │ • Tüm üniversitelerde düşük stres oranı aynıdır                 │
    │ • Tüm üniversitelerde orta stres oranı aynıdır                  │
    │ • Tüm üniversitelerde yüksek stres oranı aynıdır                │
    │                                                                 │
    │ Başka bir deyişle: Üniversite faktörü, öğrencilerin stres       │
    │ düzeyi dağılımını ETKİLEMEZ.                                    │
    └─────────────────────────────────────────────────────────────────┘

    ┌─────────────────────────────────────────────────────────────────┐
    │ H₁: Heterojenlik Hipotezi                                       │
    ├─────────────────────────────────────────────────────────────────┤
    │ • En az bir üniversitede stres düzeyi dağılımı farklıdır        │
    │                                                                 │
    │ Örneğin:                                                        │
    │ - Üniversite 4'te yüksek stresli öğrenci oranı diğerlerinden    │
    │   anlamlı şekilde fazla olabilir                                │
    │ - Veya Üniversite 3'te düşük stresli öğrenci oranı              │
    │   diğerlerinden anlamlı şekilde fazla olabilir                  │
    │                                                                 │
    │ Başka bir deyişle: Üniversite faktörü, öğrencilerin stres       │
    │ düzeyi dağılımını ETKİLER.                                      │
    └─────────────────────────────────────────────────────────────────┘

    Ki kare homojenlik testinin hesaplamaları bağımıszlık testiyle aynı olduğu için yine SciPy'da chi2_contingency fonksiyonu
    kullanılmaktadır. Yukarıdaki test şöyle yapılabilir:

    import pandas as pd
    from scipy.stats import chi2_contingency

    data = {
        'Düşük Stres': [32, 28, 35, 25],
        'Orta Stres': [58, 65, 55, 70],
        'Yüksek Stres': [40, 47, 35, 55]
    }

    universities = ['Üniversite 1', 'Üniversite 2', 'Üniversite 3', 'Üniversite 4']
    df = pd.DataFrame(data, index=universities)
    observed_values = df[['Düşük Stres', 'Orta Stres', 'Yüksek Stres']].values

    chi2, p_value, dof, expected_values = chi2_contingency(observed_values)

    print(f"Ki-kare değeri: {chi2:.4f}")
    print(f"p değeri: {p_value:.4f}")
    print(f"serbestlik derecesi: {dof}")
    print(f"beklenenen değerler: {expected_values}")

    Buradan şöyle bir çıktı elde edilmiştir:

    Ki-kare değeri: 6.5457
    p değeri: 0.3649
    serbestlik derecesi: 6
    beklenenen değerler: [[28.62385321 59.1559633  42.22018349]
    [30.82568807 63.70642202 45.46788991]
    [27.52293578 56.88073394 40.59633028]
    [33.02752294 68.25688073 48.71559633]]

    Bu örneği ki kare bağımıszlık testiyle karşılaştırırsanız aslında veri tablosunun birbirinin transpozesi gibi olduğunu 
    görebilirsiniz. Çünkü bu iki test aynı hesaplama işlemlerini farklı hipotez cümleleriyle gerçekleştirmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Ki kare dağılıyla ilgili olan ve çeşitli hipotez testlerinde karşımıza çıkan önemli bir sürekli dağılıma da F dağılımı
    denilmektedir. F dağılımının ismi, İngiliz istatistikçi ve genetikçi Ronald A. Fisher’a (1890-1962) ithafen verilmitir. 
    Ronald A. Fisher, modern istatistiğin kurucularından biri olarak kabul edilir ve varyans analizi (ANOVA) gibi temel 
    istatistiksel yöntemleri geliştirmiştir.

    F dağılımı, iki ki-kare dağılımının oranından türetilen sürekli bir olasılık dağılımıdır.  Bu dağılım istatistikteki 
    pek çok konuda karşımıza çıkmaktadır. F dağılımıyla ilişkili olan istatistiksel yöntemlerden bazıları şunlardır.

    - İki anakütlenin varyansının karşılaştırılması
    - Varyans analizi (ANOVA)
    - Regresyon analizinin anlamlılık testi
    - Homojenlik testleri
    
    F dağılımı özellikle varyanslarla ilgili çıkarımlar yapmak için kullanılmaktadır.

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    F dağılımı iki ki kare dağılmış rassal değişkenin oranından elde edilmektedir. U ve V bağımsız iki ki-kare rastgele 
    değişkeni olsun:

    • U ∼ χ²ₐ₁  (serbestlik derecesi d₁)
    
    • V ∼ χ²ₐ₂  (serbestlik derecesi d₂)

    F dağılımı  aşağıdaki gibi formülize edilmektedir:

          U / d₁
    F = ──────────
          V / d₂

    F dağılında iki serbestlik derecesinin devreye girdiğine dikkat ediniz. Yukarıdaki eşitlikten de görüldüğü gibi F dağılımı 
    aslında iki normal dağılmış rastgele değişkenin standardize edilmiş varyanslarının oranlarını belirtmektedir. Bu nedenle 
    istatistikte iki dağılımın varyanslarının karşılaştırılması gerektiği durumlarda karşımıza çıkmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Öneğin iki dağılımın varyanslarının homojenliğini (yani aynı olup olmadığını) test etmek isteyelim. Bunun için Levene testi
    denilen bir testin kullanıldığını belirtmiştik. İşte aynı zamanda bu test F dağılımı kullanılarak F testi yoluyla da 
    yapılabilmektedir. Eğer iki dağılımın varyansları aynıysa F değerinin 1 civarında olması gerekir. 
    
    Tek yönlü ANOVA testinde de gruplar arası varyans ile grup içi varyansın karşılaştırıldığını anımsayınız. Burada da F 
    dağılımı kullanılmaktadır. Yani ANOVA işlemi sonucunda bir F değeri elde edilmeketdir. Bu F değeri de F dağılımına sokularak 
    p değeri elde edilmekte ve bu p değeri belirlenen alfa değeri ile karşılaştırılmaktadır. 

    F dağılımı ve F değeri doğrusal regresyonda da karşımıza çıkmaktadır. Doğrusak regresyonda F değeri modelin açıkladığı 
    varyans ile açıklanamayan varyansın oranını belirtmektedir. Eğer bu değer 1 civarında ise bu durum regresyon anlamsız 
    olduğu anlamına gelmektedir. Doğrusal regresyonun anlamlı olabilmesi için F değerinin 1'den büyük olması gerekir. Bu değer
    1'den ne kadar büyükse oluşturulan regresyon o kadar anlamlıdır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    F dağılımına ilişkin işlemler SciPy kütüphanesindeki scipy.stats modülü içerisinde bulunan f isimli singleton nesne ile 
    yapılmaktadır. Bu f nesnesine ilişkin sınııfn pdf, cdf, ppf isimli metotları temel dağılım işlemlerini yapmaktadır. Örneğin:
    F dağılımında iki serbestlik derecesi bulunduğunu anımsayınız. Metotlardaki dfn parametresi paydaki dağılımın serbestlik 
    derecesini dfd paranmetresi ise paydadaki dağılılım serbestlik derecesini belirtmektedir. Ayrıca bu metotların loc ve scale 
    parametreleri de vardır. Bu parametreler F dağılımının X eksininde kaydırıp genişletip daraltmaktadır. Metotların parametrşk 
    yapıları şöyledir:

    pdf(x, dfn, dfd, loc=0, scale=1)
    cdf(x, dfn, dfd, loc=0, scale=1)
    ppf(q, dfn, dfd, loc=0, scale=1)

    Örneğin:

    from scipy.stats import f

    dfn = 2
    dfd = 27

    x = 3.5
    y = f.pdf(x, dfn, dfd)
    print(f'PDF: {y}')

    Burada 2 ve 27 serbestlik dereceleri içim F dağılımından elde edilen değer yazdırılmıştır. Elimizde dfn = 2 ve dfd = 27
    olan F dağılımına ilişkin 5 değeri olsun. Buna karşı gelen küöülatif olasılığı şöyle elde edebiliriz:

    from scipy.stats import f

    dfn = 2
    dfd = 27

    x = 5
    cd_val = f.cdf(x, dfn, dfd)
    print(f'{cd_val}')

    Buradan 0.985 gibi bir değer edilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    F dağılımı varyans oranlarına ilişkin bir dağılımdır. Yukarıda da belirttiğimiz gibi çeşitli istatistiksel süreçlerde 
    farklı anlamlara gelecek biçimde F değerleriyle karşılaşmaktayız. Biz bir F değeri gördüğümüzde bu F değerine konu olan 
    iki varyansın ne olduğunu anlamalıyız ve bu F değerini ona göre yorumlamalıyız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sık kullanılan parametrik olmayan hipotez testlerinden biri de "Mann-Whitney U" testidir. Bu test adeta "bağımısız örneklemler
    t testinin parametrik olmayan biçimi gibidir. Bu test özellikle ana kütlenin normal dağılmadığı, varyans homojenliğinin 
    sağlanmadığı ya da bağımlı değişkenin en az sıralı (ordinal) ölçeğe ilişkin olduğu durumlarda kullanılmaktadır. 

    Mann-Whitney U testinin hipotezleri şöyle oluşturulmaktadır:

    H₀: İki bağımsız grup grup arasında istatistiksel olarak anlamlı bir fark yoktur.
    H₁: İki bağımsız grup arasında istatistiksel olarak anlamlı bir fark vardır.

    Mann-Whitney U testi parametrik olmadığı için ortlama kullanmaz. Aslında testte bir gruptan rastgele seçilen bir gözlemin, 
    diğer gruptan seçilenden büyük olma olasılığı dikkate alınmaktadır. Mann-Whitney U testinin uygulanabileceği için birkaç 
    örnek verelim:

    - Bir araştırmada sınav öncesinde kızlarla erkeklerin sınava yönelik anksiye düzeyleri karşılaştırılmak istenmektedir. 
    Bunun için deneklere 5'li Likert ölçeği uygulanmıştır (1=Hiç, 2=Az, 3=Orta, 4=Yüksek, 5=Çok Yüksek). Araştırmanın hipotezleri 
    şöyledir:

    H₀: Kız ve erkek öğrenciler arasında sınav öncesi anksiyete düzeyleri farklılık göstermemektedir. 
    H₁: Kız ve erkek öğrenciler arasında sınav öncesi anksiyete düzeyleri farklılık göstermektedir.

    - Bilişsel davranışçı terapi ve dinamik terapi alanlarda terapi sıonrasındaki memnuniyet durumları arasında bir fark olup 
    olmadığına yönelik bir araştırma yapılmaktadır. Buradaki bağımsız değişken terapi türüdür. Bağımlı değişken ise terapi 
    sonrasındaki memnuniyet düzeyidir (1=Hiç Memnun Değilim, 2=Az Memnunum, 3=Orta, 4=Memnunum, 5=Çok Memnunum). Araştırmanın 
    hipotezleri şöyle oluşturulabilir:

    H₀: Bilişsel davranışçı terapi ve dinamik terapi alanlarda terapi sıonrasındaki memnuniyet durumları arasında fark yoktur.
    H₁: Bilişsel davranışçı terapi ve dinamik terapi alanlarda terapi sıonrasındaki memnuniyet durumları arasında fark vardır.

    - Başka bir araştırmada sağlık çalışanlarıyla diğer çalışanlar arasında sübjektif uyku kalitesi arasında bir fark olup 
    olmadığı tespit edilmek istenmiştir. Burada bağımlı değişken 4 sıralı kategoriden oluşmaktadır: 1=Çok Kötü, 2=Kötü, 3=Orta, 
    4=İyi, 5=Çok İyi. Hipotezler de şöyledir:

    H₀: Sağlık çalışanları ile diğer çalışanlar arasında uyku kalitesi bakımından fark yoktur.
    H₁: Sağlık çalışanları ile diğer çalışanlar arasında uyku kalitesi bakımından fark vardır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Özellikle psikolojide ve genel olarak sosyaş bilimlerde bazı rassal değişkenlerin "sıralı (ordinal)" ölçeğe mi yoksa 
    "aralıklı (interval)" ölçeğe mi sahip olduğu konusunda gri bölgeler oluşabilmektedir. Örneğin "Beck Depresyon Envanteri"
    denilen ölçekte kişiler her soruya dört seçenekten biri ile yanıt verirler. Sorulardan elde edilmiş puanlar toplanarak 
    toplam bir puan elde edilmektedir. Buradaki toplam puan daha çok aralıklı ölçeğe ilişkin gibi gözükmektedir. Çünkü 
    puanlar arasındaki farklılıklar eşit depresyon farklılıkları olmak zorunda değildir. Yani burada her toplam puan 
    (testte alınabilecek toplam puanlar 0 ile 63 arasındadır) adeta sıralı bir kategori belirtmektedir. Öte yandan bu puanların
    sanki aralık ölçeğindeymiş gibi ele alınmasına rastlanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Mann-Whitney U testinin manuel olarak nasıl yütürüldüğünü açıklayalım. Bu açıklamada şöyle bir araştırmayı 
    kullanacağız: Terapi alanlarla terapi almayanların Beck Depresyon Envanteri puanları elde edilmiş olsun. Terapi alanlar
    3 kişiden alayanlar ise 4 kişiden oluşmaktadır:

    Grup A (Terapi Almayanlar): 28, 32, 25, 30
    Grup B (Terapi Alanlar): 18, 22, 14, 26, 20

    Burada bitki boyları sıralı ölçek gibi ele alınmaktadır. Mann-Whitney U testinde bağımlı değişken aralıklı ölçeklere ve 
    oransal ölçeklere ilişkin olabilir. Ancak bu durumda bunlar yine sanki sıralı verilermiş gibi ele alınmaktadır. 

    1) Önce tüm değerler sıraya dizilir ve bunlara bir sıra numarası (rank) veilir. Örneğin:

    ┌───────────┬──────┬───────────────┐
    │ BDE Puanı │ Grup │ Sıra No (Rank)│
    ├───────────┼──────┼───────────────┤
    │    14     │  B   │       1       │
    │    18     │  B   │       2       │
    │    20     │  B   │       3       │
    │    22     │  B   │       4       │
    │    25     │  A   │       5       │
    │    26     │  B   │       6       │
    │    28     │  A   │       7       │
    │    30     │  A   │       8       │
    │    32     │  A   │       9       │
    └───────────┴──────┴───────────────┘

    Mann-Whitney U testinde ölçek aralıklı olmak zorund aolmadığı için puanlar yerine onların sıraları kullanılmaktadır. 

    2) Bağımısz değişken için (örneğimizdeki "terapi alanlar" ve "terapi almayanlar") sıra toplamları elde edilir:

    R_A (Grup A): 5 + 7 + 8 + 9 = 29
    R_B (Grup B): 1 + 2 + 3 + 4 + 6 = 16

    3) Buradan bağımsız değişkenler için U değerleri hesaplanır. U değerlerinin hesaplama formülü şöyledir:

    U = n₁n₂ + nᵢ(nᵢ + 1)/2 − Rᵢ

    Buradaki n1 ve n2 gruplardaki eleman sayılarını belirtmektedir. Örneğimizde n1 = 3, n2 = 4'tür. Buradaki nᵢ(nᵢ + 1)/2 
    değeri 1'den ilgili grunbun eleman sayısına kadarki sayıların toplamını belirtmektedir. Örneğimizde A grubu için bu değer
    3 * 4 / 2 = 6, B için 4 * 5 / 2 = 10'dur. Formüldeki Ri ilgili grubun sıra toplamını belirtmektedir. Bu durumda A 
    grubunun ve B grubunun U değerleri şöyledir:

    ┌───────────────────────────────────────────────────────────┐
    │              Adım 3: U Değerlerini Hesapla                │
    ├───────────────────────────────────────────────────────────┤
    │                                                           │
    │   • Grup A için (UA):                                     │
    │     UA = (4 * 5) + [4 * (4 + 1) / 2] - 29                 │
    │     UA = 20 + 10 - 29 = 1                                 │
    │                                                           │
    │   • Grup B için (UB):                                     │
    │     UB = (4 * 5) + [5 * (5 + 1) / 2] - 16                 │
    │     UB = 20 + 15 - 16 = 19                                │
    │                                                           │
    ├───────────────────────────────────────────────────────────┤
    │   Sonuç: U = 1 (Küçük olan değer seçilir)                 │
    └───────────────────────────────────────────────────────────┘
        
    Şimdi alfa = 0.05 (Çift Yönlü) için U tablosunda n_1 = 4 ve n_2 = 5$ kesişimine baktığımızda orada 1 değerini görürüz. 
    Örneğimizde 1 <= 1 olduğu için  (p < 0.05) H0 hipotezi reddedilmektedir. Yani gruplar arasında farklılık vardır. 
    alfa = 0.05 için U tablosu şöyledir:

    ┌────────────────────────────────────────────────────────────────────────────┐
    │          MANN-WHITNEY U KRİTİK DEĞERLER TABLOSU (alfa = 0.05)              │
    ├─────────────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┤
    │    n2 \ n1  │  3   │  4   │  5   │  6   │  7   │  8   │  9   │  10  │  11  │
    ├─────────────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┤
    │      3      │  -   │  -   │  0   │  1   │  1   │  2   │  2   │  3   │  3   │
    │      4      │  -   │  0   │  1   │  2   │  3   │  4   │  4   │  5   │  6   │
    │      5      │  0   │  1   │  2   │  3   │  5   │  6   │  7   │  8   │  9   │
    │      6      │  1   │  2   │  3   │  5   │  6   │  8   │  10  │  11  │  13  │
    │      7      │  1   │  3   │  5   │  6   │  8   │  10  │  12  │  14  │  16  │
    │      8      │  2   │  4   │  6   │  8   │  10  │  13  │  15  │  17  │  19  │
    │      9      │  2   │  4   │  7   │  10  │  12  │  15  │  17  │  20  │  23  │
    │      10     │  3   │  5   │  8   │  11  │  14  │  17  │  20  │  23  │  26  │
    └─────────────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘

    Burada 4 ve 5 kesişimlerinde 1 değeri vardır. Karşılaştırmaya elde ettiğimiz en düşük U değeri ile bu 1 değeri solnaktadır. 
    Burada U tablosunun oluşturulması biraz ayrıntılı bilgi gerektirmektedir. Bunu bir örnek üzerinde açıklayalım: Diyelim ki 
    elimizde n1=3 ve n2=2 kişilik iki grup olsun. Toplam 5 veri puanımız vardır. Bu 5 puanın gruplara dağıtılabileceği toplam 
    kombinasyon sayısı şu formülle hesaplanır C(5, 3) = 10. Sonra bu Bu 10 farklı olası kümelerin her biri için tek tek aşağıda
    belirtildiği gibi U değerleri hesaplanır:

    ┌─────────────────────────────────────────────────────────────────────────┐
    │           n1=3, n2=2 İÇİN TÜM OLASI DİZİLİMLER VE U HESABI              │
    ├────────────┬────────────────────┬────────────────────────────┬──────────┤
    │ Senaryo No │ Dizilim (Sıralama) │ B'den Önce Gelen A Sayısı  │ U Değeri │
    ├────────────┼────────────────────┼────────────────────────────┼──────────┤
    │     1      │   A  A  A  B  B    │ (3) + (3)                  │    6     │
    │     2      │   A  A  B  A  B    │ (2) + (3)                  │    5     │
    │     3      │   A  A  B  B  A    │ (2) + (2)                  │    4     │
    │     4      │   A  B  A  A  B    │ (1) + (3)                  │    4     │
    │     5      │   A  B  A  B  A    │ (1) + (2)                  │    3     │
    │     6      │   A  B  B  A  A    │ (1) + (1)                  │    2     │
    │     7      │   B  A  A  A  B    │ (0) + (3)                  │    3     │
    │     8      │   B  A  A  B  A    │ (0) + (2)                  │    2     │
    │     9      │   B  A  B  A  A    │ (0) + (1)                  │    1     │
    │     10     │   B  B  A  A  A    │ (0) + (0)                  │    0     │
    └────────────┴────────────────────┴────────────────────────────┴──────────┘
    
    Burada tablonun üçüncü sütununda her B için ondan önce gelen A toplamı hesaplanmıştır. Örneğin A  B  A  A  B  diziliminde 
    ilk B'den önce gelen A'ların sayısı 1 tane, ikinci B'den önce gelen A'ların sayısı da 3 tanedir. Burada şöyle de 
    düşünebilirsiniz: Sonra gelen önce geleni yenmiştir. Dolayısıyla bu sütun B değerleri kaç kere A değerlerini yenmiştir 
    gibi de yorumlanabilir. Aslında yukarıda açıkladığımız U formülü de aynı hesabı yapmaktadır:

    U = n₁n₂ + nᵢ(nᵢ + 1)/2 − Rᵢ

    Burada n1 * n2 toplam maçların sayısıdır. Yani A grubundaki her eleman B grubundaki elemanla sırasal yarışa sokulsa toplam 
    kaç yarışmanın yapılacağını belirtmektedir. Formüldeki nᵢ * (nᵢ + 1) / 2 değeri ise taban puanı belirtmektedir. Yani ilgili 
    gruptaki elemanlar en düşük sıralarda olsa (1, 2, 3, ...) alabilecekleri minimum değer toplamını belirtmektedir. Buradaki 
    Rᵢ değeri sıra numaralarının toplamıdır. Bu durumda aslında U değerinin kendisi ilgili gruptaki değerlerin diğer gruptaki 
    değerleri yenme sayısıdır. 

    Nihai U tablosunun oluşturulması olasılık hesabıyla yapılmaktadır. Yani önce 1 alfa belirlenir. Sonra bu alfa değerine 
    varılana kadar U değerleri toplanır. Aşağıda bu olasılık tablosunun n1 = 3 ve n2 = 2 için nasıl hazırlandığı verilmiştir: 

    ┌─────────────────────────────────────────────────────────────────────────────┐
    │          U TABLOSU İNŞA SÜRECİ (Alfa = 0.05 Eşiği Nasıl Belirlenir?)        │
    ├─────────┬─────────┬───────────┬────────────┬────────────┬───────────────────┤
    │ Örneklem│ Toplam  │ U Değeri  │ Tekil      │ Kümülatif  │ Tabloya Yazılan   │
    │ (n1,n2) │ Dizilim │ (Sıralama)│ Olasılık   │ Olasılık   │ Kritik Değer      │
    ├─────────┼─────────┼───────────┼────────────┼────────────┼───────────────────┤
    │         │         │     0     │   0.100    │   0.100    │                   │
    │  3 , 2  │   10    │     1     │   0.100    │   0.200    │  HİÇBİRİ (p>0.05) │
    │         │         │    ...    │    ...     │    ...     │   Tablo: " - "    │
    ├─────────┼─────────┼───────────┼────────────┼────────────┼───────────────────┤
    │         │         │     0     │   0.028    │   0.028    │                   │
    │  3 , 5  │   56    │     1     │   0.055    │   0.083    │   U_Kritik = 0    │
    │         │         │     2     │   0.083    │   0.166    │  (Sadece 0.028<0.05)│
    ├─────────┼─────────┼───────────┼────────────┼────────────┼───────────────────┤
    │         │         │     0     │   0.008    │   0.008    │                   │
    │         │         │     1     │   0.016    │   0.024    │   U_Kritik = 2    │
    │  4 , 5  │   126   │     2     │   0.024    │   0.048    │  (0.048 en yakın) │
    │         │         │     3     │   0.040    │   0.088    │  (0.088 > 0.05!)  │
    ├─────────┼─────────┼───────────┼────────────┼────────────┼───────────────────┤
    │         │         │     0     │   0.001    │   0.001    │                   │
    │         │         │    ...    │    ...     │    ...     │   U_Kritik = 8    │
    │  5 , 5  │   252   │     7     │   0.016    │   0.032    │  (8'den sonrası   │
    │         │         │     8     │   0.024    │   0.056    │   0.05'i geçiyor) │
    └─────────┴─────────┴───────────┴────────────┴────────────┴───────────────────┘
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Mann-Whitney U testi SciPy kütüphanesinde scipy.stats modülündeki mannwhitneyu fonksiyonu kullanılmaktadır. Fonksiyonun 
    parametrik yapısı şöyledir:

    mannwhitneyu(x, y, use_continuity=True, alternative='two-sided', axis=0, method='auto', *, 
        nan_policy='propagate', keepdims=False)

    Fonksiyonun ilk iki parametresi iki grubun değerlerini almaktadır. Fonksiyon ikili bir demete geri dönmektedir. Demetin 
    ilk elemanına "U istatistiği" denilmektedir. Bu eleman birinci grubun her elemanının ikinci grubun her elemanına kaç 
    kere üstünlük sağladığını (uyani kaç kere ondan sonra geldiğini) belirtmektedir. Demetin ikinci elemanı p değerini 
    belirtir. Bu p değeri alfa değri ile karşılaştırılır. Eğer p < alfa ise H0 hipotezi reddedilir, değilse H0 hipotezi 
    reddedilmez. Bizim yukarıdaki Beck Depresyon Envanteri örneğimizi bu fonksiyona sokalım:

    from scipy.stats import mannwhitneyu

    a = [25, 28, 30, 32]
    b = [14, 18, 20, 22, 26]

    stat, p_value = mannwhitneyu(a, b)
    print(stat)
    print(p_value)

    Buradan şu çıktı elde edilmektedir:

    19.0
    0.031746031746031744

    p değerinin alfa = 0.05 alfa değerinden küçük olduğunu görüyorsunuz. Bu durumda iki grubun sırasal puanları arasında 
    anlamlı bir farklılık vardır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Kruskal-Wallis H testi üzerinde duralım. Mann-Whitney U testi tipik olarak bağımlı değişkenin "sırasal (ordinal)"
     ölçeğe ilişkn olduğu iki gruplu karşılaştırmalar için kullanılmaktadır. Eğer grup sayısı ikiden fazla ise "Kruskal-Wallis 
     H" testi denilen hipotez testi test tercih edilmektedir. Kruskal-Wallis H testi adeta ANOVA testinin parametrik olmayan 
     biçimi gibidir. Mann-Whitney U testinde iki grup arasındaki farklılık sorgulanırken Kruskal-Wallis H testinde ikiden 
     fazla grup arasındaki farklılıklar sorgulanmaktadır. Kruskal-Wallis H testinin hipotezleri biçimsel biçimde şöyle 
     oluşturulabilir:

    H₀: η₁ = η₂ = η₃ = ... = ηₖ (Tüm grupların medyanları birbirine eşittir; gruplar aynı dağılımdan gelmektedir.)
    H₁: ∃ i, j : ηᵢ ≠ ηⱼ (En az bir grubun medyanı diğerlerinden farklıdır; gruplar arasında anlamlı bir fark vardır.)

    Tabii bu test de bağımısız örneklemler için kullanılmaktadır. Yani grupların üyeleri tamamen birbirinen farklı olmalıdır. 
    Bu testin uygulanmasını gerektirecek birkaç örnek verelim:

    - Örneğin "bilişsel davranışçı terapi (cognitive behavioral therapy), "kabul ve kararlılık terapisi (acceptance and 
    commitment therapy)" ve kontrol grubundan oluşan bir araştırma yapılıyor olsun. Araştırmada sosyal kaygı (social anxiety)
    için bu terapi yöntemleri arasında etki bakımındna bir fark bulunup bulunmadığı tespit edilmek istensin. Burada terapi 
    almış kişilere bir sosyaş kaygı ölçeği uygulanmaktadır. Bu ölçekten alınan puanlar normal dağılmadığı için tek yönlü ANOVA 
    yerine Kruskal-Wallis H testinin uygulanmasına karar verilmiştir. Örneğin kişilerin bu kaygı ölçeğinden aldıkları puanlar 
    şöyle olabilir:

    ┌───────────────────┬───────────────────┬───────────────────┐
    │     BDT Grubu     │     ACT Grubu     │   Kontrol Grubu   │
    ├───────────────────┼───────────────────┼───────────────────┤
    │ Puan: 45 (Sıra: 4)│ Puan: 52 (Sıra: 6)│ Puan: 78 (Sıra:12)│
    │ Puan: 38 (Sıra: 2)│ Puan: 60 (Sıra: 9)│ Puan: 85 (Sıra:15)│
    │ Puan: 42 (Sıra: 3)│ Puan: 55 (Sıra: 7)│ Puan: 82 (Sıra:14)│
    │ Puan: 30 (Sıra: 1)│ Puan: 58 (Sıra: 8)│ Puan: 80 (Sıra:13)│
    │ Puan: 48 (Sıra: 5)│ Puan: 65 (Sıra:10)│ Puan: 75 (Sıra:11)│
    ├───────────────────┼───────────────────┼───────────────────┤
    │ Sıra Toplamı: 15  │ Sıra Toplamı: 40  │ Sıra Toplamı: 65  │
    └───────────────────┴───────────────────┴───────────────────┘

    Görüldüğü gibi burada da alınan puanlar sıralanıp sırasal ilişki hipotez teztine sokulmaktadır. 

    - Örneğin güvenli, kaygılı ve kaçıngan bağlanma stillerine sahip bireylerin özsaygı ölçeğinden aldıkları puanlar 
    arasında fark olup olmadığı incelenmek isteniyor olsun. Özsaygı puanları aşırı uç değerler (outliers) içerdiği için 
    ve normal dağılmadığı için Kruskal-Wallis H testinin uygulanması uygun görülmüştür. Burada da ölçekten alınan puanlar
    birer sıra belirtecek biçimde kullanılmaktadır. Burada özsaygı puanlarının bağlanma stilleri arasında bir farlklılık 
    içerip içermediğine bakılmaktadır.

    - Deneysel bir çalışmada katılımcılar üç gruba ayrılmıştır: Az Uyuyan (4 saat), Normal Uyuyan (8 saat) ve Çok Uyuyan (
     12 saat). Katılımcıların bir dikkat testindeki hata sayıları karşılaştırılmıştır. Hata sayıları tam sayı (count data) 
     olduğu ve normal dağılmadığı için Kruskal-Wallis H testinin uygulanması uygun görülmüştür. Burada dikkat testinden alınan 
     puanların gruplar arasında bir farklılık gösterip gösterilmedeği araştırılmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kruskal-Wallis H testi aslında ANOVA testine oldukça benzemektedir. ANOVA değerler işleme sokulurken bu testte sıra 
    numaraları işleme sokulmaktadır. Yine bu testte de aslında gruplar arası varyand değeri ile grup içi varyans değeri 
    oranlanmaktadır. 

         Gruplar Arası Varyans
    ─────────────────────────────
         Gruplar İçi Varyans

    Grup içi varyans asalında sabitir. Toplam varyans şöyle ifade edilebilir:

    Toplam varyans = Gruplar arası varyans + Grup içi varyans

    Buradaki toplam varyans aslında 1'den N' kadar sayıların varyansı biçimindedir. 1'den N'e kadar sayıların varyansı da 
    (N²-1)/12'dir. (1'den N'e kadar olan sayıların varyasnları için önce 1'den N'e kadar olan sayıların karelerinin toplamına 
    ilişkin formül kullanılır, sonra varyans formülü de kullanılarak bu eşitlik elde edilmektedir. 1'den N'e kadar sayıların 
    varyanslarının neden (N²-1)/12 olduğunu LLM'leri kullanarak anlaytabilirsiniz.) Bu durumda:

    (N²-1)/12 = gruplar arası varyans + grup içi varyans
    
    Kruskal-Wallis H testi manuel olarak aşağıdaki adımlardan geçilerek gerçekleştirilmektedir. Burada biz şöyle bir hipotez 
    testi uygulamaya çalışalım: Üç farklı ders işleme yöntemi sonucunda aynı sınava sokulan kişilerin aldıkları puanlar 
    dikkate alınarak ders işleme yöntemlerinin ders başarısı üzerinde bir farklılık yaratıp taratmadığına bakılmak istenmektedir. 
    Gruplar şöyşe oluşturulmuştur:

    Grup A: Geleneksel Yöntem
    Grup B: Aktif Öğrenme
    Grup C: Çevrimiçi Öğrenme

    Bu gruplardaki kişilerin sınavdan aldıkları puanlar da şöyledir:

    Grup A (Geleneksel)    Grup B (Aktif)    Grup C (Çevrimiçi)
    ─────────────────────  ────────────────  ──────────────────
        65                    78                 72
        70                    85                 68
        68                    82                 75
        72                    90                 70
        66                    88                 73
        69                    80                 71

    Burada sınav puanları aralıklı ölçeğe ilişkin değil sıralı ölçeğe ilişkin değerler gibi ele alınmaktadır. 

    1) Tüm veriler küçükten büyüğe sıraya dizilir. Böylece her katılımcının bir sıra numarası oluşur:

    Sıra No.  Puan  Grup   Sıra Değeri (Rank)
    ────────  ────  ────   ──────────────────
    1           65    A            1
    2           66    A            2
    3           68    A            3.5  (bağ var: 68, 68)
    4           68    C            3.5
    5           69    A            5
    6           70    A            6.5  (bağ var: 70, 70)
    7           70    C            6.5
    8           71    C            8
    9           72    A            9.5  (bağ var: 72, 72)
    10          72    C            9.5
    11          73    C           11
    12          75    C           12
    13          78    B           13
    14          80    B           14
    15          82    B           15
    16          85    B           16
    17          88    B           17
    18          90    B           18

    Burada sıra numarası oluşturulurken aynı değerler varsa onların numaralarının ortalaması alınmaktadır. Örneğin yukarıda 
    tabloda iki tane 68 puan vardır. Bunlar normalde 3'üncü 4'üncü sıradadır. Bunlara farklı sıra numarası atamak uygun olmadığı 
    için (3 + 4) / 2 = 3.5 biçiminde numara atanmıştır. 

    2) Her grubun sıra toplamları elde edilir:

    Grup A Sıraları:
    1 + 2 + 3.5 + 6.5 + 5 + 9.5 = 27.5
    R₁ = 27.5

    Grup B Sıraları:
    13 + 14 + 15 + 16 + 17 + 18 = 93
    R₂ = 93
    
    Grup C Sıraları:
    3.5 + 6.5 + 8 + 9.5 + 11 + 12 = 50.5
    R₃ = 50.5

    3) Her grubun sıra ortalaması elde edilir. Ortalama işlemleri grubun sıra toplamlarının gruptaki eleman sayısına bölümüyle
    elde edilmektedir:

    R̄₁ = R₁/n₁ = 27.5/6 = 4.583
    R̄₂ = R₂/n₂ = 93/6 = 15.500
    R̄₃ = R₃/n₃ = 50.5/6 = 8.417

    4) Artık bu değerlerden hareketle bir H değeri hesaplanır. H formülü şöyledir:

          12          k   R²ᵢ
    H = ─────────  ×  Σ  ──── - 3(N+1)
        N(N+1)        i=1  nᵢ

    Buraki H formülünün elde edilmesi biraz karışıktır. Bunun için başka kaynaklara ya da LLM'lere başvurabilirsiniz. 
    H formülündeki R²ᵢ grupların sıra toplamlarının karesini, nᵢ değeri ise gruplardaki eleman sayısını belşirtmektedir. N ise 
    toplam eleman sayısını belirtmektedir. Şimdi H değerini manule olarak hesaplayalım:   

    Grup A: R₁²/n₁ = (27.5)²/6 = 756.25/6 = 126.042
    Grup B: R₂²/n₂ = (93)²/6 = 8649/6 = 1441.500
    Grup C: R₃²/n₃ = (50.5)²/6 = 2550.25/6 = 425.042

    Toplam: 126.042 + 1441.500 + 425.042 = 1992.584

    Soldaki terimin değeri de şöyle hesaplanabilir:

    12/(N(N+1)) = 12/(18×19) = 12/342 = 0.03509

    Nihai H değeri de şöyle hesaplanmaktadır:

    H değerini hesaplama
    H = 0.03509 × 1992.584 - 3(18+1)
    H = 69.921 - 3(19)
    H = 69.921 - 57
    H = 12.921

    5) Eğer veriler içerisinde birden fazla aynı değer bulunuyorsa (buna "bağ (tie" da denilmektedir)) Bu durumda H değerinin 
    düzeltme faktörüne bölünmesi gerekmektedir:

                                   H
    Düzeltilmiş H değeri =  ─────────────
                            1 - T/(N³-N)

    Burada T değeri de şöyle hesaplanmaktadır:

    T = Σ(t³ - t)

    Örneğimizde düzeltilmiş H değeri şöyle elde edilmeektedir:

    N³ - N = 18³ - 18 = 5832 - 18 = 5814
    1 - T/(N³-N) = 1 - 18/5814 = 1 - 0.003096 = 0.996904
    Düzeltilmiş H değeri = 12.921/0.996904 = 12.961

    H değeri ile düzeltilmiş H değeri arasındaki farkın küçük olduğuna dikkat ediniz.

    6) Artık elimizde H değeri ya da düzeltiş H değeri bulunmaktadır. Artık ki kare tablosuna başvurularak alfa değeir için 
    (tipik olarak alfa = 0.05) H ya da düzeltilmiş H değerinin tablodaki karşılığına bakılarak elde edilen p değeri ile 
    bu alfa değeri karşılaştırılır. Burada serbestlik derecesi toplam grup sayısının 1 eksiği olarak (yani örneğimizde 2)
    alınmalıdır. Örneğimizde α = 0.05 için χ² tablo değeri (df=2): 5.991'dur. Hesaplanan düzeltilmiş H değerinin χ² tablosunda
    karşılık değeri p ≈ 0.00206 biçimindedir. p < α olduğu için H0 hipotezi reddedilmektedir. Yani gruplar arasında medyan 
    bakımından farklılık vardır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kruskal-Wallis H testi SciPy kütüphanesindeki satats modülü içerisinde bulunan kruskal fonksiyonuyla yapılabilmektedir. 
    Fonksiyonun parametrik yapısı şöyledir:

    kruskal(*samples, nan_policy='propagate', axis=0, keepdims=False)

    Fonksiyonun birinci parametresinde * olduğuna dikkat ediniz. Fonksiyon bizden gruplara ilişkin ölçüm değerlerini 
    ayrı argümanlar olarak girmemizi istemektedir. Fonksiyon H değeri ve p değerinden oluşan iki elemanlı bir demetle geri 
    dönmektedir. Örneğin:

    import numpy as np
    from scipy.stats import kruskal

    group_a = np.array([65, 70, 68, 72, 66, 69])     # Geleneksel Yöntem
    group_b = np.array([78, 85, 82, 90, 88, 80])     # Aktif Öğrenme
    group_c = np.array([72, 68, 75, 70, 73, 71])     # Çevrimiçi Öğrenme

    h, p_val = kruskal(group_a, group_b, group_c)
    print(h)
    print(p_val)

    Buradan şöyle bir sonuç elde edilmiştir:

    12.955314009661834
    0.0015374086036501278

    Bu değer manuel hesaplanan değerden çok az farklıdır. Bu durum büyük ölçüde manuel hesaplamada kullanılan düzeltme 
    formülü ile kruskal fonksiyonun kullandığı formülün farklı olmasından kaynaklanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy.stats import kruskal

group_a = np.array([65, 70, 68, 72, 66, 69])     # Geleneksel Yöntem
group_b = np.array([78, 85, 82, 90, 88, 80])     # Aktif Öğrenme
group_c = np.array([72, 68, 75, 70, 73, 71])     # Çevrimiçi Öğrenme

h, p_val = kruskal(group_a, group_b, group_c)
print(h)
print(p_val)

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu bölümünde bulut sistemlerinin yapay zeka ve makine öğrenmesindeki kullanımlarını açıklayacağız. Bulut (cloud)
    kavramıeski bir akvram değildir. Bu kavram 2000'lerin ikinci yarısında yılında ciddi olarak kullanılmaya başlanmış ve 
    2010'lu yıllarda yaygınlaşmıştır. Bulut sisteminden kastedilen şey "depolama alanlarının" ve "çalıştırma birimlerinin" 
    yerel makineler yerine Internet'te çeşitli kurumların sağladığı suncucularda bulundurulmasıdır. Bugün bulut sistemleri 
    yalnızca depolama alanı ve hesaplama biriminin yanı sıra bunlarla ilişkili pek çok hizmetiş de barındıran geniş bir 
    ekosistem haline gelmiştir. Bir bulut sisteminin tipik özellikleri şunlardır:

    Hizmetin uzak dağıtık sistemlerden sağlanması: Bir sunucu kurmak ve yazılım satın almak yerine, gereksinim duyulan tüm 
    bileşenler bulut sisteminden sağlanmaktadır. Bu nedenle bu sistemler kişilerin ya da kurumların büyük başlangıç yatırımları 
    yapmalarının önüne geçmektedir. 

    Esneklik ve Ölçeklenebilirlik: Depolama alanınız dolduğunda fiziksel bir disk almak yerine, tek tıkla kapasitenizi 
    artırabilirsiniz. Daha fazla bilgiişlem gücüne sahip olabilirsiniz. Burada ölçekleme demekle girdi miktarı arttıkça sistemin
    yeni duruma uyumlandırılmasının kolaylığı anlatılmaktadır. 

    Kullandığın Kadarını Öde: Bu sistemlerde tıpkı elektrik veya su faturası gibi, sadece kullanılan veri miktarı veya 
    işlem gücü kadar ödeme yapılmaktadır.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesi uygulamaları için çeşitli kurumlar tarafından bulut temelli hizmetler sunulmaktadır. Bu 
    bulut hizmetlerinin en yaygın kullanılanları şunlardır:

    - Google Cloud Platform (Vertex AI)
    - Amazon Web Services (Sage Maker)
    - Microsoft Azure
    - IBM Watson

    Bu servislerin hepsinin ortak birtakım özellikleri ve amaçları vardır:

    - Bu platformlar bize CPU ve bellek sağlamaktadır. Dolayısıyla bizim makine öğrenmesi işlemleri için ayrı bir makine tahsis 
    etmemize gerek kalmaz. Pek çok modelin eğitimi günlerce sürebilmektedir. Bunun için makinenin evde ya da iş yerinde tutulması 
    uygun olamayabilir. 
    
    - Bu platformlar "ölçeklenebilir (scalable)" çözümler sunmaktadır. Yani kiralanan birimler büyütülüp küçültülebilmektedir. 

    - Bu platformlar "deployment" için kullanılabilmektedir. Yani burada eğitilen modellerin Web API'leriyle birer servis 
    olarak uzaktan kullanılması mümkn olabilmektedir. (Örneğin biz bir makine öğrenmesi uygulamasını buralarda konuşlandırabiliriz. 
    Kestirim işlemlerini cep telefonumuzdaki uygulamalardan yapabiliriz. Böylece uygulamamız mobil aygıtlardan da web tabanlı 
    olarak kullanılabilir hale gelmektedir.)

    - Bu platformlar kendi içerisinde "Automated ML" araçlarını da bulundurmaktadır. Dolayısıyla aslında konunun teorisini 
    bilmeyen kişiler de bu Automated ML araçlarını kullanarak işlemlerini yapabilmektedir. 

    Yukarıdaki platformlar (IBM Watson dışındaki) aslında çok genel amaçlı platformlardır. Yani platformlarda pek çok değişik 
    hizmet de verilmektedir. Bu platformalara "yapay ze makine öğrenmesi" unsurları son 10 senedir eklenmiş durumdadır. Yani 
    bu platformlardaki yapay zeka ve makine öğrenmesi kısımları bu platformların birer alt sistemi gibidir. Bu platformların 
    pek çok ayrıntısı olduğunu hatta bunlar için sertifikasyon sınavlarının yapıldığını belirtmek istiyoruz. Biz kurusumuzda 
    bu platformların ayrıntılarıyla ilgilenmeyeceğiz. Bunların amaca yönelik kullanımları üzerinde duracağız. 

    Tabii yukarıdaki platformlar ticari platformlardır. Yani kullanım için ücret ödenmektedir. Ücret ödemesi "kullanım miktarına"
    göre yapılmaktadır. Yani ne kadar kullanılırsa o kadar ücret ödenmektedir. (Bu bakımdan eğitimdeki modelleriizi unutursanız, 
    bu eğitimler para yazmaya devam edecektir. Denemeleriniz bittiğinde bu tür hesaplamaların durmuş olduğundan emin olmalısınız.) 
    Tabii bu platformlarda da birtakım işlemler bedava yapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bütün bulut sistemlerinde makine öğrenmesi işlemleri yapılırken birbirleriyle ilişkili üç etkinlik yürütülür: 
    
    Data + Model + Hesaplama 

    Üzerinde çalışacağımız veriler genellikle bu bulut sistemlerinde onların bu iş için ayrılan bir servisi yoluyla upload 
    edilir. Model manuel ya da otomatik bir biçimde oluşturulmaktadır. Bulut sistemleri kendi içerisindeki dağıtık biçimde 
    bulunan bilgisayarlar yoluyla model üzerinde eğitim, kestirim gibi işlemler yapmamıza olanak vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Google Cloud Platform (kısaca GCP) Amazon AWS ve Microsoft Azure platformlarının doğrudan rekabetçisi konumundadır. 
   GCP 2008’de kurulmuştur. Aslında diğer platformlarda olan servislerin benzerleri GCP’de de bulunmaktadır. GCP’ye erişmek 
   için bir Google hesabının açılmış olması gerekir. 

    GCP’nin ana sayfası şöyeldir:

    https://cloud.google.com/

    GCP işlemlerini yapabilmek için kontrol panele (konsol ortamına) girmek gerekir. Konsol ekranına yukarıdaki bağlantıdan 
    girebileceğiniz gibi doğrudan aşağıdaki bağlantıdan da erişebilirsiniz:

    https://console.cloud.google.com
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   GCP’de tüm işlemler bir proje eşliğinde yapılmaktadır. Çünkü işletmeler değişik projeler için değişik hizmetler alabilmektedir. 
   Projenin yaratımı hemen konsole sayfasından yapılabilmektedir. Projeyi yarattıktan sonra aktif hale getirmek (select etmek) 
   gerekir. Proje aktif hale geldiğinde proje sayfasına geçilmiş olur. Tabii proje yaratmak için bizim Google'a kredi kartımızı 
   vermiş olmamız gerekir. Yukarıda da belirttiğimiz gibi biz kredi kartını vermiş olsak bile Google kullanana kadar para çekmemektedir.
   Projenin "dashboard" denilen ana bir sayfası vardır. Burada projeye ilişkin pek çok özet bilgi ve hızlı erişim bağlantıları 
   bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP’nin -tıpkı diğer platformlarda olduğu gibi- “CPU + RAM” kiralaması yapan ve “Compute Engine” denilen bir servisi vardır. 
    Benzer biçimde yine veritablarını host etmek için ve birtakım dosyaları saklamak için kullanılabilecek “Cloud Storage” 
    hizmeti de bulunmaktadır. 

    GCP içerisinde birtakım servislerin erişebileceği bir depolama (storage) alanına gereksim duyulmaktadır. Bunun için “Cloud 
    Storage” hizmetini seçmek gerekir. (Ancak Google bu noktada sınırlı bedava bir hizmet verecek olsa da kredi kartı bilgilerini 
    istemektedir.) Tıpkı AWS’de olduğu gibi burada da “bucket” kavramı kullanılmıştır. Kullanıcının önce bir “bucket yaratması” 
    gerekmektedir. Bucket adeta cloud alanı için bize ayrılmış bir disk ya da klasör gibi düşünülebilir. Dosyalar bucket'lerin 
    içerisinde bulunmaktadır. Bucket yaratılması sırasında yine diğer bulut platformlarında olduğu gibi bazı sorular sorulmaktadır. 
    Örneğin verilere hangi bölgeden erişileceği, verilere hangi sıklıkta erişileceği gibi. Bucket’e verilecek isim yine AWS’de 
    olduğu gibi GCP genelinde tek (unique) olmak zorundadır. Bir bucket yaratıldıktan sonra artık biz yerel makinemizdeki 
    dosyaları bucket'e upload edebiliriz. Bucket'lere upload edilen dosyalara birer URL verilmektedir. Böylece bu dosyalara 
    erişim hakları olanlar bu URL'ler yoluyla erişebilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında GCP üzerinde işlem yapmak için çeşitli komut satırı araçları da bulundurulmuştur. Biz bu komut satırı araçlarını 
    yerel makinemize install edip işlemleri hiç Web arayüzünü kullanmadan bu araçlarla da yapabilmekteyiz. Bu araçlar bizim 
    istediğimiz komutları bir script biçiminde de çalıştırabilmektedir. Aslında bu komut satırı araçları "Cloud Shell" ismiyle
    Web tabanlı olarak uzak makinede de çalıştırılabilmektedir. Bunun için console.cloud.google.com bağlantısında sağ üst 
    köşeden ilgili düğmeyi (ikonik durumda da olabilir) tıklayabilirisniz. 
    
    Aslında "Google Cloud CLI" isimi paket indirilirse çalışma doğrudan yerel makineden de yapılabilmektedir. "Google Cloud CLI" 
    paketini aşağıdaki bağlantıdan indirebilirsiniz:

    https://docs.cloud.google.com/sdk/docs/install-sdk

    Örneğin gsutil programı ile yerel makinemizdeki "covid.csv" dosyasını GCP'deki bucket'imiz içerisine şöyle kopyalayabiliriz:

    gsutil cp covid.csv gs://kaanaslan-test-bucket
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP içerisindeki Auto ML aracına "Vertex AI" denilmektedir. Vertex AI aracına erişmek için GCP kontrol panelindeki ana 
    menüyü kullanabilirisiniz. Vertex AI'ın ana kontrol sayfasına "Dashboard" denilmektedir. Dolayısıyla bizim Dashboard'a geçmemiz gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tipik olarak Vertex-AI'da işlem yapma aşamaları şöyledir:

    1) Veri kümesi bucket içerisine upload edilir. (Bu işlem Dataset oluşturulurken de yapılabilmektedir.)
    2) Dataset oluşturulur.
    3) Eğitim işlemi yapılır
    4) Deployment ve Test işlemleri yapılır
    5) Kestirim işlemleri yapılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Vertex AI'da otomatik makine öğrenmesi için (başka hizmetler de bulunmaktadır) ilk yapılacak şey bir "Dataset" yaratmaktır. 
    Dataset bizim modelimizi temsil etmektedir. Bunun için Vertex AI sayfasındaki "Datasets" sekmesi seçilir. Buradan Create 
    düğmesine basılır. Burada Dataset için bölge bir bölgenin seçilmesi grekmektedir. (Bu bölgenin bucket ile aynı bölgede 
    olması gerekmez ancak aynı bölgede olması daha uygundur.) Dataset'e bir isim verilir. Sonra problemin türü seçilir. 
    Yukarıda problem türüne ilişkin dört sekme bulunmaktadır: Image, Tabular, Text, Video. CSV dosyasından hareketle bir 
    regresyon ya da sınıflandırma işlemi yapılacaksa "Tabular" sekmesinden "Regression/Classification" seçilmelidir. Dataset 
    yaratıldıktan sonra artık bu dataset'in bir CSV dosyası ilişkilendirilmesi gerekmektedir. Tabii Vertex AI bucket'teki 
    CSV dosyalarını da kullanabilmektedir. Burada üç seçenek bulunmaktadır:

    * Upload CSV files from your computer
    * Select CSV files from Cloud Storage
    * Select a table or view from BigQuery

    Biz yerel bilgiyasarımızdaki bir CSV dosyasını seçersek zaten bu CSV dosyası önce bucket içerisine kopyalanmaktadır. 
    Eğer zaten CSV dosyamız bir bucket içerisindeyse doğrudan bucket içerisindeki CSV dosyasını "Select CSV files from Cloud 
    Storage" seçeneği ile seçebiliriz. BigQuery GCP içerisindeki veritabanı biçiminde organize edilmiş olan başka bir depolama 
    birimidir. 

    Vertex AI'daki otomatik makine öğrenmesi aracı diğer platformlardakinden biraz daha katıdır. Örneğin 1000 satırdan daha 
    az veri kümelerini Vertex AI otomatik makine öğrenmesi aracı hiç kabul etmemektedir. (Bu tür durumlarda error mesajı 
    belli bir zaman sonra orata çıkabilmektedir. Bu da boşuna zaman ve ücret ödenmesi anlamına gelmektedir. Bu nedenle 
    küçük veri kümelerini hiç eğitmeye çalışmayınız.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Dataset oluşturulduktan sonra artık eğitim işlemine geçilebilir. Bunun için Vertex AI içerisindeki "Traning" sekmesi 
    kullanılmaktadır. Training sayfasına geçildiğinde "Train New Model" düğmesi ile eğitim belirlemelerinin yapıldığı bölüme 
    geçilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Training işleminde peşi sıra birkaç aşamadan geçilmektedir. "Training method" aşamasında bize hangi veri kümesi için eğitim 
    yapılacağı ve problemin bir "sınıflandırma problemi mi yoksa regresyon problemi mi" olduğu sorulur. Bundan sonra "Model 
    details" aşamasına geçilir. Bu aşamada bize veri kümesindeki kestirilecek sütunun hangisi olduğu sorulmaktadır. Bu aşamada 
    "Advanced Options" düğmesine basıldığında "test" ve "sınama" verilerinin miktarları belirlenebilmektedir. Default durumda 
    test verileri ve sınama verileri veri kümesinin %10'u biçiminde alınmaktadır. "Join featurestore" aşamasından doğrudan 
    "Continue" ile geçilebilir. Bundan sonra karşımıza "Training options" aşaması gelecektir. Burada eğitimde hangi sütunların 
    kullanılacağı bize sorulmaktadır. Yine bu aşamada da "Advanced Options" seçeneği vardır. Burada bize Loss fonksiyonu 
    sorulmatadır. Tabii bunlar default değerlerle geçilebilir. En sonunda "Compute and pricing" aşamasına gelinir. Burada 
    dikkat etmek gerekir. Çünkü Google eğitimde harcanan zamanı ücretlendirmektedir. Google'ın ücretlendirme yöntemi aşağıdaki 
    bağlantıdan imncelenebilir:

    https://cloud.google.com/vertex-ai/pricing

    Burada "Budget" eğitim için maksimum ne kadar zaman ayrılacağını belirtmektedir. Klasik tabular verilerde en az zaman 1 
    saat olarak, resim sınıflandırma gibi işlemlerde en az zaman 3 olarak girilebilmektedir. 

    En sonunda "Start Training" ile eğitim başlatılır. Eğitimler uzun sürebildiği için bitiminde e-posta ile bildirim 
    yapılmaktadır. 

    Eğitim bittikten sonra biz eğitim hakkında bilgileri Training sekmesinden ilgili eğitimin üzerine tıklayarak görebiliriz. 
    Eğer problem regresyon problemi ise modelin başarısı çeşitli metrik değerlerle gösterilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kesitirim işlemlerinin yapılabilmesi için önce modelin "deploy edilmesi ve bir endpoint oluşturulması" gerekmektedir. 
    Modelin deploy edilmesi demek cloud sistemi içerisinde dışarıdan kullanıma hazır hale getirilmesi demektir. Böylece biz 
    kestirimi uzaktan programlama yoluyla da yapabiliriz. Deployment işlemi "Prediction" sekmesinden girilerek yapılabileceği gibi 
    "Model Registry" sekmesninden de yapılabilmektedir. EndPoint yaratımı sırasında bize Endpoint için bir isim sorulmaktadır. Sonra 
    model için bir isim verilmekte ve ona bir versiyon numarası atanmaktadır. Buradaki "Minimum number of compute nodes" ne kadar yüksek 
    tutulursa erişim o kadar hızlı yapılmaktadır. Ancak node'ların sayısı doğrudna ücretlendirmeyi etkilemektedir. Dolayısıyla burada 
    en düşük sayı olan 1 değerini girebilirsiniz. Daha sonra bize modelin deploy edileceği makinenin özellikleri sorulmaktadır. 
    Burada eğitimin başka bir makinede yapıldığına ancak sonucun kestirilmesi için başka bir makinenin kullanıldığına dikkat ediniz. 
    Modelimiz deploy edildikten sonra kullanım miktarı kadar ücretlendirme yapılmaktadır. Dolaysıyla denemelerinizden sonra 
    bu deployment işlemini silebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işleminden sonra artık predict işlemi yapılabilir. Bu işlem tamamen görsel arayüzle yapılabileceği gibi 
    Web API'leriyle ya da bunları kullanan Python koduyla da yapılabilmektedir. Eğer deploy edilmiş modelde kestirim 
    işlemini programlama yoluyla yapacaksanız bunun için öncelikle aşağıdaki paketi kurmanız gerekmektedir:

    pip install google-cloud-aiplatform

    Bundan sonra aşağıdaki import işlemini yapıp modüldeki init fonksiyonunun uygun parametrelerle çağrılması gerekmeketdir:

    from google.cloud import aiplatform

    aiplatform.init(....)

    predict işlemi için Training sekmesinden Deploy & Test sekmesini kullanmak gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Birden fazla predict işlemi "batch predict" denilen sekme ile yapılmaktadır. Uygulamacı kestirim için yine bir CSV dosyası 
    oluşturur. Bu CSV dosyasına bucket'e upload eder. Sonra "Batch predict" sekmesinden bu CSV dosyasına referans ederek 
    işlemi başlatır. Sonuçlar yine bu işlem sırasında belirlenen bucket'ler içerisinde CSV dosyaları biçiminde oluşturulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Vertex AI ile resim sınıflandırma işlemi yapalım. Resim sınıflandırma gibi bir işlem şu aşamalardan geçilerek 
    gerçekleştirilmektedir:

    1) Resimler Google cloud'ta bir bucket'e upload edilir. 
    2) Resimler bir CSV dosyası haline getirilir. Tabii burada resmin içerisindeki data'lar değil onun bucket'teki yeri 
    kullanılmaktadır. 
    3) Bu CSV dosyasından hareketle Dataset oluşturulur. 
    4) Training işlemi yapılır.
    5) Deployment ve EndPoint ataması yapılır 
    6) Kestirim işlemi görsel atayüz yoluyla ya da WEB API'leri ya da Pythonkoduyla yapılır.

    Burada Dataset oluşturulurken bizden bir CSV dosyası istenmektedir. Bu CSV dosyası aşağıdaki gibi bir formatta oluşturulmalıdır:

    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı

    Örneğin:

    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (1).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (10).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (100).jpg,boot
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Metin sınıflandırmaları da bemzer biçmde yapılabilmektedir. Burada iki seçenek söz konusudur. Metinler ayrı dosyalarda 
    bulunudurulup dosyalar bucket içerisine upload edilebilir yine resim sınıflandırma örneğinde olduğu gibi CSV dosyası 
    metinlere ilişkin dosyalardan ve onların sınıflarından oluşturulabilir. Ya da doğrudan metinlerin kendisi ve onların sınıfları da
    CSV dosyasının içerisinde bulunabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Amazon firması Cloud paltformlarına ilk giren büyük firmalardandır. Amazon'un cloud platformuna AWS (Amazon Web Services)
    denilmektedir. AWS iki yüzün üzerinde servis barındıran dev bir platformdur. Platformun pek çok ayrıntısı vardır. Bu nedenle 
    platformun öğrenilmesi ayrı bir uzmanlık alanı haline gelmiştir. Biz kurusumuzda platformun yapay zeka ve makine öğrenmesi 
    için nasıl kullanılacağı üzerinde özet bir biçimde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS ortamında makine öğrenmesi etkinlikleri işleyiş olarak aslında daha önce görmüş olduğumuz Google Cloud Platform'a 
    oldukça benzemektedir.  Google Cloud Platform'daki "Vertex AI" servisinin Amazonda'ki mantıksal karşılığı "SageMaker"
    isimli servistir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin ana kontrol paneline aşağıdaki bağlantı ile erişilebilir:
    
    console.aws.amazon.com

    Tabii AWS hizmeti almak için yine bir kayıt aşaması gerekmektedir. AWS kaydı sırasında işlemlker için bizden kredi kartı 
    bilgileri istenmektedir. Ancak AWS diğerlerinde olduğu gibi "kullanılan kadar paranın ödendiği" bir platformdur.

    AWS'nin konsol ekranına giriş yapıldığında zaten bize son kullandığmız servisleri listelemektedir. Ancak ilk kez giriş 
    yapıyorsanız menüden "SageMaker" servisini seçmelisiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker'a geçildiğinde onun da bir "dash board" denilen kontrol paneli vardır. Burada biz notebook'lar yaratıp uzaktan 
    manuel işlemler yapabiliriz. Ancak SageMaker'ın Auto ML aracına "AutoPilot" denilmektedir. SageMaker'ı görsel olarak daha
    zahmetsiz kullanabilmek için ismine "Studio" denilen Web tabanlı bir IDE geliştirilmiştir. Son yıllarda "Google'ın collab'ına"
    benzer "Studio Lab" denilen bedava bir ortam da eklenemiştir. Kullanıcılar genellikle işlemlerini bu Studio IDE'siyle yapmaktadır. 
    SageMaker içerisinde "Studio"ya geçebilmek için en az bir "kullanıcı profilinin (user profile)" yaratılmış olması gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin veri saklamak için çeşitli servisleri vardır. Makine öğrenmesiş için en önemli depolama servisi S3 denilen servistir. 
    S3 servisinde de tıpkı GCP'de olduğu gibi "bucket" adı altında bir çeşit folder'lar oluşturulmaktadır. Sonra bu bucket'lere 
    dosyalar upload edilmektedir. Amazon "veri merkezlerini (data centers)" "bölge (zone)" denilen alnlarla bölümlere ayırmıştır. 
    Server'lar bu bölgelerin içerisindeki veri merkezlerinin içerisinde bulunmaktadır. Tıpkı GCP'de olduğu her bölgede her türlü 
    servis verilmeyebilmetedir. Kullanıcılar coğrafi bakımdan kendilerine yakın bölgeri seçerlerse erişim daha hızlı olabilmektedir. 

    Bir bucket yaratmak için ona "dünya genelinde tek olan (unique)" bir isim vermek gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker Studio'da Auto-ML etkinlikleri için Autopilot denilen uygulama kullanılmaktadır. Dolayısıyla Auto-ML işlemi için 
    AutoML seçilebilir. Autopilot'ta bir Auto-ML çalışması yapmak için bir "experiment" oluşturmak gerekir. Experiment 
    oluşturabilmek için "File/New/Create AutoML Experiment" seçilebilir ya da doğrudan Auto ML (Autopilot) penceresinde de 
    "Create Autopilot Experiment" seçilebilir. Yeni bir experiment yaratılırken bize onun ismi ve CSV dostasının bucket'teki 
    yeri sorulmaktadır. Sonra Next tuşuna basılarak bazı gerekli öğeler belirlenir. Örneğin tahmin edilecek hedef sütun ve 
    kestirimde kullanılacak sütunlar bu aşamada bize sorulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eğitim işlemi bittiğinde veri dosyanın bulunduğu bucket içerisinde bir klasör yaratılır ve bu klasör içerisinde model ile 
    ilgili çeşitli dosyalar bulundurulur. Buradaki iki dosya önemlidir:

    SageMakerAutopilotDataExplorationNotebook.ipynb
    SageMakerAutopilotCandidateDefinitionNotebook.ipynb
    
    Buradaki "SageMakerAutopilotDataExplorationNotebook.ipynb" dosyası içerisinde veriler hakkında istatistiksel bşrtakım özellikler 
    raporlanır. "SageMakerAutopilotCandidateDefinitionNotebook.ipynb" dosyasının içerisinde ise Autopilot'ın bulduğu en iyi modellerin
    nasıl işleme sokulacağına ilişkin açıklamalar bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kestirim yapabilmek için EndPoint oluşturulmuş olması gerekmektedir. Tıpkı GCP'de olduğu gibi modelin çalıştırılabilmesi için 
    bir server'a deploy edilmesi egrekmektedir. Deploy işlemi sonucunda bize bir EndPoint verilir. Biz de bu EndPoint'i kullanarak 
    Web arayüzü ile ya da Python programı ile uzaktan kestirimde bulunabiliriz. 
    
    AWS'de uzaktan Python ile işlem yapabilmek için "sagemaker" ve "boto3" gibi kütüphaneler oluşturulmuştur. Kütüphaneler şöyle yüklenebilir.  

    pip install sagemaker
    pip install boto3

    sagemaker kütüphanesi Web arayüzü ile yapılanları programlama yoluyla yapabilmekt için boto3 kütüphanesi ise uzaktan
    kesitirm (prediction) gibi işlemleri yapabilmek için kullanılmaktadır. 

    sagemaker kütüphanesi ile uzaktan işlemlerin yapılması kütüphanenin dokümantasyonlarında açıklanmıştır. Aşağıdaki 
    bağlantıyı kullanarak kodlar üzerinde değişiklikler yaparak ve kodlarda ilgili yerleri doldurarak uzaktan işlemler yapabilirsiniz:

    https://sagemaker.readthedocs.io/en/stable/overview.html
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   boto3 kütüphanesi ile uzaktan işlemler yapılırken önce bir Session nesnesinin yaratılması gerekmektedir. Sessin nesnesi yaratılırken 
   bizim AWS kaynaklarına ulaşabilmemiz için iki parola bilgisine sahip olmamız gerekir. Birincisi “aws_access_key_id” ve 
   ikincisi de “aws_secret_access_key”. Amazon servisleri uzaktan erişimler için “public key/private key” kriptografi uygulamaktadır. 
   Bu parola bilgileri Session nesnesi yaratılırken aşağıdaki verilebilir:

    import boto3

    session = boto3.Session(aws_access_key_id=XXXXX', aws_secret_access_key='YYYYY')

    Session nesnesi yaratıldıktan sonra hangi servisin kullanılacağını belirten bir kaynak nesnesi yaratılır. Örneğin:

    s3 = session.resource('s3')

    Aslında bu kaynak nesneleri session nesnesi yaratılmdan doğrudan da yaratılabilmektedir. Ancak parolaların bu durumda 
    “~/.aws/credentials” dosyasına aşağıdaki formatta yazılması gerekir:

    [default]
    aws_access_key_id = YOUR_ACCESS_KEY
    aws_secret_access_key = YOUR_SECRET_KEY

    Burada yukarıdaki iki anahtarı elde etme işlemi sırasıyla şu adımlarla yapılmaktadır:

    1) https://console.aws.amazon.com/iam/ Adresinden IAM işlemlerine gelinir. 
    2) Users sekmesi seçilir
    3) Kullanıcı ismi seçilir
    4) "Security credentials" sekmesi seçilir. 
    5) Buradan Create Acces Key seçilir. 

    İşlemler sırasında eğer yukarıdaki anahtarlar girilmek istenmiyorsa (bu amahtarların görülmesi istenmeyebilir) yukarıda da belirttiğimiz
    gibi bu anahtarlar özel bir dosyanın içerisine yazılabilir. Oradan otomatik alınabilir. Eğer bu anahtarlar ilgili dosyanın 
    içerisine yazılmışsa Session nesnesi yaratılırken parametre bu iki anahtarı girmemize gerek kalmaz. Örneğin:

    session = boto3.Session()

    Bu dosya bu bilgiler Amazon'un komut satırından çalışan aws programıyla da girilebilmektedir. Amazon'un komut satırından çalışan aws programını aşağıdaki 
    bağlantıdan inmdirerek kurabilirsiniz:

    https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

    Bu iki anahtarı ilgili dosyaya yazmak için aws programı şöyle kullanılabilir:

    aws configure

    Biz programlama yoluyla uzaktan bu bucket işlemlerini yapabiliriz. Örneğin tüm bucket'lerin isimleri aşağıdaki gibi elde edilebilir:

    for bucket in s3.buckets.all():
        print(bucket.name)
    
    Belli bir bucket'teki dosya aşağıdaki gibi download edilebilmektedir:

    s3 = boto3.client('s3')
    s3.download_file('kaanaslan-test-bucket', 'x.txt', 'y.txt')

    Burada söz konusu bucket içerisindeki "x.txt" dosyası "y.txt" biçiminde download edilmiştir. 

    Uzaktan predict işlemi yine boto3 kütüphanesi ile yapılabilmektedir. Aşağıda buna bir ilişkin bir örnek verilmiştir:

    import boto3

    session = boto3.Session(aws_access_key_id='AKIAWMMTXFTMCYOF352A',aws_secret_access_key='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', region_name='eu-central-1')

    predict_data = '''6,148,72,35,0,33.6,0.627,50
    1,85,66,29,0,26.6,0.351,31'
    '''

    client = session.client('runtime.sagemaker')
    response = client.invoke_endpoint(EndpointName='diabetes-test', ContentType='text/csv', Accept='text/csv', Body=predict_data)

    result = response['Body'].read().decode()
    print(result)

    Burada Session sınıfının client metodu kullanılarak bir sagemaker nesnesi elde edilmiştir. Sonra bu nesne üzerinde invoke_endpoint
    metodu çağrılmıştır. Tabii arka planda aslında işlemler Web Servisleriyle yürütülmektedir. Bu boto3 kütüphanesi bu işlemleri kendi 
    içerisinde yapmaktadır. Gelen mesajdaki Body kısmının elde edilip yazdırıldığında dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Microsoft Azure 2009 yılında kurulan bir bulut sistemidir. 2014 yılında Microsoft bu Azure sistemine yapay zeka ve makine 
   öğrenmesine ilişkin servisleri eklemiştir. Microsoft Azure daha önce görmüş olduğumuz Google Cloud Platform ve Amazon AWS 
   sistemine benzetilebilir. Benzer hizmetler Azure üzerinde de mevcuttur. Azure ML de hiç kod yazmadan fare hareketleriyle 
   ve Auto MLaraçlarıyla kullanılabilmektedir. Tıpkı Google Cloud Platform ve Amazon SageMaker’da olduğu gibi bir SDK eşliğinde 
   tüm yapılan görsel işlemler programlama yoluyla da yapılabilmektedir. Microsoft Azure ML için tıpkı GCP ve AWS’de olduğu gibi 
   sertifikasyon süreçleri oluşturmuştur. Bu konuda sertifika sınavları da yapmaktadır. Yani Azure sistemi bütün olarak bakıldığında 
   çok ayrıntılara sahip bir sistemdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure kullanımı için ilk yapılacak şey bir e-posta ile Microsoft hesabı açmaktır. Sonra bu hesap kullanılarak Azure hesabı 
    açılmalıdır. Azure hesabı açılırken "bedava" ve "ödediğn kadar kullan" biçiminde seçenekler karşımıza gelmektedir. Bedava 
    kullanımın pek çok kısıtları vardır. Bu nedenle deneme hesabınızı "ödediğin kadar kullan" seçeneği ile oluşturabilirisiniz. 
    Ancak kullanmadığınız servisleri her ihtimale karşı kapatmayı unutmayınız. Azure sistemine abona olunduktan sonra bir 
    kullajıcı için "abone ismi" oluşturulmaktadır. 
        
    Azure sistemini e-posta ve parola ile girildikten sonra ana yönetim sayfası portal sayfasıdır. Portal sayfasına 
    doğrudan aşağıdaki bağlantı ile girilebilmektedir:
    
    https://portal.azure.com

    Azure'ün ana sayfasına geçtikten sonra buradan Yapay Zeka ve Makine Öğrenmesi için "Azure Machine Leraning" seçilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure Machine Learning sayfasına geçildiğinde önce bir "Workspace" seçiminin yapılması gerekmektedir. Workspace yapılacak çalışmalar 
    için bir klasör gibi bir organizasyon oluşturmaktadır. Yeni bir Workspace oluşturabilmek için "Oluştur (Create)" düğmesine basılır. 
    Ancak bir "workspace" oluştururken bizim bir "kaynak grubuna (resource group)" ihtiyacımız vardır. Bu nedenle önceden bir 
    kaynak grubu oluşturulmuş olmalıdır. Kaynak grubu oluşturabilmek için ana menüden (hamburger menüden) "Kaynak Grupları (Resource Gropus)"
    seçilir. Kaynak grubu birtakım kaynakların oluşturduğu gruptur. Workscpace de bir kaynaktır. Dolayısıyla workspace'ler 
    kaynak gruplarının (resource groups) bulunurlar. Kaynak Grupları menüsünden "Oluştur (Create)" seçilerek kaynak grubu oluşturma
    sayfasına geçilir. Yaratılacak kaynak grubuna bir isim verilir. Bütün bu isimler dünya genelinde tek olmak zorundadır. 
    Kaynak Grupları diğer cloud sistemlerind eolduğu gibi bölgelerle ilişkilendirilmiştir. Bu nedenle kaynak grubu yaratılırken 
    o kaynak grubunun bölgesi de belirtilir. 

    Workspace oluştururken bizden bazı bilgilerin girilmesi istenmektedir. Ancak bu bilgiler default biçimde de oluşturulabilmektedir. 
    Ancak bizim workspace'e bir isim vermemiz ve onun yer alacağı kaynak grubunu (resource group) belirtmemiz gerekir. Bu adımlardan 
    sonra nihayet workspace oluşturulacaktır. Tabii bir workspace oluşturduktan saonra tekrar tekrar workspace oluşturmaya genellikle 
    gerek yoktur. Farklı çalışmaları aynı workspace içerisinde saklayabiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Microsoft da tıpkı Amazon'da olduğu gibi makine öğrenmesiişlemleri için Web tabanlı bir IDE benzeri sistem oluşturmuştur. 
    Buna "Machine Learning Studio" ya da kısaca "Studio" denilmektedir. Workspace'i seçip "Studio düğmesine basarak Studi IDE'sine 
    geçebiliriz. Auto ML işlemleri için Studio'da "Automated ML" sekmesine tıklanılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Machine Learning Studio'da Auto ML işlemlerini başlatmak için "Automated ML" sayfasında "New Automated ML Job" seçilir. 
    Daha önceden de gördüğümüz gibi bu tür Cloud Platformlarında dört ana unsur vardır:

    1) Storage (Eğitim için veri kümesisi barındırmak için ve eğitim sonucunda oluşturulacak dosyaları barındırmak için)
    2) CPU (Eğitimi yapabilmek için gereken makine)
    3) Model (Çeşitli yöntemlerle veri kümesine uygun en iyi ML modeli)
    4) Deployment ya da EndPoint (Hedef modelin konuşlandırılması ve Web Servisler yoluyla uzaktan kullanılabilir hale getirilmesi)

    Microsoft Azure sisteminde de "New Automated ML Job" işleminin ilk aşamasında bizden hangi veri kümesi üzerinde ML çalışması yapılacağı 
    sorulmaktadır. Biz bu aşamada yeni bir veri kümesini Azure'ün Storage sistemine upload edebiliriz. Ya da bu upload etme işlemi 
    daha önceden oluşturulabilir. Azure sisteminde upload edilmiş veri kümelerine "data asset" denilmektedir. "New Automated ML Job"
    işleminde toplam dört aşama bulunmaktadır:

    1) Select data asset: Bu aşamada üzerinde çalışılacak veri kümesi belirtilir. Yukarıda söz ettiğimiz gibi bu veri kümesi daha 
    önceden "data asset" biçiminde oluşturulmuş olabilir ya da bu aşamada oluşturulabilir. 

    2) Configure job: Burada işlem için önemli bazı belirlemeler yapılmaktadır. Örneğin yapılacak işleme bir isim verilmektedir. 
    Veri kümesindeki tahmin edilecek hedef sütun belirtilmektedir. Eğitim için kullanılacak makinenin türü de bu aşamada 
    sorulmaktadır. Makine türü için "Compute Instance" seçilebilir. Tabii bizim daha önceden yaratmış olduğumuz bir hesapalama 
    maknesi (compute instance) bulunmuyor olabilir. Bu durumda bir hesaplama makinesinin (yani eğitimde kullanılacak makinenin)
    yaratılması gerekecektir. Tabii aslında bir hesaplama makinesini (compute instance) daha önce de yaratmış olabiliriz. Hesaplama 
    makinesini daha önceden bu işlemden bağımsız olarak yaratmak için Manage/Compute sekmesi kullanılmaktadır. 

    3) Select task and settings: Burada bize problemin türü sorulmaaktadır. Tabii Azure hedef sütundan hareketle aslında problemin
    bir sınıflandırma (lojistik regresyon) problemi mi yoksa regresyon problemi mi olduğunu belirleyebilmektedir. 

    4) Hyperparameter Configuration: Bu aşamad bize sınama yönteminin ne olacağı ve test verilerinin nasıl oluşturulacağı sorulmaktadır.

    Bu aşamalardan geçildikten sonra Auto-ML en iyi modelleri bulmak için işlemleri başlatır. İşlemler bitince yine bize bildirimde bulunulmaktadır. 

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Auto-ML aracı işini bitirdikten sonra EndPoint oluşturulmultur. Biz Studo'da EndPoints sekmesine gelerek ilgili endpoint'in 
    üzerine tıkladığımızda yukarıdaki menüde "Details", "Test", "Consume" gibi seçenekler bulunur. Burada "Test" seçildiğinde 
    GUI'den kestirim yapılabilmektedir. "Consume" kısmında Web servisleri ile predict işlemi yapan bir Python kodu bulundurulmaktadır. 
    Ancak bu kodda api_key kısmı boş bir string'tir. Buradaki API key "Consume" sekmesinden "Primary Key" kısmından alınabilir. 

    Örneğin burada Consume sekmesindeki kod aşağıdaki gibidir. Ancak bu kodlarda dikkat edilmesi gereken nokta şudur: Bu kodlarda
    hesaba erişim için gereken "api key" boş bırakılmıştır. Bizim bu API key'i alıp buraya kopyalamamız gerekir. Daha önceden de 
    belirttiğimiz gibi bu API key Consume sekmesinden Primary Key alanından elde edilebilmektedir.  Buradaki "consume kodu" şaşağıda
    verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import urllib.request
import json
import os
import ssl

def allowSelfSignedHttps(allowed):
    # bypass the server certificate verification on client side
    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):
        ssl._create_default_https_context = ssl._create_unverified_context

allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.

# Request data goes here
# The example below assumes JSON formatting which may be updated
# depending on the format your endpoint expects.
# More information can be found here:
# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script
data =  {
  "Inputs": {
    "data": [
      {
        "age": 0,
        "sex": "example_value",
        "bmi": 0.0,
        "children": 0,
        "smoker": "example_value",
        "region": "example_value"
      }
    ]
  },
  "GlobalParameters": 0.0
}

body = str.encode(json.dumps(data))

url = 'https://kaanaslantestworkspace-wyndx.northeurope.inference.ml.azure.com/score'
# Replace this with the primary/secondary key or AMLToken for the endpoint
api_key = ''
if not api_key:
    raise Exception("A key should be provided to invoke the endpoint")

# The azureml-model-deployment header will force the request to go to a specific deployment.
# Remove this header to have the request observe the endpoint traffic rules
headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'automl457b251af41-1' }

req = urllib.request.Request(url, body, headers)

try:
    response = urllib.request.urlopen(req)

    result = response.read()
    print(result)
except urllib.error.HTTPError as error:
    print("The request failed with status code: " + str(error.code))

    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure
    print(error.info())
    print(error.read().decode("utf8", 'ignore'))

#----------------------------------------------------------------------------------------------------------------------------
    Aslında Microaoft tarafından hazırlanmış olan azureml isimli başka bir kütüphane daha bulunmaktadır. İşlemler bu kütüphane ile
    daha az kod yazarak da yapılabilmektedir. Bu kütüphane için aşağıdaki paketlerin yüklenmesi gerekmektedir:

    pip install azureml
    pip install azureml-core
    pip install azureml-data

    Aşağıda azureml kullanılarak kestirim işlemine örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import json
from pathlib import Path
from azureml.core.workspace import Workspace, Webservice
 
service_name = 'automl245eb70546-1',
ws = Workspace.get(
    name='KaanWorkspace',
    subscription_id='c06cf27d-0995-4edd-919a-47fd40f4a7ae',
    resource_group='KaanAslanResourceGroup'
)
service = Webservice(ws, service_name)
sample_file_path = '_samples.json'
 
with open(sample_file_path, 'r') as f:
    sample_data = json.load(f)
score_result = service.run(json.dumps(sample_data))
print(f'Inference result = {score_result}')

#----------------------------------------------------------------------------------------------------------------------------
    Azure'ün Machine Learning servisinde diğerlerinde henüz olmayan "designer" özelliği de bulunmaktadır. Bu designed sayesinde 
    sürükle bırak işlemleriyle hiç kod yazmadan makşne öğrenmesi modeli görsel biçimde oluşturulabilmektedir. Bunun için 
    Machine Learning Stdudio'da soldaki "Designer" sekmesi seçilir. Bu sekme seçildiğinde karşımıza bazı seçenekler çıkacaktır. 
    Biz hazır bazı şablonlar kullanarak ve şablonları değiştirerek işlemler yapabiliriz ya da sıfırdan tüm modeli kendimiz 
    oluşturabiliriz. 

    Model oluştururken sol taraftaki pencerede bulunan iki sekme kullanılmaktadır. Data sekmesi bizim Azure yükledeiğimiz veri 
    kümelerini göstermektedir. Component sekmesi ise sürüklenip bırakılacak bileşnleri belirtmektedir. Her bileşen dikdörtgensel 
    bir kutucuk ile temsil edilmiştir. Kod yazmak yerine bu bileşenler tasarım ekranına sürüklenip bırakılır. Sonra da bu bileşenler
    birbirlerine bağlanır. Sürüklenip bırakılan bileşenlerin üzerine tıklanıp farenin sağ tuşu ile bağlam menüsünden bileşene özgü 
    özellikler görüntülenebilir. Bu bağlam menüsünde pek çok bileşen için en önemli seçenek "Edit node name" seçenğidir. Bu seçenekte
    bileşene ilişkin özenmli bazı özellikler set edilmektedir. 

    Eğer model için şablon kullanmayıp sıfırdan işlemler yapmak istiyorsak işlemlere Data sekmesinde ilgili veri kümesini sürükleyip
    tasarım ekranına bırakmakla başlamalıyız. 

    Veri kümesini belirledikten sonra biz veri kümesindeki bazı sütunlar üzerinde işlem yapmak isteyebiliriz. Bunun için 
    "Select Columns in Dataset" bileşeni seçilir. Veri kümesinin çıktısı bu bileşene fare hareketi ile bağlanır. Sonra 
    "Select Columns in Dataset" bileşeninde bağlam menüsünden "Edit node name" seçilir. Buradan da "Edit columns" seçilerek 
    sütunlar belirlenir. 

    Bu işlemden sonra eksik veriler üzerinde işlemlerin yapılması isteniyorsa "Clean Missing Data" bileşni seçilerek tasarım 
    ekranına bırakılır. 

    Bundan sonra veri kümesini eğitim ve test olmak üzere ikiye ayırabiliriz. Bunun için "Split Data" bileşeni kullanılmaktadır. 
    Bu bileşende "Edit node names" yapıldığında bölmenin yüzdelik değerleri ve bölmenin nasıl yapılacağına yönelik bazı 
    belirlemeler girilebilir. 

    Bu işlemden sonra "Özellik Ölçeklemesi (Feature Scaling)" yapılabilir. Bunun için "Normalize Data" bileşeni seçilir. 
    Bu bileşende "Edit node names" seçildiğinde biz ölçekleme üzerinde belirlemeleri yapabilecek duruma geliriz. 

    Bu aşamalardan sonra artık sıra modelin eğitimine gelmiştir. Modelin eğitimi için "Train Model" bileşeni kullanılmaktadır. 
    Bu bileşenin iki girişi vardır. Girişlerden biri "Dataset" girişidir. Bu girişe veri test veri kümesi bağlanır. Bileşenin
    birinci girişi olan "Untrained model" girişine ise problemin türünü belirten bir bileşen bağlanır. Çeşitli problem türleri 
    için çeşitli bileşenler vardır. Örneğin ikili sınıflandırma problemleri için "Two class logistic regression" bileşeni kullanılır. 
    Eğitime ilişkin hyper parametreler bu bileşende belirtilmektedir. 

    Eğtim işleminden sonra sıra modelin test edilmesine gelmiştir. Modelin testi için "Score Model" bileşeni kullanılır. Score Model
    bileşeninin iki girişi vardır: "Trainded Model" ve "Dataset" girişleri. "Train Model" bleşeninin çıkışı "Trained Model" girişine, 
    "Split Data" bileşeninin ikinci çıkışı ise "Dataset" girişine bağlanır.

    Model Designer'da oluşturulduktan sonra artık sıra modelin eğitilmesine gelmiştir. "Configure & Submit" düğmesine basılır. 
    Burada artık eğitim için kullanılacak CPU kaynağı belirlenir. (Anımsayacağınız gibi Automated araçlarda bizim belirlememiz
    gereken üç unsur "Data" + "Model" + "CPU" biçimindeydi.) 

    Nihayet işlemleri başlatıp deployment işlemi için "Inference Pipeline"" seçeneği seçilmelidir. 

    Azure Designer'daki tüm bileşenler (components) aşağıdaki Microsoft bağlantısında dokümante edilmiştir:

    https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference?view=azureml-api-2

    Biz Inference Pipeline işlemini yaptığımızda Azure bize modelimizi kestirimde kullanılabecek hale getirmektedir. Bunun 
    için model bir "Web Service Output" bileşeni eklemektedir. Bu "Web Service Output" bileşeni kestirim çıktılarının 
    bir web servis biçiminde verileceği anlamına gelmektedir. Eskiden Azure aynı zamanda "Inference Pipeline" seçildiğinde
    modelimize bir "Web Service Input" bileşeni de ekliyordu. Bu bileşen de girdilerin web service tarafından alınacağını
    belirtmekteydi. Designer'ın yeni versiyonlarında "Infererence Pipeline" yapıldığında bu "Web Service Input" bileşeni artık
    eklenmemektedir. Web Service Input bileşeni eklendikten sonra artık bizim "Select Columns in Dataset" bileşeninden 
    kestirilecek sütunu çıkartmamız gerekmektedir. Ayrıca bu Web Service Input bileşeninin çıktısının artık "Select Columns in Dataset"
    bileşenine değil doğrudan ApplyTransformation bileşenine bağlanması gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Desginer'ın aytıntılı kullanımı için Microsoft dokümanlarını inceleyebilirsiniz. Örneğin aşağıdaki dokümanda lojistik olmayan 
    regresyon probleminin adım adım designer yardımıyla oluşturulması anlatılmaktadır:

    https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-train-score?view=azureml-api-1

#----------------------------------------------------------------------------------------------------------------------------
    IBM'in Cloud Platformu da en çok kullanılan platformlardan biridir. İşlevsellik olarak diğerlerine benzemektedir. IBM'de 
    cloud platformu içerisinde makine öğrenmesine ilişkin servisler bulundurmaktadır. Genel olarak IBM'in bu servislerine
    Watson denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformu için yine bir hesap açılması gerekir. Hesap açma ve sign in işlemleri cloud.ibm.com adresinden 
    yapılmaktadır. Hesap açılırken girilen e-posta adresi aynı zmaanda "IBMid" olarak kullanılmaktadır. IBMid cloud platformunda
    "user id" gibi kullanılmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformuna login olunduktan sonra karşımıza bir "Dashboard" sayfası çıkmaktadır. Burada ilk yapılacak şey 
    "Create Resource" seçilerek kaynak yaratılmasıdır. Buradan "Watson Studio" seçilir. Bedava hesp ile ancak bir tane "Watson Studio"
    kaynağı oluşturulabilmektedir. Kaynak oluşturulduktan sonra "Launch in IBM Cloud Pak for Data" düğmesine basılarak
    "IBM Watson Studio" ortamına geçilmektedir. IBM Watson Studio bir çeşit "Web Tabanlı IDE" gibi düşünülebilir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Watson Studio'ya geçildiğinde öncelikle bir projenin yaratılması gerekmektedir. Bunun için "New Project" seçilir. 
    Proje bir isim verilir. Sonra projenin yaratımı gerçekleşir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Proje yaratıldıktan sonra "New Asset" düğmesi le yeni bir proje öğesi (asset) oluşturulmalıdır. Burada Otomatik ML işlemleri 
    için "Auto AI" seçilebilir.
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Auto AI seçildiğinde yaratılacak işlem (experiment) için bir isim verilmelidir. Sonra bu işlem (experiment)" bir ML servisi 
    ile ilişkilendirilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Watson Studio'da Auto AI projesinde bizden öncelikle veri kümesinin yüklenmesi istenmektedir. Veri kümesi yüklendikten sonra 
    bu veri kümesinin ardışık "time series" verilerinden oluşup oluşmadığı bize sorulmaktadır. Bundan biz kesitim yapılacak sütun
    belirleriz. Uygulamacı problemin türüne göre problemin çeşitli meta parametrelerini kendisi set edebilmektedir. Bu işlem 
    "Experiment Settings" düğmesiyle yapılmaktadır. Nihayet uygulamacı "Run Experiment" seçeneği ile eğitimi başlatır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Modeller oluşturulduktan sonra bunlar performansa göre iyiden kötüye doğru sıralanmaktadır. Bu modeller save edilebilir. 
    Modeller save edilirken istenirse model kodları bir Jupyter Notebook olarak da elde edilebilmektedir. Bu notebook yerel makineye
    çekilip IBM Python kütüphanesi kurulduktan sonra yerel makinede de çalıştırılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işlemi için önce bir "deployment space" yaratılır. Ancak bu deployment space'te Assets kısmında deploy edilecek 
    modelin çıkması için dah önceden Model sayfasında "Promote to Deployment Space" seçilmelildir. Bundan sonra Deployment Space'te
    Assets kısmında ilgili model seçilerek "Create Deployment" düğmesi ile deployment işlemi yapılır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Deployment sonrası yine Wen Servisleri yoluyla model kullanılabilmektedir. Bu işlem Watson Web API'leriyle yapılabilecğei gibi
    diğer cloud platfotrmlarında olduğu gibi bu Web API'lerini kullanan Python kütüphaneleriyle de yapılabilmektedir. Kütüphane 
    aşağıdaki gibi install edilebilir:

    pip install ibm-watson-machine-learning   
#----------------------------------------------------------------------------------------------------------------------------



