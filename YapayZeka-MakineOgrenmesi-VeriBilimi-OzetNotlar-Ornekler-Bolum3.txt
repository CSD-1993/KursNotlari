#----------------------------------------------------------------------------------------------------------------------------
                                    
                                     Yapay Zeka, Makine öğrenmesi ve Veri Bilimi
                                                        Kursu
            
                                          Sınıfta Yapılan Örnekler ve Özet Notlar 
                                                       
                                                       3.Bölüm

                                                 Eğitmen: Kaan ASLAN
                                        
        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:           

        Aslan, K. (2025), "Yapay Zekâ, Makine Öğrenmesi ve Veri Bilimi Kursu", Sınıfta Yapılan Örnekler ve Özet Notlar, 
            C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları okurken editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                            Son Güncelleme: 19/10/2025 - Pazar

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            162. Ders - 19/10/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu son bölümünde bazı istatistik konuları üzerinde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Belli bir hipotezin (yani varsayımın) belli bir anlam düzeyinde (significance level) test edilmesini sağlayan istatistiksel 
    yöntemlere "hipotez testleri (hypothesis testing)" denilmektedir. Hipotez testleri başta sosyal bilimleri ve sağlık bilimleri
    olmak üzere endüstri mühendisliği, ekonomi ve işletme gibi pek çok alanda yaygın olarak kullanılmaktadır. 
    
    Örneğin yeni bir ilacın kan şekerini düşürdüğü iddia edilsin. Üretilecek olan bu ilacın gerçekten kan şekerini düşürüp 
    düşürmediğini nasıl anlayabiliriz? İlk akla gelecek yöntem bu ilacı çeşitli kişilere uygulayıp sonuçları karşılaştırmaktır. 
    Bunun için bir grup denek seçeriz. Deneklerin kan şekerini ilaç uygulanmadan önce ve uygulandıktan sonra ölçebiliriz. 
    Bu ölçümler arasında anlamlı bir fark olup olmadığına bakabiliriz. Ancak ne kadarlık bir fark bizim ilacın kan şekerini 
    düşürdüğünü kabul etmemiz için yeterli olacaktır? Örneğin kişinin tokluk kan şekeri 120 olsun. Bu ilaç bir kişide kan 
    şekerini 119'a diğer bir kişide 118'e düşürmüşse ancak diğer bir kişide ise hiçbir etki yaratmamışsa biz bu ilacın kan 
    şekerini düşürdüğünü söyleyebilir miyiz? Şüphesiz bizim hatırı sayılır bir düşüşü dikkate almamız gerekir. Öte yandan 
    ilaç herkeste tamamen aynı düşüşü de sağlamayabilecektir. O zaman bir biçimde bizim ortalama düşüşü dikkate almamız 
    gerekebilir. İşte önce eski durumu ölçüp sonra işlemi uygulayıp yeni durumu ölçüp aradaki farkların ortalamasına belli 
    bir güven düzeyi içerisinde bakarak sonuç çıkartan istatistiksel hipotez testleri geliştirilmiştir. Böyle bir test ile
    ilacın kan şekerini düşürme konusundaki etkisi herkesin kabul edebileceği biçimde nesnel olarak gösterilebilmektedir.
    (Gerçek yaşamda bir ilacın piyasaya sürülmesi için pek çok farklı testin belli prosedürlere uygun olarak yapılması 
    gerekmektedir.)
    
    Her hipotez testi tipik bazı deney kalıplarından elde edilen sonuçları test etmek için geliştirilmiştir. (Örneğin "kan 
    şekerini düşüren ilaç" örneğimize "ön test son test (pre-post test)" deney tasarımı denilmektedir.) Bu nedenle yalnızca 
    hipotez testlerinin nasıl uygulandığını bilmek yetmez, hangi durumlarda hangi testlerin uygulanması gerektiğini de bilmek 
    gerekir. Biz de bu bölümde çok kullanılan bazı deney kalıplarına yönelik hipotezlerin testleri üzerinde duracağız ve 
    bu testlerin Python'da programlama yoluyla nasıl yapıldığını uygulamalı olarak göreceğiz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri örneklemden hareketle ana kütle hakkında çıkarımlar yapmaktadır. Yukarıdaki örnekte biz ilacın kan şekerini 
    düşürüp düşürmediğini o ilacı belli sayıda kişilere uygulayarak test ettik. Ancak ilaç aslında herkese uygulanmalıydı. 
    Fakat bunun imkanın olmadığı da açıktır. O halde bu tür durumlarda ilacı belli sayıda kişiye uygulayıp onlardan elde edilen 
    sonuca dayalı olarak herkes hakkında genel bir hüküm verme yoluna gidilmektedir. Özetle hipotez testleri örneklemler üzerinde 
    uygulanmaktadır. Ancak test sonucunda ana kütle hakkında çıkarımlarda bulunulmaktadır. Örneğin bir havuzun değişik yerlerinden 
    damlalıkla su alıp onlardan birtakım ölçüm değerleri elde etmiş olalım. Sağlıklı bir suda kabul edilen değerleri de zaten 
    biliyor olalım. Havuzdaki suyun sağlıklı olup olmadığına havuzdaki tüm suya bakarak karar vermemekteyiz. Havuzdan belli 
    sayıda örnek alarak buna karar vermekteyiz. 

    2000'li yılların başlarında bilgisayar donanımlarının ve veri aktarım teknolojisinin gelişmesiyle birlikte "örneklemden 
    hareketle anakütle hakkında kestirimde bulunmaya" alternatif olarak "tüm anakütleyi ya da onun büyük kısmını göz önüne 
    alarak kestirimde bulunmayı" hedefleyen bir yaklaşım da ortaya atılmıştır. Büyük veri (big data) olarak isimlendirilen 
    bu yaklaşım pek çok uygulamada başarıyla kullanılmıştır. Büyük veri yaklaşımı aynı zamanda komşu disiplinleri de etkilemiş 
    makine öğrenmesi ve veri bilimi alanının gelişmesine de katkı sağlamıştır. Ancak büyük veri yaklaşımının her alanda 
    kullanılması mümkün değildir. Bu nedenle örnekleme temelli yöntemler geçerliliğini kaybetmiş değildir. Zaman içerisinde 
    örnekleme temelli yöntemlerle büyük veri yöntemlerinin hibrit bir biçimde uygulandığı yöntemler de geliştirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Hipotez testleri için araştırıcı öncelikle bir hipotez oluşturur. Bu hipoteze (yani uygulamacının arzu ettiği sonuca ilişkin 
    hipoteze) istatistikte genellikle H1 hipotezi ya da "alternatif hipotez" denilmektedir. Ancak "bir değişiklik olmadığına 
    yönelik", "eski duruma yönelik", "statükoya yönelik" hipoteze de H0 hipotezi ya da "null hipotez" denilmektedir. O halde 
    hipotez testlerinde iki hipotez vardır: H0 ve H1 hipotezleri. Bizim iddia ettiğimiz hipoteze H1 hipotezi, zaten var olan 
    duruma ilişkin hipoteze ise H0 hipotezi denilmektedir. Bu hipotezlere ilişkin birkaç örnek verelim:

        - Kan şekerini düşürdüğünü iddia ettiğimiz ilaç için ilacı uygulamadan ve ilacı uyguladıktan sonra ölçümler yapalım
        ("ön test son test" deney kalıbı). Sonra bu ölçümlerin ortalamasını hesaplayalım. Buradaki H0 ve H1 hipotezleri şöyle 
        oluşturulabilir:

        H0 Hipotezi: İlaç uygulanmadan önceki ortalama kan şekeri ile ilaç uygulandıktan sonraki ortalama kan şekeri arasında 
        anlamlı (belirlenen anlam düzeyi dikkate alındığında) bir farklılık yoktur. Bu hipotez matematiksel sembollerle şöyle 
        ifade edilebilir:

        H0: μ₀ = μ₁

        H1 Hipotezi: İlaç uygulanmadan önceki ortalama kan şekeri ilaç uygulandıktan sonraki ortalama kan şekerinden anlamlı 
        (belirlenen anlam düzeyinde dikkate alındığında) bir biçimde daha yüksektir. Bu hipotez matematiksel sembollerle şöyle 
        ifade edilebilir:
        
        H1: μ₀ > μ₁ 
        
        Burada μ₀ ilaç uygulanmadan önceki kan şekeri ortalamasını μ₁ ise ilaç uyguladıktan sonraki kan şekeri ortalamasını 
        belirtmektedir. 

    - Bir egzersizin uykusuzluğu azalttığına yönelik bir hipotez söz konusu olsun. Burada yine rastgele kişiler seçerek 
    onların egzersizden önceki uyku sürelerini ve egzersizden sonraki uyku sürelerini ölçebiliriz. Bunların ortalamalarını 
    hipotez testi ile karşılaştırmak isteyebiliriz. H0 ve H1 hipotezleri yine şöyle oluşturulabilir:

    H0: μ₀ = μ₁
    H1: μ₀ < μ₁ 

    Burada yine μ₀ egzersiz uygulanmadan önceki ortalamayı μ₁ ise egzersiz uygulandıktan sonraki ortalamayı belirtmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri genellikle iki gruba ayrılarak ele alınmaktadır:

    1) Parametrik hipotez testleri
    2) Parametrik olmayan hipotez testleri

    Parametrik hipotez testleri oransal (ratio) ve aralık (interval) ölçeklerine ilişkin değişkenlere uygulanabilmektedir. 
    Bu testlerde ortalama, standart sapma gibi dağılım parametreleri kullanılmaktadır ve örneklemin belli bir dağılıma (tipik 
    olarak normal dağılıma) uygun olması koşulu vardır. Parametrik olmayan testler herhangi bir ölçek türündeki değişkenlere 
    uygulanabilmektedir. Bu testlerin temel özelliği, örneklemden elde edilen değerlerin belli bir dağılıma uygunluk varsayımı 
    gerektirmemesidir. Kategorik (nominal) ve sıralı (ordinal) ölçekteki veriler için parametrik testler uygulanamadığından, 
    bu tür verilerde parametrik olmayan testler kullanılır. Ancak parametrik olmayan testler, dağılım varsayımlarını karşılamayan 
    oransal ve aralık ölçekli verilere de yaygın olarak uygulanmaktadır.
    
    Oransal ve aralık ölçekli değişkenlerde eğer dağılım varsayımları (normallik, varyans homojenliği gibi) karşılanıyorsa, 
    parametrik testler tercih edilmelidir. Çünkü parametrik testler daha yüksek istatistiksel güce (power) sahiptir ve daha 
    hassas sonuçlar verir. Ancak bu varsayımlar ihlal edildiğinde parametrik olmayan testler daha uygun ve güvenilir sonuçlar 
    sağlar.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Parametrik hipotez testleri büyük ölçüde "merkezi limit teoremine (central limit theorem)" dayandırılmaktadır. Anımsanacağı 
    gibi merkezi limit teoremi, özet olarak, bir ana kütleden seçilen yeterince büyük örneklem ortalamalarının normal dağılıma 
    yakınsadığı anlamına gelmektedir. Biz kursumuzda her bir hipotez testinin dayandığı istatistiksel ve matematiksel temel 
    üzerinde durmayacağız. Eğer konuyu matematiksel ve teorik ayrıntılarıyla öğrenmek istiyorsanız başka kaynaklara başvurabilirsiniz. 
    Kursumuzda biz daha çok uygulamaya yönelik bilgiler aktaracağız.

    Parametrik hipotez testlerinin en yaygın kullanılanları şunlardır:

    - Tek örneklem t-testi
    - Bağımsız örneklem t-testi
    - Eşleştirilmiş (bağımlı) örneklem t-testi
    - Tek yönlü ANOVA
    - İki yönlü ANOVA
    - Tekrarlı ölçümler için ANOVA
    - Pearson korelasyon testi
    - Doğrusal regresyon analizi
    - F-testi (varyans karşılaştırması)
    - Levene testi (varyans homojenliği)
    - Welch t-testi (eşit varyans varsayımı olmadan)

    Parametrik olmayan hipotez testlerinin ise en yaygın kullanılanları şunlardır:

    - Mann–Whitney U testi (Wilcoxon rank-sum testi)
    - Wilcoxon signed-rank testi
    - Kruskal–Wallis H testi
    - Friedman testi
    - Spearman sıra korelasyon testi
    - Kendall Tau korelasyon testi
    - Ki-kare (χ²) bağımsızlık testi
    - Ki-kare uygunluk testi
    - Fisher'in kesin testi
    - Sign testi
    - Median testi
    - Theil–Sen kestirimi (parametrik olmayan regresyon)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sosyal bilimlerde ve sağlık bilimlerinde hipotez testleri için genellikle paket programlar kullanılmaktadır. Bunları 
    kullanmak için programlama bilmeye gerek yoktur. Bu alanda çok kullanılan paket programlardan bazıları şunlardır:

    - SPSS (PSPP isimli GNU versiyonu vardır)
    - SAS
    - Minitab
    - Matlab (Octave isimli GNU versiyonu var)

    R Programlama Dili oldukça basit "domain specific" matematiksel ve istatistiksel bir dilidir. Ancak profesyonel dünyada 
    paket programlardan ve R dilinden ziyade Python tercih edilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistiksel hipotez testleri için oluşturulmuş çeşitli kütüphaneler vardır. NumPy, Pandas ve scikit-learn hipotez 
    testlerini desteklemektedir. Python'da hipotez testleri için en yaygın kullanılan kütüphane statsmodels isimli kütüphanedir. 
    Ancak genel amaçlı nümerik analiz kütüphanesi olan SciPy kütüphanesi de hipotez testlerinin bazılarını bünyesinde barındırmaktadır. 
    Biz SciPy kütüphanesini zaten daha önce çeşitli konularda kullanmıştık. 
    
    statsmodels kütüphanesini şöyle kurabilirsiniz:

    pip install statsmodels
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri istatistikte uzun süredir iyi bilinen konulardandır. Bu nedenle konunun zaman içerisinde geniş ve yeterli 
    bir yabancı ve yerli literatürü oluşturulmuştur. Bu konuda  "Sosyal Bilimler İçin Veri Analizi El Kitabı (Şener Büyüköztürk)" 
    isimli kitabı kaynak kitap olarak kullanabilirsiniz. Bu kitap yöntemlerin istatistiksel ve matematiksel temeli üzerinde 
    ayrıntıya girmemekle birlikte "hangi durumlarda hangi hipotez testlerinin kullanılacağı konusunda" iyi bir bilgi sunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinin çoğunda α (alfa) ve p değeri denilen iki değer söz konusu olmaktadır. α değeri hata düzeyini belirtmektedir. 
    (Örneğin normal dağılım söz konusu olduğunda kuyruk kısmındaki alanın büyüklüğünü belirtmektedir.) Örneğin α = 0.05 demek 
    yüzde %95'lik bir güven düzeyi demektir. α değeri ne kadar küçültülürse testin güvenilirliği o kadar artmaktadır. Test 
    sonucunda bir p değeri elde edilmektedir. Test sonucunda elde edilen bu p değeri H0 hipotezinin reddedilip reddedilmeyeceğini 
    belirtmektedir. α değerini uygulamacı belirler, p değeri ise test işleminin sonucundan elde edilmektedir. p değeri "H0 
    hipotezinin doğru olduğu varsayımında yapılan hata miktarını" belirtmektedir. Örneğin test sonucunda p = 0.01 çıkmış olsun. 
    Bu değer bizim belirlediğimiz α değerinden küçüktür. Yani H0 hipotezinin reddedilme sınırın altına kalmaktadır. Dolayısıyla 
    H0 hipotezi reddedilir. Matematiksel olarak gösterirsek:

    - Eğer p <= α ise H0 hipotezi reddedilir.
    - Eğer p > α ise H0 hipotezi reddedilmez.

    Bunu bir tablo biçiminde de aşağıdaki gibi verebiliriz:

    ┌───────────────┬──────────────────┬──────────────────────────────────────────┐
    │ Karşılaştırma │      Karar       │                  Yorum                   │
    ├───────────────┼──────────────────┼──────────────────────────────────────────┤
    │  p ≤ α        │ H₀ reddedilir    │ Sonuç istatistiksel olarak anlamlıdır    │
    ├───────────────┼──────────────────┼──────────────────────────────────────────┤
    │  p > α        │ H₀ reddedilemez  │ Veri, H₀'ı reddetmek için yeterli değil  │
    └───────────────┴──────────────────┴──────────────────────────────────────────┘

    Hipotez testleri genel olarak H0 hipotezinin reddedilip reddedilmeyeceği üzerine kurulmaktadır. H0 hipotezinin reddedilmesi 
    H1 hipotezinin kabul edileceği anlamına gelmemektedir. Ancak anlatımlarda çoğu kez H0 ya da H1 hipotezinin reddedilmesi 
    diğerinin kabul edilmesi biçiminde ifade edilmektedir. Fakat aslında doğru ifade biçiminin eğer "H0 hipotezi reddediliyorsa" 
    "H1 lehine anlamlı kanıt vardır" biçiminde, eğer "H0 hipotezi reddedilmiyorsa, "H1 hipotezi lehine anlamlı kanıt yoktur" 
    biçiminde olması gerekmektedir. 

    Görüldüğü gibi α değeri bir eşik değer (threshold) belirtmektedir. α adeta "benim tolere edebileceğim hata miktarı" anlamına 
    gelmektedir. 1 - α değerine genellikle istatistikte "güven düzeyi (confidence level)" denilmektedir. Yani örneğin %95 
    güven düzeyi ile α = 0.05 aynı anlama gelmektedir. 

    Peki araştırmalarda α için genellikle hangi değerler kullanılmaktadır? Şüphesiz bu durum testin önemine göre değişebilir. 
    Tipik kullanılan α değeri 0.05'tir. Ancak örneğin sağlık bilimlerinde tolerans daha düşük olduğu için α değeri 0.01 
    olarak belirlenebilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinden en temel olanlardan birisi "bağımsız örneklemler t-testi" denilen testtir. Bu teste İngilizce 
    "independent samples t-test" ya da "two-sample t-test" ya da "unpaired t-test" de denilmektedir. Bu test iki bağımsız 
    grubun elde ettiği puanların ortalamasının karşılaştırılması amacıyla kullanılmaktadır. Bu test için tipik deney kalıbı 
    şöyledir: İki bağımsız grup vardır. Bir deney bu iki gruba ya da gruplarından birine ayrı ayrı uygulanmaktadır. Deney 
    sonucunda "grupların elde ettiği ortalama puanlar arasında anlamlı bir fark var mı" diye bakılmaktadır. Tipik olarak 
    gruplardan biri "kontrol grubu" olabilmektedir. Kontrol grubu üzerinde bir işlem uygulanmaz. Yalnızca diğer grup üzerinde 
    işlem uygulanır. Ya da iki grup üzerinde ayrı işlemler uygulanır. Amaç iki grubun ortalama puanlarının karşılaştırılmasıdır. 
    Bağımsız örneklemler t-tesindeki "bağımsız (independent)" ya da "eşleşmemiş (unpaired)" terimi iki kümenin elemanlarının 
    birbirinden farklı olması anlamına gelmektedir. Yani söz konusu deneydeki iki grubun elemanları birbirinden farklıdır. 
    
    Aşağıda bağımsız örneklemler t-tesinin kullanılabileceği birkaç deney kalıbı veriyoruz: 

    - Bir sınıfta girilen sınavdaki matematik puanlarının kız öğrencilerle erkek erkek öğrenciler arasında farklılık gösterip
    göstermediği istatistik bakımdan test edilmek istenmektedir. Bunun için bağımsız örneklemler t-testi kullanılabilir. 
    Burada kız ve erkek öğrencilerin bağımsız gruplar oluşturduğuna dikkat ediniz. 

    - Bir hastalığın tedavisi için iki alternatif ilaç vardır. Amaç bu iki ilacın tedavi süresi bakımından yarattığı etkinin 
    birbirinden farklı olup olmadığını anlamaktır. İki ilaç iki farklı gruba verilir. İki farklı gruptaki kişilerin iyileşme 
    süreleri elde edilir. Bunların ortalamaları arasında anlamlı bir fark olup olmadığına bağımsız örneklemler t-testi uygulanarak 
    karar verilebilir.  

    - Sigara içenlerle içmeyenler arasında akciğer kapasitesi bakımından bir farklılık olup olmadığı araştırılmak istensin. 
    Sigara içenler ve içmeyenler iki bağımsız gruptur. Bunların akciğer kapasiteleri ölçülür. Bağımsız örneklemler t-testi 
    ile ortalamalar arasında bir farklılığın olup olmadığına karar verilir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde verilen karar için dört durum söz konusudur:

    1) H0 reddedilmemelidir, H0 reddedilmemiştir.
    2) H0 reddedilmemelidir, H0 reddedilmiştir.         ===> Tip 1 Hatası
    3) H0 reddedilmelidir, H0 reddedilmemiştir.         ===> Tip 2 Hatası
    4) H0 reddedilmelidir, H0 reddedilmiştir.

    İstatistiksl olarak anlatım doğru olmasa da bu durumları pratik olarak şöyle de ifade edebiliriz:

    1) H0 kabul edilmelidir, H0 kabul edilmiştir.
    2) H0 kabul edilmelidir, H0 kabul edilmemiştir.     ===> Tip 1 Hatası
    3) H0 kabul edilmemelidir, H0 kabul edilmiştir.     ===> Tip 2 Hatası
    4) H0 kabul edilmemelidir, H0 kabul edilmemiştir.

    İşte H0'ın reddedilmemesi gerektiği halde H0'ın reddedildiği duruma, başka bir deyişle H0'ın kabul edilmesi gerektiği 
    halde H1'in kabul edilmesi durumuna "Tip 1 Hatası (Type 1 Error)", H0'ın reddedilmesi gerektiği halde H0'ın kabul edidiği,
    başka bir deyişle H1'in kabul edilmesi gerektiği halde H0'ın kabul edildiği duruma "Tip 2 Hatası (Type 2 Error)" denilmektedir. 
    "Tip 1 Hatası" yerine "Alfa Hatası", "Tip 2 Hatası" yerine "Beta Hatası" deyimleri de kullanılmaktadır. Tip 1 ve Tip 2 
    hataları bir tablo halinde aşağıdaki gibi de gösterebiliriz:

    ╔════════════╦════════════════════════════╦════════════════════════════════════╗
    ║            ║        Testin Kararı       ║            Gerçek Durum            ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║            ║ H₀ reddedilmez (kabul)     ║ H₀ reddedilir                      ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║ H₀ doğru   ║ Doğru karar                ║ Tip I hata (alfa hatası)           ║
    ║            ║ (H₀ doğru, reddedilmedi)   ║ (Yanlış alarm)                     ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║ H₀ yanlış  ║  Tip II hata (β hatası)    ║  Doğru karar                       ║
    ║            ║ (Gerçek farkı göremedik)   ║ (Fark var, doğru tespit edildi)    ║
    ╚════════════╩════════════════════════════╩════════════════════════════════════╝
 
    Tip 1 hatanın "false positive", "Tip 2 hatanın ise "false negative" anlamına da geldiğine dikkat ediniz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-tesinin varsayımları (yani sağlanması gereken şartlar) şunlardır:

    1) Gruplar birbirinden bağımsız olmalıdır.
    2) Her iki grubun ana kütlesi normal dağılmış olmalıdır.
    3) Grupların geldiği anakütlenin varyansları eşit olmalıdır (σ₁² = σ₂²)

    Bu koşulların sağlanıp sağlanmadığına başka hipotez testleriyle karar verilebilir. Kursumuz ilk konularında normal 
    dağılım testinin nasıl yapıldığını görmüştük. Bağımsız iki grubun varyanslarının eşitliği (buna varyans homojenliği de
    denilmektedir) için çeşitli hipotez testleri kullanılmaktadır. Bu bakımdan en yaygın kullanılan hipotez testi "Levene 
    testi" ve "F-testi" denilen testlerdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-tesinin dayandığı fikir oldukça basittir. İki grup ortalamaları arasındaki fark elde edilir. Bu 
    ortalamalara ilişkin farkların normal dağıldığı fikriyle elde edilen bu farkın uç bir fark olup olmadığına belirlenen 
    α düzeyinde bakılmaktadır. Tabii iki ortalama arasındaki fark bu ortalamaların standart sapmaları da dikkate alınarak 
    anlamlandırılmaktadır. Bağımsız örneklemler t-testi için H0 ve H1 hipotezleri şöyledir:

    H₀: μ₁ = μ₂     
    H₁: μ₁ ≠ μ₂

    Test işlemi yapıldıktan sonra elde edilen p değerinden hareketle H0 hipotezinin reddedilip edilmeyeceği belirlenir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-testi SciPy kütüphanesinin stats modülündeki ttest_ind fonksiyonuyla uygulanabilmektedir. Fonksiyonun 
    parametrik yapısı şöyledir:

    ttest_ind(a, b, *, axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, 
            alternative='two-sided', trim=0, method=None, keepdims=False)

    Fonksiyonun ilk iki parametresi bağımsız örneklemler t-testi uygulanacak grup değerlerini belirtmektedir. Diğer parametreler
    default değerler almıştır. Fonksiyon geri dönüş değeri olarak TTestResult sınıfı türünden bir nesne vermektedir. Aslında 
    bu sınıf tuple sınıfından türetilmiştir. Dolayısıyla isimli bir demet gibidir ve demet gibi açılabilmektedir. Bu sınıfın 
    pvalue örnek özniteliği test sonucunda elde edilen p değerini, df örnek özniteliği serbestlik derecesini ve statistic örnek 
    özniteliği ise t istatistik değerlerini vermektedir. Fonksiyonun geri dönüş değeri iki elemanlı bir demet biçiminde de 
    kullanılabilir. Bu demetin ilk elemanı t istatistiğini, ikinci elemanı p değerini vermektedir. Bu iki eleman açıma sokulabilir. 
    Örneğin:

    from scipy.stats import ttest_ind

    group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
    group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

    stat, p = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)

    Buradan şöyle bir çıktı elde edilmiştir:

    t-istatistiği: 3.8249455333812685
    p-değeri: 0.005054073879038671

    Çıktıdaki p değerine bakıldığında oldukça düşük olduğu görülmektedir. α değerinin 0.05 seçilmiş olduğunu varsayarsak p <= α 
    durumu oluşmaktadır. Bu durumda H0 hipotezi reddedilir. Bu da grup ortalamaları arasında istatistiksel olarak anlamlı 
    bir fark olduğu anlamına gelmektedir. Yani ortalamalar arasındaki bu farklılık normal olarak karşılanacak bir farklılık değildir. 

    statsmodels isimli kütüphanede de bağımsız örneklemler t-testi statsmodels.stats.weightstats modülündeki ttest_ind 
    fonksiyonuyla uygulanmaktadır. Fonksiyonun parametrik yapısı şöyledir:

    statsmodels.stats.weightstats.ttest_ind(x1, x2, alternative='two-sided', usevar='pooled', 
            weights=(None, None), value=0)

    Fonksiyonun SciPy'daki ttest_ind fonksiyonuna çok benzediğine dikkat ediniz. Fonksiyonun geri dönüş değeri 3 elemanlı 
    bir demettir. Demetin elemanları sırasıyla t istatistiğini, p değerini ve serbestlik derecesini vermektedir. Örneğin:

    from statsmodels.stats.weightstats import ttest_ind

    group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
    group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

    stat, p, df = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)
    print('serbestlik derecesi:', df)

    Buradan şöyle çıktı elde edilmiştir:

    t-istatistiği: 3.8249455333812716
    p-değeri: 0.005054073879038644
    serbestlik derecesi: 8.0

    Buradaki serbestlik derecesi iki grubun elemanları toplamından 2 eksik olan değerdir. 
#----------------------------------------------------------------------------------------------------------------------------

from statsmodels.stats.weightstats import ttest_ind

group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

stat, p, df = ttest_ind(group1, group2)

print('t-istatistiği:', stat)
print('p-değeri:', p)
print('serbestlik derecesi:', df)

#----------------------------------------------------------------------------------------------------------------------------
    Daha önce üzerinde çalıştığımız zambak veri kümesini anımsayınız. Bu veri kümesinde zambaklar üç sınıfa ayrılmıştı. Veri 
    kümesi şu görünümdeydi:

    Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
    1,5.1,3.5,1.4,0.2,Iris-setosa
    2,4.9,3.0,1.4,0.2,Iris-setosa
    3,4.7,3.2,1.3,0.2,Iris-setosa
    4,4.6,3.1,1.5,0.2,Iris-setosa
    ...
    104,6.3,2.9,5.6,1.8,Iris-virginica
    105,6.5,3.0,5.8,2.2,Iris-virginica
    106,7.6,3.0,6.6,2.1,Iris-virginica
    107,4.9,2.5,4.5,1.7,Iris-virginica
    ...
    52,6.4,3.2,4.5,1.5,Iris-versicolor
    53,6.9,3.1,4.9,1.5,Iris-versicolor
    54,5.5,2.3,4.0,1.3,Iris-versicolor
    55,6.5,2.8,4.6,1.5,Iris-versicolor
    56,5.7,2.8,4.5,1.3,Iris-versicolor
    57,6.3,3.3,4.7,1.6,Iris-versicolor

    Burada araştırmacı "Iris-stosa" ile "Iris-virginica" zambak türlerinin PetalLengthCm özelliklerine ilişkin ortalamalarının 
    anlamlı bir biçimde farklı olup olmadığını test etmek istesin. Hipotezler şöyle oluşturulabilir:

    H₀: "Iris-stosa" ve "Iris-virginica" zambak türlerinin PetalLengthCm ortalamaları birbirine eşittir.
    H₁: "Iris-stosa" ve "Iris-virginica" zambak türlerinin PetalLengthCm ortalamaları birbirine eşit değildir. 

    Testi statsmodels kütüphanesini kullanarak şöyle yapabiliriz:

    df = pd.read_csv('iris.csv')

    group1 = df.loc[df['Species'] == 'Iris-setosa', 'PetalLengthCm']
    group2 = df.loc[df['Species'] == 'Iris-virginica', 'PetalLengthCm']

    stat, p, df = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)
    print('serbestlik derecesi:', df)

    Elde edilen çıktı şöyledir:

    t-istatistiği: -39.46866259397271
    p-değeri: 5.717463758170621e-62
    Serbestlik Derecesi (df): 98.0

    Görüldüğü gibi p değeri çok çok küçüktür. Bu durumda H0 hipotezi reddedilir. Yani iki zambak türünün PetalLengthCm 
    ortalamaları anlamlı bir biçimde birbirinden farklıdır. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from statsmodels.stats.weightstats import ttest_ind

df = pd.read_csv('iris.csv')

group1 = df.loc[df['Species'] == 'Iris-setosa', 'PetalLengthCm']
group2 = df.loc[df['Species'] == 'Iris-virginica', 'PetalLengthCm']

stat, p, df = ttest_ind(group1, group2)

print('t-istatistiği:', stat)
print('p-değeri:', p)
print('serbestlik derecesi:', df)

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-tesinin dayandığı istatistiksel temel hakkında da bazı açıklamalar yapalım. İki örneğin elde 
    edildiği anakütle X ve Y olduğunu varsayalım. Bunların normal dağılmış olduğunu varsaymıştık:

    X ~ N(μ₁, σ₁²) 
    Y ~ N(μ₂, σ₂²) 

    Örneklem ortalamalarının beklenen değerleri şöyle olacaktır (merkezi limit teoreminden):

    E(X̄) = μ₁
    E(Ȳ) = μ₂

    Örneklem ortalamalarının varyansları da şöyle olacaktır (merkezi limit teoreminden):

    Var(X̄) = σ₁²/n₁
    Var(Ȳ) = σ₂²/n₂

    Bu durumda örneklem ortalamaları arasındaki farkların beklenen değeri şöyle olur:

    E(X̄ - Ȳ) = E(X̄) - E(Ȳ) = μ₁ - μ₂

    Örneklem ortalamalarının varyansı şöyle olacaktır:

    Var(X̄ - Ȳ) = Var(X̄) + Var(Ȳ) = σ₁²/n₁ + σ₂²/n₂

    Örneklem ortalamaları arasındaki farkların standart hatasını şöyle ifade edebiliriz:

    SE(X̄ - Ȳ) = √(σ₁²/n₁ + σ₂²/n₂)

    Eğer anakütle varyansları biliniyorsa örneklem farklarına ilişkin standart normal dağılım şöyle elde edilebilir:

         (X̄ - Ȳ) - (μ₁ - μ₂)
    Z =  ----------------------
          √(σ₁²/n₁ + σ₂²/n₂)

    Ancak genellikle anakütlelere ilişkin varyanslar bilinmez. Bu durumda standart normal dağılım yerine t dağılımı 
    kullanılmalıdır:

    s₁² = Σ(Xᵢ - X̄)² / (n₁ - 1)
    s₂² = Σ(Yⱼ - Ȳ)² / (n₂ - 1)

    Anakütle varyanslarının eşit olduğunu kabul ettiğimizde ağırlıklandırılmış bileşik varyans (pooled variance)
    şöyle hesaplanmaktadır:

    s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2)

    Nihayetinde t istatistiği de şu hale gelecektir:

       (X̄ - Ȳ) - (μ₁ - μ₂)
   t = -----------------------
       sₚ√(1/n₁ + 1/n₂)

    Serbestlik derecesi de df = n₁ + n₂ - 2 olacaktır. 
    
    Eğer anakütle varyansları eşit kabul edilmezse buna Welch t-testi denir. Bu duurmda t istatistiği ve serbestlik derecesi 
    şu hale gelmektedir:

        (X̄ - Ȳ) - (μ₁ - μ₂)
    t = -------------------
        √(s₁²/n₁ + s₂²/n₂)

         (s₁²/n₁ + s₂²/n₂)²
    df = ---------------------------------------
         [(s₁²/n₁)²/(n₁-1)] + [(s₂²/n₂)²/(n₂-1)]

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Çok kullanılan hipotez testlerinden biri de "varyans analizi (analysis of variance)" denilen kısaca ANOVA biçiminde de
    ifade edilen hipotez testidir. Varyans analizi "tek yönlü (one-way)" ve "çift yönlü (two-way)" olmak üzere iki biçimde 
    uygulanabilmektedir. Biz burada önce tek yönlü varyans analizini sonra da çift yönlü varyans analizini ele alacağız. 

    Tek yönlü varyans analizi (One-Way ANOVA) 2'den fazla grubun ortalamaları arasında anlamlı bir farkın olup olmadığını 
    test etmek amacıyla kullanılmaktadır. Yani bu anlamda bağımsız örneklemler t-testinin çok gruplusu gibi düşünülebilir. 
    Tek yönlü ANOVA testi için sağlanması gereken koşullar şunlardır:
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
<BURADA KALDIK>
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesi uygulamaları için çeşitli kurumlar tarafından cloud temelli hizmetler sunulmaktadır. Bu cloud 
    hizmetlerinin en yaygın kullanılanları şunlardır:

    - Goodle Cloud Platform (Vertex AI)
    - Amazon Web Services (Sage Maker)
    - Microsodt Azure
    - IBM Watson

    Bu servislerin hepsinin ortak birtakım özellikleri ve amaçları vardır:

    - Bu platformlar bize CPU ve bellek sağlamaktadır. Dolayısıyla bizim makine öğrenmesi işlemleri için ayrı bir makine tahsis 
    etmemize gerek kalmaz. Pek çok modelin eğitimi günlerce sürebilmektedir. Bunun için makinenin evde tutulması uygun olamayabilir. 
    
    - Bu platformlar "ölçeklenebilir (scalable)" çözümler sunmaktadır. Yani kiralanan birimler büyütülük küçültülebilmektedir. 

    - Bu platformlar "deployment" için kullanılabilmektedir. Yani burada eğitilen modellerle ilgili işlemler Web API'leriyle
    uzaktan yapılabilmektedir. (Örneğin biz makine öğrenmesi uygulamasını buralarda konuşlandırabiliriz. predict işlemlerini 
    cep telefonumuzdaki uygulamalardan yapabiliriz. Böylece uygulamamız mobil aygıtlardan da web tabanlı olarak kullanılabilir 
    hale gelmektedir.)

    - Bu platformlar kendi içerisinde "Automated ML" araçlarını da bulundurmaktadır. Dolayısıyla aslında konunun teorisini bilmeyen 
    kişiler de bu Automated ML araçlarını kullanarak işlemlerini yapabilmektedir. 

    Yukarıdaki platformlar (IBM Watson dışındaki) aslında çok genel amaçlı platformlardır. Yani platformlarda pek çok değişik hizmet de 
    verilmektedir. Bu platformalara "yapay ze makine öğrenmesi" unsurları son 10 senedir eklenmiş durumdadır. Yani bu platformlardaki 
    yapay zeka ve makine öğrenmesi kısımşları bu platformların birer alt sistemi gibidir. Bu platformların pek çok ayrıntısı olduğunu 
    hatta bunlar için sertifikasyon sınavlarının yapıldığını belirtmek istiyoruz. 

    Tabii yukarıdaki platformlar ticari platformlardır. Yani kullanım için ücret ödenmektedir. Ücret ödemesi "kullanım miktarı ile"
    ilişkilidir. Yani ne kadar kullanılırsa o kadar ücret ödenmektedir. (Bu bakımdan modellerin eğitimini unutursanız, bu eğitimler
    bu platformun kaynaklarını kullandığı için ücretlendirilecektir. Denemeler yaparken bu tür hesaplamaları durdurduğunuzdan emin 
    olmalısınız.) Tabii bu platformlarda da birtakım işlemler bedava yapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bütün cloud sistemlerinde makine öğrenmesi işlemleri yapılırken birbirleriyle ilişkili üç etkinlik yürütülür: 
    
    Data + Model + Hesaplama 

    Üzerinde çalışacağımız veriler genellikle bu cloud sistemlerinde onların bu iş için ayrılan bir servisi yoluyla upload 
    edilir. Model manuel ya da otomatik bir biçimde oluşturulmaktadır. Cloud sistemleri kendi içerisindeki dağıtık bilgisayar
    sistemleri yoluyla model üzerinde eğitim, kestirim gibi işlemler yapmamıza olanak vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Google Cloud Platform (kısaca GCP) Amazon AWS ve Microsoft Azure platformlarının doğrudan rekabetçisi konumundadır. 
   GCP 2008’de kurulmuştur. Aslında diğer platformlarda olan servislerin tamamen benzeri GCP’de bulunmaktadır. 
   GCP’ye erişmek için bir Google hesabının açılmış olması gerekir. 

    GCP’nin ana sayfası şöyeldir:

    https://cloud.google.com/

    GCP işlemlerini yapabilmek için kontrol panele (konsol ortamına) girmek gerekir. Kontrol panel adresi de şöyledir:

    https://console.cloud.google.com

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   GCP’de tüm işlemler bir proje eşliğinde yapılmaktadır. Çünkü işletmeler değişik projeler için değişik hizmetler alabilmektedir. 
   Projenin yaratımı hemen konsole sayfasından yapılabilmektedir. Projeyi yarattıktan sonra aktif hale getirmek (select etmek) gerekir. 
   Proje aktif hale geldiğinde proje sayfasına geçilmiş olur. Tabii proje yaratmak için bizim Google'a kredi kartımızı vermiş olmamız
   gerekir. Yukarıda da belirttiğimiz gibi biz kredi kartını vermiş olsak bile Google kullanım kadar para çekmektedir. 
   Projenin "dashboard" denilen ana bir sayfası vardır. Burada projeye ilişkin pek çok özet bilgi ve hızlı erişim bağlantıları 
   bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP’nin -tıpkı diğer platformlarda olduğu gibi- “CPU + RAM” kiralaması yapan ve “Compute Engine” denilen bir servisi vardır. 
    Benzer biçimde yine veritablarını host etmek için ve birtakım dosyaları saklamak için kullanılabilecek “Cloud Storage” hizmeti 
    bulunmaktadır. 

    GCP içerisinde birtakım servislerin erişebileceği bir storage alanına gereksim duyulmaktadır. Bunun için “Cloud Storage” 
    hizmetini seçmek gerekir. Ancak Google bu noktada sınırlı bedava bir hizmet verecek olsa da kredi kartı bilgilerini istemektedir. 
    Tıpkı AWS’de olduğu gibi burada da “bucket” kavramı kullanılmıştır. Kullanıcının önce bir “bucket yaratması” gerekmektedir.
    Bucket adeta cloud alanı için bize ayrılmış bir disk ya da klasör gibi düşünülebilir. Dosyalar bucket'lerin içerisinde bulunmaktadır.   
    Bucket yaratılması sırasında yine diğerlerinde olduğu gibi bazı sorular sorulmaktadır. Örneğin verilere hangi bölgeden erişileceği, 
    verilere hangi sıklıkta erişileceği gibi. Bucket’e verilecek isim yine AWS’de olduğu gibi GCP genelinde tek (unique) olmak zorundadır. 
    Bir bucket yaratıldıktan sonra artık biz yerel makinemizdeki dosyaları bucket'e aupload edebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında GCP üzerinde işlem yapmak için çeşitli komut satırı araçları da bulundurulmuştur. Biz bu komut satırı araçlarını 
    yerel makinemize install edip işlemleri hiç Web arayüzünü kullanmadan bu araçlarla da yapabilmekteyiz. Bu araçlar bizim 
    istediğimiz komutları bir script biçiminde de çalıştırabilmektedir. Aslında bu komut satırı araçları "Cloud Shell" ismiyle
    Web tabalı olarak uzak makinede de çalıştırılabilmektedir. 

    Yerel makinemize aşağıdkai bağlantıyı kullanarak gsutil programını kurabiliriz:

    https://cloud.google.com/storage/docs/gsutil_install

    Örneğin gsutil programı ile yerel makinemizdeki "cvid.csv" dosyasını GCP'deki bucket'imiz içerisine şöyle kopyalayabiliriz:

    gsutil cp covid.csv gs:/kaanaslan-test-bucket
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP içerisindeki Auto ML aracına "Vertex AI" denilmektedir. Vertex AI aracına erişmek için GCP kontrol panelindeki ana menüyü
    kullanabilirisniz. Vertex AI'ın ana kontrol sayfasına "Dashboard" denilmektedir. Dolayısıyla bizim Dashboard'a geçmemiz gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tipik olarak Vertex-AI'da işlem yapma aşamaları şöyledir:

    1) Veri kümesi bucket içerisine upload edilir. (Bu işlem Dataset oluşturulurken de yapılabilmektedir.)
    2) Dataset oluşturulur.
    3) Eğitim işlemi yapılır
    4) Deployment ve Test işlemleri yapılır
    5) Kestirim işlemleri yapılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Vertex AI'da ilk yapılacak şey bir "Dataset" yaratmaktır. Bunun için Vertex AI sayfasındaki "Datasets" sekmesi seçilir. 
    Buradan Create düğmesine basılır. Burada Dataset için bölge seçilir. (Bu bölgenin bucket ile aynı bölgede olması gerekmez
    ancak aynı bölgede olması daa uygundur.) Dataset'e bir isim verilir. Sonra problemin türü seçilir. Bir CSV dosyasından hareketle
    kestirim yapacaksak "Tabular" sekmesinden "Regression/Classification" seçilmelidir. Daytaset yaratıldıktan sonra artık bu dataset'in
    bir CSV dosyası ilişkilendirilmesi gerekmektedir. Ancak Vertex AI backet'teki CSV dosyalarını kullanabilmektedir. Burada üç seçenek 
    bulunmaktadır:

    * Upload CSV files from your computer
    * Select CSV files from Cloud Storage
    * Select a table or view from BigQuery

    Biz yerel bilgiyasarımızdaki bir CSV dosyasını seçersek zaten bu CSV dosyası önce bucket içerisine kopyalanmaktadır. 
    Eğer zaten CSV dosyasımız bir bucket içerisindeyse doğrudan bucket içerisindeki CSV dosyasını belirtebiliriz. BigQuery
    GCP içerisindeki veritabanı biçiminde organize edilmiş olan başka bir depolama birimidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Dataset oluşturulduktan sonra artık eğitim işlemine geçilebilir. Bunun için Vertex AI içerisindeki "Traning" sekmesi kullanılmaktadır. 
    Training sayfasına geçildiğinde "Create" düğmesi ile eğitim belirlemelerinin yapıldığı bölüme geçilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Training işleminde peşi sıra birkaç aşamadan geçilmektedir. "Traingng method" aşamasında bize hangi veri kümesi için eğitim 
    yapılacağı ve problemin bir "sınıflandırma problemi mi yoksa regresyon problemi mi" olduğu sorulur. Bundan sonra "Model details" 
    aşamasına geçilir. Bu aşamada bize veri kümesindeki kestirilecek sütunun hangisi olduğu sorulmaktadır. Bu aşamada "Advanced 
    Options" düğmesine basıldığında test ve sınama verilerinin miktarları belirlenebilmektedir. Default durumda test verileri ve 
    sınama verileri veri kümesinin %10'u biçiminde alınmaktadır. "Join featurestore" aşamasından doğrudan "Continue" ile geçilebilir. 
    Bundan sonra karşımıza "Training options" aşaması gelecektir. Burada eğitimde hangi sütunların kullanılacağı bize sorulmaktadır. 
    Yine bu aşamada da "Advanced Options" seçeneği vardır. Burada bize Loss fonksiyonu sorulmatadır. Tabii bunlar default değerlerle 
    geçilebilir. En sonunda "Compute and pricing" aşamasına gelinir. Burada dikkat etmek gerekir. Çünkü Google eğitimde harcanan
    zamanı ücretlendirmektedir. Google'ın ücretlendirme yöntemi aşağıdaki bağlantıdan imncelenebilir:

    https://cloud.google.com/vertex-ai/pricing

    Burada "Budget" eğitim için maksimum ne kadar zaman ayrılacağını belirtmektedir. Klasik tabular verilerde en az zaman 1 
    saat olarak, resim sınıflandırma gibi işlemlerde en az zaman 3 olarak girilebilmektedir. 

    En sonunda "Start Training" ile eğitim başlatılır. Eğitimler uzun sürebildiği için bitiminde e-posta ile bildirm yapılmaktadır. 

    Eğitim bittikten sonra biz eğitim hakkında bilgileri Training sekmesinden ilgili eğitimin üzerine tıklayarak görebiliriz. 
    Eğer problem regresyon problemi ise modelin başarısı çeşitli metrik değerlerle gösterilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kesitirim işlemlerinin yapılabilmesi için önce modelin "deploy edilmesi ve bir endpoint oluşturulması" gerekmektedir. 
    Modelin deploy edilmesi demek cloud sistemi içerisinde dışarıdan kullanıma hazır hale getirilmesi demektir. Böylece biz 
    kestirimi uzaktan programlama yoluyla da yapabiliriz. Deployment işlemi "Prediction" sekmesinden girilerek yapılabileceği gibi 
    "Model Registry" sekmesninden de yapılabilmektedir. EndPoint yaratımı sırasında bize Endpoint için bir isim sorulmaktadır. Sonra 
    model için bir isim verilmekte ve ona bir versiyon numarası atanmaktadır. Buradaki "Minimum number of compute nodes" ne kadar yüksek 
    tutulursa erişim o kadar hızlı yapılmaktadır. Ancak node'ların sayısı doğrudna ücretlendirmeyi etkilemektedir. Dolayısıyla burada 
    en düşük sayı olan 1 değerini girebilirsiniz. Daha sonra bize modelin deploy edileceği makinenin özellikleri sorulmaktadır. 
    Burada eğitimin başka bir makinede yapıldığına ancak sonucun kestirilmesi için başka bir makinenin kullanıldığına dikkat ediniz. 
    Modelimiz deploy edildikten sonra kullanım miktarı kadar ücretlendirme yapılmaktadır. Dolaysıyla denemelerinizden sonra 
    bu deployment işlemini silebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işleminden sonra artık predict işlemi yapılabilir. Bu işlem tamamen görsel arayüzle yapılabileceği gibi 
    Web API'leriyle ya da bunları kullanan Python koduyla da yapılabilmektedir. Eğer deploy edilmiş modelde kestirim 
    işlemini programlama yoluyla yapacaksanız bunun için öncelikle aşağıdaki paketi kurmanız gerekmektedir:

    pip install google-cloud-aiplatform

    Bundan sonra aşağıdaki import işlemini yapıp modüldeki init fonksiyonunun uygun parametrelerle çağrılması gerekmeketdir:

    from google.cloud import aiplatform

    aiplatform.init(....)

    predict işlemi için Training sekmesinden Deploy & Test sekmesini kullanmak gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Birden fazla predict işlemi "batch predict" denilen sekme ile yapılmaktadır. Uygulamacı kestirim için yine bir CSV dosyası 
    oluşturur. Bu CSV dosyasına bucket'e upload eder. Sonra "Batch predict" sekmesinden bu CSV dosyasına referans ederek 
    işlemi başlatır. Sonuçlar yine bu işlem sırasında belirlenen bucket'ler içerisinde CSV dosyaları biçiminde oluşturulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Vertex AI ile resim sınıflandırma işlkemi yapalım. Resim sınıflandırma gibi bir işlem şu aşamalardan geçilerek 
    gerçekleştirilmektedir:

    1) Resimler Google cloud'ta bir bucket'e upload edilir. 
    2) Resimler bir CSV dosyası haline getirilir. Tabii burada resmin içerisindeki data'lar değil onun bucket'teki yeri 
    kullanılmaktadır. 
    3) Bu CSV dosyasından hareketle Dataset oluşturulur. 
    4) Training işlemi yapılır.
    5) Deployment ve EndPoint ataması yapılır 
    6) Kestirim işlemi görsel atayüz yoluyla ya da WEB API'leri ya da Pythonkoduyla yapılır.

    Burada Dataset oluşturulurken bizden bir CSV dosyası istenmektedir. Bu CSV dosyası aşağıdaki gibi bir formatta oluşturulmalıdır:

    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı

    Örneğin:

    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (1).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (10).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (100).jpg,boot
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Metin sınıflandırmaları da nemzer biçmde yapılabilmektedir. Burada iki seçenek söz konusudur. Metinler ayrı dosyalarda 
    bulunudurulup dosyalar bucket içerisine upload edilebilir yine resim sınıflandırma örneğinde olduğu gibi CSV dosyası 
    metinlere ilişkin dosyalardan ve onların sınıflarından oluşturulabilir. Ya da doğrudan metinlerin kendisi ve onların sınıfları da
    CSV dosyasının içerisinde bulunabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Amazon firması Cloud paltformlarına ilk giren büyük firmalardandır. Amazon'un cloud platformuna AWS (Amazon Web Services)
    denilmektedir. AWS iki yüzün üzerinde servis barındıran dev bir platformdur. Platformun pek çok ayrıntısı vardır. Bu nedenle 
    platformun öğrenilmesi ayrı bir uzmanlık alanı haline gelmiştir. Biz kurusumuzda platformun yapay zeka ve makine öğrenmesi 
    için nasıl kullanılacağı üzerinde özet bir biçimde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS ortamında makine öğrenmesi etkinlikleri işleyiş olarak aslında daha önce görmüş olduğumuz Google Cloud Platform'a 
    oldukça benzemektedir.  Google Cloud Platform'daki "Vertex AI" servisinin Amazonda'ki mantıksal karşılığı "SageMaker"
    isimli servistir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin ana kontrol paneline aşağıdaki bağlantı ile erişilebilir:
    
    console.aws.amazon.com

    Tabii AWS hizmeti almak için yine bir kayıt aşaması gerekmektedir. AWS kaydı sırasında işlemlker için bizden kredi kartı 
    bilgileri istenmektedir. Ancak AWS diğerlerinde olduğu gibi "kullanılan kadar paranın ödendiği" bir platformdur.

    AWS'nin konsol ekranına giriş yapıldığında zaten bize son kullandığmız servisleri listelemektedir. Ancak ilk kez giriş 
    yapıyorsanız menüden "SageMaker" servisini seçmelisiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker'a geçildiğinde onun da bir "dash board" denilen kontrol paneli vardır. Burada biz notebook'lar yaratıp uzaktan 
    manuel işlemler yapabiliriz. Ancak SageMaker'ın Auto ML aracına "AutoPilot" denilmektedir. SageMaker'ı görsel olarak daha
    zahmetsiz kullanabilmek için ismine "Studio" denilen Web tabanlı bir IDE geliştirilmiştir. Son yıllarda "Google'ın collab'ına"
    benzer "Studio Lab" denilen bedava bir ortam da eklenemiştir. Kullanıcılar genellikle işlemlerini bu Studio IDE'siyle yapmaktadır. 
    SageMaker içerisinde "Studio"ya geçebilmek için en az bir "kullanıcı profilinin (user profile)" yaratılmış olması gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin veri saklamak için çeşitli servisleri vardır. Makine öğrenmesiş için en önemli depolama servisi S3 denilen servistir. 
    S3 servisinde de tıpkı GCP'de olduğu gibi "bucket" adı altında bir çeşit folder'lar oluşturulmaktadır. Sonra bu bucket'lere 
    dosyalar upload edilmektedir. Amazon "veri merkezlerini (data centers)" "bölge (zone)" denilen alnlarla bölümlere ayırmıştır. 
    Server'lar bu bölgelerin içerisindeki veri merkezlerinin içerisinde bulunmaktadır. Tıpkı GCP'de olduğu her bölgede her türlü 
    servis verilmeyebilmetedir. Kullanıcılar coğrafi bakımdan kendilerine yakın bölgeri seçerlerse erişim daha hızlı olabilmektedir. 

    Bir bucket yaratmak için ona "dünya genelinde tek olan (unique)" bir isim vermek gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker Studio'da Auto-ML etkinlikleri için Autopilot denilen uygulama kullanılmaktadır. Dolayısıyla Auto-ML işlemi için 
    AutoML seçilebilir. Autopilot'ta bir Auto-ML çalışması yapmak için bir "experiment" oluşturmak gerekir. Experiment 
    oluşturabilmek için "File/New/Create AutoML Experiment" seçilebilir ya da doğrudan Auto ML (Autopilot) penceresinde de 
    "Create Autopilot Experiment" seçilebilir. Yeni bir experiment yaratılırken bize onun ismi ve CSV dostasının bucket'teki 
    yeri sorulmaktadır. Sonra Next tuşuna basılarak bazı gerekli öğeler belirlenir. Örneğin tahmin edilecek hedef sütun ve 
    kestirimde kullanılacak sütunlar bu aşamada bize sorulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eğitim işlemi bittiğinde veri dosyanın bulunduğu bucket içerisinde bir klasör yaratılır ve bu klasör içerisinde model ile 
    ilgili çeşitli dosyalar bulundurulur. Buradaki iki dosya önemlidir:

    SageMakerAutopilotDataExplorationNotebook.ipynb
    SageMakerAutopilotCandidateDefinitionNotebook.ipynb
    
    Buradaki "SageMakerAutopilotDataExplorationNotebook.ipynb" dosyası içerisinde veriler hakkında istatistiksel bşrtakım özellikler 
    raporlanır. "SageMakerAutopilotCandidateDefinitionNotebook.ipynb" dosyasının içerisinde ise Autopilot'ın bulduğu en iyi modellerin
    nasıl işleme sokulacağına ilişkin açıklamalar bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kestirim yapabilmek için EndPoint oluşturulmuş olması gerekmektedir. Tıpkı GCP'de olduğu gibi modelin çalıştırılabilmesi için 
    bir server'a deploy edilmesi egrekmektedir. Deploy işlemi sonucunda bize bir EndPoint verilir. Biz de bu EndPoint'i kullanarak 
    Web arayüzü ile ya da Python programı ile uzaktan kestirimde bulunabiliriz. 
    
    AWS'de uzaktan Python ile işlem yapabilmek için "sagemaker" ve "boto3" gibi kütüphaneler oluşturulmuştur. Kütüphaneler şöyle yüklenebilir.  

    pip install sagemaker
    pip install boto3

    sagemaker kütüphanesi Web arayüzü ile yapılanları programlama yoluyla yapabilmekt için boto3 kütüphanesi ise uzaktan
    kesitirm (prediction) gibi işlemleri yapabilmek için kullanılmaktadır. 

    sagemaker kütüphanesi ile uzaktan işlemlerin yapılması kütüphanenin dokümantasyonlarında açıklanmıştır. Aşağıdaki 
    bağlantıyı kullanarak kodlar üzerinde değişiklikler yaparak ve kodlarda ilgili yerleri doldurarak uzaktan işlemler yapabilirsiniz:

    https://sagemaker.readthedocs.io/en/stable/overview.html
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   boto3 kütüphanesi ile uzaktan işlemler yapılırken önce bir Session nesnesinin yaratılması gerekmektedir. Sessin nesnesi yaratılırken 
   bizim AWS kaynaklarına ulaşabilmemiz için iki parola bilgisine sahip olmamız gerekir. Birincisi “aws_access_key_id” ve 
   ikincisi de “aws_secret_access_key”. Amazon servisleri uzaktan erişimler için “public key/private key” kriptografi uygulamaktadır. 
   Bu parola bilgileri Session nesnesi yaratılırken aşağıdaki verilebilir:

    import boto3

    session = boto3.Session(aws_access_key_id=XXXXX', aws_secret_access_key='YYYYY')

    Session nesnesi yaratıldıktan sonra hangi servisin kullanılacağını belirten bir kaynak nesnesi yaratılır. Örneğin:

    s3 = session.resource('s3')

    Aslında bu kaynak nesneleri session nesnesi yaratılmdan doğrudan da yaratılabilmektedir. Ancak parolaların bu durumda 
    “~/.aws/credentials” dosyasına aşağıdaki formatta yazılması gerekir:

    [default]
    aws_access_key_id = YOUR_ACCESS_KEY
    aws_secret_access_key = YOUR_SECRET_KEY

    Burada yukarıdaki iki anahtarı elde etme işlemi sırasıyla şu adımlarla yapılmaktadır:

    1) https://console.aws.amazon.com/iam/ Adresinden IAM işlemlerine gelinir. 
    2) Users sekmesi seçilir
    3) Kullanıcı ismi seçilir
    4) "Security credentials" sekmesi seçilir. 
    5) Buradan Create Acces Key seçilir. 

    İşlemler sırasında eğer yukarıdaki anahtarlar girilmek istenmiyorsa (bu amahtarların görülmesi istenmeyebilir) yukarıda da belirttiğimiz
    gibi bu anahtarlar özel bir dosyanın içerisine yazılabilir. Oradan otomatik alınabilir. Eğer bu anahtarlar ilgili dosyanın 
    içerisine yazılmışsa Session nesnesi yaratılırken parametre bu iki anahtarı girmemize gerek kalmaz. Örneğin:

    session = boto3.Session()

    Bu dosya bu bilgiler Amazon'un komut satırından çalışan aws programıyla da girilebilmektedir. Amazon'un komut satırından çalışan aws programını aşağıdaki 
    bağlantıdan inmdirerek kurabilirsiniz:

    https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

    Bu iki anahtarı ilgili dosyaya yazmak için aws programı şöyle kullanılabilir:

    aws configure

    Biz programlama yoluyla uzaktan bu bucket işlemlerini yapabiliriz. Örneğin tüm bucket'lerin isimleri aşağıdaki gibi elde edilebilir:

    for bucket in s3.buckets.all():
        print(bucket.name)
    
    Belli bir bucket'teki dosya aşağıdaki gibi download edilebilmektedir:

    s3 = boto3.client('s3')
    s3.download_file('kaanaslan-test-bucket', 'x.txt', 'y.txt')

    Burada söz konusu bucket içerisindeki "x.txt" dosyası "y.txt" biçiminde download edilmiştir. 

    Uzaktan predict işlemi yine boto3 kütüphanesi ile yapılabilmektedir. Aşağıda buna bir ilişkin bir örnek verilmiştir:

    import boto3

    session = boto3.Session(aws_access_key_id='AKIAWMMTXFTMCYOF352A',aws_secret_access_key='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', region_name='eu-central-1')

    predict_data = '''6,148,72,35,0,33.6,0.627,50
    1,85,66,29,0,26.6,0.351,31'
    '''

    client = session.client('runtime.sagemaker')
    response = client.invoke_endpoint(EndpointName='diabetes-test', ContentType='text/csv', Accept='text/csv', Body=predict_data)

    result = response['Body'].read().decode()
    print(result)

    Burada Session sınıfının client metodu kullanılarak bir sagemaker nesnesi elde edilmiştir. Sonra bu nesne üzerinde invoke_endpoint
    metodu çağrılmıştır. Tabii arka planda aslında işlemler Web Servisleriyle yürütülmektedir. Bu boto3 kütüphanesi bu işlemleri kendi 
    içerisinde yapmaktadır. Gelen mesajdaki Body kısmının elde edilip yazdırıldığında dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Microsoft Azure 2009 yılında kurulan bir bulut sistemidir. 2014 yılında Microsoft bu Azure sistemine yapay zeka ve makine 
   öğrenmesine ilişkin servisleri eklemiştir. Microsoft Azure daha önce görmüş olduğumuz Google Cloud Platform ve Amazon AWS 
   sistemine benzetilebilir. Benzer hizmetler Azure üzerinde de mevcuttur. Azure ML de hiç kod yazmadan fare hareketleriyle 
   ve Auto MLaraçlarıyla kullanılabilmektedir. Tıpkı Google Cloud Platform ve Amazon SageMaker’da olduğu gibi bir SDK eşliğinde 
   tüm yapılan görsel işlemler programlama yoluyla da yapılabilmektedir. Microsoft Azure ML için tıpkı GCP ve AWS’de olduğu gibi 
   sertifikasyon süreçleri oluşturmuştur. Bu konuda sertifika sınavları da yapmaktadır. Yani Azure sistemi bütün olarak bakıldığında 
   çok ayrıntılara sahip bir sistemdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure kullanımı için ilk yapılacak şey bir e-posta ile Microsoft hesabı açmaktır. Sonra bu hesap kullanılarak Azure hesabı 
    açılmalıdır. Azure hesabı açılırken "bedava" ve "ödediğn kadar kullan" biçiminde seçenekler karşımıza gelmektedir. Bedava 
    kullanımın pek çok kısıtları vardır. Bu nedenle deneme hesabınızı "ödediğin kadar kullan" seçeneği ile oluşturabilirisiniz. 
    Ancak kullanmadığınız servisleri her ihtimale karşı kapatmayı unutmayınız. Azure sistemine abona olunduktan sonra bir 
    kullajıcı için "abone ismi" oluşturulmaktadır. 
        
    Azure sistemini e-posta ve parola ile girildikten sonra ana yönetim sayfası portal sayfasıdır. Portal sayfasına 
    doğrudan aşağıdaki bağlantı ile girilebilmektedir:
    
    https://portal.azure.com

    Azure'ün ana sayfasına geçtikten sonra buradan Yapay Zeka ve Makine Öğrenmesi için "Azure Machine Leraning" seçilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure Machine Learning sayfasına geçildiğinde önce bir "Workspace" seçiminin yapılması gerekmektedir. Workspace yapılacak çalışmalar 
    için bir klasör gibi bir organizasyon oluşturmaktadır. Yeni bir Workspace oluşturabilmek için "Oluştur (Create)" düğmesine basılır. 
    Ancak bir "workspace" oluştururken bizim bir "kaynak grubuna (resource group)" ihtiyacımız vardır. Bu nedenle önceden bir 
    kaynak grubu oluşturulmuş olmalıdır. Kaynak grubu oluşturabilmek için ana menüden (hamburger menüden) "Kaynak Grupları (Resource Gropus)"
    seçilir. Kaynak grubu birtakım kaynakların oluşturduğu gruptur. Workscpace de bir kaynaktır. Dolayısıyla workspace'ler 
    kaynak gruplarının (resource groups) bulunurlar. Kaynak Grupları menüsünden "Oluştur (Create)" seçilerek kaynak grubu oluşturma
    sayfasına geçilir. Yaratılacak kaynak grubuna bir isim verilir. Bütün bu isimler dünya genelinde tek olmak zorundadır. 
    Kaynak Grupları diğer cloud sistemlerind eolduğu gibi bölgelerle ilişkilendirilmiştir. Bu nedenle kaynak grubu yaratılırken 
    o kaynak grubunun bölgesi de belirtilir. 

    Workspace oluştururken bizden bazı bilgilerin girilmesi istenmektedir. Ancak bu bilgiler default biçimde de oluşturulabilmektedir. 
    Ancak bizim workspace'e bir isim vermemiz ve onun yer alacağı kaynak grubunu (resource group) belirtmemiz gerekir. Bu adımlardan 
    sonra nihayet workspace oluşturulacaktır. Tabii bir workspace oluşturduktan saonra tekrar tekrar workspace oluşturmaya genellikle 
    gerek yoktur. Farklı çalışmaları aynı workspace içerisinde saklayabiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Microsoft da tıpkı Amazon'da olduğu gibi makine öğrenmesiişlemleri için Web tabanlı bir IDE benzeri sistem oluşturmuştur. 
    Buna "Machine Learning Studio" ya da kısaca "Studio" denilmektedir. Workspace'i seçip "Studio düğmesine basarak Studi IDE'sine 
    geçebiliriz. Auto ML işlemleri için Studio'da "Automated ML" sekmesine tıklanılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Machine Learning Studio'da Auto ML işlemlerini başlatmak için "Automated ML" sayfasında "New Automated ML Job" seçilir. 
    Daha önceden de gördüğümüz gibi bu tür Cloud Platformlarında dört ana unsur vardır:

    1) Storage (Eğitim için veri kümesisi barındırmak için ve eğitim sonucunda oluşturulacak dosyaları barındırmak için)
    2) CPU (Eğitimi yapabilmek için gereken makine)
    3) Model (Çeşitli yöntemlerle veri kümesine uygun en iyi ML modeli)
    4) Deployment ya da EndPoint (Hedef modelin konuşlandırılması ve Web Servisler yoluyla uzaktan kullanılabilir hale getirilmesi)

    Microsoft Azure sisteminde de "New Automated ML Job" işleminin ilk aşamasında bizden hangi veri kümesi üzerinde ML çalışması yapılacağı 
    sorulmaktadır. Biz bu aşamada yeni bir veri kümesini Azure'ün Storage sistemine upload edebiliriz. Ya da bu upload etme işlemi 
    daha önceden oluşturulabilir. Azure sisteminde upload edilmiş veri kümelerine "data asset" denilmektedir. "New Automated ML Job"
    işleminde toplam dört aşama bulunmaktadır:

    1) Select data asset: Bu aşamada üzerinde çalışılacak veri kümesi belirtilir. Yukarıda söz ettiğimiz gibi bu veri kümesi daha 
    önceden "data asset" biçiminde oluşturulmuş olabilir ya da bu aşamada oluşturulabilir. 

    2) Configure job: Burada işlem için önemli bazı belirlemeler yapılmaktadır. Örneğin yapılacak işleme bir isim verilmektedir. 
    Veri kümesindeki tahmin edilecek hedef sütun belirtilmektedir. Eğitim için kullanılacak makinenin türü de bu aşamada 
    sorulmaktadır. Makine türü için "Compute Instance" seçilebilir. Tabii bizim daha önceden yaratmış olduğumuz bir hesapalama 
    maknesi (compute instance) bulunmuyor olabilir. Bu durumda bir hesaplama makinesinin (yani eğitimde kullanılacak makinenin)
    yaratılması gerekecektir. Tabii aslında bir hesaplama makinesini (compute instance) daha önce de yaratmış olabiliriz. Hesaplama 
    makinesini daha önceden bu işlemden bağımsız olarak yaratmak için Manage/Compute sekmesi kullanılmaktadır. 

    3) Select task and settings: Burada bize problemin türü sorulmaaktadır. Tabii Azure hedef sütundan hareketle aslında problemin
    bir sınıflandırma (lojistik regresyon) problemi mi yoksa regresyon problemi mi olduğunu belirleyebilmektedir. 

    4) Hyperparameter Configuration: Bu aşamad bize sınama yönteminin ne olacağı ve test verilerinin nasıl oluşturulacağı sorulmaktadır.

    Bu aşamalardan geçildikten sonra Auto-ML en iyi modelleri bulmak için işlemleri başlatır. İşlemler bitince yine bize bildirimde bulunulmaktadır. 

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Auto-ML aracı işini bitirdikten sonra EndPoint oluşturulmultur. Biz Studo'da EndPoints sekmesine gelerek ilgili endpoint'in 
    üzerine tıkladığımızda yukarıdaki menüde "Details", "Test", "Consume" gibi seçenekler bulunur. Burada "Test" seçildiğinde 
    GUI'den kestirim yapılabilmektedir. "Consume" kısmında Web servisleri ile predict işlemi yapan bir Python kodu bulundurulmaktadır. 
    Ancak bu kodda api_key kısmı boş bir string'tir. Buradaki API key "Consume" sekmesinden "Primary Key" kısmından alınabilir. 

    Örneğin burada Consume sekmesindeki kod aşağıdaki gibidir. Ancak bu kodlarda dikkat edilmesi gereken nokta şudur: Bu kodlarda
    hesaba erişim için gereken "api key" boş bırakılmıştır. Bizim bu API key'i alıp buraya kopyalamamız gerekir. Daha önceden de 
    belirttiğimiz gibi bu API key Consume sekmesinden Primary Key alanından elde edilebilmektedir.  Buradaki "consume kodu" şaşağıda
    verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import urllib.request
import json
import os
import ssl

def allowSelfSignedHttps(allowed):
    # bypass the server certificate verification on client side
    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):
        ssl._create_default_https_context = ssl._create_unverified_context

allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.

# Request data goes here
# The example below assumes JSON formatting which may be updated
# depending on the format your endpoint expects.
# More information can be found here:
# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script
data =  {
  "Inputs": {
    "data": [
      {
        "age": 0,
        "sex": "example_value",
        "bmi": 0.0,
        "children": 0,
        "smoker": "example_value",
        "region": "example_value"
      }
    ]
  },
  "GlobalParameters": 0.0
}

body = str.encode(json.dumps(data))

url = 'https://kaanaslantestworkspace-wyndx.northeurope.inference.ml.azure.com/score'
# Replace this with the primary/secondary key or AMLToken for the endpoint
api_key = ''
if not api_key:
    raise Exception("A key should be provided to invoke the endpoint")

# The azureml-model-deployment header will force the request to go to a specific deployment.
# Remove this header to have the request observe the endpoint traffic rules
headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'automl457b251af41-1' }

req = urllib.request.Request(url, body, headers)

try:
    response = urllib.request.urlopen(req)

    result = response.read()
    print(result)
except urllib.error.HTTPError as error:
    print("The request failed with status code: " + str(error.code))

    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure
    print(error.info())
    print(error.read().decode("utf8", 'ignore'))

#----------------------------------------------------------------------------------------------------------------------------
    Aslında Microaoft tarafından hazırlanmış olan azureml isimli başka bir kütüphane daha bulunmaktadır. İşlemler bu kütüphane ile
    daha az kod yazarak da yapılabilmektedir. Bu kütüphane için aşağıdaki paketlerin yüklenmesi gerekmektedir:

    pip install azureml
    pip install azureml-core
    pip install azureml-data

    Aşağıda azureml kullanılarak kestirim işlemine örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import json
from pathlib import Path
from azureml.core.workspace import Workspace, Webservice
 
service_name = 'automl245eb70546-1',
ws = Workspace.get(
    name='KaanWorkspace',
    subscription_id='c06cf27d-0995-4edd-919a-47fd40f4a7ae',
    resource_group='KaanAslanResourceGroup'
)
service = Webservice(ws, service_name)
sample_file_path = '_samples.json'
 
with open(sample_file_path, 'r') as f:
    sample_data = json.load(f)
score_result = service.run(json.dumps(sample_data))
print(f'Inference result = {score_result}')

#----------------------------------------------------------------------------------------------------------------------------
    Azure'ün Machine Learning servisinde diğerlerinde henüz olmayan "designer" özelliği de bulunmaktadır. Bu designed sayesinde 
    sürükle bırak işlemleriyle hiç kod yazmadan makşne öğrenmesi modeli görsel biçimde oluşturulabilmektedir. Bunun için 
    Machine Learning Stdudio'da soldaki "Designer" sekmesi seçilir. Bu sekme seçildiğinde karşımıza bazı seçenekler çıkacaktır. 
    Biz hazır bazı şablonlar kullanarak ve şablonları değiştirerek işlemler yapabiliriz ya da sıfırdan tüm modeli kendimiz 
    oluşturabiliriz. 

    Model oluştururken sol taraftaki pencerede bulunan iki sekme kullanılmaktadır. Data sekmesi bizim Azure yükledeiğimiz veri 
    kümelerini göstermektedir. Component sekmesi ise sürüklenip bırakılacak bileşnleri belirtmektedir. Her bileşen dikdörtgensel 
    bir kutucuk ile temsil edilmiştir. Kod yazmak yerine bu bileşenler tasarım ekranına sürüklenip bırakılır. Sonra da bu bileşenler
    birbirlerine bağlanır. Sürüklenip bırakılan bileşenlerin üzerine tıklanıp farenin sağ tuşu ile bağlam menüsünden bileşene özgü 
    özellikler görüntülenebilir. Bu bağlam menüsünde pek çok bileşen için en önemli seçenek "Edit node name" seçenğidir. Bu seçenekte
    bileşene ilişkin özenmli bazı özellikler set edilmektedir. 

    Eğer model için şablon kullanmayıp sıfırdan işlemler yapmak istiyorsak işlemlere Data sekmesinde ilgili veri kümesini sürükleyip
    tasarım ekranına bırakmakla başlamalıyız. 

    Veri kümesini belirledikten sonra biz veri kümesindeki bazı sütunlar üzerinde işlem yapmak isteyebiliriz. Bunun için 
    "Select Columns in Dataset" bileşeni seçilir. Veri kümesinin çıktısı bu bileşene fare hareketi ile bağlanır. Sonra 
    "Select Columns in Dataset" bileşeninde bağlam menüsünden "Edit node name" seçilir. Buradan da "Edit columns" seçilerek 
    sütunlar belirlenir. 

    Bu işlemden sonra eksik veriler üzerinde işlemlerin yapılması isteniyorsa "Clean Missing Data" bileşni seçilerek tasarım 
    ekranına bırakılır. 

    Bundan sonra veri kümesini eğitim ve test olmak üzere ikiye ayırabiliriz. Bunun için "Split Data" bileşeni kullanılmaktadır. 
    Bu bileşende "Edit node names" yapıldığında bölmenin yüzdelik değerleri ve bölmenin nasıl yapılacağına yönelik bazı 
    belirlemeler girilebilir. 

    Bu işlemden sonra "Özellik Ölçeklemesi (Feature Scaling)" yapılabilir. Bunun için "Normalize Data" bileşeni seçilir. 
    Bu bileşende "Edit node names" seçildiğinde biz ölçekleme üzerinde belirlemeleri yapabilecek duruma geliriz. 

    Bu aşamalardan sonra artık sıra modelin eğitimine gelmiştir. Modelin eğitimi için "Train Model" bileşeni kullanılmaktadır. 
    Bu bileşenin iki girişi vardır. Girişlerden biri "Dataset" girişidir. Bu girişe veri test veri kümesi bağlanır. Bileşenin
    birinci girişi olan "Untrained model" girişine ise problemin türünü belirten bir bileşen bağlanır. Çeşitli problem türleri 
    için çeşitli bileşenler vardır. Örneğin ikili sınıflandırma problemleri için "Two class logistic regression" bileşeni kullanılır. 
    Eğitime ilişkin hyper parametreler bu bileşende belirtilmektedir. 

    Eğtim işleminden sonra sıra modelin test edilmesine gelmiştir. Modelin testi için "Score Model" bileşeni kullanılır. Score Model
    bileşeninin iki girişi vardır: "Trainded Model" ve "Dataset" girişleri. "Train Model" bleşeninin çıkışı "Trained Model" girişine, 
    "Split Data" bileşeninin ikinci çıkışı ise "Dataset" girişine bağlanır.

    Model Designer'da oluşturulduktan sonra artık sıra modelin eğitilmesine gelmiştir. "Configure & Submit" düğmesine basılır. 
    Burada artık eğitim için kullanılacak CPU kaynağı belirlenir. (Anımsayacağınız gibi Automated araçlarda bizim belirlememiz
    gereken üç unsur "Data" + "Model" + "CPU" biçimindeydi.) 

    Nihayet işlemleri başlatıp deployment işlemi için "Inference Pipeline"" seçeneği seçilmelidir. 

    Azure Designer'daki tüm bileşenler (components) aşağıdaki Microsoft bağlantısında dokümante edilmiştir:

    https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference?view=azureml-api-2


    Biz Inference Pipeline işlemini yaptığımızda Azure bize modelimizi kestirimde kullanılabecek hale getirmektedir. Bunun 
    için model bir "Web Service Output" bileşeni eklemektedir. Bu "Web Service Output" bileşeni kestirim çıktılarının 
    bir web servis biçiminde verileceği anlamına gelmektedir. Eskiden Azure aynı zamanda "Inference Pipeline" seçildiğinde
    modelimize bir "Web Service Input" bileşeni de ekliyordu. Bu bileşen de girdilerin web service tarafından alınacağını
    belirtmekteydi. Designer'ın yeni versiyonlarında "Infererence Pipeline" yapıldığında bu "Web Service Input" bileşeni artık
    eklenmemektedir. Web Service Input bileşeni eklendikten sonra artık bizim "Select Columns in Dataset" bileşeninden 
    kestirilecek sütunu çıkartmamız gerekmektedir. Ayrıca bu Web Service Input bileşeninin çıktısının artık "Select Columns in Dataset"
    bileşenine değil doğrudan ApplyTransformation bileşenine bağlanması gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Desginer'ın aytıntılı kullanımı için Microsoft dokümanlarını inceleyebilirsiniz. Örneğin aşağıdaki dokümanda lojistik olmayan 
    regresyon probleminin adım adım designer yardımıyla oluşturulması anlatılmaktadır:

    https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-train-score?view=azureml-api-1

#----------------------------------------------------------------------------------------------------------------------------
    IBM'in Cloud Platformu da en çok kullanılan platformlardan biridir. İşlevsellik olarak diğerlerine benzemektedir. IBM'de 
    cloud platformu içerisinde makine öğrenmesine ilişkin servisler bulundurmaktadır. Genel olarak IBM'in bu servislerine
    Watson denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformu için yine bir hesap açılması gerekir. Hesap açma ve sign in işlemleri cloud.ibm.com adresinden 
    yapılmaktadır. Hesap açılırken girilen e-posta adresi aynı zmaanda "IBMid" olarak kullanılmaktadır. IBMid cloud platformunda
    "user id" gibi kullanılmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformuna login olunduktan sonra karşımıza bir "Dashboard" sayfası çıkmaktadır. Burada ilk yapılacak şey 
    "Create Resource" seçilerek kaynak yaratılmasıdır. Buradan "Watson Studio" seçilir. Bedava hesp ile ancak bir tane "Watson Studio"
    kaynağı oluşturulabilmektedir. Kaynak oluşturulduktan sonra "Launch in IBM Cloud Pak for Data" düğmesine basılarak
    "IBM Watson Studio" ortamına geçilmektedir. IBM Watson Studio bir çeşit "Web Tabanlı IDE" gibi düşünülebilir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Watson Studio'ya geçildiğinde öncelikle bir projenin yaratılması gerekmektedir. Bunun için "New Project" seçilir. 
    Proje bir isim verilir. Sonra projenin yaratımı gerçekleşir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Proje yaratıldıktan sonra "New Asset" düğmesi le yeni bir proje öğesi (asset) oluşturulmalıdır. Burada Otomatik ML işlemleri 
    için "Auto AI" seçilebilir.
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Auto AI seçildiğinde yaratılacak işlem (experiment) için bir isim verilmelidir. Sonra bu işlem (experiment)" bir ML servisi 
    ile ilişkilendirilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Watson Studio'da Auto AI projesinde bizden öncelikle veri kümesinin yüklenmesi istenmektedir. Veri kümesi yüklendikten sonra 
    bu veri kümesinin ardışık "time series" verilerinden oluşup oluşmadığı bize sorulmaktadır. Bundan biz kesitim yapılacak sütun
    belirleriz. Uygulamacı problemin türüne göre problemin çeşitli meta parametrelerini kendisi set edebilmektedir. Bu işlem 
    "Experiment Settings" düğmesiyle yapılmaktadır. Nihayet uygulamacı "Run Experiment" seçeneği ile eğitimi başlatır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Modeller oluşturulduktan sonra bunlar performansa göre iyiden kötüye doğru sıralanmaktadır. Bu modeller save edilebilir. 
    Modeller save edilirken istenirse model kodları bir Jupyter Notebook olarak da elde edilebilmektedir. Bu notebook yerel makineye
    çekilip IBM Python kütüphanesi kurulduktan sonra yerel makinede de çalıştırılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işlemi için önce bir "deployment space" yaratılır. Ancak bu deployment space'te Assets kısmında deploy edilecek 
    modelin çıkması için dah önceden Model sayfasında "Promote to Deployment Space" seçilmelildir. Bundan sonra Deployment Space'te
    Assets kısmında ilgili model seçilerek "Create Deployment" düğmesi ile deployment işlemi yapılır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Deployment sonrası yine Wen Servisleri yoluyla model kullanılabilmektedir. Bu işlem Watson Web API'leriyle yapılabilecğei gibi
    diğer cloud platfotrmlarında olduğu gibi bu Web API'lerini kullanan Python kütüphaneleriyle de yapılabilmektedir. Kütüphane 
    aşağıdaki gibi install edilebilir:

    pip install ibm-watson-machine-learning   
#----------------------------------------------------------------------------------------------------------------------------



