#----------------------------------------------------------------------------------------------------------------------------
                                    
                                     Yapay Zeka, Makine öğrenmesi ve Veri Bilimi
                                                        Kursu
            
                                          Sınıfta Yapılan Örnekler ve Özet Notlar 
                                                       
                                                       3.Bölüm

                                                 Eğitmen: Kaan ASLAN
                                        
        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:           

        Aslan, K. (2025), "Yapay Zekâ, Makine Öğrenmesi ve Veri Bilimi Kursu", Sınıfta Yapılan Örnekler ve Özet Notlar, 
            C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları okurken editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                            Son Güncelleme: 19/10/2025 - Pazar

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            162. Ders - 19/10/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu son bölümünde bazı istatistik konuları üzerinde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Belli bir hipotezin (yani varsayımın) belli bir anlam düzeyinde (significance level) test edilmesini sağlayan istatistiksel 
    yöntemlere "hipotez testleri (hypothesis testing)" denilmektedir. Hipotez testleri başta sosyal bilimleri ve sağlık bilimleri
    olmak üzere endüstri mühendisliği, ekonomi ve işletme gibi pek çok alanda yaygın olarak kullanılmaktadır. 
    
    Örneğin yeni bir ilacın kan şekerini düşürdüğü iddia edilsin. Üretilecek olan bu ilacın gerçekten kan şekerini düşürüp 
    düşürmediğini nasıl anlayabiliriz? İlk akla gelecek yöntem bu ilacı çeşitli kişilere uygulayıp sonuçları karşılaştırmaktır. 
    Bunun için bir grup denek seçeriz. Deneklerin kan şekerini ilaç uygulanmadan önce ve uygulandıktan sonra ölçebiliriz. 
    Bu ölçümler arasında anlamlı bir fark olup olmadığına bakabiliriz. Ancak ne kadarlık bir fark bizim ilacın kan şekerini 
    düşürdüğünü kabul etmemiz için yeterli olacaktır? Örneğin kişinin tokluk kan şekeri 120 olsun. Bu ilaç bir kişide kan 
    şekerini 119'a diğer bir kişide 118'e düşürmüşse ancak diğer bir kişide ise hiçbir etki yaratmamışsa biz bu ilacın kan 
    şekerini düşürdüğünü söyleyebilir miyiz? Şüphesiz bizim hatırı sayılır bir düşüşü dikkate almamız gerekir. Öte yandan 
    ilaç herkeste tamamen aynı düşüşü de sağlamayabilecektir. O zaman bir biçimde bizim ortalama düşüşü dikkate almamız 
    gerekebilir. İşte önce eski durumu ölçüp sonra işlemi uygulayıp yeni durumu ölçüp aradaki farkların ortalamasına belli 
    bir güven düzeyi içerisinde bakarak sonuç çıkartan istatistiksel hipotez testleri geliştirilmiştir. Böyle bir test ile
    ilacın kan şekerini düşürme konusundaki etkisi herkesin kabul edebileceği biçimde nesnel olarak gösterilebilmektedir.
    (Gerçek yaşamda bir ilacın piyasaya sürülmesi için pek çok farklı testin belli prosedürlere uygun olarak yapılması 
    gerekmektedir.)
    
    Her hipotez testi tipik bazı deney kalıplarından elde edilen sonuçları test etmek için geliştirilmiştir. (Örneğin "kan 
    şekerini düşüren ilaç" örneğimize "ön test son test (pre-post test)" deney tasarımı denilmektedir.) Bu nedenle yalnızca 
    hipotez testlerinin nasıl uygulandığını bilmek yetmez, hangi durumlarda hangi testlerin uygulanması gerektiğini de bilmek 
    gerekir. Biz de bu bölümde çok kullanılan bazı deney kalıplarına yönelik hipotezlerin testleri üzerinde duracağız ve 
    bu testlerin Python'da programlama yoluyla nasıl yapıldığını uygulamalı olarak göreceğiz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri örneklemden hareketle ana kütle hakkında çıkarımlar yapmaktadır. Yukarıdaki örnekte biz ilacın kan şekerini 
    düşürüp düşürmediğini o ilacı belli sayıda kişilere uygulayarak test ettik. Ancak ilaç aslında herkese uygulanmalıydı. 
    Fakat bunun imkanın olmadığı da açıktır. O halde bu tür durumlarda ilacı belli sayıda kişiye uygulayıp onlardan elde edilen 
    sonuca dayalı olarak herkes hakkında genel bir hüküm verme yoluna gidilmektedir. Özetle hipotez testleri örneklemler üzerinde 
    uygulanmaktadır. Ancak test sonucunda ana kütle hakkında çıkarımlarda bulunulmaktadır. Örneğin bir havuzun değişik yerlerinden 
    damlalıkla su alıp onlardan birtakım ölçüm değerleri elde etmiş olalım. Sağlıklı bir suda kabul edilen değerleri de zaten 
    biliyor olalım. Havuzdaki suyun sağlıklı olup olmadığına havuzdaki tüm suya bakarak karar vermemekteyiz. Havuzdan belli 
    sayıda örnek alarak buna karar vermekteyiz. 

    2000'li yılların başlarında bilgisayar donanımlarının ve veri aktarım teknolojisinin gelişmesiyle birlikte "örneklemden 
    hareketle anakütle hakkında kestirimde bulunmaya" alternatif olarak "tüm anakütleyi ya da onun büyük kısmını göz önüne 
    alarak kestirimde bulunmayı" hedefleyen bir yaklaşım da ortaya atılmıştır. Büyük veri (big data) olarak isimlendirilen 
    bu yaklaşım pek çok uygulamada başarıyla kullanılmıştır. Büyük veri yaklaşımı aynı zamanda komşu disiplinleri de etkilemiş 
    makine öğrenmesi ve veri bilimi alanının gelişmesine de katkı sağlamıştır. Ancak büyük veri yaklaşımının her alanda 
    kullanılması mümkün değildir. Bu nedenle örnekleme temelli yöntemler geçerliliğini kaybetmiş değildir. Zaman içerisinde 
    örnekleme temelli yöntemlerle büyük veri yöntemlerinin hibrit bir biçimde uygulandığı yöntemler de geliştirilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri için araştırıcı öncelikle bir hipotez oluşturur. Bu hipoteze (yani uygulamacının arzu ettiği sonuca 
    ilişkin hipoteze) istatistikte genellikle H1 hipotezi ya da "alternatif hipotez" denilmektedir. Ancak "bir değişiklik 
    olmadığına yönelik", "eski duruma yönelik", "statükoya yönelik" hipoteze de H0 hipotezi ya da "null hipotez" denilmektedir. 
    O halde hipotez testlerinde iki hipotez vardır: H0 ve H1 hipotezleri. Bizim iddia ettiğimiz hipoteze H1 hipotezi, zaten 
    var olan duruma ilişkin hipoteze ise H0 hipotezi denilmektedir. Bu hipotezlere ilişkin birkaç örnek verelim:

        - Kan şekerini düşürdüğünü iddia ettiğimiz ilaç için ilacı uygulamadan ve ilacı uyguladıktan sonra ölçümler yapalım
        ("ön test son test" deney kalıbı). Sonra bu ölçümlerin ortalamasını hesaplayalım. Buradaki H0 ve H1 hipotezleri şöyle 
        oluşturulabilir:

        H0 Hipotezi: İlaç uygulanmadan önceki ortalama kan şekeri ile ilaç uygulandıktan sonraki ortalama kan şekeri arasında 
        anlamlı (belirlenen anlam düzeyi dikkate alındığında) bir farklılık yoktur. Bu hipotez matematiksel sembollerle şöyle 
        ifade edilebilir:

        H0: μ₀ = μ₁

        H1 Hipotezi: İlaç uygulanmadan önceki ortalama kan şekeri ilaç uygulandıktan sonraki ortalama kan şekerinden anlamlı 
        (belirlenen anlam düzeyinde dikkate alındığında) bir biçimde daha yüksektir. Bu hipotez matematiksel sembollerle şöyle 
        ifade edilebilir:
        
        H1: μ₀ > μ₁ 
        
        Burada μ₀ ilaç uygulanmadan önceki kan şekeri ortalamasını μ₁ ise ilaç uyguladıktan sonraki kan şekeri ortalamasını 
        belirtmektedir. 

    - Bir egzersizin uykusuzluğu azalttığına yönelik bir hipotez söz konusu olsun. Burada yine rastgele kişiler seçerek 
    onların egzersizden önceki uyku sürelerini ve egzersizden sonraki uyku sürelerini ölçebiliriz. Bunların ortalamalarını 
    hipotez testi ile karşılaştırmak isteyebiliriz. H0 ve H1 hipotezleri yine şöyle oluşturulabilir:

    H0: μ₀ = μ₁
    H1: μ₀ < μ₁ 

    Burada yine μ₀ egzersiz uygulanmadan önceki ortalamayı μ₁ ise egzersiz uygulandıktan sonraki ortalamayı belirtmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri genellikle iki gruba ayrılarak ele alınmaktadır:

    1) Parametrik hipotez testleri
    2) Parametrik olmayan hipotez testleri

    Parametrik hipotez testleri oransal (ratio) ve aralık (interval) ölçeklerine ilişkin değişkenlere uygulanabilmektedir. 
    Bu testlerde ortalama, standart sapma gibi dağılım parametreleri kullanılmaktadır ve örneklemin belli bir dağılıma (tipik 
    olarak normal dağılıma) uygun olması koşulu vardır. Parametrik olmayan testler herhangi bir ölçek türündeki değişkenlere 
    uygulanabilmektedir. Bu testlerin temel özelliği, örneklemden elde edilen değerlerin belli bir dağılıma uygunluk varsayımı 
    gerektirmemesidir. Kategorik (nominal) ve sıralı (ordinal) ölçekteki veriler için parametrik testler uygulanamadığından, 
    bu tür verilerde parametrik olmayan testler kullanılır. Ancak parametrik olmayan testler, dağılım varsayımlarını karşılamayan 
    oransal ve aralık ölçekli verilere de yaygın olarak uygulanmaktadır.
    
    Oransal ve aralık ölçekli değişkenlerde eğer dağılım varsayımları (normallik, varyans homojenliği gibi) karşılanıyorsa, 
    parametrik testler tercih edilmelidir. Çünkü parametrik testler daha yüksek istatistiksel güce (power) sahiptir ve daha 
    hassas sonuçlar verir. Ancak bu varsayımlar ihlal edildiğinde parametrik olmayan testler daha uygun ve güvenilir sonuçlar 
    sağlar.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Parametrik hipotez testleri büyük ölçüde "merkezi limit teoremine (central limit theorem)" dayandırılmaktadır. Anımsanacağı 
    gibi merkezi limit teoremi, özet olarak, bir ana kütleden seçilen yeterince büyük örneklem ortalamalarının normal dağılıma 
    yakınsadığı anlamına gelmektedir. Biz kursumuzda her bir hipotez testinin dayandığı istatistiksel ve matematiksel temel 
    üzerinde durmayacağız. Eğer konuyu matematiksel ve teorik ayrıntılarıyla öğrenmek istiyorsanız başka kaynaklara başvurabilirsiniz. 
    Kursumuzda biz daha çok uygulamaya yönelik bilgiler aktaracağız.

    Parametrik hipotez testlerinin en yaygın kullanılanları şunlardır:

    - Tek örneklem t-testi
    - Bağımsız örneklem t-testi
    - Eşleştirilmiş (bağımlı) örneklem t-testi
    - Tek yönlü ANOVA
    - İki yönlü ANOVA
    - Tekrarlı ölçümler için ANOVA
    - Pearson korelasyon testi
    - Doğrusal regresyon analizi
    - F-testi (varyans karşılaştırması)
    - Levene testi (varyans homojenliği)
    - Welch t-testi (eşit varyans varsayımı olmadan)

    Parametrik olmayan hipotez testlerinin ise en yaygın kullanılanları şunlardır:

    - Mann–Whitney U testi (Wilcoxon rank-sum testi)
    - Wilcoxon signed-rank testi
    - Kruskal–Wallis H testi
    - Friedman testi
    - Spearman sıra korelasyon testi
    - Kendall Tau korelasyon testi
    - Ki-kare (χ²) bağımsızlık testi
    - Ki-kare uygunluk testi
    - Fisher'in kesin testi
    - Sign testi
    - Median testi
    - Theil–Sen kestirimi (parametrik olmayan regresyon)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Sosyal bilimlerde ve sağlık bilimlerinde hipotez testleri için genellikle paket programlar kullanılmaktadır. Bunları 
    kullanmak için programlama bilmeye gerek yoktur. Bu alanda çok kullanılan paket programlardan bazıları şunlardır:

    - SPSS (PSPP isimli GNU versiyonu var)
    - SAS
    - Minitab
    - Matlab (Octave isimli GNU versiyonu var)

    R Programlama Dili oldukça basit "domain specific" matematiksel ve istatistiksel bir dilidir. Ancak profesyonel dünyada 
    paket programlardan ve R dilinden ziyade Python tercih edilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    İstatistiksel hipotez testleri için oluşturulmuş çeşitli kütüphaneler vardır. NumPy, Pandas ve scikit-learn hipotez 
    testlerini desteklemektedir. Python'da hipotez testleri için en yaygın kullanılan kütüphane statsmodels isimli kütüphanedir. 
    Ancak genel amaçlı nümerik analiz kütüphanesi olan SciPy kütüphanesi de hipotez testlerinin bazılarını bünyesinde barındırmaktadır. 
    Biz SciPy kütüphanesini zaten daha önce çeşitli konularda kullanmıştık. 
    
    statsmodels kütüphanesini şöyle kurabilirsiniz:

    pip install statsmodels
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testleri istatistikte uzun süredir iyi bilinen konulardandır. Bu nedenle konunun zaman içerisinde geniş ve yeterli 
    bir yabancı ve yerli literatürü oluşturulmuştur. Bu konuda  "Sosyal Bilimler İçin Veri Analizi El Kitabı (Şener Büyüköztürk)" 
    isimli kitabı kaynak kitap olarak kullanabilirsiniz. Bu kitap yöntemlerin istatistiksel ve matematiksel temeli üzerinde 
    ayrıntıya girmemekle birlikte "hangi durumlarda hangi hipotez testlerinin kullanılacağı konusunda" iyi bir bilgi sunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde iki önemli kavram vardır: α (alfa) ve p değeri. α, araştırmacı tarafından belirlenen tip I hata 
    olasılığıdır. Yani H0 doğru iken H0’ı reddetme olasılığıdır. p değeri ise test sonucunda elde edilen bir değerdir ve "H0 
    hipotezi doğru kabul edilirse, gözlenen test istatistiği kadar uç bir değer elde etme olasılığını" bileritmektedir. α değeri 
    ne kadar küçültülürse testin güvenilirliği o kadar artar. Örneğin test sonucunda p = 0.01 çıkmış olsun. Bu değer bizim 
    belirlediğimiz α değerinden küçüktür. Yani H0 hipotezinin reddedilme sınırın altına kalmaktadır. Dolayısıyla H0 hipotezi 
    reddedilir. Matematiksel olarak gösterirsek:

    - Eğer p < α ise H0 hipotezi reddedilir.
    - Eğer p ≥ α ise H0 hipotezi reddedilmez.

    Bunu bir tablo biçiminde de aşağıdaki gibi verebiliriz:

    ┌───────────────┬──────────────────┬──────────────────────────────────────────┐
    │ Karşılaştırma │      Karar       │                  Yorum                   │
    ├───────────────┼──────────────────┼──────────────────────────────────────────┤
    │  p < α        │ H₀ reddedilir    │ Sonuç istatistiksel olarak anlamlıdır    │
    ├───────────────┼──────────────────┼──────────────────────────────────────────┤
    │  p ≥ α        │ H₀ reddedilemez  │ Veri, H₀'ı reddetmek için yeterli değil  │
    └───────────────┴──────────────────┴──────────────────────────────────────────┘

    Hipotez testlerine p < α bölgesine "reddetme aralığı (rejection level)",  p ≥ α bölgesine ise kabul aralığı (acceptance 
    region) da denilmektedir.  p değerinin bir koşullu olasılık belirttiğine dikkat ediniz:

    p = P(gözlenen değer ∣ H₀ doğru)

    Başka bir deyişle p değeri "H0 doğru kabul edilirse bu veriyi elde etme olasılığımız nedir?" sorusunun yanıtını vermektedir. 

    Hipotez testleri genel olarak H0 hipotezinin reddedilip reddedilmeyeceği üzerine kurulmaktadır. H0 hipotezinin reddedilmesi, 
    H1 hipotezinin kesin olarak kabul edildiği anlamına gelmez; yalnızca verilerin H1 lehine anlamlı kanıt sunduğu anlamına 
    gelir. Ancak anlatımlarda çoğu kez H0 ya da H1 hipotezinin reddedilmesidiğerinin kabul edilmesi biçiminde ifade edilmektedir. 
    Fakat aslında doğru ifade biçiminin eğer "H0 hipotezi reddediliyorsa" "H1 lehine anlamlı kanıt vardır" biçiminde, eğer 
    "H0 hipotezi reddedilmiyorsa, "H1 hipotezi lehine anlamlı kanıt yoktur" biçiminde olması gerekmektedir. 

    Görüldüğü gibi α değeri bir eşik değerini belirtir ve "tolere edilebilir tip I hata oranı" anlamına gelmektedir. Güven 
    aralıklarında kullanılan güven düzeyi (confidence level) ise 1 − α şeklinde tanımlanır. Örneğin α = 0.05 seçildiğinde güven 
    düzeyi %95 olur ancak güven aralığı (confidence interval) bu güven düzeyine göre hesaplanan bir sayı aralığıdır ve α’nın 
    kendisiyle aynı şey değildir.

    Peki araştırmalarda α için genellikle hangi değerler kullanılmaktadır? Şüphesiz bu durum testin önemine göre değişebilir. 
    Tipik kullanılan α değeri 0.05'tir. Ancak örneğin sağlık bilimlerinde tolerans daha düşük olduğu için α değeri 0.01 
    olarak belirlenebilmektedir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde verilen karar için dört durum söz konusudur:

    1) H0 reddedilmemelidir, H0 reddedilmemiştir.
    2) H0 reddedilmemelidir, H0 reddedilmiştir.         ===> Tip 1 Hatası
    3) H0 reddedilmelidir, H0 reddedilmemiştir.         ===> Tip 2 Hatası
    4) H0 reddedilmelidir, H0 reddedilmiştir.

    İstatistiksl olarak anlatım doğru olmasa da bu durumları pratik olarak şöyle de ifade edebiliriz:

    1) H0 kabul reddedilmemelidir, H0 kabul edilmiştir.
    2) H0 kabul edilmelidir, H0 kabul edilmemiştir.     ===> Tip 1 Hatası
    3) H0 kabul edilmemelidir, H0 kabul edilmiştir.     ===> Tip 2 Hatası
    4) H0 kabul edilmemelidir, H0 kabul edilmemiştir.

    İşte H0'ın reddedilmemesi gerektiği halde H0'ın reddedildiği duruma, hatalı bir deyişle H0'ın kabul edilmesi gerektiği 
    halde H1'in kabul edilmesi durumuna "Tip 1 Hatası (Type 1 Error)", H0'ın reddedilmesi gerektiği halde H0'ın reddedilmediği 
    durumuna,  hatalı bir deyişle H1'in kabul edilmesi gerektiği halde H0'ın kabul edildiği duruma "Tip 2 Hatası (Type 2 Error)" 
    denilmektedir. "tip 1 Hatası" yerine "alfa hatası", "tip 2 hatası" yerine "beta hatası" deyimleri de kullanılmaktadır. 
    tip 1 ve tip 2 hataları bir tablo halinde aşağıdaki gibi de gösterebiliriz:

    ╔════════════╦════════════════════════════╦════════════════════════════════════╗
    ║            ║        Testin Kararı       ║            Gerçek Durum            ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║            ║ H₀ reddedilmez (kabul)     ║ H₀ reddedilir                      ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║ H₀ doğru   ║ Doğru karar                ║ Tip I hata (alfa hatası)           ║
    ║            ║ (H₀ doğru, reddedilmedi)   ║ (Yanlış alarm)                     ║
    ╠════════════╬════════════════════════════╬════════════════════════════════════╣
    ║ H₀ yanlış  ║  Tip II hata (β hatası)    ║  Doğru karar                       ║
    ║            ║ (Gerçek farkı göremedik)   ║ (Fark var, doğru tespit edildi)    ║
    ╚════════════╩════════════════════════════╩════════════════════════════════════╝
 
    Tip 1 hatanın "false positive", "Tip 2 hatanın ise "false negative" anlamına da geldiğine dikkat ediniz.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinden en temel olanlardan birisi "bağımsız örneklemler t-testi" denilen testtir. Bu teste İngilizce 
    "independent samples t-test" ya da "two-sample t-test" ya da "unpaired t-test" de denilmektedir. Bu test iki bağımsız 
    grubun elde ettiği puanların ortalamasının karşılaştırılması amacıyla kullanılmaktadır. Bu test için tipik deney kalıbı 
    şöyledir: İki bağımsız grup vardır. Bir deney bu iki gruba ya da gruplarından birine ayrı ayrı uygulanmaktadır. Deney 
    sonucunda "grupların elde ettiği ortalama puanlar arasında anlamlı bir fark var mı" diye bakılmaktadır. Tipik olarak 
    gruplardan biri "kontrol grubu" olabilmektedir. Kontrol grubu üzerinde bir işlem uygulanmaz. Yalnızca diğer grup üzerinde 
    işlem uygulanır. Ya da iki grup üzerinde ayrı işlemler uygulanır. Amaç iki grubun ortalama puanlarının karşılaştırılmasıdır. 
    Bağımsız örneklemler t-tesindeki "bağımsız (independent)" ya da "eşleşmemiş (unpaired)" terimi iki kümenin elemanlarının 
    birbirinden farklı olması anlamına gelmektedir. Yani söz konusu deneydeki iki grupta ortak eleman yoktur. 
    
    Aşağıda bağımsız örneklemler t-tesinin kullanılabileceği birkaç deney kalıbı veriyoruz: 

    - Bir sınıfta girilen sınavdaki matematik puanlarının kız öğrencilerle erkek erkek öğrenciler arasında farklılık gösterip
    göstermediği istatistik bakımdan test edilmek istenmektedir. Bunun için bağımsız örneklemler t-testi kullanılabilir. 
    Burada kız ve erkek öğrencilerin bağımsız gruplar oluşturduğuna dikkat ediniz. 

    - Bir hastalığın tedavisi için iki alternatif ilaç vardır. Amaç bu iki ilacın tedavi süresi bakımından yarattığı etkinin 
    birbirinden farklı olup olmadığını anlamaktır. İki ilaç iki farklı gruba verilir. İki farklı gruptaki kişilerin iyileşme 
    süreleri elde edilir. Bunların ortalamaları arasında anlamlı bir fark olup olmadığına bağımsız örneklemler t-testi uygulanarak 
    karar verilebilir.  

    - Sigara içenlerle içmeyenler arasında akciğer kapasitesi bakımından bir farklılık olup olmadığı araştırılmak istensin. 
    Sigara içenler ve içmeyenler iki bağımsız gruptur. Bunların akciğer kapasiteleri ölçülür. Bağımsız örneklemler t-testi 
    ile ortalamalar arasında bir farklılığın olup olmadığına karar verilir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-testinin varsayımları (uygulanabilmesi için sağlanması gereken şartlı kastediyoruz) şunlardır:

    1) Gruplar birbirinden bağımsız olmalıdır.
    2) Her iki grubun ana kütlesi normal dağılmış olmalıdır.
    3) Grupların geldiği anakütlenin varyansları eşit olmalıdır (σ₁² = σ₂²)

    Bu koşulların sağlanıp sağlanmadığına başka hipotez testleriyle karar verilebilir. Kursumuz ilk konularında normal dağılım 
    testinin nasıl yapıldığını görmüştük. Bağımsız iki grubun varyanslarının eşitliği (buna varyans homojenliği de denilmektedir) 
    için çeşitli hipotez testleri kullanılmaktadır. Bu bakımdan en yaygın kullanılan hipotez testi "Levene testi" ve "F-testi" 
    denilen testlerdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-tesinin dayandığı fikir oldukça basittir. İki grup ortalamaları arasındaki fark elde edilir. Bu 
    ortalamalara ilişkin farkların normal dağıldığı kabulüyle elde edilen bu farkın uç bir fark olup olmadığına belirlenen 
    α düzeyinde bakılmaktadır. Tabii iki ortalama arasındaki fark bu ortalamaların standart sapmaları da dikkate alınarak 
    anlamlandırılmaktadır. Bağımsız örneklemler t-testi için H0 ve H1 hipotezleri şöyledir:

    H₀: μ₁ = μ₂     
    H₁: μ₁ ≠ μ₂

    Test işlemi yapıldıktan sonra elde edilen p değerinden hareketle H0 hipotezinin reddedilip reddedilmeyeceği belirlenir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bağımsız örneklemler t-testi SciPy kütüphanesinin stats modülündeki ttest_ind fonksiyonuyla uygulanabilmektedir. Fonksiyonun 
    parametrik yapısı şöyledir:

    ttest_ind(a, b, *, axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, 
            alternative='two-sided', trim=0, method=None, keepdims=False)

    Fonksiyonun ilk iki parametresi bağımsız örneklemler t-testi uygulanacak grup değerlerini belirtmektedir. Diğer parametreler
    default değerler almıştır. Fonksiyon geri dönüş değeri olarak TTestResult sınıfı türünden bir nesne vermektedir. Aslında 
    bu sınıf tuple sınıfından türetilmiştir. Dolayısıyla isimli bir demet gibidir ve demet gibi açılabilmektedir. Bu sınıfın 
    pvalue örnek özniteliği test sonucunda elde edilen p değerini, df örnek özniteliği serbestlik derecesini ve statistic örnek 
    özniteliği ise t istatistik değerlerini vermektedir. Fonksiyonun geri dönüş değeri iki elemanlı bir demet biçiminde de 
    kullanılabilir. Bu demetin ilk elemanı t istatistiğini, ikinci elemanı p değerini vermektedir. Bu iki eleman açıma sokulabilir. 
    Örneğin:

    from scipy.stats import ttest_ind

    group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
    group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

    stat, p = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)

    Buradan şöyle bir çıktı elde edilmiştir:

    t-istatistiği: 3.8249455333812685
    p-değeri: 0.005054073879038671

    Çıktıdaki p değerine bakıldığında oldukça düşük olduğu görülmektedir. α değerinin 0.05 seçilmiş olduğunu varsayarsak 
    p <= α durumu oluşmaktadır. Bu durumda H0 hipotezi reddedilir. Bu da grup ortalamaları arasında istatistiksel olarak 
    anlamlı bir fark olduğu anlamına gelmektedir. Yani ortalamalar arasındaki bu farklılık normal olarak karşılanacak bir 
    farklılık değildir. 

    statsmodels isimli kütüphanede de bağımsız örneklemler t-testi statsmodels.stats.weightstats modülündeki ttest_ind 
    fonksiyonuyla uygulanmaktadır. Fonksiyonun parametrik yapısı şöyledir:

    statsmodels.stats.weightstats.ttest_ind(x1, x2, alternative='two-sided', usevar='pooled', 
            weights=(None, None), value=0)

    Fonksiyonun SciPy'daki ttest_ind fonksiyonuna çok benzediğine dikkat ediniz. Fonksiyonun geri dönüş değeri 3 elemanlı 
    bir demettir. Demetin elemanları sırasıyla t istatistiğini, p değerini ve serbestlik derecesini vermektedir. Örneğin:

    from statsmodels.stats.weightstats import ttest_ind

    group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
    group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

    stat, p, df = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)
    print('serbestlik derecesi:', df)

    Buradan şöyle çıktı elde edilmiştir:

    t-istatistiği: 3.8249455333812716
    p-değeri: 0.005054073879038644
    serbestlik derecesi: 8.0

    Buradaki serbestlik derecesi iki grubun elemanları toplamından 2 eksik olan değerdir. 
#----------------------------------------------------------------------------------------------------------------------------

from statsmodels.stats.weightstats import ttest_ind

group1 = [5.2, 4.8, 6.1, 5.9, 5.4]
group2 = [4.3, 4.5, 4.0, 4.9, 4.4]

stat, p, df = ttest_ind(group1, group2)

print('t-istatistiği:', stat)
print('p-değeri:', p)
print('serbestlik derecesi:', df)

#----------------------------------------------------------------------------------------------------------------------------
    Daha önce üzerinde çalıştığımız zambak veri kümesini anımsayınız. Bu veri kümesinde zambaklar üç sınıfa ayrılmıştı. Veri 
    kümesi şu görünümdeydi:

    Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
    1,5.1,3.5,1.4,0.2,Iris-setosa
    2,4.9,3.0,1.4,0.2,Iris-setosa
    3,4.7,3.2,1.3,0.2,Iris-setosa
    4,4.6,3.1,1.5,0.2,Iris-setosa
    ...
    104,6.3,2.9,5.6,1.8,Iris-virginica
    105,6.5,3.0,5.8,2.2,Iris-virginica
    106,7.6,3.0,6.6,2.1,Iris-virginica
    107,4.9,2.5,4.5,1.7,Iris-virginica
    ...
    52,6.4,3.2,4.5,1.5,Iris-versicolor
    53,6.9,3.1,4.9,1.5,Iris-versicolor
    54,5.5,2.3,4.0,1.3,Iris-versicolor
    55,6.5,2.8,4.6,1.5,Iris-versicolor
    56,5.7,2.8,4.5,1.3,Iris-versicolor
    57,6.3,3.3,4.7,1.6,Iris-versicolor

    Burada araştırmacı "Iris-stosa" ile "Iris-virginica" zambak türlerinin PetalLengthCm özelliklerine ilişkin ortalamalarının 
    anlamlı bir biçimde farklı olup olmadığını test etmek istesin. Hipotezler şöyle oluşturulabilir:

    H₀: "Iris-stosa" ve "Iris-virginica" zambak türlerinin PetalLengthCm ortalamaları birbirine eşittir.
    H₁: "Iris-stosa" ve "Iris-virginica" zambak türlerinin PetalLengthCm ortalamaları birbirine eşit değildir. 

    Testi statsmodels kütüphanesini kullanarak şöyle yapabiliriz:

    df = pd.read_csv('iris.csv')

    group1 = df.loc[df['Species'] == 'Iris-setosa', 'PetalLengthCm']
    group2 = df.loc[df['Species'] == 'Iris-virginica', 'PetalLengthCm']

    stat, p, df = ttest_ind(group1, group2)

    print('t-istatistiği:', stat)
    print('p-değeri:', p)
    print('serbestlik derecesi:', df)

    Elde edilen çıktı şöyledir:

    t-istatistiği: -39.46866259397271
    p-değeri: 5.717463758170621e-62
    Serbestlik Derecesi (df): 98.0

    Görüldüğü gibi p değeri çok çok küçüktür. Bu durumda H0 hipotezi reddedilir. Yani iki zambak türünün PetalLengthCm 
    ortalamaları anlamlı bir biçimde birbirinden farklıdır. 
#----------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from statsmodels.stats.weightstats import ttest_ind

df = pd.read_csv('iris.csv')

group1 = df.loc[df['Species'] == 'Iris-setosa', 'PetalLengthCm']
group2 = df.loc[df['Species'] == 'Iris-virginica', 'PetalLengthCm']

stat, p, df = ttest_ind(group1, group2)

print('t-istatistiği:', stat)
print('p-değeri:', p)
print('serbestlik derecesi:', df)

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de bağımsız örneklemler t-tesinin dayandığı istatistiksel temel hakkında bazı açıklamalar yapalım. İki örneğin 
    elde edildiği anakütlenin X ve Y olduğunu varsayalım. Bunların normal dağılmış olduğunu varsaymıştık:

    X ~ N(μ₁, σ₁²) 
    Y ~ N(μ₂, σ₂²) 

    Örneklem ortalamalarının beklenen değerleri şöyle olacaktır (merkezi limit teoreminden):

    E(X̄) = μ₁
    E(Ȳ) = μ₂

    Örneklem ortalamalarının varyansları da şöyle olacaktır (merkezi limit teoreminden):

    Var(X̄) = σ₁²/n₁
    Var(Ȳ) = σ₂²/n₂

    Bu durumda örneklem ortalamaları arasındaki farkların beklenen değeri şöyle olur:

    E(X̄ - Ȳ) = E(X̄) - E(Ȳ) = μ₁ - μ₂

    Örneklem ortalamaları arasındaki farkların varyansı şöyle olacaktır:

    Var(X̄ - Ȳ) = Var(X̄) + Var(Ȳ) = σ₁²/n₁ + σ₂²/n₂

    Burada varyansların farklarının alınmadığına varyansların toplandığına dikkat ediniz. Örneklem ortalamaları arasındaki 
    farkların standart hatasını (yani örneklem farklarına ilişkin dağılımın standart sapmasını) şöyle ifade edebiliriz:

    SE(X̄ - Ȳ) = √(σ₁²/n₁ + σ₂²/n₂)

    Eğer anakütle varyansları biliniyorsa örneklem farklarına ilişkin standart normal dağılım şöyle elde edilebilir:

         (X̄ - Ȳ) - (μ₁ - μ₂)
    Z =  ----------------------
          √(σ₁²/n₁ + σ₂²/n₂)

    Ancak genellikle anakütlelere ilişkin varyanslar bilinmez. Bu durumda standart normal dağılım yerine t dağılımı 
    kullanılmalıdır:

    s₁² = Σ(Xᵢ - X̄)² / (n₁ - 1)
    s₂² = Σ(Yⱼ - Ȳ)² / (n₂ - 1)

    Anakütle varyanslarının eşit olduğunu kabul ettiğimizde ağırlıklandırılmış bileşik varyans (pooled variance)
    şöyle hesaplanmaktadır:

    s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2)

    Nihayetinde t istatistiği de şu hale gelecektir:

       (X̄ - Ȳ) - (μ₁ - μ₂)
   t = -----------------------
       sₚ√(1/n₁ + 1/n₂)

    Serbestlik derecesi de df = n₁ + n₂ - 2 olacaktır. 
    
    Eğer anakütle varyansları eşit kabul edilmezse buna "Welch t-testi" denir. Bu duurmda t istatistiği ve serbestlik derecesi 
    şu hale gelmektedir:

        (X̄ - Ȳ) - (μ₁ - μ₂)
    t = -------------------
        √(s₁²/n₁ + s₂²/n₂)

         (s₁²/n₁ + s₂²/n₂)²
    df = ---------------------------------------
         [(s₁²/n₁)²/(n₁-1)] + [(s₂²/n₂)²/(n₂-1)]
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Çok kullanılan hipotez testlerinden biri de "varyans analizi (analysis of variance)" denilen kısaca ANOVA biçiminde de
    ifade edilen hipotez testidir. Varyans analizi "tek yönlü (one-way)" ve "çift yönlü (two-way)" olmak üzere iki biçimde 
    uygulanabilmektedir. Biz burada önce tek yönlü varyans analizini sonra da çift yönlü varyans analizini ele alacağız. 

    Tek yönlü varyans analizi (one-way ANOVA) 2'den fazla grubun ortalamaları arasında anlamlı bir farkın olup olmadığını 
    test etmek amacıyla kullanılmaktadır. Yani bu anlamda bağımsız örneklemler t-testinin çok gruplusu gibi düşünülebilir. 
    Tek yönlü ANOVA testi için sağlanması gereken koşullar bağımıszlık örneklemler t-tesi ile benzerdir:

    1) Gruplardaki elemanlar birbirinden bağımsız olmalıdır. (Yani bir eleman birden fazla grupta bulunmamalıdır.)
    2) Gruplara ilişkin ana kütleler normal dağılmış olmalıdır.
    3) Gruplara ilişkin anakütle varyansları eşit olmalıdır. 

    Tabii yine bu koşulların sağlanıp sağlanmadığına başka hiptez testleriyle bakılmaktadır. Anımsayacağınız gibi normallik 
    testi için "Kolmogorov-Simirnov" ya da "Shapiro Wilk" testi, varyans homojenliği için ise "Levene" testi ya da "F testi"  
    kullanılabiliyodu.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA birden fazla bağımısz grubun ortalamalarının biribirine eşit olup olmadığını istatistiksel olarak test 
    etmekte kullanılmaktadır. Yani bu bağlamda bağımızsız örneklemler t-testinin çok gruplusu gibidir. Tek yönlü ANOVA testinde
    H0 ve H1 hipotezleri şöyle oluşturulmaktadır:

    H₀: μ₁ = μ₂ = μ₃ = ... = μₖ
    H₁: En az bir grup ortalaması diğerlerinden farklıdır.

    Örneğin aynı ürünün üç farklı renkteki ambalajlarının satış miktarı üzerinde anlamlı bir farklılığa sahip olup olmadığını
    anlamaya çalışalım. Burada ambalajlar şunlar olsun:

    Grup 1: Kırmızı ambalaj
    Grup 2: Mavi ambalaj
    Grup 3: Yeşil ambalaj

    Bu ambalajlara sahip ürünlerin aynı mağazadaki günlük satış miktarlarını elde etmiş olalım. Bir haftalık süre içerisinde 
    deneyi sürdürelim. Elimizde toplam her ambalajdan 7 adet satış değeri bulunacaktır. Bunların ortalamasını elde elde edip
    tek yönlü ANOVA testi ile bunların herahngi birisinin diğerin farklı bir satış miktarına sahip olup olmadığını anlamak 
    isteyebiliriz. Buradaki hipotezlerimiz şöyle oluşturulabilir:

    H0: Kırmızı, Mavi ve Yeşil ambalajların ortalama satışları arasında bir farklılık yoktur.
    H1: Kırmızı, Mavi ve Yeşil ambalajların ortalama satışları arasında en az biri diğerinden farklıdır.

    Örneğin darklı gübre türlerinin bitki büyümesine etkisinin olup olmadığı yine tek yönlü ANOVA testi ile test edilebilir.

    Pekiyi elimizde 3 farklı grubun ortalamaları olsa biz bu ortlamaların en az birinin diğerlerinden farklı olup olmadığını 
    bağımsız örneklemler t-testi ile anlayabilir miyiz? İşte bağımısz örneklemler t-testi iki grup arasında yürütüldüğü 
    için bizim C(5, 2) kadar farklı t-testi yapmamız gerekir. Bu hem zahmetlidir hem de "tip 1 hata (type 1 error)" olasılığını 
    yükseltmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA testi SciPy kütüphanesindeki stats modülünde bulunan f_oneway fonksiyonuyla uygulanabilmektedir. Fonksiyonun
    parametrik yapısı şöyledir:

    f_oneway(*samples, axis=0, equal_var=True, nan_policy='propagate', keepdims=False)

    Burada fonksiyonun parametresinin *'lı olduğuna dikkat ediniz. Yani fonksiyonu çağırırken biz gruplara ilişkin değerleri 
    farklı argümanlar olarak girebiliriz. Diğer parametreler default değer almıştır. Fonksiyon iki elemanlı bir demete geri 
    dönmektedir. Demetin birinci elemanı F değerini, ikinci elemanı ise F değerinin F dağılımına sokularak elde edildiği 
    p değerini vermektedir. Uygulamacı bu p değerine bakar. Bu p değeri belirlediği α değerinden (tipik olarak 0.05) küüçükse 
    H0 hipotezini reddetmekte değilse reddetmemektedir. Başka bir deyişle bu p değeri α değerinden küçükse grup ortalamalarından 
    en az biri diğerinden farklıdır. Eğer bu değeri α değerindne küçük değilse grup ortalamalarının birbirine eşit olduğu 
    kabul edilmektedir. Bu durumu özetle şöyle ifade edebiliriz:

    ┌───────────────┬─────────────────────────────────────────────────────────┐
    │ Karşılaştırma │ Yorum                                                   │
    ├───────────────┼─────────────────────────────────────────────────────────┤
    │ p ≤ α         │ H₀ reddedilir → Gruplar arasında anlamlı fark vardır.   │
    ├───────────────┼─────────────────────────────────────────────────────────┤
    │ p > α         │ H₀ reddedilemez → Gruplar arasında anlamlı fark yoktur. │
    └───────────────┴─────────────────────────────────────────────────────────┘

    F değerinin ne anlam ifade ettiğini izleyen paragraflarda ele alacağız. İlerleyen kısımlarda da F dağılımı hakkında bilgiler 
    vereceğiz. 

    Örneğin:

    grup_A = [20, 21, 19, 22, 20]
    grup_B = [30, 31, 29, 32, 30]
    grup_C = [25, 27, 26, 28, 27]

    f_val, p_val = stats.f_oneway(grup_A, grup_B, grup_C)

    print('F istatistiği:', f_val)
    print('p-değeri:', p_val)

    Burada üç ayrı gruptaki değerlerin ortalamaları arasında anlamlı bir fark olup olmadığına bakılmıştır. Elde p değeri 
    ile hipotez testi karara bağlanabilir:

    if p_val < 0.05:
        print('Sonuç: Gruplar arasında anlamlı fark vardır (H₀ reddedilir).')
    else:
        print('Sonuç: Gruplar arasında anlamlı fark yoktur (H₀ reddedilmez).')
#----------------------------------------------------------------------------------------------------------------------------

from scipy import stats

grup_A = [20, 21, 19, 22, 20]
grup_B = [30, 31, 29, 32, 30]
grup_C = [25, 27, 26, 28, 27]

f_val, p_val = stats.f_oneway(grup_A, grup_B, grup_C)

print('F istatistiği:', f_val)
print('p-değeri:', p_val)

if p_val < 0.05:
    print('Sonuç: Gruplar arasında anlamlı fark vardır (H₀ reddedilir).')
else:
    print('Sonuç: Gruplar arasında anlamlı fark yoktur (H₀ reddedilmez).')
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA testi statsmodels kütüphanesindeki stats modülünde bulunan anova_lm fonksiyonuyla da uygulanabilmektedir. 
    Fonksiyonun parametrik yapısı şöyledir.

    statsmodels.stats.anova.anova_lm(*args, **kwargs)

    Fonksiyonun birinci parametresi en küçük kareler yöntemiyle elde edilen model nesnesini belirtmektedir. Bu nesne asıl 
    işlemi yapan stats.anova modülündeki anova_lm fonksiyonuna verilmektedir. Örneğin:

    data = {
        'group': ['A'] * 5 + ['B'] * 5 + ['C'] * 5,
        'height': [20, 21, 19, 22, 20,   # Group A
                30, 31, 29, 32, 30,   # Group B
                25, 27, 26, 28, 27]   # Group C
    }

    df = pd.DataFrame(data)
    model = ols('height ~ C(group)', data=df).fit()

    Burada oluşturulan DataFrame nesnesi şöyledir:

       group  height
    0      A      20
    1      A      21
    2      A      19
    3      A      22
    4      A      20
    5      B      30
    6      B      31
    7      B      29
    8      B      32
    9      B      30
    10     C      25
    11     C      27
    12     C      26
    13     C      28
    14     C      27

    Burada ols fonksiyonu ile elde edilen nesne anova_lm fonksiyonuna verilir:

    anova_table = sm.stats.anova_lm(model, typ=2)

    Burada da ANOVA tablosu elde dilmektedir. Fonksiyondaki typ parametresi tek yönlü ANOVA için genellikle 2 olarak girilmektedir. 
    Fonksiyondan bir ANOVA tablosu elde edilir. Bu ANOVA tablosu bir DataFrame nesnesi biçimindedir. Örneğimizde şöyle bir 
    DataFrame nesnesi elde edilmiştir:

                sum_sq    df     F        PR(>F)
    C(group)    254.8      2.0  98.0       3.687291e-08
    Residual    15.6    12.0   NaN        NaN
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de tek yönlü ANOVA testinin dayandığı istatistiksel temel üzerine açıklamalar yapalım. Tek yönlü ANOVA testi için 
    iki varyans kaynağı hesaplanmaktadır: Gruplar içi varyans değeri ve gruplar arası varyans değeri. Bu iki varyans değeri 
    hesaplandıktan sonra, gruplar arası varyans değeri gruplar içi varyans değerine bölünerek bir F değeri (test istatistiği) 
    elde edilmektedir. Sonra da elde edilen hesaplanmış F değeri, serbestlik derecelerine göre F tablosundan bulunan kritik 
    değer ile karşılaştırılmaktadır:

            Gruplar Arası Varyans
    F = ─────────────────────────────
            Gruplar İçi Varyans

    Gruplar arası varyans değerini hesaplamak için önce kareler toplamının elde edilmesi gerekir. Gruplar arası kareler 
    toplamı şöyle hesaplanmaktadır:

    Gruplar arası kareler toplamı  = Σnⱼ(X̄ⱼ - X̄)²

    Burada X̄ tüm gruplardaki elemanların, X̄ⱼ ise j'inci grubun ortalamasıdır. Burada her grup ortalamasının genel ortalamadan 
    farkının karelerinin ağırlıklı biçimde toplandığını görüyorsunuz. Buradaki nⱼ, j'inci grubun eleman sayısını belirtmektedir. 
    Grup içi kareler toplamı ise şöyle hesaplanmaktadır:

    Grup içi kareler toplamı = ΣΣ(Xᵢⱼ - X̄ⱼ)²
     
    Burada X̄ⱼ j'inci grubun ortalamasını, Xᵢⱼ ise j'inci gruptaki elemanları belirtmektedir. Gruplar arası varyans şöyle 
    hesaplanmaktadır:

    Gruplar arası varyans = Gruplar arası kareler toplamı / (toplam grup sayısı - 1) 

    Burada toplam grup sayısı - 1 değeri serbestlik derecesini belirtmektedir. 

    Gruplar içi varyans da şöyle hesaplanmaktadır:

    Gruplar içi varyans = Gruplar içi kareler toplamı / (toplam eleman sayısı - grup sayısı)

    İşte bunun sonucu olarak yukarıda belirttiğimiz gibi önce bir F değeri elde edilmektedir. Bu F değeri de F dağılımına 
    sokularak p değeri elde edilecek, p değeri de belirlenen alfa değeri ile karşılaştırılacaktır. Şimdi burada yapılanları 
    adım adım örneklendirelim. Elimizde aşağıdaki gibi üç grup olsun:

    Grup A: 20, 21
    Grup B: 30, 31, 29, 32
    Grup C: 32, 33, 33 

    Öncelikle grup ortalamasını ve toplam ortalamayı bulalım.

    A ort. = (20 + 21) / 2 = 20.5
    B ort. = (30 + 31 + 29 + 32) / 4 = 30.5
    C ort. = (32 + 33 + 33) / 3 = 32.67

    Genel ortalama da şöyledir:

    (20 + 21 + 30 + 31 + 29 + 32 + 32 + 33 + 33) / 9 ​=  29

    Şimdi de gruplar arası kareler toplamını bulalım:

    A: 2 * (20.5 − 29)² = 144.50
    B: 4 * (30.5 − 29)² = 9.00
    C: 3 * (32.67 − 29)² = 40.41

    Gruplar arası kareler toplamı = 144.50 + 9.00 + 40.41 = 193.91

    Şimdi de gruplar içi kareler toplamını bulalım:

    A Grubu: (20 − 20.5)² + (21 − 20.5)² = 0.50
    B Grubu: (30 − 30.5)² + (31 − 30.5)² + (29 − 30.5)² + (32 − 30.5)² = 5.00
    C Grubu: (32 − 32.67)² + (33 − 32.67)² + (33 − 32.67)² = 0.67 

    Gruplar içi kareler toplamı = SS_W = 0.50 + 5 + 0.67 = 6.17

    Şimdi de varyansları hesaplayalım. Gruplar arası varyans hesaplarken gruplar arası kareler toplamı, (toplam eleman sayısı 
    - grup sayısı) değerine bölünür:

    Gruplar arası varyans = 193.91 / (3 - 1) =  96.96
    Gruplar içi varyans = 6.17 / (9 - 3) = 1.03

    Artık F değeri elde edilebilir:

             96.96
    F = ──────────────── = 94.14
             1.03

    İşte karar buradaki F değerine bakılarak verilmektedir. F dağılımında df1=2, df2=6 ve α = 0.05 için F değeri 5.14'tür. 
    Bizdeki F değeri ise bu değerden yüksektir (yani 5.14 < 94.14). Daha biçimsel açıklamayı şöyle yapabiliriz: Elde ettiğimiz 
    F değeri için F dağılımından elde edilen p değeri yaklaşık 0.000077'tir. Sonuç olarak 0.00007 < 0.05 olduğu için H0 hipotezi 
    reddedilmektedir. Yani ortalamalar arasında anlamlı farklılıklar vardır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinde "post-hoc" testler denilen bir kavram sıkça kullanılmaktadır. "Post-hoc" terimi Latince "bundan sonra"
    ya da "bu durum oluştuktan sonra" gibi anlamlara gelmektedir. Bir hipotez testi gerçekleştirildikten sonra daha fazla 
    bilgi edinmek için yapılabilecek ilave testlere "post-hoc" testler denilmektedir. Biz kursumuzda hipotez testlerinden 
    sonra uygulanabilecek post-hoc testler üzerinde fazlaca durmayacağız.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tek yönlü ANOVA testinde eğer H0 hipotezi reddedilirse (yani ortalamalar arasında anlamlı farklılıklar varsa) birkaç "post-hoc" 
    test önerilmektedir. Bu durumda "post-hoc" testler "hangi iki grubun birbirinden farklı" olduğu konusunda bilgiler vermektedir. 
    Aşağıda hangi durumlarda hangi "post-hoc" testin kullanılabileceğini bir tablo biçiminde veriyoruz:

    ┌─────────────────────┬───────────────────────┬─────────────────────┬───────────────────────────────┬──────────────────────────┐
    │ Post-hoc Testi      │ Varyans Homojenliği?  │ Örneklem Büyüklüğü  │ Avantajı                      │ Not                      │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Tukey HSD           │ Gerekli               │ Benzer              │ En güvenilir klasik yöntem    │ Varsayımlar sağlanmalı   │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Bonferroni / Holm   │ Gerekmez              │ Fark etmez          │ Basit ve sağlam               │ Konservatif              │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Scheffé             │ Gerekli               │ Fark etmez          │ En korumacı                   │ Gücü düşük               │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Games–Howell        │ Gerekmez              │ Fark etmez          │ Heterojen varyans için en iyi │ Modern, güçlü            │
    ├─────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────────┼──────────────────────────┤
    │ Dunnett             │ Fark etmez            │ Fark etmez          │ Kontrole karşı en güçlü yöntem│ Sadece kontrol kıyasları │
    └─────────────────────┴───────────────────────┴─────────────────────┴───────────────────────────────┴──────────────────────────┘
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Çift yönlü ANOVA, iki faktörün etkilerini ve birlikte etkilerini karşılaştırmak için kullanılmaktadır. Çift yönlü ANOVA'da 
    deneyde iki faktör vardır. Faktörler kategorik bağımsız değişkenlerdir. Örneğin tipik olarak aktörlerden biri grupları, 
    diğeri ise gruplara uygulanan işlemleri belirtebilir. Örneğin kadın ve erkekler üç farklı egzersiz deneyine sokulmuş olsun:

    Faktör 1 (Cinsiyet): Kadın, Erkek
    Faktör 2 (Egzersiz Tipi): Kardiyo, Kuvvet, Kombine 
     
    Burada toplam 6 ayrı ölçüm değeri elde edilecektir. Ölçülen değerlerin kilo kaybı olduğunu varsayalım. Deneklerden 
    aşağıdaki gibi veriler elde ettiğimizi düşünelim:

    ┌──────────┬──────────┬───────────────┬─────────────────┐
    │ Denek No │ Cinsiyet │ Egzersiz Tipi │ Kilo Kaybı (kg) │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │    1     │  Kadın   │    Kardiyo    │      3.2        │
    │    2     │  Kadın   │    Kardiyo    │      4.1        │
    │    3     │  Kadın   │    Kardiyo    │      3.8        │
    │    4     │  Kadın   │    Kardiyo    │      4.5        │
    │    5     │  Kadın   │    Kardiyo    │      3.6        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │    6     │  Kadın   │    Kuvvet     │      2.8        │
    │    7     │  Kadın   │    Kuvvet     │      3.2        │
    │    8     │  Kadın   │    Kuvvet     │      2.5        │
    │    9     │  Kadın   │    Kuvvet     │      3.0        │
    │   10     │  Kadın   │    Kuvvet     │      2.9        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   11     │  Kadın   │    Kombine    │      5.1        │
    │   12     │  Kadın   │    Kombine    │      5.8        │
    │   13     │  Kadın   │    Kombine    │      5.3        │
    │   14     │  Kadın   │    Kombine    │      6.0        │
    │   15     │  Kadın   │    Kombine    │      5.5        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   16     │  Erkek   │    Kardiyo    │      4.5        │
    │   17     │  Erkek   │    Kardiyo    │      5.2        │
    │   18     │  Erkek   │    Kardiyo    │      4.8        │
    │   19     │  Erkek   │    Kardiyo    │      5.0        │
    │   20     │  Erkek   │    Kardiyo    │      4.9        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   21     │  Erkek   │    Kuvvet     │      4.2        │
    │   22     │  Erkek   │    Kuvvet     │      4.8        │
    │   23     │  Erkek   │    Kuvvet     │      4.5        │
    │   24     │  Erkek   │    Kuvvet     │      4.6        │
    │   25     │  Erkek   │    Kuvvet     │      4.4        │
    ├──────────┼──────────┼───────────────┼─────────────────┤
    │   26     │  Erkek   │    Kombine    │      6.8        │
    │   27     │  Erkek   │    Kombine    │      7.2        │
    │   28     │  Erkek   │    Kombine    │      6.5        │
    │   29     │  Erkek   │    Kombine    │      7.0        │
    │   30     │  Erkek   │    Kombine    │      6.9        │
    └──────────┴──────────┴───────────────┴─────────────────┘

    Görüldüğü gibi bu deney kalıbında gruplar bulunmaktadır. Ancak bu gruplar da kendi aralarında ayrışmaktadır. Burada 
    "Kardiyo", "Kuvvet" ve "Kombine" biçiminde üç ayrı grup vardır. Ancak bu gruplar da kendi aralarında "Kadın" ve "Erkek" 
    olmak üzere ikiye ayrılmaktadır. İki yönlü ANOVA'ya iki faktörlü ANOVA da denilmektedir. Yukarıdaki ölçümlerin ortalamaları 
    aşağıdaki gibi bir tabloyla ifade edilebilir:

    ┌──────────────────────────────────────────────────────────────┐
    │              GRUPLARA GÖRE ORTALAMALAR (kg)                  │
    ├──────────┬──────────┬──────────┬──────────┬──────────────────┤
    │ Cinsiyet │ Kardiyo  │  Kuvvet  │ Kombine  │  Satır Ortalaması│
    ├──────────┼──────────┼──────────┼──────────┼──────────────────┤
    │  Kadın   │   3.84   │   2.88   │   5.54   │      4.09        │
    │  Erkek   │   4.88   │   4.50   │   6.88   │      5.42        │
    ├──────────┼──────────┼──────────┼──────────┼──────────────────┤
    │  Sütun   │   4.36   │   3.69   │   6.21   │      4.75        │
    │ Ort.     │          │          │          │   (Genel Ort.)   │
    └──────────┴──────────┴──────────┴──────────┴──────────────────┘

    İki yönlü ANOVA'da üç farklı hipotez sınanmaktadır:

    1) Birinci faktörün seviyeleri arasında ortalamalar bakımından anlamlı bir farklılık var mı?
    2) İkinci faktörün seviyeleri arasında ortalamalar  ortalamalar arasında bir farklılık var mı?
    3) İki faktör arasında etkileşim etkisi var mı? Yani birinci faktörün etkisi, ikinci faktörün seviyelerine bağlı olarak 
    değişiyor mu?

    Yukarıdaki örnek bağlamında hipotezler şöyle ifade edilebilir:

    1) Kadın ve erkeklerin kilo kayıplarının ortalamaları arasında anlamlı bir farklılık var mı?
    2) Üç farklı egzersiz tipinin kilo kaybı ortalamaları arasında anlamlı bir farklılık var mı?
    3) Cinsiyet ve egzersiz tipi arasında etkileşim var mı? Yani egzersiz tipinin etkisi cinsiyete göre değişiyor mu? Bu 
    maddedeki hipotezler şöyle oluşturulabilir:

    H₀: Etkileşim yoktur. Egzersiz tipinin etkisi kadın ve erkeklerde aynıdır.
    H₁: Etkileşim vardır. Egzersiz tipinin etkisi cinsiyete göre değişir.

    ANOVA'daki üçümncü hipoteze dikkat ediniz. Burada "egzersiz tipinin cinsiyete göre değişip değişmeyeceği tek bir 
    hipotez biçiminde oluşturulmaktadır. Örneğin bazı egzersiz tipleri cinsiyete göre değişmiyor ancak bazıları değişiyor 
    olabilir. Ancak test tüm sütunları kapsayacak biçimde yapılmaktadır. Yani iki yönlü ANOVA'nın üçüncü hipotezi iki 
    faktör arasında etkileşiminin nerede oluştuğu hakkında bilgi vermemektedir. Bu tespit ancak ayrıca uygulanacak post-hoc 
    testlerle açıklığa kavuşturulabilmektedir. 

    İki yönlü ANOVA uygulamak için sağlanması gereken koşullar şunlardır:

    1) Bağımsızlık. Bir katılımcının verisi diğer katılımcıların verisini etkilememeli. Başka bir deyişle yukarıdaki matrisin
    bir hücresindeki bir katılımcı başka hücresinde bulunmamalıdır. Örneğin bir kadın hem kardiyo hem de kuvvet egzersizine 
    sokulamaz. 

    2) Her bir hücredeki anakütlelerin normal dağılmış olması gerekmektedir. (Yani yukarıdaki örnekte matrisin 6 hücresi de 
    kendi içinde normal dağılmış olmalıdır.)

    3) Tüm hücrelerin anakütlelerine ilişkin varyanslar birbirine eşit olmalıdır.    

    İki yönlü ANOVA testi iki faktör olduğundan dolayı iki yönlüdür. Eğer faktör sayısı üç olursa buna üç yönlü ANOVA, faktör 
    sayısı dört ya da aha fazla olursa da buna çok yönlü ANOVA (multiway ANOVA) denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SciPy kütüphanesinde iki yönlü ANOVA testini yapan bir fonksiyon bulunmamaktadır. iki yönlü ANOVA testi için Statsmodels 
    kütüphanesinde statsmodels.stats.anova modülündeki anova_lm fonksiyonu kullanılmaktadır. İşlemler sırasıyla şöyle 
    yapılmaktadır:

    1) Veri kümesi bir Pandas DataFrame nesnesi olarak oluşturulur. Burada sütunlardan biri birinci faktörü, diğeri ikinci 
    faktörü ve diğeri de elde edilen puanları belirtmektedir. Yukarıdaki veri kümesi için DataFrame nesnesi şöyle oluşturulabilir:

    data = {
        'Denek': range(1, 31),
        'Cinsiyet': ['Kadın']*15 + ['Erkek']*15,
        'Egzersiz': ['Kardiyo']*5 + ['Kuvvet']*5 + ['Kombine']*5 + ['Kardiyo']*5 + ['Kuvvet']*5 + ['Kombine']*5,
        'Kilo_Kaybi': [3.2, 4.1, 3.8, 4.5, 3.6,  # Kadın-Kardiyo
                    2.8, 3.2, 2.5, 3.0, 2.9,  # Kadın-Kuvvet
                    5.1, 5.8, 5.3, 6.0, 5.5,  # Kadın-Kombine
                    4.5, 5.2, 4.8, 5.0, 4.9,  # Erkek-Kardiyo
                    4.2, 4.8, 4.5, 4.6, 4.4,  # Erkek-Kuvvet
                    6.8, 7.2, 6.5, 7.0, 6.9]  # Erkek-Kombine
    }

    df = pd.DataFrame(data)

    Bu DataFrame nesnesi print edildiğinde şunlar görülecektir:

        Denek Cinsiyet Egzersiz  Kilo_Kaybi
    0        1    Kadın  Kardiyo          3.2
    1        2    Kadın  Kardiyo          4.1
    2        3    Kadın  Kardiyo          3.8
    3        4    Kadın  Kardiyo          4.5
    4        5    Kadın  Kardiyo          3.6
    5        6    Kadın  Kuvvet           2.8
    6        7    Kadın  Kuvvet           3.2
    7        8    Kadın  Kuvvet           2.5
    8        9    Kadın  Kuvvet           3.0
    9       10    Kadın  Kuvvet           2.9
    10      11    Kadın  Kombine          5.1
    11      12    Kadın  Kombine          5.8
    12      13    Kadın  Kombine          5.3
    13      14    Kadın  Kombine          6.0
    14      15    Kadın  Kombine          5.5
    15      16    Erkek  Kardiyo          4.5
    16      17    Erkek  Kardiyo          5.2
    17      18    Erkek  Kardiyo          4.8
    18      19    Erkek  Kardiyo          5.0
    19      20    Erkek  Kardiyo          4.9
    20      21    Erkek  Kuvvet           4.2
    21      22    Erkek  Kuvvet           4.8
    22      23    Erkek  Kuvvet           4.5
    23      24    Erkek  Kuvvet           4.6
    24      25    Erkek  Kuvvet           4.4
    25      26    Erkek  Kombine          6.8
    26      27    Erkek  Kombine          7.2
    27      28    Erkek  Kombine          6.5
    28      29    Erkek  Kombine          7.0
    29      30    Erkek  Kombine          6.9

    2) Bir model nesnesi oluşturulur. Model nesnesi oluşturmak için statsmodels.formula.api modülündeki ols fonksiyonu 
    kullanılmaktadır. Bu fonksiyonla model nesnesi oluşturulurken Pandas dataframe nesnesinin sütunlarının ne anlam 
    ifade ettiği belli bir sentaksla belirtilmektedir. Örneğimiz için model yazısı şöyle oluşturulabilir:

    "Kilo_Kaybi ~ C(Cinsiyet) + C(Egzersiz) + C(Cinsiyet):C(Egzersiz)"

    Burada DataFrame içerisindeki Kilo_Kaybi sütununun bağımlı değişken olduğu, birinci faktörün "Cinsiyet", ikinci faktörün 
    "Egzersiz" sütunu olduğunu ve "Cinsiyet" ile "Egzersiz" sütunları arasında ilişki olduğu belirtilmektedir. ols nesnesi 
    yaratıldıktan sonra sınıfın fit metodu ile fit işlemi yapılmalıdır. fit metodu birtakım gerekli işlemleri yapıp nesnenin 
    özniteliklerinde saklamaktadır. Örneğin:

    model = ols('Kilo_Kaybi ~ C(Cinsiyet) + C(Egzersiz) + C(Cinsiyet):C(Egzersiz)', data=df).fit()

    3) Fit edilmiş model nesnesi anova_lm fonksiyonuna argüman olarak verilir. Bu fonksiyondan DataFrame biçiminde ANOVA 
    tablosu elde edilir:

    anova_table = anova_lm(model, typ=2)

    anova_lm fonksiyonunun typ parametresi 1, 2 ya da 3 girilmektedir. Yukarıdaki örnekte olduğu gibi iki yönlü ANOVA testlerinde 
    bu parametre için 2 girilmelidir. Buradan elde edilen DataFrame nesnesi şöyledir:

                                sum_sq     df           F         PR(>F)
    C(Cinsiyet)              13.333333    1.0  127.591707  4.325899e-11
    C(Egzersiz)              34.072667    2.0  163.027113  1.078726e-14
    C(Cinsiyet):C(Egzersiz)   0.420667    2.0     2.012759  1.555575e-01
    Residual                  2.508000  24.0           NaN            NaN

    Bu tablodayu yorumlamak için en önemli iki sütun F değerine ve F değerine karşı gelen p değerine ilişkin tütunlardır.
    p değerlerine baktığımızda bu testten şu sonuçları çıkartabiliriz:

    1) Cinsiyetler arasında elde edilen kilo kaybı ortalamaları bakımından anlamlı bir fark vardır (p değeri çok küçük)
    2) Egzersiz yöntemleri arasında ortalama kilo kaybı bakımından anlamlı bir fark vardır (p değeri çok küçük)
    3) Egzersiz tipinin etkisi kadınlarda ve erkeklerde benzerdir. Yani bir egzersiz tipi kadınlarda ne kadar etkiliyse 
    erkeklerde de o kadar etkilidir (p değeri yeteri kadar küçük değil).
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Hipotez testlerinden elde edilen sonuçların tez gibi, makale gibi çalışmalarda akademik bir dille ifade edilmesi gerekmektedir. 
    Bunun için çeşitli akademik yazım formatları kullanılmaktadır. Örneğin sosyal bilimlerde "APA (American Psychological 
    Association)" fen bilimlerinde "IEEE formatı" yaygın kullanılmaktadır. Bugün artık LLM'ler sayesinde bu tür raporların 
    uygun formatlarda ifade edilmesi oldukça kolaylaşmıştır. Örneğin yukarıdaki iki yönlü ANOVA uygulanan deney kalıbı için 
    APA formatına uygun aşağıdaki gibi bir özet oluşturulabilir:
     
    *******************************
    Cinsiyet Ana Etkisi
     
    Cinsiyet değişkeninin kilo kaybı üzerinde istatistiksel olarak anlamlı bir ana etkisi bulunmuştur, F(1, 24) = 127.59, 
    p < .001, η² = .265. Erkekler (M = 5.42, SD = 1.29) kadınlardan (M = 4.09, SD = 1.20) anlamlı derecede daha fazla kilo 
    kaybetmişlerdir. Etki büyüklüğü (η² = .265) büyük düzeyde bir etki olduğunu göstermektedir (Cohen, 1988).
     
    Egzersiz Tipi Ana Etkisi

    Egzersiz tipi değişkeninin kilo kaybı üzerinde istatistiksel olarak anlamlı bir ana etkisi tespit edilmiştir, 
    F(2, 24) = 163.03, p < .001, η² = .677. Etki büyüklüğü çok yüksek düzeydedir ve egzersiz tipinin kilo kaybındaki varyansın 
    %67.7'sini açıkladığını göstermektedir. Kombine egzersiz programı (M = 6.21, SD = 0.75) en yüksek kilo kaybını sağlarken, 
    bunu kardiyo (M = 4.36, SD = 0.64) ve kuvvet antrenmanı (M = 3.69, SD = 0.76) izlemiştir.

    Etkileşim Etkisi

    Cinsiyet ve egzersiz tipi arasındaki etkileşim istatistiksel olarak anlamlı bulunmamıştır, F(2, 24) = 2.01, p = .156, 
    η² = .008. Bu bulgu, egzersiz tiplerinin kilo kaybı üzerindeki etkisinin kadınlar ve erkekler için benzer olduğunu 
    göstermektedir.

    Model Uyumu

    Genel model istatistiksel olarak anlamlı bulunmuştur ve kilo kaybındaki toplam varyansın %95.0'ını açıklamaktadır 
    (R² = .950, düzeltilmiş R² = .940).

    **********************************
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Biz yukarıda bağımsız örneklemler için t-testini ve ANOVA testini inceledik. Eğer deney koşulları aynı deneklere uygulanıyorsa
    ya da genel olarak bu denekler bağımsız değilse bu durumda "eşleşmiş (paired)" örneklemler testleri uygulanmaktadır. Bu 
    tür testlerin tipik deney kalıbı "ön test - son test" kalıbıdır. Bu deney kalıbında denekler üzerinde henüz ilgili işlem 
    uygulanmadan bir ölçüm uygulanır. İlgili işlem uygulandıktan sonra da yeniden ölçüm uygulanır. Bu iki ölçümün ortalamaları 
    arasındaki farklara bakılır. Tabii bu deney kalıbından amaç uygulanan işlemin anlamlı bir farklılığa yol açıp açmadığıdır. 
    Örneğin kan şekerini düşürdüğü iddia edilen yeni bir ilaç söz konusu olsun. Deneklerin önce kan şekerleri ölçülür (ön test), 
    sonra ilaç uygulanır, sonra da deneklerin yeniden şekerleri ölçülür (son test). Eğer ilaç etkiliyse arada anlamlı bir fark 
    olmalıdır. İşte bu testlerle "ortalama bakımından anlamlı bir fark var mı" diye bakılmaktadır. Ön test - son test deney kalıbı 
    araştırmalarda oldukça sık uygulanmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş t-testinin H0 ve H1 hipotezleri şöyle oluşturulmaktadır:

    H0 Hipotezi: Ön test ve son test ölçümleri birbirleriyle aynıdır. Bu durum sembolik olarak şöyle ifade edilebilir:

    H₀: μ_D = 0

    H1 Hipotezi: Ön test ve son test ölçümleri birbirinden farklıdır. Bu durum matematiksel olarak şöyle ifade edilebilir:

    H₁: μ_D ≠ 0

    Burada μ_D son test ile ön test arasındaki farkı belirtmektedir. 

    Eşleştirilmiş örneklemler t-testinin kullanılabilmesi için şu koşulların sağlanması gerekmektedir:

    1) Test uygulanan gruplar aynı olmalıdır. Yani aynı kişilere ön test ve son test uygulanmalıdır. 
    2) Ön test son test puan farklarının normal dağılmış olması gerekmektedir. 
    3) Ölçümlerde aşırı uç değerlerin (outliers) bulunmaması gerekir.
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
                                            163. Ders - 23/10/2025 - Pazar
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş örneklemler t-testi için SciPy kütüphanesindeki stats modülünde bulunan ttest_rel fonksiyonu kullanılmaktadır. 
    Bu fonksiyon bizden parametre olarak ön test ve son test değerlerini alır (hangi parametrenin ön test hangisinin son test 
    olduğunun bir önemi yoktur) bize iki elemanlı TestResult türünden bir demet geri döndürür. Demetin ilk elemanı t istatistiğini 
    ikinci elemanı da p değerini vermektedir. Yine bu p değeri önceden belirlenmiş alfa değerinden küçükse H0 hipotezi reddedilmekte, 
    değilse H0 hipotezi reddedilmemektedir. 
    
    Örneğin refleks geliştirme eğitiminde eğitimin bir faydasının dokunmadığını anlamak için deneklere önce uyaranlar verilip 
    onların tepki süreleri ölçülmektedir. Sonra deneklere refleks eğitimi verilip yeniden tepki süresi ölçüm yapılmaktadır. 
    Burada ön test puanlarıyla son test puanları arasında anlamlı bir farklılığın oluşması beklenmektedir. Örneğin:

    pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
    post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

    t_stat, p_val = stats.ttest_rel(pre, post)

    print('t-istatistiği:', t_stat)
    print('p-değeri:', p_val)
    print('Ortalama iyileşme (ms):', np.mean(pre - post))

    Burada p değeri çok küçük çıkmıştır dolayısıyla Ho hipotezi reddedilir. Yani ön test son test puanları arasında anlamlı 
    farklılıklar vardır. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy import stats

pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

t_stat, p_val = stats.ttest_rel(pre, post)

print('t-istatistiği:', t_stat)
print('p-değeri:', p_val)
print('Ortalama iyileşme (ms):', np.mean(pre - post))

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş t-testi statsmodels kütüphanesiyle de gerçekleştirilebilmektedir. Bunun için statsmodels.stats.weightstats
    modülündeki DescrStatsW sınıfı kullanılmaktadır. Uygulamacı bu sınıf türünden bir nesne yaratır. Ancak nesneyi yaratırken 
    ön test ile son test farkını argüman olarak verilmelidir:

    pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
    post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

    diff = pre - post
    d_stats = DescrStatsW(diff)

    Buradan elde edilen nesne ile sınıfın ttest_mean fonksiyonu 0 argümanıyla çağrılmalıdır:

    t_stat, p_val, df = d_stats.ttest_mean(0)

    ttest_mean metodundan üçlü bir demet elde edilmektedir. Demetin birinci elemanı t istatistiğini, ikinci elemanı p değerini 
    ve üçüncü elemanı da serbestlik derecesini belirtmektedir:

    print('t-istatistiği:', t_stat)
    print('p-değeri:', p_val)
    print('Serbestlik derecesi (df):', df)
    print('Ortalama iyileşme (ms):', np.mean(diff))

    Burada p değerinin çok düşük çıkması H0 hipotezinin reddedileceği anlamına gelmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

import numpy as np
from scipy import stats

pre = np.array([252, 310, 275, 290, 305, 299, 265, 280, 295, 310, 300, 285])
post = np.array([238, 292, 260, 276, 288, 285, 250, 263, 278, 293, 284, 270])

t_stat, p_val = stats.ttest_rel(pre, post)

print("t-istatistiği:", t_stat)
print("p-değeri:", p_val)
print("Ortalama iyileşme (ms):", np.mean(pre - post))

from statsmodels.stats.weightstats import DescrStatsW

diff = pre - post
d_stats = DescrStatsW(diff)
t_stat, p_val, df = d_stats.ttest_mean(0)  

print('t-istatistiği:', t_stat)
print('p-değeri:', p_val)
print('Serbestlik derecesi (df):', df)
print('Ortalama iyileşme (ms):', np.mean(diff))

#----------------------------------------------------------------------------------------------------------------------------
    Eşleştirilmiş örneklemlerde yalnızca ön test - son test değil ikiden fazla test de uygulanabilmektedir. Bu deney kalıbında 
    aynı deneklere ikiden fazla test uygulanmaktadır. Amaç tüm bu testlerden elde edilen skorlar arasında bir farklılığın 
    olup olmadığının istatistiksel bakımdan belirlenmesidir. Bunun için kullanılan hipotez  testine "tekrarlı ölçümlerde tek 
    yönlü ANOVA (one-way ANOVA for repeated measures)" denilmektedir. Örneğin aynı denek grubuna gün içerisinde farklı zamanlarda 
    aynı deney uygulanmış olabilir. Bunlar arasında zamana dayalı bir farklılığın oluşup oluşmadığı tespit edilmeye çalışabilir. 
    Örneğin günün farklı saatlerinde kişilerin bilişsel performansları arasında bir farklılık olup olmadığı tespit edilmek 
    istensin. Bu amaçla aynı kişilere saat 9'da saat 12'de saat 16'da saat 20'de ve saat 24'te aynı testler uygulnamış olsun. 
    Eğer testin uygulandığı zaman bilişsel başarıyı etkilemiyorsa bu testlerden alınan puan farklarının istatistiksel bakımdan a
    nlamlı olmaması gerekir. (Bu tür deneylerde eğer deneklere aynı test verilirse denekler bunları öğrenebilir ve zamanla 
    test performansı artabilir. Genellikle bu tür çalışmalarda testlerin eşdeğer zorlukta alternatif versiyonları oluşturulmaktadır.)
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------


#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------


#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Yapay zeka ve makine öğrenmesi uygulamaları için çeşitli kurumlar tarafından cloud temelli hizmetler sunulmaktadır. Bu cloud 
    hizmetlerinin en yaygın kullanılanları şunlardır:

    - Goodle Cloud Platform (Vertex AI)
    - Amazon Web Services (Sage Maker)
    - Microsodt Azure
    - IBM Watson

    Bu servislerin hepsinin ortak birtakım özellikleri ve amaçları vardır:

    - Bu platformlar bize CPU ve bellek sağlamaktadır. Dolayısıyla bizim makine öğrenmesi işlemleri için ayrı bir makine tahsis 
    etmemize gerek kalmaz. Pek çok modelin eğitimi günlerce sürebilmektedir. Bunun için makinenin evde tutulması uygun olamayabilir. 
    
    - Bu platformlar "ölçeklenebilir (scalable)" çözümler sunmaktadır. Yani kiralanan birimler büyütülük küçültülebilmektedir. 

    - Bu platformlar "deployment" için kullanılabilmektedir. Yani burada eğitilen modellerle ilgili işlemler Web API'leriyle
    uzaktan yapılabilmektedir. (Örneğin biz makine öğrenmesi uygulamasını buralarda konuşlandırabiliriz. predict işlemlerini 
    cep telefonumuzdaki uygulamalardan yapabiliriz. Böylece uygulamamız mobil aygıtlardan da web tabanlı olarak kullanılabilir 
    hale gelmektedir.)

    - Bu platformlar kendi içerisinde "Automated ML" araçlarını da bulundurmaktadır. Dolayısıyla aslında konunun teorisini bilmeyen 
    kişiler de bu Automated ML araçlarını kullanarak işlemlerini yapabilmektedir. 

    Yukarıdaki platformlar (IBM Watson dışındaki) aslında çok genel amaçlı platformlardır. Yani platformlarda pek çok değişik hizmet de 
    verilmektedir. Bu platformalara "yapay ze makine öğrenmesi" unsurları son 10 senedir eklenmiş durumdadır. Yani bu platformlardaki 
    yapay zeka ve makine öğrenmesi kısımşları bu platformların birer alt sistemi gibidir. Bu platformların pek çok ayrıntısı olduğunu 
    hatta bunlar için sertifikasyon sınavlarının yapıldığını belirtmek istiyoruz. 

    Tabii yukarıdaki platformlar ticari platformlardır. Yani kullanım için ücret ödenmektedir. Ücret ödemesi "kullanım miktarı ile"
    ilişkilidir. Yani ne kadar kullanılırsa o kadar ücret ödenmektedir. (Bu bakımdan modellerin eğitimini unutursanız, bu eğitimler
    bu platformun kaynaklarını kullandığı için ücretlendirilecektir. Denemeler yaparken bu tür hesaplamaları durdurduğunuzdan emin 
    olmalısınız.) Tabii bu platformlarda da birtakım işlemler bedava yapılabilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Bütün cloud sistemlerinde makine öğrenmesi işlemleri yapılırken birbirleriyle ilişkili üç etkinlik yürütülür: 
    
    Data + Model + Hesaplama 

    Üzerinde çalışacağımız veriler genellikle bu cloud sistemlerinde onların bu iş için ayrılan bir servisi yoluyla upload 
    edilir. Model manuel ya da otomatik bir biçimde oluşturulmaktadır. Cloud sistemleri kendi içerisindeki dağıtık bilgisayar
    sistemleri yoluyla model üzerinde eğitim, kestirim gibi işlemler yapmamıza olanak vermektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Google Cloud Platform (kısaca GCP) Amazon AWS ve Microsoft Azure platformlarının doğrudan rekabetçisi konumundadır. 
   GCP 2008’de kurulmuştur. Aslında diğer platformlarda olan servislerin tamamen benzeri GCP’de bulunmaktadır. 
   GCP’ye erişmek için bir Google hesabının açılmış olması gerekir. 

    GCP’nin ana sayfası şöyeldir:

    https://cloud.google.com/

    GCP işlemlerini yapabilmek için kontrol panele (konsol ortamına) girmek gerekir. Kontrol panel adresi de şöyledir:

    https://console.cloud.google.com

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   GCP’de tüm işlemler bir proje eşliğinde yapılmaktadır. Çünkü işletmeler değişik projeler için değişik hizmetler alabilmektedir. 
   Projenin yaratımı hemen konsole sayfasından yapılabilmektedir. Projeyi yarattıktan sonra aktif hale getirmek (select etmek) gerekir. 
   Proje aktif hale geldiğinde proje sayfasına geçilmiş olur. Tabii proje yaratmak için bizim Google'a kredi kartımızı vermiş olmamız
   gerekir. Yukarıda da belirttiğimiz gibi biz kredi kartını vermiş olsak bile Google kullanım kadar para çekmektedir. 
   Projenin "dashboard" denilen ana bir sayfası vardır. Burada projeye ilişkin pek çok özet bilgi ve hızlı erişim bağlantıları 
   bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP’nin -tıpkı diğer platformlarda olduğu gibi- “CPU + RAM” kiralaması yapan ve “Compute Engine” denilen bir servisi vardır. 
    Benzer biçimde yine veritablarını host etmek için ve birtakım dosyaları saklamak için kullanılabilecek “Cloud Storage” hizmeti 
    bulunmaktadır. 

    GCP içerisinde birtakım servislerin erişebileceği bir storage alanına gereksim duyulmaktadır. Bunun için “Cloud Storage” 
    hizmetini seçmek gerekir. Ancak Google bu noktada sınırlı bedava bir hizmet verecek olsa da kredi kartı bilgilerini istemektedir. 
    Tıpkı AWS’de olduğu gibi burada da “bucket” kavramı kullanılmıştır. Kullanıcının önce bir “bucket yaratması” gerekmektedir.
    Bucket adeta cloud alanı için bize ayrılmış bir disk ya da klasör gibi düşünülebilir. Dosyalar bucket'lerin içerisinde bulunmaktadır.   
    Bucket yaratılması sırasında yine diğerlerinde olduğu gibi bazı sorular sorulmaktadır. Örneğin verilere hangi bölgeden erişileceği, 
    verilere hangi sıklıkta erişileceği gibi. Bucket’e verilecek isim yine AWS’de olduğu gibi GCP genelinde tek (unique) olmak zorundadır. 
    Bir bucket yaratıldıktan sonra artık biz yerel makinemizdeki dosyaları bucket'e aupload edebiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Aslında GCP üzerinde işlem yapmak için çeşitli komut satırı araçları da bulundurulmuştur. Biz bu komut satırı araçlarını 
    yerel makinemize install edip işlemleri hiç Web arayüzünü kullanmadan bu araçlarla da yapabilmekteyiz. Bu araçlar bizim 
    istediğimiz komutları bir script biçiminde de çalıştırabilmektedir. Aslında bu komut satırı araçları "Cloud Shell" ismiyle
    Web tabalı olarak uzak makinede de çalıştırılabilmektedir. 

    Yerel makinemize aşağıdkai bağlantıyı kullanarak gsutil programını kurabiliriz:

    https://cloud.google.com/storage/docs/gsutil_install

    Örneğin gsutil programı ile yerel makinemizdeki "cvid.csv" dosyasını GCP'deki bucket'imiz içerisine şöyle kopyalayabiliriz:

    gsutil cp covid.csv gs:/kaanaslan-test-bucket
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    GCP içerisindeki Auto ML aracına "Vertex AI" denilmektedir. Vertex AI aracına erişmek için GCP kontrol panelindeki ana menüyü
    kullanabilirisniz. Vertex AI'ın ana kontrol sayfasına "Dashboard" denilmektedir. Dolayısıyla bizim Dashboard'a geçmemiz gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Tipik olarak Vertex-AI'da işlem yapma aşamaları şöyledir:

    1) Veri kümesi bucket içerisine upload edilir. (Bu işlem Dataset oluşturulurken de yapılabilmektedir.)
    2) Dataset oluşturulur.
    3) Eğitim işlemi yapılır
    4) Deployment ve Test işlemleri yapılır
    5) Kestirim işlemleri yapılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Vertex AI'da ilk yapılacak şey bir "Dataset" yaratmaktır. Bunun için Vertex AI sayfasındaki "Datasets" sekmesi seçilir. 
    Buradan Create düğmesine basılır. Burada Dataset için bölge seçilir. (Bu bölgenin bucket ile aynı bölgede olması gerekmez
    ancak aynı bölgede olması daa uygundur.) Dataset'e bir isim verilir. Sonra problemin türü seçilir. Bir CSV dosyasından hareketle
    kestirim yapacaksak "Tabular" sekmesinden "Regression/Classification" seçilmelidir. Daytaset yaratıldıktan sonra artık bu dataset'in
    bir CSV dosyası ilişkilendirilmesi gerekmektedir. Ancak Vertex AI backet'teki CSV dosyalarını kullanabilmektedir. Burada üç seçenek 
    bulunmaktadır:

    * Upload CSV files from your computer
    * Select CSV files from Cloud Storage
    * Select a table or view from BigQuery

    Biz yerel bilgiyasarımızdaki bir CSV dosyasını seçersek zaten bu CSV dosyası önce bucket içerisine kopyalanmaktadır. 
    Eğer zaten CSV dosyasımız bir bucket içerisindeyse doğrudan bucket içerisindeki CSV dosyasını belirtebiliriz. BigQuery
    GCP içerisindeki veritabanı biçiminde organize edilmiş olan başka bir depolama birimidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Dataset oluşturulduktan sonra artık eğitim işlemine geçilebilir. Bunun için Vertex AI içerisindeki "Traning" sekmesi kullanılmaktadır. 
    Training sayfasına geçildiğinde "Create" düğmesi ile eğitim belirlemelerinin yapıldığı bölüme geçilebilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Training işleminde peşi sıra birkaç aşamadan geçilmektedir. "Traingng method" aşamasında bize hangi veri kümesi için eğitim 
    yapılacağı ve problemin bir "sınıflandırma problemi mi yoksa regresyon problemi mi" olduğu sorulur. Bundan sonra "Model details" 
    aşamasına geçilir. Bu aşamada bize veri kümesindeki kestirilecek sütunun hangisi olduğu sorulmaktadır. Bu aşamada "Advanced 
    Options" düğmesine basıldığında test ve sınama verilerinin miktarları belirlenebilmektedir. Default durumda test verileri ve 
    sınama verileri veri kümesinin %10'u biçiminde alınmaktadır. "Join featurestore" aşamasından doğrudan "Continue" ile geçilebilir. 
    Bundan sonra karşımıza "Training options" aşaması gelecektir. Burada eğitimde hangi sütunların kullanılacağı bize sorulmaktadır. 
    Yine bu aşamada da "Advanced Options" seçeneği vardır. Burada bize Loss fonksiyonu sorulmatadır. Tabii bunlar default değerlerle 
    geçilebilir. En sonunda "Compute and pricing" aşamasına gelinir. Burada dikkat etmek gerekir. Çünkü Google eğitimde harcanan
    zamanı ücretlendirmektedir. Google'ın ücretlendirme yöntemi aşağıdaki bağlantıdan imncelenebilir:

    https://cloud.google.com/vertex-ai/pricing

    Burada "Budget" eğitim için maksimum ne kadar zaman ayrılacağını belirtmektedir. Klasik tabular verilerde en az zaman 1 
    saat olarak, resim sınıflandırma gibi işlemlerde en az zaman 3 olarak girilebilmektedir. 

    En sonunda "Start Training" ile eğitim başlatılır. Eğitimler uzun sürebildiği için bitiminde e-posta ile bildirm yapılmaktadır. 

    Eğitim bittikten sonra biz eğitim hakkında bilgileri Training sekmesinden ilgili eğitimin üzerine tıklayarak görebiliriz. 
    Eğer problem regresyon problemi ise modelin başarısı çeşitli metrik değerlerle gösterilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kesitirim işlemlerinin yapılabilmesi için önce modelin "deploy edilmesi ve bir endpoint oluşturulması" gerekmektedir. 
    Modelin deploy edilmesi demek cloud sistemi içerisinde dışarıdan kullanıma hazır hale getirilmesi demektir. Böylece biz 
    kestirimi uzaktan programlama yoluyla da yapabiliriz. Deployment işlemi "Prediction" sekmesinden girilerek yapılabileceği gibi 
    "Model Registry" sekmesninden de yapılabilmektedir. EndPoint yaratımı sırasında bize Endpoint için bir isim sorulmaktadır. Sonra 
    model için bir isim verilmekte ve ona bir versiyon numarası atanmaktadır. Buradaki "Minimum number of compute nodes" ne kadar yüksek 
    tutulursa erişim o kadar hızlı yapılmaktadır. Ancak node'ların sayısı doğrudna ücretlendirmeyi etkilemektedir. Dolayısıyla burada 
    en düşük sayı olan 1 değerini girebilirsiniz. Daha sonra bize modelin deploy edileceği makinenin özellikleri sorulmaktadır. 
    Burada eğitimin başka bir makinede yapıldığına ancak sonucun kestirilmesi için başka bir makinenin kullanıldığına dikkat ediniz. 
    Modelimiz deploy edildikten sonra kullanım miktarı kadar ücretlendirme yapılmaktadır. Dolaysıyla denemelerinizden sonra 
    bu deployment işlemini silebilirsiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işleminden sonra artık predict işlemi yapılabilir. Bu işlem tamamen görsel arayüzle yapılabileceği gibi 
    Web API'leriyle ya da bunları kullanan Python koduyla da yapılabilmektedir. Eğer deploy edilmiş modelde kestirim 
    işlemini programlama yoluyla yapacaksanız bunun için öncelikle aşağıdaki paketi kurmanız gerekmektedir:

    pip install google-cloud-aiplatform

    Bundan sonra aşağıdaki import işlemini yapıp modüldeki init fonksiyonunun uygun parametrelerle çağrılması gerekmeketdir:

    from google.cloud import aiplatform

    aiplatform.init(....)

    predict işlemi için Training sekmesinden Deploy & Test sekmesini kullanmak gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Birden fazla predict işlemi "batch predict" denilen sekme ile yapılmaktadır. Uygulamacı kestirim için yine bir CSV dosyası 
    oluşturur. Bu CSV dosyasına bucket'e upload eder. Sonra "Batch predict" sekmesinden bu CSV dosyasına referans ederek 
    işlemi başlatır. Sonuçlar yine bu işlem sırasında belirlenen bucket'ler içerisinde CSV dosyaları biçiminde oluşturulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Şimdi de Vertex AI ile resim sınıflandırma işlkemi yapalım. Resim sınıflandırma gibi bir işlem şu aşamalardan geçilerek 
    gerçekleştirilmektedir:

    1) Resimler Google cloud'ta bir bucket'e upload edilir. 
    2) Resimler bir CSV dosyası haline getirilir. Tabii burada resmin içerisindeki data'lar değil onun bucket'teki yeri 
    kullanılmaktadır. 
    3) Bu CSV dosyasından hareketle Dataset oluşturulur. 
    4) Training işlemi yapılır.
    5) Deployment ve EndPoint ataması yapılır 
    6) Kestirim işlemi görsel atayüz yoluyla ya da WEB API'leri ya da Pythonkoduyla yapılır.

    Burada Dataset oluşturulurken bizden bir CSV dosyası istenmektedir. Bu CSV dosyası aşağıdaki gibi bir formatta oluşturulmalıdır:

    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı
    dosyanın_bucketteki_yeri,sınıfı

    Örneğin:

    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (1).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (10).jpg,boot
    gs://kaanaslan-test-bucket/ShoeVsSandalVsBootDataset/Boot/boot (100).jpg,boot
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Metin sınıflandırmaları da nemzer biçmde yapılabilmektedir. Burada iki seçenek söz konusudur. Metinler ayrı dosyalarda 
    bulunudurulup dosyalar bucket içerisine upload edilebilir yine resim sınıflandırma örneğinde olduğu gibi CSV dosyası 
    metinlere ilişkin dosyalardan ve onların sınıflarından oluşturulabilir. Ya da doğrudan metinlerin kendisi ve onların sınıfları da
    CSV dosyasının içerisinde bulunabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Amazon firması Cloud paltformlarına ilk giren büyük firmalardandır. Amazon'un cloud platformuna AWS (Amazon Web Services)
    denilmektedir. AWS iki yüzün üzerinde servis barındıran dev bir platformdur. Platformun pek çok ayrıntısı vardır. Bu nedenle 
    platformun öğrenilmesi ayrı bir uzmanlık alanı haline gelmiştir. Biz kurusumuzda platformun yapay zeka ve makine öğrenmesi 
    için nasıl kullanılacağı üzerinde özet bir biçimde duracağız. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS ortamında makine öğrenmesi etkinlikleri işleyiş olarak aslında daha önce görmüş olduğumuz Google Cloud Platform'a 
    oldukça benzemektedir.  Google Cloud Platform'daki "Vertex AI" servisinin Amazonda'ki mantıksal karşılığı "SageMaker"
    isimli servistir.  
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin ana kontrol paneline aşağıdaki bağlantı ile erişilebilir:
    
    console.aws.amazon.com

    Tabii AWS hizmeti almak için yine bir kayıt aşaması gerekmektedir. AWS kaydı sırasında işlemlker için bizden kredi kartı 
    bilgileri istenmektedir. Ancak AWS diğerlerinde olduğu gibi "kullanılan kadar paranın ödendiği" bir platformdur.

    AWS'nin konsol ekranına giriş yapıldığında zaten bize son kullandığmız servisleri listelemektedir. Ancak ilk kez giriş 
    yapıyorsanız menüden "SageMaker" servisini seçmelisiniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker'a geçildiğinde onun da bir "dash board" denilen kontrol paneli vardır. Burada biz notebook'lar yaratıp uzaktan 
    manuel işlemler yapabiliriz. Ancak SageMaker'ın Auto ML aracına "AutoPilot" denilmektedir. SageMaker'ı görsel olarak daha
    zahmetsiz kullanabilmek için ismine "Studio" denilen Web tabanlı bir IDE geliştirilmiştir. Son yıllarda "Google'ın collab'ına"
    benzer "Studio Lab" denilen bedava bir ortam da eklenemiştir. Kullanıcılar genellikle işlemlerini bu Studio IDE'siyle yapmaktadır. 
    SageMaker içerisinde "Studio"ya geçebilmek için en az bir "kullanıcı profilinin (user profile)" yaratılmış olması gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    AWS'nin veri saklamak için çeşitli servisleri vardır. Makine öğrenmesiş için en önemli depolama servisi S3 denilen servistir. 
    S3 servisinde de tıpkı GCP'de olduğu gibi "bucket" adı altında bir çeşit folder'lar oluşturulmaktadır. Sonra bu bucket'lere 
    dosyalar upload edilmektedir. Amazon "veri merkezlerini (data centers)" "bölge (zone)" denilen alnlarla bölümlere ayırmıştır. 
    Server'lar bu bölgelerin içerisindeki veri merkezlerinin içerisinde bulunmaktadır. Tıpkı GCP'de olduğu her bölgede her türlü 
    servis verilmeyebilmetedir. Kullanıcılar coğrafi bakımdan kendilerine yakın bölgeri seçerlerse erişim daha hızlı olabilmektedir. 

    Bir bucket yaratmak için ona "dünya genelinde tek olan (unique)" bir isim vermek gerekir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    SageMaker Studio'da Auto-ML etkinlikleri için Autopilot denilen uygulama kullanılmaktadır. Dolayısıyla Auto-ML işlemi için 
    AutoML seçilebilir. Autopilot'ta bir Auto-ML çalışması yapmak için bir "experiment" oluşturmak gerekir. Experiment 
    oluşturabilmek için "File/New/Create AutoML Experiment" seçilebilir ya da doğrudan Auto ML (Autopilot) penceresinde de 
    "Create Autopilot Experiment" seçilebilir. Yeni bir experiment yaratılırken bize onun ismi ve CSV dostasının bucket'teki 
    yeri sorulmaktadır. Sonra Next tuşuna basılarak bazı gerekli öğeler belirlenir. Örneğin tahmin edilecek hedef sütun ve 
    kestirimde kullanılacak sütunlar bu aşamada bize sorulmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Eğitim işlemi bittiğinde veri dosyanın bulunduğu bucket içerisinde bir klasör yaratılır ve bu klasör içerisinde model ile 
    ilgili çeşitli dosyalar bulundurulur. Buradaki iki dosya önemlidir:

    SageMakerAutopilotDataExplorationNotebook.ipynb
    SageMakerAutopilotCandidateDefinitionNotebook.ipynb
    
    Buradaki "SageMakerAutopilotDataExplorationNotebook.ipynb" dosyası içerisinde veriler hakkında istatistiksel bşrtakım özellikler 
    raporlanır. "SageMakerAutopilotCandidateDefinitionNotebook.ipynb" dosyasının içerisinde ise Autopilot'ın bulduğu en iyi modellerin
    nasıl işleme sokulacağına ilişkin açıklamalar bulunmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Kestirim yapabilmek için EndPoint oluşturulmuş olması gerekmektedir. Tıpkı GCP'de olduğu gibi modelin çalıştırılabilmesi için 
    bir server'a deploy edilmesi egrekmektedir. Deploy işlemi sonucunda bize bir EndPoint verilir. Biz de bu EndPoint'i kullanarak 
    Web arayüzü ile ya da Python programı ile uzaktan kestirimde bulunabiliriz. 
    
    AWS'de uzaktan Python ile işlem yapabilmek için "sagemaker" ve "boto3" gibi kütüphaneler oluşturulmuştur. Kütüphaneler şöyle yüklenebilir.  

    pip install sagemaker
    pip install boto3

    sagemaker kütüphanesi Web arayüzü ile yapılanları programlama yoluyla yapabilmekt için boto3 kütüphanesi ise uzaktan
    kesitirm (prediction) gibi işlemleri yapabilmek için kullanılmaktadır. 

    sagemaker kütüphanesi ile uzaktan işlemlerin yapılması kütüphanenin dokümantasyonlarında açıklanmıştır. Aşağıdaki 
    bağlantıyı kullanarak kodlar üzerinde değişiklikler yaparak ve kodlarda ilgili yerleri doldurarak uzaktan işlemler yapabilirsiniz:

    https://sagemaker.readthedocs.io/en/stable/overview.html
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   boto3 kütüphanesi ile uzaktan işlemler yapılırken önce bir Session nesnesinin yaratılması gerekmektedir. Sessin nesnesi yaratılırken 
   bizim AWS kaynaklarına ulaşabilmemiz için iki parola bilgisine sahip olmamız gerekir. Birincisi “aws_access_key_id” ve 
   ikincisi de “aws_secret_access_key”. Amazon servisleri uzaktan erişimler için “public key/private key” kriptografi uygulamaktadır. 
   Bu parola bilgileri Session nesnesi yaratılırken aşağıdaki verilebilir:

    import boto3

    session = boto3.Session(aws_access_key_id=XXXXX', aws_secret_access_key='YYYYY')

    Session nesnesi yaratıldıktan sonra hangi servisin kullanılacağını belirten bir kaynak nesnesi yaratılır. Örneğin:

    s3 = session.resource('s3')

    Aslında bu kaynak nesneleri session nesnesi yaratılmdan doğrudan da yaratılabilmektedir. Ancak parolaların bu durumda 
    “~/.aws/credentials” dosyasına aşağıdaki formatta yazılması gerekir:

    [default]
    aws_access_key_id = YOUR_ACCESS_KEY
    aws_secret_access_key = YOUR_SECRET_KEY

    Burada yukarıdaki iki anahtarı elde etme işlemi sırasıyla şu adımlarla yapılmaktadır:

    1) https://console.aws.amazon.com/iam/ Adresinden IAM işlemlerine gelinir. 
    2) Users sekmesi seçilir
    3) Kullanıcı ismi seçilir
    4) "Security credentials" sekmesi seçilir. 
    5) Buradan Create Acces Key seçilir. 

    İşlemler sırasında eğer yukarıdaki anahtarlar girilmek istenmiyorsa (bu amahtarların görülmesi istenmeyebilir) yukarıda da belirttiğimiz
    gibi bu anahtarlar özel bir dosyanın içerisine yazılabilir. Oradan otomatik alınabilir. Eğer bu anahtarlar ilgili dosyanın 
    içerisine yazılmışsa Session nesnesi yaratılırken parametre bu iki anahtarı girmemize gerek kalmaz. Örneğin:

    session = boto3.Session()

    Bu dosya bu bilgiler Amazon'un komut satırından çalışan aws programıyla da girilebilmektedir. Amazon'un komut satırından çalışan aws programını aşağıdaki 
    bağlantıdan inmdirerek kurabilirsiniz:

    https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

    Bu iki anahtarı ilgili dosyaya yazmak için aws programı şöyle kullanılabilir:

    aws configure

    Biz programlama yoluyla uzaktan bu bucket işlemlerini yapabiliriz. Örneğin tüm bucket'lerin isimleri aşağıdaki gibi elde edilebilir:

    for bucket in s3.buckets.all():
        print(bucket.name)
    
    Belli bir bucket'teki dosya aşağıdaki gibi download edilebilmektedir:

    s3 = boto3.client('s3')
    s3.download_file('kaanaslan-test-bucket', 'x.txt', 'y.txt')

    Burada söz konusu bucket içerisindeki "x.txt" dosyası "y.txt" biçiminde download edilmiştir. 

    Uzaktan predict işlemi yine boto3 kütüphanesi ile yapılabilmektedir. Aşağıda buna bir ilişkin bir örnek verilmiştir:

    import boto3

    session = boto3.Session(aws_access_key_id='AKIAWMMTXFTMCYOF352A',aws_secret_access_key='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', region_name='eu-central-1')

    predict_data = '''6,148,72,35,0,33.6,0.627,50
    1,85,66,29,0,26.6,0.351,31'
    '''

    client = session.client('runtime.sagemaker')
    response = client.invoke_endpoint(EndpointName='diabetes-test', ContentType='text/csv', Accept='text/csv', Body=predict_data)

    result = response['Body'].read().decode()
    print(result)

    Burada Session sınıfının client metodu kullanılarak bir sagemaker nesnesi elde edilmiştir. Sonra bu nesne üzerinde invoke_endpoint
    metodu çağrılmıştır. Tabii arka planda aslında işlemler Web Servisleriyle yürütülmektedir. Bu boto3 kütüphanesi bu işlemleri kendi 
    içerisinde yapmaktadır. Gelen mesajdaki Body kısmının elde edilip yazdırıldığında dikkat ediniz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
   Microsoft Azure 2009 yılında kurulan bir bulut sistemidir. 2014 yılında Microsoft bu Azure sistemine yapay zeka ve makine 
   öğrenmesine ilişkin servisleri eklemiştir. Microsoft Azure daha önce görmüş olduğumuz Google Cloud Platform ve Amazon AWS 
   sistemine benzetilebilir. Benzer hizmetler Azure üzerinde de mevcuttur. Azure ML de hiç kod yazmadan fare hareketleriyle 
   ve Auto MLaraçlarıyla kullanılabilmektedir. Tıpkı Google Cloud Platform ve Amazon SageMaker’da olduğu gibi bir SDK eşliğinde 
   tüm yapılan görsel işlemler programlama yoluyla da yapılabilmektedir. Microsoft Azure ML için tıpkı GCP ve AWS’de olduğu gibi 
   sertifikasyon süreçleri oluşturmuştur. Bu konuda sertifika sınavları da yapmaktadır. Yani Azure sistemi bütün olarak bakıldığında 
   çok ayrıntılara sahip bir sistemdir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure kullanımı için ilk yapılacak şey bir e-posta ile Microsoft hesabı açmaktır. Sonra bu hesap kullanılarak Azure hesabı 
    açılmalıdır. Azure hesabı açılırken "bedava" ve "ödediğn kadar kullan" biçiminde seçenekler karşımıza gelmektedir. Bedava 
    kullanımın pek çok kısıtları vardır. Bu nedenle deneme hesabınızı "ödediğin kadar kullan" seçeneği ile oluşturabilirisiniz. 
    Ancak kullanmadığınız servisleri her ihtimale karşı kapatmayı unutmayınız. Azure sistemine abona olunduktan sonra bir 
    kullajıcı için "abone ismi" oluşturulmaktadır. 
        
    Azure sistemini e-posta ve parola ile girildikten sonra ana yönetim sayfası portal sayfasıdır. Portal sayfasına 
    doğrudan aşağıdaki bağlantı ile girilebilmektedir:
    
    https://portal.azure.com

    Azure'ün ana sayfasına geçtikten sonra buradan Yapay Zeka ve Makine Öğrenmesi için "Azure Machine Leraning" seçilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Azure Machine Learning sayfasına geçildiğinde önce bir "Workspace" seçiminin yapılması gerekmektedir. Workspace yapılacak çalışmalar 
    için bir klasör gibi bir organizasyon oluşturmaktadır. Yeni bir Workspace oluşturabilmek için "Oluştur (Create)" düğmesine basılır. 
    Ancak bir "workspace" oluştururken bizim bir "kaynak grubuna (resource group)" ihtiyacımız vardır. Bu nedenle önceden bir 
    kaynak grubu oluşturulmuş olmalıdır. Kaynak grubu oluşturabilmek için ana menüden (hamburger menüden) "Kaynak Grupları (Resource Gropus)"
    seçilir. Kaynak grubu birtakım kaynakların oluşturduğu gruptur. Workscpace de bir kaynaktır. Dolayısıyla workspace'ler 
    kaynak gruplarının (resource groups) bulunurlar. Kaynak Grupları menüsünden "Oluştur (Create)" seçilerek kaynak grubu oluşturma
    sayfasına geçilir. Yaratılacak kaynak grubuna bir isim verilir. Bütün bu isimler dünya genelinde tek olmak zorundadır. 
    Kaynak Grupları diğer cloud sistemlerind eolduğu gibi bölgelerle ilişkilendirilmiştir. Bu nedenle kaynak grubu yaratılırken 
    o kaynak grubunun bölgesi de belirtilir. 

    Workspace oluştururken bizden bazı bilgilerin girilmesi istenmektedir. Ancak bu bilgiler default biçimde de oluşturulabilmektedir. 
    Ancak bizim workspace'e bir isim vermemiz ve onun yer alacağı kaynak grubunu (resource group) belirtmemiz gerekir. Bu adımlardan 
    sonra nihayet workspace oluşturulacaktır. Tabii bir workspace oluşturduktan saonra tekrar tekrar workspace oluşturmaya genellikle 
    gerek yoktur. Farklı çalışmaları aynı workspace içerisinde saklayabiliriz. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Microsoft da tıpkı Amazon'da olduğu gibi makine öğrenmesiişlemleri için Web tabanlı bir IDE benzeri sistem oluşturmuştur. 
    Buna "Machine Learning Studio" ya da kısaca "Studio" denilmektedir. Workspace'i seçip "Studio düğmesine basarak Studi IDE'sine 
    geçebiliriz. Auto ML işlemleri için Studio'da "Automated ML" sekmesine tıklanılır. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Machine Learning Studio'da Auto ML işlemlerini başlatmak için "Automated ML" sayfasında "New Automated ML Job" seçilir. 
    Daha önceden de gördüğümüz gibi bu tür Cloud Platformlarında dört ana unsur vardır:

    1) Storage (Eğitim için veri kümesisi barındırmak için ve eğitim sonucunda oluşturulacak dosyaları barındırmak için)
    2) CPU (Eğitimi yapabilmek için gereken makine)
    3) Model (Çeşitli yöntemlerle veri kümesine uygun en iyi ML modeli)
    4) Deployment ya da EndPoint (Hedef modelin konuşlandırılması ve Web Servisler yoluyla uzaktan kullanılabilir hale getirilmesi)

    Microsoft Azure sisteminde de "New Automated ML Job" işleminin ilk aşamasında bizden hangi veri kümesi üzerinde ML çalışması yapılacağı 
    sorulmaktadır. Biz bu aşamada yeni bir veri kümesini Azure'ün Storage sistemine upload edebiliriz. Ya da bu upload etme işlemi 
    daha önceden oluşturulabilir. Azure sisteminde upload edilmiş veri kümelerine "data asset" denilmektedir. "New Automated ML Job"
    işleminde toplam dört aşama bulunmaktadır:

    1) Select data asset: Bu aşamada üzerinde çalışılacak veri kümesi belirtilir. Yukarıda söz ettiğimiz gibi bu veri kümesi daha 
    önceden "data asset" biçiminde oluşturulmuş olabilir ya da bu aşamada oluşturulabilir. 

    2) Configure job: Burada işlem için önemli bazı belirlemeler yapılmaktadır. Örneğin yapılacak işleme bir isim verilmektedir. 
    Veri kümesindeki tahmin edilecek hedef sütun belirtilmektedir. Eğitim için kullanılacak makinenin türü de bu aşamada 
    sorulmaktadır. Makine türü için "Compute Instance" seçilebilir. Tabii bizim daha önceden yaratmış olduğumuz bir hesapalama 
    maknesi (compute instance) bulunmuyor olabilir. Bu durumda bir hesaplama makinesinin (yani eğitimde kullanılacak makinenin)
    yaratılması gerekecektir. Tabii aslında bir hesaplama makinesini (compute instance) daha önce de yaratmış olabiliriz. Hesaplama 
    makinesini daha önceden bu işlemden bağımsız olarak yaratmak için Manage/Compute sekmesi kullanılmaktadır. 

    3) Select task and settings: Burada bize problemin türü sorulmaaktadır. Tabii Azure hedef sütundan hareketle aslında problemin
    bir sınıflandırma (lojistik regresyon) problemi mi yoksa regresyon problemi mi olduğunu belirleyebilmektedir. 

    4) Hyperparameter Configuration: Bu aşamad bize sınama yönteminin ne olacağı ve test verilerinin nasıl oluşturulacağı sorulmaktadır.

    Bu aşamalardan geçildikten sonra Auto-ML en iyi modelleri bulmak için işlemleri başlatır. İşlemler bitince yine bize bildirimde bulunulmaktadır. 

#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Auto-ML aracı işini bitirdikten sonra EndPoint oluşturulmultur. Biz Studo'da EndPoints sekmesine gelerek ilgili endpoint'in 
    üzerine tıkladığımızda yukarıdaki menüde "Details", "Test", "Consume" gibi seçenekler bulunur. Burada "Test" seçildiğinde 
    GUI'den kestirim yapılabilmektedir. "Consume" kısmında Web servisleri ile predict işlemi yapan bir Python kodu bulundurulmaktadır. 
    Ancak bu kodda api_key kısmı boş bir string'tir. Buradaki API key "Consume" sekmesinden "Primary Key" kısmından alınabilir. 

    Örneğin burada Consume sekmesindeki kod aşağıdaki gibidir. Ancak bu kodlarda dikkat edilmesi gereken nokta şudur: Bu kodlarda
    hesaba erişim için gereken "api key" boş bırakılmıştır. Bizim bu API key'i alıp buraya kopyalamamız gerekir. Daha önceden de 
    belirttiğimiz gibi bu API key Consume sekmesinden Primary Key alanından elde edilebilmektedir.  Buradaki "consume kodu" şaşağıda
    verilmiştir.
#----------------------------------------------------------------------------------------------------------------------------

import urllib.request
import json
import os
import ssl

def allowSelfSignedHttps(allowed):
    # bypass the server certificate verification on client side
    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):
        ssl._create_default_https_context = ssl._create_unverified_context

allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.

# Request data goes here
# The example below assumes JSON formatting which may be updated
# depending on the format your endpoint expects.
# More information can be found here:
# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script
data =  {
  "Inputs": {
    "data": [
      {
        "age": 0,
        "sex": "example_value",
        "bmi": 0.0,
        "children": 0,
        "smoker": "example_value",
        "region": "example_value"
      }
    ]
  },
  "GlobalParameters": 0.0
}

body = str.encode(json.dumps(data))

url = 'https://kaanaslantestworkspace-wyndx.northeurope.inference.ml.azure.com/score'
# Replace this with the primary/secondary key or AMLToken for the endpoint
api_key = ''
if not api_key:
    raise Exception("A key should be provided to invoke the endpoint")

# The azureml-model-deployment header will force the request to go to a specific deployment.
# Remove this header to have the request observe the endpoint traffic rules
headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'automl457b251af41-1' }

req = urllib.request.Request(url, body, headers)

try:
    response = urllib.request.urlopen(req)

    result = response.read()
    print(result)
except urllib.error.HTTPError as error:
    print("The request failed with status code: " + str(error.code))

    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure
    print(error.info())
    print(error.read().decode("utf8", 'ignore'))

#----------------------------------------------------------------------------------------------------------------------------
    Aslında Microaoft tarafından hazırlanmış olan azureml isimli başka bir kütüphane daha bulunmaktadır. İşlemler bu kütüphane ile
    daha az kod yazarak da yapılabilmektedir. Bu kütüphane için aşağıdaki paketlerin yüklenmesi gerekmektedir:

    pip install azureml
    pip install azureml-core
    pip install azureml-data

    Aşağıda azureml kullanılarak kestirim işlemine örnek verilmiştir. 
#----------------------------------------------------------------------------------------------------------------------------

import json
from pathlib import Path
from azureml.core.workspace import Workspace, Webservice
 
service_name = 'automl245eb70546-1',
ws = Workspace.get(
    name='KaanWorkspace',
    subscription_id='c06cf27d-0995-4edd-919a-47fd40f4a7ae',
    resource_group='KaanAslanResourceGroup'
)
service = Webservice(ws, service_name)
sample_file_path = '_samples.json'
 
with open(sample_file_path, 'r') as f:
    sample_data = json.load(f)
score_result = service.run(json.dumps(sample_data))
print(f'Inference result = {score_result}')

#----------------------------------------------------------------------------------------------------------------------------
    Azure'ün Machine Learning servisinde diğerlerinde henüz olmayan "designer" özelliği de bulunmaktadır. Bu designed sayesinde 
    sürükle bırak işlemleriyle hiç kod yazmadan makşne öğrenmesi modeli görsel biçimde oluşturulabilmektedir. Bunun için 
    Machine Learning Stdudio'da soldaki "Designer" sekmesi seçilir. Bu sekme seçildiğinde karşımıza bazı seçenekler çıkacaktır. 
    Biz hazır bazı şablonlar kullanarak ve şablonları değiştirerek işlemler yapabiliriz ya da sıfırdan tüm modeli kendimiz 
    oluşturabiliriz. 

    Model oluştururken sol taraftaki pencerede bulunan iki sekme kullanılmaktadır. Data sekmesi bizim Azure yükledeiğimiz veri 
    kümelerini göstermektedir. Component sekmesi ise sürüklenip bırakılacak bileşnleri belirtmektedir. Her bileşen dikdörtgensel 
    bir kutucuk ile temsil edilmiştir. Kod yazmak yerine bu bileşenler tasarım ekranına sürüklenip bırakılır. Sonra da bu bileşenler
    birbirlerine bağlanır. Sürüklenip bırakılan bileşenlerin üzerine tıklanıp farenin sağ tuşu ile bağlam menüsünden bileşene özgü 
    özellikler görüntülenebilir. Bu bağlam menüsünde pek çok bileşen için en önemli seçenek "Edit node name" seçenğidir. Bu seçenekte
    bileşene ilişkin özenmli bazı özellikler set edilmektedir. 

    Eğer model için şablon kullanmayıp sıfırdan işlemler yapmak istiyorsak işlemlere Data sekmesinde ilgili veri kümesini sürükleyip
    tasarım ekranına bırakmakla başlamalıyız. 

    Veri kümesini belirledikten sonra biz veri kümesindeki bazı sütunlar üzerinde işlem yapmak isteyebiliriz. Bunun için 
    "Select Columns in Dataset" bileşeni seçilir. Veri kümesinin çıktısı bu bileşene fare hareketi ile bağlanır. Sonra 
    "Select Columns in Dataset" bileşeninde bağlam menüsünden "Edit node name" seçilir. Buradan da "Edit columns" seçilerek 
    sütunlar belirlenir. 

    Bu işlemden sonra eksik veriler üzerinde işlemlerin yapılması isteniyorsa "Clean Missing Data" bileşni seçilerek tasarım 
    ekranına bırakılır. 

    Bundan sonra veri kümesini eğitim ve test olmak üzere ikiye ayırabiliriz. Bunun için "Split Data" bileşeni kullanılmaktadır. 
    Bu bileşende "Edit node names" yapıldığında bölmenin yüzdelik değerleri ve bölmenin nasıl yapılacağına yönelik bazı 
    belirlemeler girilebilir. 

    Bu işlemden sonra "Özellik Ölçeklemesi (Feature Scaling)" yapılabilir. Bunun için "Normalize Data" bileşeni seçilir. 
    Bu bileşende "Edit node names" seçildiğinde biz ölçekleme üzerinde belirlemeleri yapabilecek duruma geliriz. 

    Bu aşamalardan sonra artık sıra modelin eğitimine gelmiştir. Modelin eğitimi için "Train Model" bileşeni kullanılmaktadır. 
    Bu bileşenin iki girişi vardır. Girişlerden biri "Dataset" girişidir. Bu girişe veri test veri kümesi bağlanır. Bileşenin
    birinci girişi olan "Untrained model" girişine ise problemin türünü belirten bir bileşen bağlanır. Çeşitli problem türleri 
    için çeşitli bileşenler vardır. Örneğin ikili sınıflandırma problemleri için "Two class logistic regression" bileşeni kullanılır. 
    Eğitime ilişkin hyper parametreler bu bileşende belirtilmektedir. 

    Eğtim işleminden sonra sıra modelin test edilmesine gelmiştir. Modelin testi için "Score Model" bileşeni kullanılır. Score Model
    bileşeninin iki girişi vardır: "Trainded Model" ve "Dataset" girişleri. "Train Model" bleşeninin çıkışı "Trained Model" girişine, 
    "Split Data" bileşeninin ikinci çıkışı ise "Dataset" girişine bağlanır.

    Model Designer'da oluşturulduktan sonra artık sıra modelin eğitilmesine gelmiştir. "Configure & Submit" düğmesine basılır. 
    Burada artık eğitim için kullanılacak CPU kaynağı belirlenir. (Anımsayacağınız gibi Automated araçlarda bizim belirlememiz
    gereken üç unsur "Data" + "Model" + "CPU" biçimindeydi.) 

    Nihayet işlemleri başlatıp deployment işlemi için "Inference Pipeline"" seçeneği seçilmelidir. 

    Azure Designer'daki tüm bileşenler (components) aşağıdaki Microsoft bağlantısında dokümante edilmiştir:

    https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference?view=azureml-api-2


    Biz Inference Pipeline işlemini yaptığımızda Azure bize modelimizi kestirimde kullanılabecek hale getirmektedir. Bunun 
    için model bir "Web Service Output" bileşeni eklemektedir. Bu "Web Service Output" bileşeni kestirim çıktılarının 
    bir web servis biçiminde verileceği anlamına gelmektedir. Eskiden Azure aynı zamanda "Inference Pipeline" seçildiğinde
    modelimize bir "Web Service Input" bileşeni de ekliyordu. Bu bileşen de girdilerin web service tarafından alınacağını
    belirtmekteydi. Designer'ın yeni versiyonlarında "Infererence Pipeline" yapıldığında bu "Web Service Input" bileşeni artık
    eklenmemektedir. Web Service Input bileşeni eklendikten sonra artık bizim "Select Columns in Dataset" bileşeninden 
    kestirilecek sütunu çıkartmamız gerekmektedir. Ayrıca bu Web Service Input bileşeninin çıktısının artık "Select Columns in Dataset"
    bileşenine değil doğrudan ApplyTransformation bileşenine bağlanması gerekmektedir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Desginer'ın aytıntılı kullanımı için Microsoft dokümanlarını inceleyebilirsiniz. Örneğin aşağıdaki dokümanda lojistik olmayan 
    regresyon probleminin adım adım designer yardımıyla oluşturulması anlatılmaktadır:

    https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-train-score?view=azureml-api-1

#----------------------------------------------------------------------------------------------------------------------------
    IBM'in Cloud Platformu da en çok kullanılan platformlardan biridir. İşlevsellik olarak diğerlerine benzemektedir. IBM'de 
    cloud platformu içerisinde makine öğrenmesine ilişkin servisler bulundurmaktadır. Genel olarak IBM'in bu servislerine
    Watson denilmektedir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformu için yine bir hesap açılması gerekir. Hesap açma ve sign in işlemleri cloud.ibm.com adresinden 
    yapılmaktadır. Hesap açılırken girilen e-posta adresi aynı zmaanda "IBMid" olarak kullanılmaktadır. IBMid cloud platformunda
    "user id" gibi kullanılmaktadır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Cloud platformuna login olunduktan sonra karşımıza bir "Dashboard" sayfası çıkmaktadır. Burada ilk yapılacak şey 
    "Create Resource" seçilerek kaynak yaratılmasıdır. Buradan "Watson Studio" seçilir. Bedava hesp ile ancak bir tane "Watson Studio"
    kaynağı oluşturulabilmektedir. Kaynak oluşturulduktan sonra "Launch in IBM Cloud Pak for Data" düğmesine basılarak
    "IBM Watson Studio" ortamına geçilmektedir. IBM Watson Studio bir çeşit "Web Tabanlı IDE" gibi düşünülebilir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    IBM Watson Studio'ya geçildiğinde öncelikle bir projenin yaratılması gerekmektedir. Bunun için "New Project" seçilir. 
    Proje bir isim verilir. Sonra projenin yaratımı gerçekleşir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Proje yaratıldıktan sonra "New Asset" düğmesi le yeni bir proje öğesi (asset) oluşturulmalıdır. Burada Otomatik ML işlemleri 
    için "Auto AI" seçilebilir.
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Auto AI seçildiğinde yaratılacak işlem (experiment) için bir isim verilmelidir. Sonra bu işlem (experiment)" bir ML servisi 
    ile ilişkilendirilmelidir. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Watson Studio'da Auto AI projesinde bizden öncelikle veri kümesinin yüklenmesi istenmektedir. Veri kümesi yüklendikten sonra 
    bu veri kümesinin ardışık "time series" verilerinden oluşup oluşmadığı bize sorulmaktadır. Bundan biz kesitim yapılacak sütun
    belirleriz. Uygulamacı problemin türüne göre problemin çeşitli meta parametrelerini kendisi set edebilmektedir. Bu işlem 
    "Experiment Settings" düğmesiyle yapılmaktadır. Nihayet uygulamacı "Run Experiment" seçeneği ile eğitimi başlatır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Modeller oluşturulduktan sonra bunlar performansa göre iyiden kötüye doğru sıralanmaktadır. Bu modeller save edilebilir. 
    Modeller save edilirken istenirse model kodları bir Jupyter Notebook olarak da elde edilebilmektedir. Bu notebook yerel makineye
    çekilip IBM Python kütüphanesi kurulduktan sonra yerel makinede de çalıştırılabilir. 
#----------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------------------------------------------
    Deployment işlemi için önce bir "deployment space" yaratılır. Ancak bu deployment space'te Assets kısmında deploy edilecek 
    modelin çıkması için dah önceden Model sayfasında "Promote to Deployment Space" seçilmelildir. Bundan sonra Deployment Space'te
    Assets kısmında ilgili model seçilerek "Create Deployment" düğmesi ile deployment işlemi yapılır. 
#----------------------------------------------------------------------------------------------------------------------------
   
#----------------------------------------------------------------------------------------------------------------------------
    Deployment sonrası yine Wen Servisleri yoluyla model kullanılabilmektedir. Bu işlem Watson Web API'leriyle yapılabilecğei gibi
    diğer cloud platfotrmlarında olduğu gibi bu Web API'lerini kullanan Python kütüphaneleriyle de yapılabilmektedir. Kütüphane 
    aşağıdaki gibi install edilebilir:

    pip install ibm-watson-machine-learning   
#----------------------------------------------------------------------------------------------------------------------------



