/*----------------------------------------------------------------------------------------------------------------------

                                       C ve Sistem Programcıları Derneği

     "Linux Kernel - İşletim Sistemlerinin Tasarımı ve Gerçekleştirilmesi" Kursunda Yapılan Örnekler ve Özet Notlar
                                                   1. Bölüm

                                             Eğitmen: Kaan ASLAN

        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:

    Aslan, K. (2025), "Linux Kernel - İşletim Sistemlerinin Tasarımı ve Gerçekleştirilmesi Kursu, Sınıfta Yapılan 
        Örnekler ve Özet Notlar", C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları sabit genişlikli font kullanan programlama editörleri ile açınız.)
                        (Editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                    Son Güncelleme: 28/10/2025 - Salı

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										1. Ders 19/07/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kurs katılımcılarıyla tanışıldı.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursun amacı, kapsamı ve genel işleyişi hakkında açıklamalar yapıldı. Yardımcı kaynaklar (kitaplar, dokümanlar ve 
    web sayfaları) tanıtıldı.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuz için bir sanal makineye herhangi bir Linux dağıtımının kurulması gerekmektedir. Dağıtım minimalist biçimde 
    kurulabilir. Ancak Linux içerisinde C derleyicisinin (gcc ya da clang) ve binary utility araçlarının bulunuyor olması 
    gerekir. Kursumuzda Linux kaynak kodları üzerinde değişiklikler yapıp çekirdeği yeniden derleyerek çeşitli denemeler 
    yapacağız. Bu nedenle kurduğunuz sistemin bir kopyasını da saklamanızı salık veriyoruz.

    Kurusumuzda Debian türevi bir Linux dağıtımının kurulmuş olduğu varsayılacaktır. Biz bazı konularda açıklamalar 
    yaparken Debian dağıtımının "apt" paket sistemini kullanacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzda Linux işletim sisteminin kaynak kodlarında gezinmek için https://elixir.bootlin.com/linux/ sitesini 
    kullanacağız. Bu sitede Linux'un 0.01 versiyonundan günümüzdeki en son versiyonuna kadar bütün resmi versiyonlarının 
    kaynak kodları bulunmaktadır ve bu kaynak kodlar üzerinde gezintiler (navigation) yapılabilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										2. Ders 20/07/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri bilgisayar donanımının kaynaklarını yöneten, bilgisayar donanımı ile kullanıcı arasında arayüz 
    oluşturan sistem programlarıdır. Bilgisayar bilimlerinin akademik öncülerinin çoğu işletim sistemlerini bir kaynak 
    yöneticisi (resource manager) olarak tanımlamıştır.

            +----------------------+
            | Uygulama Programları |
            +----------------------+
        +-----------------------------+
        |       İşletim Sistemi       |
        +-----------------------------+
    +-----------------------------------+
    |        Bilgisayar Donanımı        |
    +-----------------------------------+

    İşletim sistemlerinin yönettiği kaynakların en önemlileri şunlardır:

    - CPU: İşletim sistemi hangi programın ne zaman, ne kadar süre için CPU'ya atanacağına karar verip bu işlemleri 
    gerçekleştirmektedir.

    - Ana Bellek (Main Memory (RAM)): İşletim sistemi programların ana belleğin neresine yükleneceğine karar verir 
    ve ana bellek kullanımını düzenler.

    - İkincil Bellekler: İşletim sistemi bir dosya sistemi (file system) oluşturarak dosyaların parçalarını ikincil 
    belleklerde etkin bir biçimde tutar ve kullanıcılara bir dosya kavramıyla sunar.

    - Çevre Birimleri (klavye, fare, yazıcı vs.): İşletim sistemi fare, klavye, yazıcı gibi çevre birimlerini yöneterek 
    onları kullanıma hazır hale getirir. Yardımcı işlemcileri (denetleyicileri) programlayarak onların işlev görmesini 
    sağlamaktadır.

    - Ağ İşlemleri: İşletim sistemi ağa ilişkin donanım birimlerini yöneterek dışarıdan gelen bilgileri onları talep 
    eden programlara iletir.

    İşletim sistemleri kaynak yönetimine göre alt sistemlere ayrılarak da incelenebilmektedir. Örneğin işletim 
    sisteminin "çizelgeleyici (scheduler)" alt sistemi demekle CPU yönetimini sağlayan alt sistemi kastedilmektedir. 
    Ana bellek yönetimi (memory management) yine soyutlanarak incelenen önemli alt sistemlerden biridir. İşletim 
    sistemlerinin ikincil bellek yönetimine "dosya sistemi (file system)" da denilmektedir. Tabii bütün bu sistemler 
    birbirinden kopuk olarak değil birbirleriyle ilişkili bir biçimde işlev görmektedir. Bu durumu insanın "solunum 
    sistemi", "dolaşım sistemi", "sinir sistemi", "boşaltım sistemi" gibi alt sistemlerine benzetebiliriz. Bu alt 
    sistemlerin birinde bile çalışma bozukluğu oluşsa insan yaşamını yitirebilmektedir.

    İşletim sistemleri yapı olarak iki kısımdan oluşmaktadır: Çekirdek (kernel) ve kabuk (shell). Çekirdek işletim 
    sisteminin donanımı kontrol eden ve kaynakları yöneten motor kısmıdır. Aslında işletim sistemi denildiğinde akla 
    çekirdek gelmektedir. Kabuk ise işletim sisteminin kullanıcı ile arayüz oluşturan önyüzüdür. Örneğin UNIX/Linux 
    sistemlerinde bash gibi komut satırı, GNOME, KDE gibi pencere yöneticileri, Windows'taki masaüstü (Explorer), 
    macOS'teki masaüstü (Aqua) bu işletim sistemlerinin kabuk kısımlarını oluşturmaktadır.

    +-----------------------------------+
    |           Kabuk (Shell)           |
    |   +---------------------------+   |
    |   |                           |   |
    |   |    Çekirdek (Kernel)      |   |
    |   |                           |   |
    |   +---------------------------+   |
    +-----------------------------------+

    Pekiyi işletim sistemi bu kadar temel donanım yönetimini sağlıyorsa işletim sistemi olmadan programlama yapılabilir 
    mi? İşletim sistemi olmadan programlama faaliyetine halk arasında "bare metal programlama" denilmektedir. Bare metal 
    programlama gömülü sistemlerde, mikrodenetleyicilerin kullanıldığı uygulamalarda kullanılmaktadır. Bare metal 
    programlama genellikle özel bir amaca hizmet edecek biçimde yapılmaktadır. Amaçlar fazlalaştığı zaman ve sistem 
    karmaşıklaştığı zaman artık işletim sistemlerine gereksinim duyulmaktadır.

    Bazı kontrol yazılımları işletim sistemlerinin bazı etkinliklerini de sağlamaktadır. Bir kontrol yazılımının 
    işletim sistemi olarak isimlendirilmesi için yukarıda açıkladığımız kaynak yönetimlerinin önemli bir bölümününü 
    sağlıyor olması gerekir. Bu kaynak yönetimlerinin çoğunu sağlamayan kontrol yazılımlarına genel olarak "firmware" 
    de denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri çeşitli biçimlerde sınıflandırılabilmektedir:

    - Proses Yönetimine Göre: Aynı anda tek bir programı çalıştıran işletim sistemlerine "tek prosesli (single processing)", 
    aynı anda birden fazla programı çalıştırabilen işletim sistemlerine ise "çok prosesli (multiprocessing) işletim sistemleri 
    denilmektedir. Örneğin DOS işletim sistemi tek prosesli bir sistemdi. Biz bu işletim sisteminde bir programı çalıştırırdık 
    ancak çalıştırdığımız program sonlanınca başka bir programı çalıştırabilirdik. Halbuki Windows, UNIX/Linux, MacOS gibi 
    işletim sistemleri çok prosesli işletim sistemleridir.

    - Kullanıcı Sayısına Göre: Birden fazla farklı kullanıcının çalışabildiği sistemlere "çok kullanıcılı (multiuser)", 
    tek bir kullanıcının çalışabildiği sistemlere "tek kullanıcılı (single user)" sistemler denilmektedir. Genellikle 
    çok prosesli işletim sistemleri aynı zamanda çok kullanıcılı sistemlerdir. Birden fazla kullanıcının söz konusu 
    olduğu sistemlerde kullanıcıların yetkilerinin ayarlanması, kullanıcıların birbirlerinin alanlarına erişmesinin 
    engellenmesi, sistem kaynaklarını belli oranlarda bölüşmesi gerekebilmektedir. Örneğin DOS tek kullanıcılı bir 
    sistemdi. Halbuki Windows, UNIX/Linux ve macOS sistemleri çok kullanıcılı sistemlerdir.

    - Çekirdek Yapısına Göre: İşletim sistemleri çekirdek yapısına göre "tek parçalı çekirdekli (monolithic kernel)" 
    ve "mikro çekirdekli (microkernel)" olmak üzere ikiye ayrılmaktadır. Tek parçalı çekirdekli işletim sisteminin büyük 
    kısmı çekirdek modunda çalışır. Mikro çekirdekli sistemlerde ise çekirdek modunda çalışan kısım minimize edilmeye 
    çalışılmıştır. Aslında tek parçalı ve mikro çekirdekli tesarımları bir spektrum olarak düşünebiliriz. (Örneğin bu 
    spektrumda bazı çekirdekler tek parçalı tarafa yakın bazıları ise mikro tarafa yakın olabilmektedir.)

    - Dışsal Olaylarla Yanıt Verebilme Özelliğine Göre: İşletim sistemleri dışsal olaylara yanıt verme bakımından gerçek 
    zamanlı olan (real-time) ve gerçek zamanlı olmayan (non-real-time) sistemler olmak üzere ikiye ayrılabilir. Dışsal 
    olaylara hızlı bir biçimde yanıt verebilecek çekirdek yapısına sahip olan işletim sistemlerine "gerçek zamanlı (real-time) 
    işletim sistemleri denilmektedir. Gerçek zamanlı işletim sistemleri de kendi aralarında "katı (hard real-time)" ve 
    "gevşek (soft real-time)" işletim sistemleri olmak üzere ikiye ayrılabilmektedir. Katı gerçek zamanlı sistemler dışsal 
    olaylara yanıt verme bakımından çok güvenilir olma iddiasındadır. Gevşek gerçek zamanlı sistemler ise bu konuda daha 
    toleranslıdır.

    - Dağıtıklık Durumuna Göre: İşletim sistemleri dağıtıklık durumuna göre "dağıtık olan (distributed)" ve "dağıtık olmayan 
    (non-distributed)" sistemler biçiminde ikiye ayrılabilmektedir. Dağıtık işletim sistemlerinde sistem birden fazla 
    bilgisayardan oluşan tek bir sistem gibi davranmaktadır. Örneğin 10 tane makineyi tek bir sistem olarak düşünebilirsiniz. 
    Bu durumda bu bilgisayarların kaynakları (örneğin diskleri ve CPU'ları) bu 10 makine tarafından paylaşılmaktadır. 
    Windows, UNIX/Linux ve macOS dağıtık işletim sistemleri değildir. Ancak bu sistemlerde dağıtık uygulamalar yapılabilmektedir.

    - Donanım Özelliğine Göre: Neredeyse her yaygın masaüstü işletim sisteminin bir mobil versiyonu da oluşturulmuştur. 
    IOS (Iphone Operating System) ve ipadOS Apple firmasının (yani macOS sistemlerinin) mobil işletim sistemleridir. Android 
    bir çeşit mobil Linux sistemi olarak değerlendirilebilir. Android projesinde Linux çekirdeği alınmış, biraz özelleştirilmiş, 
    bazı parçaları atılmış, buna bir mobil arayüz giydirilmiş ve sistem akıllı telefonlara ve tabletlere uygun hale getirilmiştir. 
    Nokia eskiden Symbian sistemlerinde büyük bir pazar payına sahipti. Ancak bu firma akıllı telefon geçişini iyi yönetemedi. 
    MeeGo ve Maemo gibi işletim sistemlerini denedi. Sonra ekonomik sıkıntılar sonucunca büyük ölçüde Microsoft tarafından 
    satın alındı. Windows'un mobil versiyonuna genel olarak Windows CE denilmektedir. Windows CE'nin akıllı telefonlar ve 
    tabletler için özelleştirilmiş biçimine ise Windows Mobile ve Windows Phone denilmektedir. Ancak Microsoft 2010 yılında 
    Windows Mobile işletim sistemini 2017'de de Windows Phone işletim sistemini sonlandırmıştır ve bu alandaki rekabetten 
    tamamen çekilmiştir. Windows CE ise Windows IoT Core ismiyle farklı bir tasarımla evrimleşerek devam ettirilmektedir.

    - Kaynak Kod Lisansına Göre: Kaynak kod lisansına göre işletim sistemlerini kabaca "açık kaynak kodlu (open source)" 
    ve "mülkiyete bağlı (proprieatary)" olmak üzere ikiye ayırabiliriz. Açık kaynak kodlu işletim sistemleri değişik açık 
    kaynak kod lisanslarına sahip olabilmektedir. Bunların kaynak kodları indirilip üzerinde değişiklikler yapılabilmektedir. 
    Örneğin Windows işletim sistemi mülkiyete sahiptir. Oysa Linux, BSD sistemleri, Solaris, Android gibi sistemler açık 
    kaynak kodludur. macOS sistemlerinin ise çekirdeği açık diğer kısımları (örneğin kabuk kısmı ve diğer katmanları) 
    kapalıdır.

    - Kaynak Kodun Özgünlüğüne Göre: Bazı işletim sistemleri bazı işletim sistemlerinin kodları alınıp değiştirilerek 
    oluşturulmuştur (örneğin Android ve macOS'ta olduğu gibi). Bazı işletim sistemlerinin kodları ise sıfırdan yazılmıştır. 
    Kodları sıfırdan yazılan yani orijinal kod temeline dayanan işletim sistemlerinden bazıları şunlardır:

    AT&T UNIX
    DOS
    Windows
    Linux
    BSD'ler (belli bir yıldan sonra)
    Solaris
    XENIX
    VMS

    Burada orijinal mimari ile orijinal kod tabanını birbirine karıştırmamak gerekiyor. Linux UNIX işletim sisteminin 
    mimarisini temel almıştır. Ancak tüm kodları sıfırdan yazılmıştır. Yani orijinal AT&T UNIX sistemindeki kaynak kodların 
    bir bölümü kopyalanarak kullanılmamıştır.

    - GUI Çalışma Desteğine Göre: Bazı işletim sistemleri GUI çalışma modelini doğrudan desteklerken bazıları desteklememektedir. 
    Örneğin Windows sistemleri çekirdekle entegre edilmiş bir GUI çalışma modeli sunmaktadır. UNIX/Linux sistemleri de 
    X Window (ya da X11) ve Wayland katmanlarıyla benzer bir model sunmaktadır. Fakat örneğin DOS işletim sisteminin böyle 
    bir doğal GUI desteği yoktu.

    - Ağ Üzerinde Hizmet Alıp Verme Rollerine Göre: İşletim sistemlerini ağ altında hizmet alıp verme rollerine göre 
    "istemci (client) ve sunucu (server) biçiminde de iki gruba ayırabiliriz. Bazı işletim sistemlerinin istemci versiyonları 
    birbirlerinden ayrılmıştır. Bazılarında ise bu ayrım yapılmamıştır. Örneğin Windows 7, 8, 10, 11 sistemleri bu bakımdan 
    istemci (client) sistemleridir. Halbuki Windows Server 2016, 2019 sunucu sistemleri olarak piyasaya sürülmüştür. 
    Eskiden Mac OS X'in istemci ve sunucu versiyonları farklıydı. Fakat Mac OS X 10.7 (Lion) ile birlikte istemci ve 
    sunucu versiyonları birleştirildi. Linux dağıtımlarının çoğu da hem istemci hem de sunucu olarak kullanılabilmektedir. 
    Ancak bazı dağıtımların ise istemci ve sunucu versiyonları farklıdır. Pekiyi işletim sistemlerinin istemci ve sunucu 
    versiyonları arasındaki farklılıklar nelerdir? Kabaca iki tür farklılığın olduğunu söyleyebiliriz. Birincisi çekirdekle 
    ilgili farklılıklardır. Genellikle sunucu sistemlerinde çizelgeleyici alt sistemde istemci sistemlerine göre farklılıklar 
    bulunmaktadır. İkincisi ise barındırdıkları yardımcı yazılımlardır. işletim sistemlerinin sunucu versiyonları hazır 
    bazı sunucu programlarını da içermektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de işletim sistemlerinin tarihsel gelişimi üzerinde duracağız. 1940’lı yıllarda ilk elektronik bilgisayarlar 
    yapıldığında henüz bir işletim sistemi kavramı yoktu. Bu bilgisayarlara program yazacak olanlar işletim sistemi 
    faaliyetlerini de kendileri yapmak zorunda kalıyordu. (Yani şimdi mikrodenetleyicilere bare metal kod yazanlarda 
    olduğu gibi.) Transistör bulunduktan sonra 1950’li yıllarda artık elektronik bilgisayarlar yavaş yavaş transistörlerle 
    yapılmaya başlandı. Transistörlerin ortaya çıkması hem bilgisayarların kapasitelerini ve güvenilirliklerini artırmış, 
    hem de güç harcamalarını düşürmüştür.

    1950'li yıllarda IBM gibi pek çok bilgisayar üreticisi firma yalnızca donanım satıyordu. İşletim sistemi gibi programları 
    yazmak kullanıcıların yapması gereken bir işti. Böylece donanımı satın alan her kurum işletim sistemine benzeyen 
    programları da kendisi yazıyordu. Bu anlamda standart bir işletim sistemi yoktu. Bugünkü anlamda ilk işletim sisteminin 
    General Motors'un 1956 yılında IBM'in 701 sistemi için yazdığı NAA IO (North American Aviation Input Output System) 
    olduğu söylenebilir.

    1960'lara gelindiğinde IBM System/360 isminde yeni bir bilgisayar donanımı geliştirme işine girişti ve artık donanımla 
    işletim sistemini birlikte satma fikrini benimsedi. Bu donanım 1964 yılında duyuruldu ve 1965 yılında gerçekleştirildi. 
    İlk System/360 Model 30 bilgisayarları o zamanın "Solid Logic Technology (SLD)" teknolojisiyle üretilmişti. Hem öncekilerden 
    daha güçlüydü hem de daha az yer kaplıyordu. Saniyede 34500 işlem yapabiliyordu ve 8K ile 64K ana belleğe sahipti. 
    1967 yılında System/360'ın Model 60'ı piyasaya sürüldü. Bu model saniyede 16.6 milyon komut çalıştırabiliyordu ve ana 
    belleği de tipik olarak 512K, 768K ve 1 MB idi. IBM Sistem 360 donanımları için 1964 yılında ilk kez OS/360 işletim 
    sistemini geliştirdi. IBM daha sonra 1967 yılında OS/360 Model 67 için OS/360'ın TSS 360 isminde zaman paylaşımlı (time 
    sharing system) bir versiyonunu daha geliştirmiştir. IBM'in System/360 makineleri ve işletim sistemleri önemli ticari 
    başarı kazandı. System/360'ı System/370 izledi. System/360 ve System/370 için başka kurumlar da işletim sistemleri 
    geliştirmiştir. Michigan Terminal System (MTS) ve MUSIC/SP bunlar arasında önemli olanlardandır.

    1960'lı yıllarda başka firmalar da işletim sistemleri geliştirmiştir. Örneğin Control Data Corporation firmasının 
    SCOPE işletim sistemi batch işlemler yapabiliyordu. Aynı firma MACE isminde bu işletim sisteminin zaman paylaşımlı 
    bir versiyonunu da yazmıştır. Firma bu çalışmalarını 1970'li yıllarda Kronos işletim sistemiyle devam ettirmiştir. 
    Burroughs firması 1961 yılında MCP işletim sistemi ile B5000 bilgisayarlarını, GE firması da 1962 yılında GECOS işletim 
    sistemiyle GE-600 serisi bilgisayarlarını piyasaya sürdü. UNIVAC dünyanın ilk ticari bilgisayarlarını üreten firmadır. 
    Bu firma da 1962 yılında UNIVAC 1107 için EXEC I işletim sistemini yazdı. Bu işletim sistemini sırasıyla Exec 2 ve 
    Exec 8 izledi.

    DEC (Digital Equipment Corporation) eskilerin en önemli bilgisayar üretici firmalarından biriydi. (DEC 1998 yılında 
    Compaq firması tarafından Compaq' firması da 2002 yılında HP firması tarafından satın alındı.) Firmanın en önemli 
    ürünleri PDP (Programmed Data Processor) isimli bilgisayarlarıdır. Firma PDP-1'den (1959) başlayarak PDP-16'ya (1971-
    1972) kadar PDP makinelerinin 16 versiyonunu piyasaya sürmüştür. DEC'in PDP-8'inin mini bilgisayar devrimini başlattığı 
    söylenebilir. Bu model 50000'in üzerinde satışa ulaşmıştır. UNIX işletim sistemi 1969 yılında ilk kez DEC'in PDP-7 modeli 
    üzerinde yazılmıştır. 1965 yılında piyasaya sürülen DEC PDP-7 18 bitlik bir makineydi. Makine DECsys denilen işletim 
    sistemi benzeri bir yönetici programla beraber satılıyordu. DEC'in 1966 yılında çıkardığı PDP-10 26 bitlik bir makineydi 
    DEC bu modelle birlikte işletim sistemi olarak TOPS-10 isimli bir sisteme geçti.

    1960'lı yılların sonuna kadar işletim sistemleri ağırlıklı olarak sembolik makine diliyle yazılıyordu. 1960’lı yılların 
    sonlarında AT&T Bell Lab. tarafından UNIX işletim sistemi geliştirildiğinde önemli bir devrim yaşandı. UNIX işletim 
    sistemi 1973 yılında C ile yeniden yazılmıştır. Böylece artık işletim sistemlerinin yüksek seviyeli dillerle de yazılabildiği 
    görülmüştür. PDP-11'i 16 bitlik PDP-12 izledi. PDP-12 Intel'in x86 ve Motorola'nın 6800 işlemcileri için ilham kaynağı 
    olmuştur.

    1970’li yılların ikinci yarısında entegre devrelerin de geliştirilmesiyle "ev bilgisayarları (home computer)" ortaya 
    çıkmaya başladı. Bunlarda genellikle BASIC yorumlayıcıları ile iç içe geçmiş CP/M tya da GEOS işletim sistemleri 
    kullanılıyordu. 1970'li yıllarda pek çok firma farklı ev bilgisayarları üretmiştir. BBC Micro, Commodore 64, 
    Apple II, Atari, Amstrad, ZX Spectrum dönemin en ünlü ev bilgisayarlarındandı. Bu makinelerde kullanılan işlemciler 
    Intel'in 8080'i, Zilog'un Z80'i, Motorola'nın 6800'ü gibi 8 bitlik işlemcilerdi.

    DEC firması 1977 yılında VAX isimli bilgisayarı ve 32 bitlik işlemci birimini piyasaya sürdü. VAX ailesi makineler 
    o yıllarda önemli bi ticari başarı kazanmıştır. DEC VAX makineleri için VAX/VMS isimli bir işletim sistemi yazmıştı. 
    DEC bu işletim sisteminin ismini 1992 yılında OpenVMS olarak değiştirdi. DEC 1992 yılında 64 bitlik RISC tasarımı olan 
    Alpha işlemcilerini piyasaya sürdü ve OpenVMS Alpha işlemcilerine port edildi. OpenVMS hala kullanılmaya devam etmektedir. 
    Itanium ve X86-64 portları da vardır.

    Apple firması 1976 yılında kuruldu. Apple'ın ilk bilgisayarı Apple I idi. Bunu 1977'de Apple II, 1980'de de Apple III 
    izledi. Bu ilk Apple bilgisayarlarında AppleDOS isimli işletim sistemleri kullanılıyordu. Daha sonra Apple 1983'te 
    Lisa modelini piyasaya sürdü. 1983'ün sonlarında da ilk Macintosh bilgisayarını çıkarttı. Lisa ile birlikte Apple 
    grafik tabanlı işletim sistemlerine geçiş yaptı. Lisa ve sonraki Apple bilgisayarlarının hepsi grafik bir arayüze 
    sahiptir. Macintosh markası daha sonra Mac olarak telaffuz edilmeye başlandı. Lisa bilgisayarlarında kullanılan 
    işletim sistemi LisaOS ismindeydi. Apple daha sonra Macintosh bilgisayarlarının değişik versiyonlarını piyasaya sürdü. 
    Bunlardaki işletim sistemini "System Software 1 (1984), System Software 2 (1985), System Software 3 (1986), System 
    Software 4 (1987), System Software 5 (1987), System Software 6 (1988), ve System Software 7 (1991)" olarak isimlendirdi. 
    Apple System Software 7.5'ten sonra işletim sisteminin ismini "System Software" yerine Mac OS olarak değiştirdi ve 
    System Software 7.6 versiyonu Mac OS 7.6 ismiyle çıktı. Daha sonra Apple 1997 yılında Mac OS 8'i, 1999 yılında da 
    Mac OS 9'u çıkarmıştır.

    1980'li yıllarda Mac bilgisayarlarının fiyatı çok yüksekti ve satışları da iyi gitmiyordu. Çünkü Steve Jobs bilgisayarların 
    program yazmak için değil kullanmak için alınması gerektiğini düşünüyordu. Nihayet Apple'daki çalkantılar sonucunda 
    Steve Jobs 1985 yılında Apple'dan ayrılmak zorunda kaldı (kovuldu da denebilir) ve NeXT firmasını kurdu. NeXT firması 
    NeXT isimli bilgisayarları geliştirdi. Bu bilgisayarlarda NeXTSTEP isimli işletim sistemi kullanılıyordu. Daha sonra 
    bu sistem açık hale getirildi ve OPENSTEP ismini aldı. Dünyanın ilk Web tarayıcısı Tim Berners Lee tarafından Cern’de 
    NeXT bilgisayarları üzerinde gerçekleştirilmiştir.

    Steve Jobs 1997 yılında Apple’a geri döndü. Apple da NeXT firmasını 200 milyon dolara satın aldı. Sonra piyasaya 
    iMac ve Power Mac serileri çıktı. Daha sonra Steve Jobs Mac’lerin çekirdeklerini tamamen değiştirme kararı aldı. 
    Mac’ler Mac OS'un 10 versiyonu ile birlikte yeni bir çekirdeğe geçtiler. Mac OS işletim sistemlerinin 10'lu versiyonları 
    Roma rakamıyla Mac OS X biçiminde isimlendirilmiştir. Apple Mac OS X ismini 2012 yılında Mountain Lion (10.8) sürümü 
    ile OS X olarak, 2016 yılında da Sierra (10.12) sürümüyle birlikte de macOS olarak değiştirmiştir.

    DOS işletim sistemi text ekranda çalışıyordu. Microsoft da geleceğin grafik tabanlı işletim sistemlerinde olduğunu 
    gördü ve yavaş yavaş DOS'u bırakarak grafik tabanlı bir sisteme geçmeyi planladı. Bunun için Windows isimli grafik 
    arayüzün birinci versiyonunu 1985'te çıkardı. Bunu 1987'de Windows 2, 1990'da Windows 3.0 ve 1992'de de Windows 3.1 
    izledi. Bu 16 bit Windows sistemleri işletim sistemi değildi. DOS üzerinden çalıştırılan birer grafik arayüz gibiydi. 
    Microsoft daha sonra Windows'u Windows NT 3.1 ile bağımsız bir işletim sistemi haline getirdi. Microsoft bundan sonra 
    sırasıyla 1994 yılında Windows NT 3.5'i, 1995 yılında Windows NT 3.51'i ve Windows 95'i, 1998 yılında Windows 98'i, 
    2000 yılında Windows 2000 ve Windows ME'yi, 2001 yılında Windows XP'yi, 2006 yılında Windows Vista'yı, 2012 yılında 
    Windows 8'i, 2015 yılında Windows 10'u ve nihayet 2021 yılında da Windows 11'i çıkarmıştır.

    Linux işletim sistemi 1992 yılında bir dağıtım biçiminde piyasaya çıkmıştır. Linux işletim sisteminin hikayesi daha 
    geniş olarak izleyen paragraflarda ele alınmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										3. Ders 26/07/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de UNIX türevi işletim sistemlerinin tarihsel gelişimi üzerinde durmak istiyoruz. UNIX İşletim sistemi AT&T Bell 
    Laboratuvarlarında 1969-1971 yılları arasında geliştirildi. Proje ekibinin lideri Ken Thompson'du. Çalışma ekibinde 
    Dennis Ritchie, Brian Kernighan gibi önemli isimler de vardı. Ekip daha önce General Electric'in GE-645 mainframe 
    bilgisayarı için Multics işletim sistemi üzerinde çalışıyordu. (Multics işletim sisteminin geliştirilmesine 1964 yılında 
    başlandı. Projede General Electric, MIT ve Bell Lab birlikte çalışıyordu. Sonra proje Honeywell şirketi tarafından 
    devralınmıştır.)

    AT&T 1969 yılında bu projeden çekilerek kendi işletim sistemini geliştirmek istemiştir. Geliştirme çalışmasına DEC'in 
    PDP-7 makinelerinde başlanmıştır. UNIX ismi 1970 yılında Brian Kernighan tarafından Multics'ten kelime oyunu yapılarak 
    uydurulmuştur. Proje ekibi AT&T'yi DEC PDP-11 almaya ikna etti ve böylece geliştirme çalışmaları PDP-11 ile devam etti. 
    UNIX'in resmi olarak ilk sürümü Ekim 1971'de ikinci sürümü Aralık 1972'de, Üçüncü ve dördüncü sürümleri de 1973 yılında 
    yayınlanmıştır. UNIX işletim sistemi büyük ölçüde PDP'nin sembolik makine dili ve Ken Thompson'ın B isimli programlama 
    diliyle geliştirilmiştir. B programlama dili fonksiyonları alıp DEC'in makine diline dönüştürüyordu. Bu bakımdan B bir 
    yorumlayıcı değil derleyiciydi. İşte 1972 yılında Dennis Ritchie, Ken Thompson'ın B programlama dilinden hareketle C 
    Programlama dilini geliştirmiştir. UNIX işletim sisteminin dördüncü sürümü 1973 yılında yeniden C Programlama Dili ile 
    yazılmıştır. 1974 yılında UNIX'in beşinci sürümü oluşturuldu. Bu sürümlerin hepsi araştırma amaçlıydı ve "educational 
    license" ismiyle lisanslanmıştı. UNIX işletim sistemi bir araştırma projesi olarak organize edilmişti. Bu nedenle AT&T 
    kaynak kodlarını araştırma kuruluşlarına ücretsiz dağıtılmıştır. 1975 yılında UNIX'in altıncı sürümü şirketlere yönelik 
    hazırlandı. UNIX'in altıncı versiyonunun kaynak kodları 20,000 dolara (şimdikinin 120,000 doları) şirketlere sunuldu. 
    1977 yılında Bell Lab, UNIX'i Interdata 7/32 isimli 32 bit mimariye port etti. Bunu 1978'de VAX portu izledi.

    1974 yılında California Üniversitesi (Berkeley) işletim sisteminin kopyasını Bell Lab'tan aldı. 1978 yılında "Berkeley 
    Software Distribution (1BSD)" ismiyle AT&T dışındaki ilk UNIX dağıtımını gerçekleştirdi. Bu dağıtım hayatını hala FreeBSD, 
    OpenBSD ve NetBSD olarak devam ettirmektedir. 1979'da BSD'nin ikinci versiyonu (2BSD) ve 1979'un sonlarına doğru da üçüncü 
    versiyonu (3BSD) piyasaya sürüldü. Bunu 1980 yılında versiyon 4 (4BSD) izlemiştir. 1991 yılında BSD UNIX'ten AT&T kodları 
    tamamen arındırılmış ve kod bakımından özgün hale getirilmiştir. BSD'nin son versiyonu 1995'te 4.4BSD Lite Release 2 
    olarak çıkmıştır.

    1980'li yıllarda pek çok kurum ve ticari firma UNIX kodlarını lisans ücreti ödeyerek AT&T'den satın alıp kendilerine 
    yönelik UNIX sistemleri oluşturmuştur. Bunların önemli olanları şunlardır:

    AIX: IBM tarafından geliştirilmiş olan UNIX türevi sistemlerdir. İlk kez 1986 yılında piyasaya sürülmüştür. IBM AIX'i 
    System/370, RS/6000 PS2 bilgisayarlarında kullanıyordu. Bu sistemler AT&T UNIX System 5 kodları temel alınarak geliştirilmiştir. 
    AIX hala kullanılmaktadır. Son sürümü 2021 yılında 7.3 olarak piyasaya sürülmüştür. AIX PowerPC, x86 işlemcileri için 
    de port edilmiştir.

    IRIX: SGI firması tarafından AT&T ve BSD kodları değiştirilerek 1988'de oluşturulmuştur. 2006'da bırakılmıştır.

    HP-UX: HP 9000 bilgisayarları için 1982'de oluşturulmuştur. Motorola 68000 ve Itanium işlemcileri için yazılmıştır. 
    Hala devam ettirilmektedir.

    ULTRIX: DEC firmasının PDP-7, PDP-11 ve VAX donanımları için geliştirdiği UNIX sistemiydi. İlk versiyonu 1984 yılında 
    çıktı. 1995 yılında piyasadan çekildi.

    XENIX: Microsoft tarafından 1980 yılında geliştirilmeye başlanmıştır. İlk versiyonu 1980'in sonlarına doğru çıkmıştır. 
    Daha sonra SCO firması Microsoft'la bu konuda iş birliği yapmış 1987 yılında da Microsoft sistemi tamamen SCO'ya 
    devretmiştir. Bu sistemi daha sonra SCO firması, SCO-UNIX olarak devam ettirmiştir.

    SCO-UNIX: SCO firması XENIX'i Microsoft'tan alınca bunu SCO-UNIX olarak devam ettirdi. SCO-UNIX'in ilk versiyonu 1989 
    yılında çıktı. SCO sonra bunu OpenServer ismiyle devam ettirmiştir.

    FreeBSD, NETBSD ve OpenBSD: 4.3BSD sistemleri temel alınarak geliştirilmiştir. FreeBSD ve NetBSD 1993 yılında, OpenBSD 
    ise 1996 yılında piyasaya çıkmıştır. Sürdürülmeye devam etmektedir. Önemli bir UNIX varyantı durumundadır. Bu üç 
    sistem de birbirlerine çok benzemektedir. FreeBSD genel amaçlı client ve server işletim sistemi olma niyetindedir. 
    NetBSD daha taşınabilirdir ve geniş bir port'a sahiptir. Daha çok bilimsel çalışmalarda tercih edilmektedir. OpenBSD 
    güvenliğin önemli olduğu alanlarda tercih edilmektedir.

    SunOS (Solaris): Sun firmasının BSD kodlarıyla oluşturduğu UNIX türevi işletim sistemiydi. İlk versiyonu 1982 yılında 
    çıktı. SunOS işletim sistemi 5.2 versiyonundan sonra (1992) Solaris ismiyle pazarlanmaya başlamıştır. Solaris daha 
    sonra OpenSolaris biçiminde açık kaynak kodlu olarak bir süre varlığını devam ettirdi. Oracle firmasının Sun firmasını 
    2010'da satın almasından sonra bu proje de durduruldu. Bu proje Illumos ismiyle başka ekip tarafından devam ettirilmektedir.

    Linux: Linux Torvalds'ın öncülüğünde geliştirilmiş en yaygın UNIX türevi işletim sistemidir. İlk versiyonu 1991 
    yılında çıkmıştır. Hala devam ettirilmektedir. Linux'un tarihsel gelişimi izleyen bölümde ayrıntılı bir biçimde 
    açıklanmaktadır.

    Mac OS X, OS X, macOS: Carnegie Mellon üniversitesinin Mach isimli çekirdeği ile BSD UNIX sisteminin bir araya getirilmesiyle 
    oluşturulmuştur. İlk versiyonu 2001 yılında piyasaya sürülmüştür. İzleyen bölümlerde Mac OS işletim sistemlerinin 
    tarihsel gelişimi ayrıntılı olarak ele alınmaktadır.

    İzleyen paragrafta özel olarak Linux sistemlerinin tarihsel gelişimi üzerinde duracağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinin tarihsel gelişimini ele aldığımız önceki bölümde de belirttiğimiz gibi Apple firmasının Mac 
    bilgisayarları Mac OS'un 10 versiyonu ile birlikte yeni bir çekirdeğe geçtiler. Mac OS işletim sistemlerinin 10'lu 
    versiyonları Roma rakamıyla Mac OS X biçiminde isimlendirildi. Apple Mac OS X ismini 2012 yılında Mountain Lion (10.8) 
    sürümü ile OS X olarak, 2016 yılında da Sierra (10.12) sürümüyle birlikte de macOS olarak değiştirdi. Biz Mac OS X, 
    OS X ve macOS sistemlerine bu bölümde "Mac OS X türevi sistemler" de diyeceğiz.

    Mac OS X türevi işletim sistemleri UNIX türevi sistemlerdir. Bu işletim sistemlerinin çekirdeğine Darwin denilmektedir. 
    Darwin açık kaynak kodlu bir işletim sistemdir. Ancak Mac OS X türevi sistemler tam anlamıyla açık sistemler değildir. 
    Bu sistemlerin çekirdeği açık olsa da geri kalan kısımları mülkiyete sahip (proprietary) biçimdedir.

    Darwin'in hikayesi 1989 yılında NeXT'in NeXTSTEP işletim sistemiyle başladı. NeXTSTEP daha sonra OpenStep ismiyle API 
    düzeyinde standart hale getirildi. 1996'nın sonunda 1997'nin başında Steve Jobs Apple'a dönerken Apple da NeXT firmasını 
    satın aldı ve sonraki işletim sistemini OpenStep üzerine kuracağını açıkladı. Bundan sonra Apple 1997’de OpenStep üzerine 
    kurulu olan Rapsody'yi çıkardı. 1998'de de yeni işletim sisteminin Mac OS X olacağını açıkladı. Daha sonra 2000 yılında 
    Apple Rapsody'den Darwin projesini türetti. Darwin her ne kadar Mac sistemlerinin çekirdeği olarak tasarlanmışsa da ayrı 
    bir işletim sistemi olarak da yüklenebilmektedir. Ancak Darwin grafik arayüzü olmadığı için Mac programlarını çalıştıramamaktadır. 
    Daha sonra Darwin'i bağımsız bir işletimn sistemi haline getirmek amacıyla Darwin'den de çeşitli projeler türetilmiştir. 
    Bunlardan biri Apple tarafından 2002'de başlatılan OpenDarwin'dir. Bu proje 2006'da sonlandırılmıştır. 2007'de PureDarwin 
    projesi başlatılmıştır.

    Darwin'in çekirdeği XNU üzerine oturtulmuştur. XNU NeXT firması tarafından NEXTSTEP işletim sisteminde kullanılmak üzere 
    geliştirilmiş bir çekirdektir. XNU, Carnegie Mellon ("Karnegi" diye okunuyor) üniversitesi'nin Mach 3 mikrokernel çekirdeği 
    ile 4.3BSD karışımı hibrit bir sistemdir.

    Mac OS X türevi sistemlerin versiyonları şunlardır:

    - Mac OS X 10.0 (Cheetah, 2001)
    - Mac OS X 10.1 (Puma, 2001)
    - Mac OS X 10.2 (Jaguar, 2002)
    - Mac OS X 10.3 (Panther, 2003)
    - Mac OS X 10.4 (Tiger, 2005)
    - Mac OS X 10.5 (Leopard, 2007)
    - Mac OS X 10.6 (Snow Leopard, 2009)
    - Mac OS X 10.7 (Lion, 2011)
    - OS X 10.8 (Mountain Lion, 2012)
    - OS X 10.9 (Maverics, 2013)
    - OS X 10.10 (Yosemite, 2014)
    - OS X 10.11 (El Capitan, 2015)
    - macOS 10.12 (Sierra, 2017)
    - macOS 10.13 (High Sierra, 2017)
    - macOS 10.14 (Mojave, 2018)
    - macOS 10.15 (Catalina, 2019)
    - macOS 11 (Big Sur, 2020)
    - macOS 12 (Monterey, 2021)
    - macOS 13 (Ventura, 2022)
    - macOS 14	(Sonoma, 2023)
    - macOS 15	(Sequoia, 2024)

    MacOS büyük ölçüde POSIX uyumlu bir sistemdir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de UNIX/Linux dünyasında önemli bir yeri olan "GNU Projesi", "özgür yazılım" ve "açık kaynak kod akımları" üzerinde 
    durmak istiyoruz.

    1970'lerdeki mikro bilgisayarlar devrimine kadar yazılımda bir telif anlayışı yoktu. Yani yazılımın dağıtılması konusunda 
    sözleşmeler ve hukuki yaptırımlara gerek duyulmamıştı. Yazılım zaten donanımla birlikte satılıyordu ya da kuruma özel 
    yapılıyordu. 1969 yılında IBM yazılımı donanımla birlikte verdiği için rekabet kurallarına uymadığı gerekçesiyle mahkemeye 
    verilmiştir ve cezaya çarptırılmıştır. 1970'li yıllarda yazılım maliyetleri artmış, yazılım sektörü genişlemiş ve lisanslama 
    politikaları da uygulamaya sokulmuştur. Pek çok yazılım bu yıllarda özel lisanslarla piyasaya sürülmeye başlanmıştır. 
    1980'li yıllarda bu lisanslama faaliyetleri hız kazanmıştır.

    1980'li yıllarda tüm UNIX türevi sistemler çeşitli biçimlerde sınırlandırıcı lisanslara sahipti. Yani 1980'li yıllarda 
    sınırlaması olmayan UNIX türevi sistemler kalmamıştı. Bu nedenle bedava ve sınırlamasız UNIX türevi bir işletim sistemine 
    gereksinim duyulmaya başlandı. İşte durumdan vazife çıkaran ünlü Emacs editörünün yazarı Richard Stallman 1983 yılının 
    sonlarına doğru GNU projesini başlattı ve özgür yazılım (free software) fikrini oraya attı. GNU projesinin amacı açık 
    kaynak kodlu UNIX benzeri bir işletim sistemini ve geliştirme araçlarını yazmaktı. Proje fiilen 1984 yılında başlamıştır.

    Stallman 1985 yılında özgür yazılım kavramını yaygınlaştırmak amacıyla Free Software Foundation (www.fsf.org) isimli 
    kurumu kurdu ve atık GNU projesi bu kurum tarafından yürütülmeye başlandı. FSF özgür yazılım modeli için GPL (GNU Public 
    License) denilen lisansı ortaya çıkardı. Özgür yazılım akımında oluşturulan bir yazılım istenildiği gibi çalıştırılabilir, 
    kopyalanabilir, incelenebilir, dağıtılabilir, değiştirilebilir ve iyileştirilebilir. Daha açık bir biçimde özgür yazılım 
    tipik olarak aşağıdaki dört özgürlükle tanımlanmıştır:

    Özgürlük 0: Programı her türlü amaç için çalıştırma özgürlüğü
    Özgürlük 1: Programın kaynak kodunu inceleme ve değiştirebilme özgürlüğü
    Özgürlük 2: Programın kopyalarını çıkartabilme ve yeniden dağıtabilme özgürlüğü
    Özgürlük 3: Programı iyileştirebilme ve iyileştirilmiş programı yayınlama özgürlüğü

    GNU projesi bağlamında pek çok temel araç (gcc derleyici, ld bağlayıcı vs.) geliştirilmiştir. Fakat hedeflenen bir 
    çekirdek bir türlü oluşturulamamıştır.

    Aslında özgür yazılım (free software) ile açık kaynak kod (open source) akımları arasında bazı farklar olmakla birlikte 
    her iki akımın da hedefleri benzerdir. Özgür yazılım bir sosyal harekete benzetilirken açık kaynak kod akımı bir geliştirme 
    metodolojisine benzetilmektedir. Biz kursumuzda tüm bu akımları "açık kaynak kod (open source)" olarak nitelendireceğiz. 
    Özgür yazılım akımının temel lisansı GPL'dir (GNU Public Licence). Bunun yumuşatılmış LGPL (Lesser GPL) biçiminde bir 
    versiyonu da oluşturulmuştur. Ayrıca Apache, MIT, BSD gibi açık kaynak kodlu başka lisanslar da vardır. Şüphesiz bu 
    lisansların aralarında birtakım farklılıklar bulunmakla birlikte pek çok yönleri de ortaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux Torvalds Helsinki Üniversitesinde öğrenciyken bir işletim sistemi yazmaya niyetlenmiştir. O zamanlarda telif 
    uygulanmayan UNIX türevi bir işletim sistemi kalmamıştı ve GNU projesinin işletim sistemi de (GNU Hurd) bitirilememişti. 
    MINIX isimli bir işletim sistemi Andrew Tanenbaum tarafından yazılmıştı ancak bu sisteminin lisansı yalnızca akademik 
    kullanımlara izin verecek biçimde sınırlandırılmıştı. Linus Torvalds projesini USENET haber gruplarında duyurdu ve zamanla 
    kendisine gönüllü yardım edecek sistem programcıları buldu. Yazılım dünyasında bu tür girişimlerle sık karşılaşıldığı 
    halde başarı olasılığı nispeten düşük olmaktadır. Linus Torvalds'ın bu girişimi başarıya ulaşmıştır.

    1991 yılında Linux'un 0.01 sürümü oluşturuldu. 1994 yılında stabil bir biçimde Linux 1.0 versiyonu dağıtılmaya başlandı. 
    Bunu 1996 yılında Linux 2.0, 1999 yılında 2.2, 2000 yılında 2.4 ve 2003 yılında 2.6 izledi. Daha sonra Linux versiyon 
    numaralandırma sistemi değiştirilmiştir. 2011 yılında 3.0, 2015 yılında 4.0, 2019 yılında 5.0, 2022 yılında da 6.0 
    versiyonu çıkmıştır. Kursun yapıldığı zamandaki son çekirdek sürümü Ekim 2022'de çıkmış olan 6.0'dır.

    Linux çekirdeklerinin versiyonları ve bu versiyonlarda eklenen önemli özellikleri şöyle listeleyebiliriz:

    Sürüm	Tarih	        Önemli Yenilikler

    0.01	Ağustos 1991	Linus Torvalds tarafından duyurulan ilk sürüm; sadece temel fonksiyonlara sahipti.
    1.0	    Mart 1994	    İlk "resmi" sürüm; çoklu işlemci desteği yoktu. Ağ üzerinden TCP/IP desteği sağlandı.
    1.2	    Mart 1995	    x86 dışı mimarilere (Alpha, MIPS) ilk destekler geldi.
    2.0	    Haziran 1996	SMP (Simetrik Çoklu İşlemci) desteği eklendi. Daha fazla mimari desteği sunuldu.
    2.2	    Ocak 1999	    Gelişmiş ağ yığını, IPv6 desteği, daha fazla SMP ölçeklenebilirliği.
    2.4	    Ocak 2001	    USB, PCMCIA ve Bluetooth desteği, 64 GB RAM'e kadar bellek desteği (PAE ile).
    2.6	    Aralık 2003	    Yeni scheduler (O(1)), udev ile dinamik aygıt yönetimi, sysfs, Native POSIX thread kütüphanesi 
                            (NPTL), preemptive kernel
    3.0	    Temmuz 2011	    Büyük bir teknik değişiklik yok; sadece sürüm numarası sadeleştirildi (2.6.x'lerin devamı).
    4.0	    Nisan 2015	    Canlı kernel güncelleme (live patching) özelliği eklendi.
    5.0	    Mart 2019	    Yeni donanım desteği, enerji verimliliği iyileştirmeleri, Adiantum şifreleme algoritması.
    5.4     Kasım 2019	    Lockdown modu, fs-verity desteği.
    5.10    Aralık 2020	    EXT4 ve Btrfs iyileştirmeleri, AMD GPU desteği.
    5.15    Kasım 2021	    NTFS3 dosya sistemi, yeni I/O kontrolcüsü.
    6.0	    Ekim 2022	    Rust diline ilk çekirdek içi destek, Scheduler ve TCP performans iyileştirmeleri.
    6.1     Aralık 2022	    Rust desteği genişletildi, Intel AMX desteği.
    6.6     Kasım 2023	    Apple Silicon (M1/M2) desteği, yeni enerji yönetimi özellikleri.

    Linux monolithic bir çekirdek yapısına sahiptir. Büyük ölçüde POSIX uyumu bulunmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Açık kaynak kodlu yazılımlar bir araya getirilip paketlenerek istenildiği gibi dağıtılabilmektedir. Dağıtım (distribution) 
    bu anlamda kullanılan genel bir terimdir ve her türlü açık kaynak kodlu yazılım için dağıtım söz konusu olabilir. Ancak 
    biz burada Linux dağıtımları üzerinde duracağız.

    Linux temel olarak bir çekirdek geliştirme projesidir. Linux kaynak kodlarına baktığınızda tüm kodların çekirdekle 
    ilgili olduğunu görürsünüz. Çekirdeğin dışındaki tüm yazılımlar (örneğin init prosesinden başlayarak, kabuk yazılımları, 
    paket yöneticileri, pencere yöneticileri vs.) hep başka proje grupları tarafından gerçekleştirilmiş açık kaynak kodlu 
    yazılımlardır. İşte tüm bu açık kaynak kodlu yazılımların Linux çekirdeği temelinde bir araya getirilmesi ve doğrudan 
    kullanıcının install edip çalıştırabileceği biçimde paketlenmesine Linux dağıtımları denilmektedir. Linux dağıtımları 
    pencere yöneticileri (KDE, GNOME gibi), paket yöneticileri (APT, RPM, YUM, DPKG, PACMAN, ZYPPER gibi) ve diğer yararlı 
    uygulama programları bakımından farklılıklar gösterebilmektedir.

    Toplamda iki yüzün üzerinde Linux dağıtımının olduğu söylenebilir. Ancak bunlar arasında az sayıda dağıtım çok popüler 
    olmuştur. Bazı dağıtımlar bazı dağıtımlardan fork edilerek oluşturulmuştur. Aşağıda en çok kullanılan dağıtımlara 
    ilişkin dağıtım ağacını veriyoruz:

    Linux
    ├── Debian
    │   ├── Ubuntu
    │   │   ├── Linux Mint
    │   │   ├── Pop!_OS
    │   │   ├── elementary OS
    │   │   └── Zorin OS
    │   ├── Devuan       # Systemd olmayan Debian
    │   └── Kali Linux   # Güvenlik test amaçlı
    ├── Red Hat Linux (eski)
    │   ├── Fedora       # Topluluk temelli, RHEL'in test yatağı
    │   │   └── RHEL (Red Hat Enterprise Linux)
    │   │       ├── CentOS (→ 2021 sonrası CentOS Stream)
    │   │       ├── AlmaLinux
    │   │       └── Rocky Linux
    ├── Slackware
    │   └── Slax         # Hafif sürüm
    ├── Arch Linux
    │   ├── Manjaro
    │   └── EndeavourOS
    ├── Gentoo
    │   └── Calculate Linux
    ├── SUSE Linux
    │   ├── openSUSE Leap
    │   └── openSUSE Tumbleweed
    ├── Android          # Mobil, Linux çekirdeğine dayalı
    ├── Alpine Linux     # Minimal, güvenli, konteyner dostu
    └── Chrome OS
        └── Chromium OS  # Açık kaynak tabanı

    Burada en çok kullanılan Linux dağıtımlarından bahsedeceğiz.

    Debian Dağıtımı: En önemli ve en eski Linux dağıtımlarından biridir. Knoppix, Mint, Ubuntu dağıtımları Debian türevi 
    dağıtımlardır.

    Fedora: Red Hat firması tarafından çıkarılmış olan dağıtımdır. İlk kez 2003 yılında oluşturulmuştur. RPM paket 
    yöneticisini kullanır. CentOS ve Scientific Linux en önemli Fedora türevi dağıtımlardır. 2000 yılında ilk sürümü 
    yapılan Red Hat Enterprise Linux (RHEL) en önemli Fedora türevidir. Ondan da CentOS, Scientific Linux gibi dağıtımlar 
    türetilmiştir. CentOS server makinelerde en yaygın kullanılan Linux versiyonudur.

    OpenSUSE: Alman SUSE firmasının desteklediği dağıtımdır. SUSE Linux Enterprise isminde ticari bir versiyonu da vardır. 
    ZYpp, YaST ve RPM paket yöneticilerini kullanmaktadır.

    Slackware: En eski Linux dağıtımıdır. 1993 yılında oluşturulmuştur. Sürdürümü yavaş olmakla birlikte hala devam 
    etmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    1980'li yıllarda AT&T ya da BSD kodlarından türetilmiş olan ve çoğunluğu şirketlere ait olan pek çok UNIX türevi 
    işletim sistemi oluşturuldu. Bu işletim sistemleri birbirlerine çok benzemekle birlikte aralarında bazı farklılıklara 
    da sahipti. İşte IEEE durumdan vazife çıkartarak bu UNIX türevi sistemleri standardize etmek için kolları sıvadı ve 
    bunun sonucu olarak da POSIX standartlarını oluşturdu.

    POSIX sözcüğü Richard Stallman tarafından önerilmiştir. "Portable Operating System Interface for UNIX" sözcüklerinden 
    kısaltılarak uydurulmuştur ve "poziks" biçiminde okunmaktadır. POSIX standartları üzerinde çalışmalar 1985 yılında 
    başlamıştır ve ilk standartlar 1988 yılında "IEEE Std 1003.1-1988" kod numarasıyla oluşturulmuştur. POSIX her ne kadar 
    UNIX türevi sistemler için düşünülmüşse de UNIX türevi mimariye sahip olamayan sistemler için de kullanılabilecek bir 
    standarttır. (Örneğin Windows sistemleri Interix denilen alt sistemle POSIX uyumlu olarak da kullanılabilmekteydi. 
    Interix alt sistemi daha sonra Windows 8 ile birlikte Windows'tan kaldırılmıştır.)

    POSIX standartları 4 bölümden oluşmaktadır:

    1) Base Definitions: Bu bölümde temel tanımlamalar bulunmaktadır.
    2) Shell & Utilities: Bu bölümde kabuk komutları ve standart utility programlar ele alınmaktadır.
    3) System Interfaces: Bu bölümde C programcıları için hazır bulunan POSIX fonksiyonları açıklanmaktadır.
    4) Rationale: Çeşitli kuralların ve özelliklerin gerekçeleri bu bölümde açıklanmaktadır.

    POSIX standartlarının zaman içerisinde çeşitli versiyonları çıkartılmıştır. Bu versiyonlarda hem yeni POSIX fonksiyonları 
    kütüphaneye eklenmiş hem de standartlardaki bazı bozukluklar ve uyumsuzluklar düzeltilmiştir. Standardın önemli 
    versiyonları şu senelerde yayınlanmıştır: 1992, 1993, 1995, 1997, 2001, 2004, 2008, 2017. Standartlardaki en önemli 
    değişim 1993 yılında "POSIX 1.b" diye de isimlendirilen "Realtime-extensions" ile 1995 yılında "POSIX 1.c" diye 
    isimlendirilen "Thread-extensions" isimli eklemelerdir. Bu eklemelerle POSIX'e gerçek zamanlı işlemler için çeşitli 
    özelliklerle thread kullanımı eklenmiştir.

    "Single UNIX Specification" UNIX türevi sistemler için oluşturulmuş diğer önemli standarttır. Bir sistemin UNIX olarak 
    değerlendirilebilmesi için bu standartlara uygun olması gerekmektedir. Standartlar "Austin Group" isimli toplulukla 
    "Open Group" isimli dernek tarafından geliştirilmiştir. Sürdürümü Open Group tarafından yapılmaktadır. Open Group 
    hali hazırda UNIX sistemlerinin isim haklarını elinde bulundurmaktadır. Single UNIX Specification isimli standardın 
    zamanla pek çok versiyonu oluşturulmuştur.

    POSIX standartları ile "Single UNIX Specification" standartları arasında eskiden daha fazla farklılıklar vardı. Ancak 
    bugün itibari ile bu iki standart birbirlerine yaklaştırılmış ve son versiyonlarla aynı hale getirilmiştir. Single 
    UNIX Specification dokümanlarına Internet'ten Open Group'un web sitesinden erişilebilir:

    https://pubs.opengroup.org/onlinepubs/9799919799/
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzda önce Linux çekirdeğinin kaynak kodlarının genel dizi yapısı üzerinde duracağız.

    Linux çekirdeğinin kaynak kod ağacı üzerinde temel dizinler aynı kalmak üzere zaman içerisinde çeşitli değişikler 
    yapılmıştır. Biz burada çekirdeğin son versiyonlarını dikkate alarak açıklamalar yapacağız. Linux kaynak kod ağacındaki 
    temel dizinler şunlardır:

    linux-6.x/
    ├── arch/          → Mimarilere özel kodlar (x86, ARM, RISC-V, vs.)
    ├── block/         → Blok aygıt altyapısı
    ├── certs/         → Kernel modül imzalama için sertifikalar
    ├── crypto/        → Kriptografik algoritmalar ve API’ler
    ├── Documentation/ → Belgeler (API, özellikler, davranışlar)
    ├── drivers/       → Aygıt sürücüleri (net, usb, gpu, vs.)
    ├── fs/            → Dosya sistemi sürücüleri (ext4, btrfs, vs.)
    ├── include/       → Global başlık dosyaları (headers)
    ├── io_uring/      → 5.1 çekirdeği ile eklenen asenkron io_uring sistem fonksiyonları için çekirdek kodları
    ├── init/          → Kernelin ilk başlatma kodları
    ├── ipc/           → IPC mekanizmaları (semaphore, message queue, vs.)
    ├── kernel/        → Temel kernel işlevleri (zamanlayıcı, process, vs.)
    ├── lib/           → Genel amaçlı yardımcı fonksiyonlar
    ├── mm/            → Bellek yönetimi
    ├── net/           → Ağ yığını (TCP/IP, socket, protokoller)
    ├── samples/       → Örnek kodlar (örnek eBPF, modül kodları, vs.)
    ├── scripts/       → Derleme sürecine yardımcı betikler
    ├── security/      → Güvenlik altyapısı (LSM, SELinux, AppArmor, vs.)
    ├── sound/         → Ses sürücüleri (ALSA, etc.)
    ├── tools/         → Kullanıcı uzayı araçları (perf, bpftool, vs.)
    ├── usr/           → Yerleşik initramfs oluşturmak için
    ├── virt/          → Sanallaştırma (KVM, Xen, vs.)
    ├── MAINTAINERS    → Dosyaların kim tarafından korunduğu bilgisi
    ├── Makefile       → Ana derleme talimatı
    └── Kconfig        → Kernel yapılandırma seçenekleri

    arch => arch sözcüğü "architecture" sözcüğünden kısaltılmıştır. Bu dizinde işlemci mimarisine göre değişebilen 
    çekirdek kodları her bir işlemci ailesi ayrı bir dizinde olacak biçimde bulundurulmaktadır.

    block => Eskiden bu dizin "drivers" dizini içerisindeydi. Burada işletim sisteminin blok düzeyinde işlemler yapan 
    kodları bulundurulmuştur.

    certs => Bu dizinde çekirdek modüllerine ilişkin sertifikasyonlarla ilgili kodlar bulunmaktadır.

    crypto => Çekirdeğin içerisinde kullanılan şifreleme işlemlerine yönelik kaynak kodlar bulunmaktadır.

    Documentation => Burada çeşitli alt sistemlere ilişkin dokümanlar bulunmaktadır.

    drivers => Burada aygıt sürücülere ilişkin çekirdek kodları bulunmaktadır. Tabii bu aygıt sürücüler isteğe göre 
    seçilerek yalnızca bir grubu derlenmektedir.

    fs => Bu dizinde dosya sistemlerine yönelik çekirdek kodları bulundurulmaktadır.

    include => Çekirdek kodlarında include edilen tüm başlık dosyaları bu dizinin içerisinde bulundurulmaktadır. Tabii 
    bu dizinde de alt dizinler vardır.

    init => Çekirdeğin çalışabilir hale gelmesi için yapılan ilk işlemlere ilişkin kodlar burada bulundurulmaktadır.

    iu_uring => 5.1 çekirdekleri ile eklenen yüksek performanslı asenkron IO işlemleri için kullanılan sistem fonksiyonlarının 
    gerçekleştirimine ilişkin kodlar bu dizinde bulunmaktadır.

    ipc => Borular, mesaj kuyrukları gibi IPC nesnelerine ilişkin kaynak kodlar bu dizin içerisindedir.

    kernel => Bu dizin çekirdeğin en temel işlevlerine ilişkin kaynak kodları barındırmaktadır. (Çekirdek kodlarının 
    içerisinde ayrıca bir "kernel" dizinin bulunması biraz tuhaf olsa da bu aslında adeta "çekirdeğin çekirdeği" gibi 
    bir anlama gelmektedir.)

    lib => Bu dizinde çekirdeğin değişik yerlerinde kullanılan genel amaçlı utility fonksiyonların kodları bulundurulmaktadır. 
    Burada "lib" sözcüğünün statik ve dinamik kütüphane ile bir ilgisi yoktur. Zaten Linux derlenirken bu biçimde kütüphane 
    dosyaları oluşturulmamaktadır.

    mm => Çekirdeğin tüm ana bellek yönetim kodları bu dizinde bulunmaktadır.

    net => Çekirdeğin tüm "ağ (network)" alt sistemine ilişkin kodları bu dizinde bulunmaktadır.

    rust => Linux çekirdeğinde 6'lı versiyonlarla Rust Programlama Dili de kullanılmaya başlanmıştır. Bu dizinde çekirdeğe 
    ilişkin Rust kodları bulunmaktadır. Kursun yapıldığı tarihteki Linux çekirdeklerinde Rust kodları çok az yer kaplamaktadır.

    samples => Burada alt sistemlere ilişkin örnek kodlar bulunmaktadır.

    scripts => Bu dizinde çekirdek derlemesi sırasında faydalanılan script dosyaları bulundurulmuştur.

    security => Çekirdeğin sistem güvenliği ile ilgili kodları bu dizinde bulunmaktadır.

    sound => Ses işlemlerine yönelik çekirdek kodları bu dizinde bulunmaktadır.

    tools => Bu dizinde çekirdek geliştirmesinde ve analizinde kullanılan yardımcı programlar bulundurulmaktadır. Aslında
    bu dizindeki programların çekirdekle bir ilgisi yoktur. Bunlar yardımcı araçlardır. Çekirdek kodlarının içerisinde yer
    almazlar.

    usr => Bu dizinde "geçici kök dosya sistemine (initial ramdisk)" ilişkin kodlar bulunmaktadır. Bunların bazı sistemlerde 
    konfigürasyon yoluyla çekirdek kodlarına aktarılmaktadır.

    virt => Burada çekirdeğin sanallaştırmaya hizmet eden kodları bulundurulmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										4. Ders 27/07/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu bölümünde Linux sistemlerinin boot edilmesi süreci ele alınacaktır. İşletim sisteminin otomatik olarak 
    yüklenerek çalışır hale getirilmesi sürecine "boot" işlemi denilmektedir. (Boot terimi İngilizce "askeri bottan 
    (çizmeden)" gelmektedir.) Biz bilgisayar sistemine güç verdiğimizde bir süre sonra işletim sisteminin otomatik 
    yüklendiğini görmekteyiz. Aslında işletim sisteminin yüklenmesi biraz karmaşık bir süreçle gerçekleşmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Mikroişlemciler güç uygulandığında (reset edildiğinde) belli bir adresten itibaren çalışacak biçimde tasarlanmıştır. 
    Buna işlemcilerin "reset vektörü" denilmektedir. İşlemcilerin reset vektörleri genellikle fiziksel belleğin başında 
    ya da sonunda bulunmaktadır. Örneğin Intel işlemcilerinde reset vektörü belleğin sonunda, ARM işlemcilerinde ise 
    belleğin başında bulunmaktadır. İşlemci reset edildiğinde belli bir adresten çalışmaya başladığına göre orada çalışmaya 
    hazır bir kodun bulunuyor olması gerekir. Tabii bu kod RAM (DRAM) bellekte bulunamaz. Çünkü sisteme güç uygulandığında 
    RAM belleğin de içeriği sıfırlanmaktadır, ayrıca bugünkü DRAM belleklerin kullanılabilmesi için de onların donanımsal 
    olarak programlanması gerekmektedir. İşte bilgisayar sistemlerinde işlemcinin reset vektöründe tipik olarak ROM bellekler 
    bulundurulur. Eskiden bu amaçla EPROM bellekler kullanılıyordu. Artık uzunca bir süredir EEPROM (flash EPROM) bellekler 
    kullanılmaktadır. Pekiyi reset vektöründeki kod ne yapmaktadır?

    Bugün kullandığımız DRAM bellekler güç kaynağı verilir verilmez kullanıma hazır hale getirilememektedir. Onların da 
    kullanıma hazır hale getirilmesi (initialize edilmesi) gerekir. Benzer biçimde yine bazı donanım aygıtlarının kullanılmadan 
    önce kullanıma hazır hale getirilmesi (initialize edilmesi) gerekmektedir. İşte reset vektörüne yerleştirilmiş olan kodlar 
    bilgisayar sistemini kullanıma hazır hale getirecek kodları barındırmaktadır. Tabii burada donanımdan donanıma değişen 
    bazı ayrıntılar da söz konusu olabilmektedir. Reset vektöründeki kodlar genellikle donanımı üreten firmalar tarafından 
    yazılmaktadır. Bunlar çok temel kodlar olduğu için bunlara "BIOS" ya da "firmware" gibi isimler verilmiştir. Bu tür temel 
    BIOS ya da firmware kodlarının donanımı kullanıma hazır hale getirmek için yaptığı işlemlerden bazıları şunlardır:

    - DRAM bellekleri (bildiğimiz RAM) kullanıma hazır getirme
    - Çok çekirdekli sistemlerde çekirdeklerin kullanıma hazır hale getirilmesi
    - Donanım aygıtlarının ve çevre birimlerinin tespit edilmesi ve bunların tablolara (örneğin ACPI tablosu) yerleştirilmesi
    - Çevre birimlerinin (yani yardımcı işlemcilerin) kullanıma hazır hale getirilmesi
    - SSD ve HDD gibi depolama birimlerinin kullanıma hazır hale getirilmesi
    - Klavye, fare gibi giriş çıkış aygıtlarının kullanıma hazır hale getirilmesi
    - Ekran kartı gibi görüntü aygıtlarının kullanıma hazır hale getirilmesi

    Reset vektöründeki kodlar çalıştırılınca artık donanım genel olarak çalışmaya hazır bir duruma getirilmiştir. Bundan 
    sonra akışın bir biçimde sistem programcısına devredilmesi gerekir. Akışın devredilmesi tipik olarak ikincil bellekteki 
    belli bir disk bloğunun RAM'e yüklenmesi ve oradaki kodun çalıştırılması yoluyla yapılmaktadır. Tersten gidersek sistem 
    programcısı programını ikincil belleğin (diskin) bir bloğuna yerleştirir ve bu süreçler sonucunda bu program çalışır 
    hale gelir. Pekiyi işletim sistemi nasıl yüklenmektedir? ROM'daki BIOS ya da firmware kodunun işletim sistemini yüklemesi 
    genel olarak mümkün değildir. Bunun temel nedenleri şunlardır:

    - İşletim sistemleri çok büyük olabilir. ROM'daki kod bunu yapabilecek yeterlilikte olmayabilir.
    - İşletim sistemlerinin yüklenmesi sistemden sisteme değişebilen ve nispeten karmaşık bir süreçtir. ROM'daki küçük kodun 
    bunu yapması genellikle mümkün olamamaktadır.
    - İkincil bellekte birden fazla işletim sistemi bulunuyor olabilir. Bu durumda bunların hangisinin nasıl yükleneceğine 
    karar verilmesi ve karar verilen işletim sisteminin yüklenmesi ROM'daki küçük program tarafından genellikle yapılamaz.

    O halde tek çıkar yol ROM'daki programın diskteki bir önyükleyiciyi yüklemesi ve işletim sisteminin yüklenmesinin bu 
    program tarafından yapılmasıdır. İşletim sistemlerini yükleyen bu tür programlara "önyükleyici (bootloader)" denilmektedir. 
    (Biz "bootloader" terimi yerine bunun Türkçesi olan "önyükleyici" terimini de kullanacağız.) Yani yukarıda da belirttiğimiz 
    gibi sistem programcısı diskteki önceden tespit edilmiş alana kendi kodunu yerleştirir. (Genellikle BIOS ya da firmware 
    kodları küçük bir disk bloğunu yükleyip çalıştırmaktadır.) İşte sistem programcısının özel disk bloğuna yerleştirdiği 
    bu program da önyükleyici (bootloader) programını yüklemektedir. Bugün değişik platformlarda kullanılan değişik "önyükleyici" 
    programlar bulunmaktadır. Örneğin Microsoft kendi Windows sistemleri için kendi "önyükleyici" programını kullanmaktadır. 
    Buna "Windows Boot Manager (bootmgr)" denilmektedir. UNIX/Linux sistemlerinde değişik proje grupları tarafından yazılmış 
    olan değişik önyükleyici programlar kullanılabilmektedir. Örneğin Linux sistemlerinde eskiden "LILO" isimli önyükleyici 
    yoğun biçimde kullanılıyordu. Daha sonra "GRUB" isimli önyükleyici yaygın biçimde kullanılmaya başlandı. Son yıllarda 
    SYSLINUX isimli bootloader paketi de belli bir ölçüde kullanım bulmuştur. Bu önyükleyici minimalist bir tasarıma sahiptir 
    ve daha çok küçük sistemlerde tercih edilmektedir. Gömülü Linux sistemlerinde bugün en çok tercih edilen ise "Das U-Boot" 
    ya da kısaca "U-Boot (Universal Bootloader)" denilen önyükleyici programdır.

    O halde işletim sisteminin yüklenmesi pek çok donanım ve platformda aşağıdaki aşamalardan geçilerek yapılmaktadır:

    Mikroişlemci RESET ediliyor ---> ROM'daki RESET vektöründe bulunan kodlar çalışıyor ---> ROM'daki kodlar diskteki 
    önceden belirlenmiş olan bloğu RAM'e yükleyerek çalıştırıyor ---> RAM'e yüklenen bu küçük program önyükleyici 
    programı RAM'e yüklüyor ---> Önyükleyici program işletim sistemini yükleyerek işletim sisteminin başlangıç kodlarını 
    çalıştırıyor.

    Burada birkaç soru aklınıza gelebilir. Bunlardan biri ROM'daki RESET vektöründe bulunan kodların oraya kimin tarafından 
    yerleştirilmiş olduğudur. ROM'daki RESET vektöründe bulunan kodlar çok aşağı seviyeli kodlar olduğu için bunlar genellikle 
    donanımı tasarlayan kurumlar tarafından yazılıp oraya yerleştirilmektedir. Örneğin bugün kullandığımız PC sistemlerinde 
    ROM'daki bu kodlara BIOS (Basic Input Output System) denilmektedir. BIOS kodları PC donanımını tasarlayan IBM tarafından 
    yazılmıştı. Ancak bugün BIOS üretici firmalar tarafından yazılmaktadır. Bu ROM bellekler bugün EEPROM teknolojisi ile 
    üretildikleri için BIOS güncellemesi de yapılabilmektedir. Örneğin Beaglebone Black kartlarındaki ROM programı Texas 
    Instruments (TI) firması tarafından yazılmıştır. Raspberry PI kartlarındaki ROM programı ise Broadcom firması tarafından 
    yazılmış durumdadır. Özetle ROM'da bulunan bu aşağı seviyeli kodlar ilgili donanımı tasarlayan kurumlardaki sistem 
    programcıları tarafından yazılmıştır.

    Pekiyi ROM'daki program önyükleyici (bootloader) programını ikincil bellekte nasıl arayıp bulmaktadır? Bunun için 
    birkaç teknik kullanılabilmektedir. Birincisi ROM'daki programın doğrudan ikincil belleğin önceden belirlenmiş bloklarını 
    yüklemesidir. Örneğin klasik PC sistemlerinde ROM'daki (BIOS'taki) program diskin 0'ıncı bloğunu (yani ilk 512 byte'ı 
    içeren bloğu) yüklemektedir. (Ancak daha sonra PC sistemlerinde "UEFI BIOS" ismi altında eski klasik BIOS kodları 
    oldukça geliştirilmiştir. Artık bu UEFI BIOS kodları bazı dosya sistemlerini de tanımaktadır.) Örneğin Raspberry 
    Pi'da ROM'daki kodları FAT dosya sistemini tanıyabilmektedir. FAT dosya sistemi Microsoft'un DOS sistemlerinde 
    kullandığı karmaşık olmayan sade bir dosya sistemidir. İşte ROM'daki kodlar FAT gibi bir dosya sistemini tanıyorsa 
    oranın kök dizininde belli isimdeki dosyayı bulup RAM'e de yükleyebilmektedir.

    Sistem reset edildiğinde tüm boot işlemi bir bütünün parçaları gibi düşünülürse burada devreye giren programlara birer 
    aşama numarası da verilebilmektedir. Örneğin boot işleminin RESET vektöründe bulunan kısmına "birinci aşama bootloader  
    (stage-1 / first stage bootloader)", bu programın yüklediği programa "ikinci aşama bootloader (stage-2 / second stage 
    bootloader)" ve bunun da yüklediği programa (yani işletim sistemini yükleyen GRUB gibi programlara da) "üçüncü 
    aşama bootloader (stage-3 / third stage bootloader)" denilebilmektedir. Ancak bazı kaynaklar bu ROM'daki kodu boot 
    sürecinin bir parçası olarak ele almamaktadır. Dolayısıyla bu kaynaklar ROM kodunun kendisine değil, onun yüklediği 
    programa "birinci aşama bootloader (stage-1 / first stage bootloader), işletim sistemini yükleyen programa (yani 
    GRUB gibi) ise "ikinci aşama bootloader (stage-2 / second stage bootloader)" demektedir. Örneğin Wikipedia boot 
    işleminde donanım bileşenlerini hazır hale getiren reset vektöründeki programı "birinci aşama bootloader", işletim 
    sistemini yükleyen programı ise (GRUB gibi) "ikinci aşama bootloader" olarak isimlendirmiştir. Biz kursumuzda izleyen 
    paragraflarda bu terminolojiyi kullanacağız.

    Bugün pek çok masaüstü Linux dağıtımında GRUB isimli önyükleyici program kullanılmaktadır. Bu önyükleyici program 
    Linux çekirdek imajını bellek yükleyerek çalıştırmaktadır. Linux'un çekirdek parametreleri de bu önyükleyici tarafından 
    kullanıcıdan alınıp Linux'a verilmektedir. Biz kursumuzda çekirdek derlemesi yaparken ve sistemi çekirdekle boot 
    ederken önyükleyicinin GRUB olduğunu varsayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzdaki örnekler x86 tabanlı masaüstü bilgisayar kullanılarak verilemektedir. (Burada "masaüstü (desktop)" terimi 
    yalnızca kasalı bilgisayarlar için değil notebook'lar için de kullanılan genel bir terimdir.) x86 tabanlı masaüstü 
    bilgisayarlara tarihsel bakımdan "PC (Personal Computer)" da denilmektedir. (2000'lerin başında Apple Intel tabanlı 
    PC mimarisine geçtiyse de kullandığı PC donanımında bazı farklılıklar da oluşturmuştur. Sonra Apple'ın Intel mimarisini 
    de bırakıp ARM mimarisine geçtiğini biliyorsunuz. Ancak PC denildiğinde Intel ve ARM tabanlı macOS sistemleri kastedilmemektedir.)

    Bugün kullandığımız PC sistemlerinin temel donanımları 70'li yılların sonlarında ve 80'li yılların başlarında IBM 
    tarafından tasarlanmıştır. Bu bilgisayarlar 1980'in Aralık ayında IBM'in tasarladığı donanım ve Microsoft'un tasarladığı 
    DOS işletim sistemiyle piyasaya sürüldü. Zaman ilerledikçe bu PC donanımlarında bazı iyileştirmeler yapıldıysa da 
    temel mimari büyük ölçüde aynı kalmıştır.

    Eskiden PC'lerde klasik BIOS (legacy BIOS) kullanılıyordu. Ancak 2010'lardan sonra modern UEFI BIOS'lar yaygınlaştı. 
    Artık bugün ağırlıklı olarak UEFI BIOS'lar kullanılıyor. (Ancak bu modern UEFI BIOS'lar eski klasik BIOS (legacy BIOS) 
    gibi de çalışabilecek özelliklere sahip olabilmektedir). Biz burada klasik eski BIOS'a sahip PC'lerin boot sürecinden 
    bahsedeceğiz.

    Eski klasik BIOS'lara (legacy BIOS) sahip PC'ler reset edildiğinde BIOS kodları DRAM belleği ve pek çok donanım birimini 
    programladıktan sonra CMOS setup'ta belirtilen "boot sırasına (boot sequence)" göre ilgili medyanın 0'ıncı sektörünü 
    belleğe yükleyip akışı oraya devretmektedir. (PC mimarisinde diskten okunabilecek ya da diske yazılabilecek en küçük 
    birime "sektör (sector)" denilmektedir.) İkincil belleklerdeki ilk sektöre "MBR (Master Boot Record)" denilmektedir. 
    MBR'de toplam 512 byte'lık bir program bulunur. MBR'nin sonunda 64 byte'lık "Disk Bölümleme Tablosu (Disk Partition 
    Table)" vardır. MBR'deki program duruma göre ya bir önyükleyici programını yüklemekte ya da default durumda aktif 
    disk bölümünün 0'ıncı sektörünü RAM'e yükleyip akışı oraya devretmektedir. (PC sistemlerinde her disk bölümünün ilk 
    sektörüne (0'ıncı sektörüne) "boot sektör" denilmektedir. Boot sektör ilgili işletim sisteminin yüklenmesinden sorumludur.) 
    Tabii MBR'deki program daha büyük bir önyükleyici programını da yükleyebilmektedir. Bu durumda hangi işletim sisteminin 
    yükleneceği önyükleyici program tarafından bir menü yoluyla kullanıcıya da sorulabilmektedir. UEFI BIOS öncesindeki 
    eski BIOS sistemini kullanan (legacy BIOS) PC sistemlerindeki boot sürecini aşağıdaki gibi özetleyebiliriz:

    PC'ye güç veriliyor ve mikroişlemci RESET ediliyor ---> EEPROM'daki BIOS kodları temel hazırlık işlemlerini yapıyor 
    ---> EEPROM'daki BIOS kodları diskin ilk sektörünü (MBR) RAM'e yüklüyor ve akışı ona devrediyor ---> MBR'deki program 
    önyükleyiciyi RAM'e yüklüyor ---> Önyükleyici seçilen disk bölümünün boot sektörünü RAM'e yüklüyor ---> Seçilen disk 
    bölümünün boot sektörü işletim sistemini yüklüyor ---> Akış işletim sistemi kodlarına devrediliyor.

    Kursumuzda UEFI BIOS sistemlerinin işlevi temel çalışma mekanizması başka bir bölümde ele alınacaktır.

    Yukarıda da belirttiğimiz gibi bugünkü Linux yüklü olan Intel x86 tabanlı PC sistemlerinde genellikle önyükleyici 
    olarak GRUB tercih edilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinin kodlarına dokunmadan onunla ilgili bazı davranış değişikliklerinin yapılması temelde beş biçimde 
    sağlanabilmektedir:

    1) Çekirdek modunda çalışan "çekirdek modülleri (kernel modules)" ve "aygıt sürücüler (device drivers)" yoluyla: 
    İşletim sistemlerinin çoğu çekirdeğin bir parçası gibi işlev görecek kodların yüklenmesine ve çalıştırılmasına olanak 
    sağlamaktadır. Bu yöntemde çekirdeğin yeniden derlenmesine gerek yoktur. Zaten çekirdek modülleri ve aygıt sürücüleri 
    çalışmakta olan çekirdek içerisine yüklenmektedir. Çekirdek modüllerini ve aygıt sürücüleri "kasalı bilgisayarlardaki 
    genişleme yuvalarına takılan kartlara benzetebiliriz. Nasıl takılan bu kartlar donanımın bir parçası haline geliyorsa 
    çekirdek modülleri ve aygıt sürücüler de çekirdeğin bir parçası haline gelmektedir. Linux'taki çekirdek modüllerinin 
    ve aygıt sürücülerinin yazımı kursumuzun konularına dahil değildir.

    2) Çekirdek yüklenip başlatılırken (initialize ederken) ismine "çekirdek komut satırı parametreleri (kernel command 
    line parameters)" denilen parametreler yoluyla çekirdeğin davranışı değiştirilebilmektedir. Çekirdek komut satırı 
    parametreleri "önyükleyici tarafından" çekirdeğe iletilmektedir. Linux çekirdeğinin pek çok komut satırı parametresi 
    vardır. Bu parametreler yoluyla çekirdekte bazı davranış değişiklikleri oluşturulabilmektedir. Bunun için de çekirdeğin 
    yeniden derlenmesi gerekmez.

    3) Çekirdek derlenirken çekirdek kodlarına hiç dokunmadan bazı konfigürasyon parametreleri ile oynanarak çekirdekte 
    davranış değişiklikleri oluşturulabilmektedir. Çekirdek konfigüre edilirken bazı alt sistemler çekirdekten çıkartılabilmekte, 
    bazı alt sistemlerde ince ayarlar yapılabilmektedir. Tabii çekirdek konfigüre edilirken her ne kadar biz çekirdek 
    kodlarını değiştirmiyor olsak da aslında sembolik sabitler yoluyla arka planda derleme işlemine sokulan kodlar üzerinde 
    de değişikler yapılmış olmaktadır. O halde çekirdeğin konfigüre edilmesinin iki amacı vardır:

    - Çekirdek içerisindeki alt sistemlere ilişkin bileşenlerin çekirdeğe eklenmesini ve çekirdekten çıkartılmasını sağlamak.
    - Çekirdeğin üzerinde bazı davranış değişikliklerini oluşturmak.

    Çekirdek konfigüre edildikten sonra yeniden derlenmelidir. Yani bu yöntem çekirdeğin yeniden derlenmesini gerektirmektedir.

    4) Çekirdek kodlarında doğrudan değişiklikler yapılıp çekirdek yeniden derlenebilir. Bu çekirdekte davranış değişikliği 
    oluşturmak için kullanılan en aşağı seviyeli yöntemdir.

    5) Nihayet çekirdek çalışırken de çekirdeğin davranışı bazı komutlarla (bu komutlar bazı mekanizmaları ve sistem 
    fonksiyonlarını kullanmaktadır), konfigürasyon dosyalarıyla ve bazı mekanizmalarla da (örneğin "proc" ve "sys" 
    dosya sistemi yoluyla) çekirdek davranışları değiştirilebilmektedir. Tabii bunun için de çekirdeğin yeniden derlenmesi 
    gerekmez. Örneğin sistem çalışırken bir prosesin açabileceği dosya sayısını "proc" dosya sistemi yoluyla şöyle 
    değiştirebiliriz:

    $ echo 2048 | sudo tee /proc/sys/fs/file-max
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										5. Ders 02/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'ta çekirdek komut satırı parametreleri birbirinden boşluklarla ayrılmış yazılardan oluşmaktadır. Bazı parametrelerin 
    argümanları yoktur, bazılarının vardır. Eğer parametrenin bir argümanı varsa "parametre=değer" biçiminde ('=' karakteri 
    boşluksuz olarak kullanılmalıdır) yoksa yalnızca "parametre" biçiminde belirtilmektedir. Çekirdek komut satırı parametreleri 
    tek bir yazı biçiminde çekirdeğe aktarılmaktadır. Çekirdek bu komut satırı parametrelerini kendini kullanıma hazır 
    hale getirmenin (kendini initialize etmenin) ön aşamalarında parse eder ve bu değerleri yapılandırma amacıyla kullanır. 
    Tabii çekirdeğin komut satırı parametreleri tipik olarak önyükleyici (bootloader) tarafından çekirdeğe iletilmektedir. 
    Örneğin çekirdek komut satırı parametreleri aşağıdaki gibi bir görünümde olabilir:

    console=serial0,115200 console=tty1 root=PARTUUID=382d6f16-02 rootfstype=ext4 fsck.repair=yes rootwait quiet splash 
    plymouth.ignore-serial-consoles cfg80211.ieee80211_regdom=TR

    Linux çekirdeğinin onlarca farklı komut satırı parametresi vardır. Bunların çoğu spesifik konulara ilişkindir ve ancak 
    çekirdeğin yapısını iyi bilen kişiler tarafından anlamlandırılabilir. Tabii bazı parametrelerin anlamları herkes 
    tarafından anlaşılacak kadar açıktır. Çekirdek parametrelerinin dokümantasyonuna aşağıdaki bağlantıdan erişebilirsiniz:

    https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html

    Linux çekirdeği komut satırı parametrelerini parse ederken gerçekte olmayan (yani çekirdek tarafından kullanılmayan) 
    bir parametre gördüğünde onu yok saymaktadır. Ancak biz kendi parametrelerimizin de çekirdek tarafından saklanmsını 
    sağlayabiliriz.

    Biz kursumuzda çeşitli bölümlerde çekirdeğin komut satırı parametreleri hakkında açıklamalarda bulunacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bazı durumlarda çekirdeğin sıfırdan derlenmesi gerekebilmektedir. Çekirdeğin yeniden derlenmesinin gerektiği tipik 
    durumlar şunlardır:

    - Bazı çekirdek modüllerinin ve aygıt sürücülerin çekirdek imajından çıkartılması ve dolayısıyla çekirdeğin küçültülmesi 
    için.
    - Yeni birtakım modüllerin ve aygıt sürücülerin çekirdek imajına eklenmesi için.
    - Çekirdeğe tamamen başka birtakım özelliklerin ve alt sistemlerin eklenmesi için.
    - Çekirdek üzerinde çekirdek parametreleriyle sağlanamayacak bazı konfigürasyon değişikliklerinin yapılabilmesi için.
    - Çekirdek kodlarında yapılan değişikliklerin etkin hale getirilmesi için.
    - Çekirdeğe yama (patch) yapılması için.
    - Yeni çıkan çekirdek kodlarının kullanılabilir hale getirilmesi için.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin derlenmesi için öncelikle çekirdek kaynak kodlarının derleme yapılacak bilgisayara indirilmesi gerekir. 
    Bazı dağıtımlar default durumda çekirdeğin kaynak kodlarını da kurulum sırasında makineye çekmektedir. Çekirdek 
    kodları "kernel.org" sitesinde bulundurulmaktadır. Tarayıcdan "kernel.org" sitesine girip "pub/linux/kernel" 
    dizinine geçtiğinizde tüm yayınlanmış çekirdek kodlarını göreceksiniz. İndirmeyi tarayıcıdan doğrudan yapabilirsiniz. 
    Eğer indirmeyi komut satırından "wget" programıyla yapmak istiyorsanız aşağıdaki URL'yi kullanabilirsiniz:

    https://cdn.kernel.org/pub/linux/kernel/[MAJOR_VERSION].x/linux-[VERSION].tar.xz

    Buradaki MAJOR_VERSION "3", "4", "5", "6" gibi tek bir sayıyı belirtmektedir. VERSION ise çekirdeğin büyük ve küçük 
    numaralarını belirtmektedir. Örneğin biz çekirdeğin 6.9.2 versiyonunu şöyle indirebiliriz:

    $ wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.9.2.tar.xz
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek kodları indirildikten sonra onun açılması gerekir. Açma işlemi tar komutuyla aşağıdaki gibi yapılabilir:

    $ tar -xvJf linux-5.15.12.tar.xz

    Burada Linux'ta kullanılan sıkıştırma formatlarında kısaca bahsetmek istiyoruz.

    UNIX/Linux sistemlerinde kullanılan "tar" utlity programı sıkıştırma yapmamaktadır. Yalnızca dosyaları uç uca ekleyip 
    onları tek bir dosya haline getirmektedir. Bu sayede kullanılmayan dosyalar dosya sisteminde daha az yer kaplar hale 
    getirilebilir. Aynı zamanda onların iletilmesi ve kopyalanması da kolaylaştırılmış olur. Sıkışıtma programları ise 
    aslında tek bir dosyayı sıkıştırmaktadır. O halde UNIX/Linux sistemlerinde kullanıcılar bir grup dosyayı sıkıştırmak 
    için önce onları tar'layıp tek dosya haline getirirler, sonra bu dosyayı sıkıştırırlar. Bu işlemler sonucunda dosya 
    uzantısı da ".tar.gz", gibi "tar.xz" gibi dosyanın hem tar'lanmış hem de sıkıştırılmış olduğunu belirten biçimde olur. 
    Açım sırasında da önce sıkıştırılan dosyalar açılır. Buradan ".tar" dosyası elde edilir. Sonra da bu ".tar" dosyasının 
    içerisindeki dosyalar dışarı çıkartılır.

    Linux'ta çeşitli sıkıştırma yöntemleri (formatları da diyebiliriz) kullanılmaktadır. Bunların bazıları şunlardır:

    - gzip formatı (dosyaların uzantıları ".gz" biçimindedir)
    - bz2 formatı (dosyaların uzantıları ".bz2" biçimindedir)
    - xz formatı (dosyaların uzantıları ".xz" biçimindedir)
    - Klasik zip formatı (dosyaların uzantıları ".zip" ya da ".z" biçimindedir)

    Bu formatlar sıkıştırma bakımından farklı performans göstermektedir. Ancak formatın sıkıştırma performansı ne kadar 
    yüksekse işlem yapma süresi de o kadar uzun olmaktadır. Tipik olarak bu formatların sıkıştırma performansları için 
    aşağıdaki ilişki söz konusudur:

    xz > bz2 > gzip == zip

    Yani en iyi sıkıştırma "xz" formatında, daha sonra "bz2" formatında daha sonra da "gzip" formatındadır. gzip formatı ile 
    zip formatı aynı algoritmaları kullanmaktadır. Dolayısıyla bunların performansları birbirine benzerdir. Ancak yukarıda da 
    belirttiğimiz gibi sıkıştırma performansı yükselirken (yani daha iyi hale gelirken) sıkıştırma ve açma için gereken zaman 
    da uzamaktadır.

    Yukarıdaki formatlara göre sıkıştıran ve açan programlar hazır biçimde bulunmaktadır. Bu programların isimleri şunlardır:

    gzip, gunzip
    bzip2, bunzip2
    xz, unxz
    zip, unzip

    zip programının dışındaki programların hepsi tek bir dosyayı sıkıştırıp açmaktadır. Dolayısıyla yukarıda da belirttiğimiz 
    gibi eğer birden fazla dosya sıkıştırılacaksa önce onların birleştirilmesi gerekir. O halde önce "tar" programının kullanımı 
    hakkında bazı bilgiler verelim.

    tar programının pek çok komut satırı seçeneği olsa da en çok kullanıcılan seçenekler "-c" "-x" "-f" "-v" seçenekleridir. 
    "-c" tar'lamak için "-x" ise açmak için kullanılır. "-v" seçeneği programın daha fazla bilgi vermesini sağlamaktadır. 
    "-f" seçeneği bir argümanla kullanılır. Argüman ".tar" dosyasını belirtir. tar programı birden fazla dosyayı komut satırı 
    argümanıyla alabilir. Tabii birden fazla dosyayı belirtmek için kabuğun joker karakterlerinden faydalanabilirsiniz. Argüman 
    olarak dosya yerine dizinler de verilebilir. Bu durumda bu dizinin içerisindeki dosyaların hepsi tar'lanıp açılmaktadır. 
    Örneğin:

    $ tar -c -f test.tar x.txt y.txt z.txt

    Genellikle kullanıcılar seçenekleri aşağıdaki gibi birleştirmektedir:

    $ tar -cf test.tar x.txt y.txt z.txt

    Tabii burada "f" seçeneğinin seçenek listesinde en sonra olması gerekmektedir.

    gzip programı ile sıkıştırmak oldukça kolaydır. Örneğin:

    $ gzip test.tar

    Açma işlemi de şöyle yapılabilir:

    $ gunzip test.tar.gz

    Buradan biz "test.tar" dosyasını elde edeceğiz. Onu da açmamız gerekir. gzip ve gunzip programlarının eski dosyayı da 
    sildiğine dikkat ediniz. tar komutu ile hem tar'lamak hem de aynı zamanda gzip işlemini yapmak için tar komutunda "-z" 
    seçeneği kullanılmaktadır. Yani "-z" seçeneği "önce tar'la sonra gzip yap" anlamına gelmektedir. Tabii tar programı da 
    aslında kendi içerisinde gzip programını da çalıştırmaktadır. Örneğin:

    $ tar -cvzf test.tar.gz x.txt y.txt

    Burada biz dosyaları hem tar'ladık hem de sıkıştırdık. Açma işlemi de aynı biçimde yapılmaktadır:

    $ tar -xvzf test.tar.gz

    bzip2 programının kullanımı gzip programına oldukça benzemektedir. Örneğin:

    $ bzip2 test.tar

    Açım da benzer biçimde yapılmaktadır:

    $ bunzip2 test.tar.bz2

    Önce tar'layıp sonra bzip2 ile sıkıştırma işlemi tek hamlede tar programı tarafından "-j" seçeneği ile yapılabilmektedir. 
    ("-z" seçeneğinin gzip için "-j" seçeneğinin bz2 için kullanıldığına dikkat ediniz.) Örneğin:

    $ tar -cvjf test.tar.bz2 x.txt y.txt

    Açım da benzer biçimde yapılabilir:

    $ tar -xvjf test.tar.bz2

    xz programı ile sıkıştırma yapma da benzer biçimdedir. Örneğin:

    $ xz test.tar

    Açım da benzerdir:

    $ unxz test.tar.xz

    Hem tar'lamak hem de xz haline getirmek için tar programında "-J" seçeneği kullanılmaktadır. Örneğin:

    $ tar -cvJf test.tar.xz x.txt y.txt

    Açım işlemi de şöyle yapılabilir:

    $ tar -xvJf test.tar.xz
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Dağıtımlar (Ubuntu, Mint, Fedora gibi) çekirdek kodlarında küçük değişiklikler ve kendilerine özgü özelleştirmeler ve 
    yamamalar da yapabilmektedir. Debian tabanlı sistemlerde o anda makinede yüklü olan mevcut çekirdeğin ilgili dağıtıma 
    ilişkin kaynak kodlarını indirmek için aşağıdaki komutu da kullanabilirisiniz:

    $ sudo apt-get install linux-source

    Burada yükleme "/usr/src" dizinine yapılacaktır. Ancak bu komut doğrudan sıkıştırılmış dosyayı indirmektedir. Yani açım 
    yapmamaktadır. Aslında istenirse bulunulan makinedeki versiyon numarasına ilişkin dağıtıma özgü kaynak kodlar yerine 
    istenilen bir versiyona ilişkin kaynak kodlar da indirilebilir. Bunun için komutta linux-source argümanına istenilen 
    versiyonun majör, minör ve patch numarası "-majör.minör.patch" biçiminde eklenir. Örneğin:

    $ sudo apt-get install linux-source-6.8.0

    Bu biçimde indirdiğimiz çekirdek kaynak kodları Debian ya da Ubuntu depolarından indirilmektedir. Bunlar bu dağıtımlar 
    tarafından yamanmış kodlardır. Örneğin Mint'te çalışıyorsanız indirdiğiniz kodlar Ubuntu için yamanmış kodlar olacaktır.

    BBB için derleme yapmak istiyorsanız yine "kernel.org"deki kaynak kodları indirebilirsiniz. Ancak BBB için bazı 
    özelleştirmelerin de yapılmış olduğu kaynak kodların indirilip derlenmesi birtakım kolaylıklar sağlamaktadır. Bu aşağıdaki 
    komutla yapabilirsiniz:

    $ git clone https://github.com/beagleboard/linux.git

    Benzer biçimde Raspbbey Pi için de "kernel.org"deki kaynak kodlar kullanılabilir. Ancak Raspberry Pi'a özgü daha 
    güncel aygıt sürücüler ve aygıt ağacı dosyalarını içeren Linux kaynak kodlarının projenin kendi sitesinden indirilmesi 
    daha uygun olur. İndirmeyi aşağıdaki bağlantıdan yapabilirsiniz:

    $ git clone --depth=1 https://github.com/raspberrypi/linux.git
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux kaynak kodlarının versiyonlanması eskiden daha farklıydı. Çekirdeğin 2.6 versiyonlarından sonra versiyon 
    numaralandırma sistemi değiştirildi. Eskiden (2.6 ve öncesinde) versiyon numaraları çok yavaş ilerletiliyordu. 2.6 
    sonrasındaki yeni versiyonlamada versiyon numaraları daha hızlı ilerletilmeye başlanmıştır. Bugün kullanılan Linux 
    versiyonları nokta ile ayrılmış üç sayıdan oluşmaktadır:

    Majör.Minör.Patch

    Buradaki "majör numara" büyük ilerlemeleri, "minör numara" ise küçük ilerlemeleri belirtmektedir. Eskiden (2.6 ve 
    öncesinde) tek sayı olan minör numaralar "geliştirme versiyonlarını (ya da beta versiyonlarını)", çift olanlar ise 
    stabil hale getirilmiş dağıtılan versiyonları belirtiyordu. Ancak 2.6 sonrasında artık tek ve çift minör numaralar 
    arasında böyle bir anlam farklılığı kalmamıştır. Patch numarası birtakım böceklerin giderildiği ya da çok küçük 
    yeniliklerin çekirdeğe dahil edildiği versiyonları belirtmektedir. Patch numarası minör numaralardan daha küçük 
    bir ilerlemenin söz konusu olduğunu anlatmaktadır.

    Linux kaynak kodları konfigüre edilip derlendiğinde çekirdek imajının ismine bir alan daha eklenebilmektedir. Bu alana 
    biz "Extra" alanı diyeceğiz. Bu durumda çekirdek imaj ismi (kaynak kod versiyon ismi değil) şu hale gelecektir:

    Majör.Minör.Patch-Extra (Extra için -rcX, -stable, -custom, -generic gibi sözcükler kullanılabilir)

    Bu extra alanı tamamen derleme işlemini yapan kişi ya da kurumun versiyon bilgisine eklediği, onların isteklerine göre 
    belirlenmiş bir alandır. Burada Extra ile temsil edilen alanda "rcX (X burada bir sayı belirtir) "stable", "custom", 
    "generic", "realtime" gibi sözcükler bulunabilmektedir. "rc" harfleri "release candidate" sözcüklerinin kısaltmasıdır. 
    "stable" sözcüğü dağıtılan sürümün "kararlı sürüm" olduğunu belirtir. Eğer sistem programcısı çekirdekte kendisi birtakım 
    değişiklikler yapıyorsa genellikle "Extra" alanında "custom" sözcüğünü kullanır. Ayrıca "Extra" alanındaki sözcüğün başına 
    ya da sonuna versiyon ya da build numaraları getirilebilmektedir. Örneğin "custom" sözcüğünü ayrıca "-<custom_version_number>" 
    biçiminde bir versiyon numarası da izleyebilir. Buradaki numaralar sistem programcısının kendi özelleştirmesine ilişkin 
    numaralardır. "generic" sözcüğünü dağıtımlar sıkça kullanmaktadır. Bu "generic" sözcüğü çekirdeğin "genel kullanım için 
    konfigüre edilmiş olduğunu belirtmektedir. "realtime" sözcüğü genellikle gerçek zamanlı bir yapılandırmada kullanılmaktadır. 
    "generic" ve "realtime" sözcüklerinin öncesinde "-N-" biçiminde bir sayı da bulunabilmektedir. Bu sayı "dağıtıma özgü 
    yama ya da derleme numarasını belirtmektedir. Örneğin:

    6.8.0-51-generic

    Burada -51 yapılandırmaya özgü bir numara belirtmektedir. "-generic" ise yapılandırmanın genel kullanım için olduğunu 
    belirtmektedir.

    Çalışmakta olan Linux sistemi hakkında bilgiler "uname -a" komutu ile elde edilebilir. Örneğin:

    $ uname -a
    Linux kaan-virtual-machine 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

    Bu bilgi içerisinden yalnızca çekirdek versiyonu görüntülenmek isteniyorsa "uname -r" seçeneği kullanılmalıdır:

    $ uname -r
    6.8.0-51-generic

    Buradan biz çekirdeğin "6.8.0" sürümünün kullanıldığını anlıyoruz. Burada genel yapılandırılmış bir çekirdek söz 
    konusudur. "91" sayısı dağıtıma özgü yama ya da derleme numarasını belirtir.

    Daha önceden de belirttiğimiz gibi "uname" komutu bu bilgileri "/proc" dosya sisteminin içerisinde almaktadır. 
    Örneğin:

    $ cat /proc/version
    Linux version 5.15.0-91-generic (buildd@lcy02-amd64-045) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld
    (GNU Binutils for Ubuntu) 2.38) #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi büyük projelerin derlenmesi için "build otomasyon araçları" denilen araçlar kullanılmaktadır. Bunların 
    en yaygın kullanılanı "make" denilen araçtır. Linux çekirdeklerinin derlenmesi de "make" aracı ile yapılmaktadır. 
    Ancak Linux çekirdeklerinin derlenmesinde projeye özgü bazı yapılar ve yöntemler de kullanılmıştır. Buna "KConfig 
    sistemi" ya da "KBuild sistemi" denilmektedir. Biz önce çekirdek derleme işleminin hangi adımlardan geçilerek yapılacağını 
    göreceğiz. Sonra çekirdeğin önemli konfigürasyon parametreleri üzerinde biraz duracağız. Sonra da çekirdekte bazı 
    değişiklikler yapıp değiştirilmiş çekirdekle sistemin açılmasını sağlayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'ta çekirdek derlemesi tipik olarak aşağıdaki aşamalardan geçilerek gerçekleştirilmektedir:

    1) Derleme öncesinde derlemenin yapılacağı makinede bazı programların yüklenmiş olması gerekmektedir. Çünkü KBuild 
    sistemi yalnızca binary araçları değil bazı başka kütüphaneleri de kullanmaktadır. Çekirdeğin derlenmesi için gerekebilecek 
    programları şöyle yükleyebilirsiniz:

    $ sudo apt update
    $ sudo apt install build-essential libncurses-dev bison flex libssl-dev wget gcc-arm-linux-gnueabihf \
    binutils-arm-linux-gnueabihf libelf-dev dwarves

    2) Çekirdek kodları indirilerek açılır. Biz bu konuyu yukarıda ele almıştık. İndirmeyi şöyle yapabiliriz:

    $ wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.9.2.tar.xz

    Bu işlemden sonra "linux-6.9.2.tar.xz" isimli dosya indirilmiş durumdadır. Onu aşağıdaki gibi açabiliriz:

    $ tar -xvJf linux-6.9.2.tar.xz

    Bu işlemden sonra "linux-6.9.2" isminde bir dizin oluşturulacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										6. Ders 03/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    3) Çekirdek derlenmeden önce konfigüre edilmelidir. Çekirdeğin konfigüre edilmesi birtakım çekirdek özelliklerin 
    belirlenmesi anlamına gelmektedir. Konfigürasyon bilgileri çekirdek kaynak kod ağacının kök dizininde (örneğimizde 
    "linux-6.9.2" dizini) ".config" ismiyle bulunmalıdır. Bu ".config" dosyası default durumda kaynak dosyaların kök 
    dizininde bulunmamaktadır. Bunun çekirdeği derleyen kişi tarafından oluşturulması gerekmektedir. Çekirdek konfigürasyon 
    parametreleri oldukça fazladır ve bunların bazılarının anlamlandırılması özel bilgi gerektirmektedir. Biz izleyen 
    paragraflarda önemli çekirdek konfigürasyon parametrelerini açıklayacağız. Çekirdek konfigürasyon parametreleri çok 
    fazla olduğu için bunlar bazı genel amaçları karşılayacak biçimde default değerlerle önceden oluşturulmuş durumdadır. 
    Önceden oluşturulmuş default konfigürasyon dosyaları kaynak kod ağacında "arch/<mimari>/configs" dizininin içerisinde 
    bulunmaktadır. Örneğin Intel x86 mimarisi için bu default konfigürasyon dosyaları şöyledir:

    $ ls arch/x86/configs
    hardening.config  i386_defconfig  tiny.config  x86_64_defconfig  xen.config

    Burada biz 64 bit Linux sistemleri için "x86_64_defconfig" dosyasını kullanabiliriz. O halde bu dosyayı kaynak dosyaların 
    bulunduğu dizininin kök dizinine ".config" ismiyle kopyalayabiliriz:

    $ cp arch/x86/configs/x86_64_defconfig .config

    Biz bütün işlemlerde çekirdek kaynak kodlarının kök dizininde bulunduğumuzu (current working directory) varsayacağız. 
    Ancak burada bir noktaya dikkatinizi çekmek istiyoruz. Linux kaynak kodlarındaki default konfigürasyon dosyaları 
    minimalist biçimde konfigüre edilmiştir. Bu nedenle pek çok modül bu default konfigürasyon dosyalarında işaretlenmiş 
    değildir. Bu tür denemeleri zaten çalışan çekirdeğin derlenmesinde kullanılan konfigürasyon dosyalarından hareketle 
    yaparsanız daha fazla modül dosyası oluşturulabilir ancak daha az zahmet çekebilirsiniz. Linux sistemlerinde genel 
    olarak "/boot" dizini içerisinde "config-<çekirdek_sürümü>" ismiyle mevcut çekirdeğe ilişkin konfigürasyon dosyası 
    bulundurulmaktadır.

    Burada bir noktaya dikkatinizi çekmek istiyoruz. Çekirdek kaynak kodlarındaki "arch/<platform>/configs" dizinindeki 
    "x86_64_defconfig" konfigürasyon dosyası ".config" ismiyle kopyalandıktan sonra ayrıca "make menuconfig" ya da "make oldconfig" 
    gibi bir işlemle onun satırlarına delenecek çekirdekte bulunan yeni birtakım özelliklere ilişkin bazı default değerlerin 
    de eklenmesi gerekir. Bu işlem sonraki aşamada açıklayacağımız "make menuconfig" komutyuyla gerçekleştirilmektedir.

    Aslında ".config" dosyasını oluşturmanın başka alternatif yolları da vardır:

    make defconfig: Bu komut çalıştığımız sisteme uygun olan konfigürasyon dosyasını temel alarak mevcut donanım bileşenlerini 
    de gözden geçirerek sistemin açılması için gerekli minimal bir konfigürasyon dosyasını ".config" ismiyle oluşturmaktadır. 
    Örneğin biz 64 bit Intel sistemine ilişkin bir bilgisayarda çalışıyorsak "make defconfig" dediğimizde "arch/x86/configs/x86_64_defconfig" 
    dosyası temel alınarak o anda çalışılmakta olan çekirdek donanımları da göz önünde bulundurularak nispeten minimal 
    olan bir konfigürasyon dosyası oluşturmaktadır.

    make oldconfig: Bu seçeneği kullanmak için kaynak kök dizinde bir ".config" dosyasının bulunuyor olması gerekir. 
    Ancak bu seçenek "KConfig" dosyalarındaki ve kaynak dosya ağacındaki diğer değişiklikleri de göz önüne alarak bu eski 
    ".config" dosyasını eğer söz konusu mimaride birtakım değişiklikler yapılmışsa o değişikliklere uyumlandırmaktadır. Yani örneğin 
    biz eski bir ".config" dosyasını kullanıyor olabiliriz. Ancak çekirdeğin yeni versiyonlarında ek birtakım başka konfigürasyon 
    parametreleri de eklenmiş olabilir. Bu durumda "make oldconfig" bize bu eklenenler hakkında da bazı sorular sorup bunların 
    dikkate alınmasını sağlayacaktır. Başka bir deyişle "make oldconfig" eski bir konfigürasyon dosyasını yeni çekirdekler 
    için uyumlandırmaktadır.

    make <platform>_defconfig: Bu seçenek belli bir platformun default konfig dosyasını ".config" dosyası olarak save 
    etmektedir. Örneğin biz Intel makinelerinde çalışıyor olabiliriz ancak "BeagleBone Black (BBB)" için default konfigürasyon 
    dosyası oluşturmak isteyebiliriz. Eğer biz "make defconfig" yaparsak Intel tabanlı bulunduğumuz platform dikkate 
    alınarak ".config" dosyası oluşturulur. Ancak biz burada örneğin "make bb.org_defconfig" komutunu uygularsak bu durumda 
    Intel mimarisinde çalışıyor olsak da "bb.org_defconfig" konfigürasyon dosyası ".config" olarak save edilir. Tabii biz 
    aslında bu komutu kullanmak yerine ilgili platformun konfigürasyon dosyasını manuel olarak da ".config" biçiminde 
    kopyalayabiliriz.

    make modules: Bu seçenek ile yalnızca modüller derlenir. Yani bu seçenek ".config" dosyasında belirtilen aygıt sürücü 
    dosyalarını derler ancak çekirdek derlemesi yapmaz. Yalnızca "make" işlemi zaten aynı zamanda bu işlemi de yapmaktadır.

    make uninstall: "make install" işlemi ile yapılanları geri alır.

    Aşağıdaki "make xxxconfig" komutları ise seyrek kullanılmaktadır:

    make allnoconfig: Tüm seçenekleri "hayır (no)" olarak ayarlar (minimal özellikler).

    make allyesconfig: Tüm seçenekleri "evet (yes)" olarak ayarlar (maksimum özellikler).

    make allmodconfig: Tüm aygıt sürücülerin çekirdeğin dışında modül (module) biçiminde derleneceğini belirtir.

    make localmodconfig: Sistemde o anda yüklü modüllere dayalı bir yapılandırma dosyası (".config" dosyası) oluşturur.

    make silentoldconfig: Yeni seçenekler için onları görmezden gelir ve o yeni özellikler ".config" dosyasına yansıtılmaz.

    make dtbs: Kaynak kod ağacında "/arch/platform/boot/dts" dizinindeki aygıt ağacı kaynak dosyalarını derler ve "dtb" 
    dosyalarını elde eder. Gömülü sistemlerde bu işlemin yapılması ve her çekirdek versiyonuyla o versiyonun "dtb" dosyasının 
    kullanılması tavsiye edilir. Ancak gömülü sistemlerde zaten "make" işlemi aygıt ağacı dosyalarını da derlemektedir.

    Yukarıda da belirttiğimiz gibi aslında pek çok dağıtım o anda yüklü olan çekirdeğe ilişkin konfigürasyon dosyasını "/boot" 
    dizini içerisinde "config-$(uname -r)" ismiyle bulundurmaktadır. Örneğin kursun yapılmakta olduğu Mint dağıtımında "/boot" 
    dizinin içeriği şöyledir:

    $ ls /boot
    config-6.8.0-51-generic      initrd.img.old
    System.map-6.8.0-51-generic  efi
    grub                         vmlinuz
    initrd.img                   vmlinuz-6.8.0-51-generic
    initrd.img-6.8.0-51-generic

    Buradaki "config-6.8.0-51-generic" dosyası çalışmakta olduğumuz çekirdekte kullanılan konfigürasyon dosyasıdır. Buradaki 
    "config-6.8.0-51-generic" dosyası sistem açılırken herhangi bir biçimde kullanılmamaktadır. (Yani bu dosyayı silseniz 
    hiçbir sorun oluşmaz.) Bu dosya o çekirdeği yeniden derleyecek kişiler için kolaylık sağlamak amacıyla bulundurulmaktadır.

    Daha önceden de belirttiğimiz gibi eğer çalışılan sistemdeki konfigürasyon dosyasını temel alacaksanız bu dosyayı Linux kaynak 
    kodlarının bulunduğu kök dizine ".config" ismiyle kopyalayabilirsiniz:

    $ cp /boot/config-$(uname -r) .config

    Fakat eski bir konfigürasyon dosyasını yeni bir çekirdekle kullanmak için ayrıca "make oldconfig" işleminin de yapılması 
    gerekmektedir. Sonraki maddede göreceğimiz "make menuconfig" işlemi aynı zamanda "make oldconfig" işlemini de kendi 
    içerisinde barındırmaktadır.

    4) Şimdi elimizde pek çok değerin set edilmiş olduğu ".config" isimli bir konfigürasyon dosyası vardır. Artık bu konfigürasyon 
    dosyasından hareketle yalnızca istediğimiz özellikleri değiştirebiliriz. Bunun için "make menuconfig" komutunu kullanabiliriz:

    $ make menuconfig

    Bu komut ile birlikte text ekranda konfigürasyon seçenekleri listelenecektir. Tabii buradaki seçenekler ".config" dosyasındaki 
    içerikten hareketle oluşturulmuş durumdadır. Bunların üzerinde değişiklikler yaparak ".config" dosyasını yeniden save edebiliriz. 
    Aslında "make menuconfig" işlemi hiç ".config" dosyası oluşturulmadan doğrudan da yapılabilmektedir. Bu durumda hangi sistemde 
    çalışılıyorsa o sisteme özgü default config dosyası temel alınmaktadır. Biz en azından "General stup/Local version - append 
    to kernel release" seçeneğine "-custom" gibi bir sonek girmenizi böylece yeni çekirdeğe "-custom" soneki iliştirmenizi tavsiye 
    ederiz. Yukarıda da belirttiğimiz gibi "make menuconfig" işlemi zaten "make oldconfig" işlemini de kendi içerisinde barındırmaktadır.

    Pekiyi biz hazır bir ".config" dosyasını kaynak kod ağacının kök dizinine kopyaladıktan sonra hiç "make menuconfig" ya da 
    "make oldconfig" yazmazsak ne olur? Bu durumda sorun çıkmayabilir. Eğer kaynak kod çekirdeği yeniyse "make" işlemi sırasında 
    "make oldconfig" gibi bir işlem de yapılmaktadır. Ancak biz ".config" dosyasını kaynak kod ağacının kök dizinine kopyaladıktan 
    sonra "make menuconfig" ya da "make oldconfig" işlemini yapmanızı salık veriyoruz. "make menuconfig" işlemini yapıyorssanız 
    ayrıca "make oldconfig" işlemini yapmanıza gerek yoktur.

    ".config" dosyası elde edildiğinde çekirdek imzalamasını ortadan kaldırmak için dosyayı açıp aşağıdaki özellikleri belirtildiği 
    gibi değiştirebilirsiniz (bunların bazıları zaten default durumda aşağıdaki gibi de olabilir):

    CONFIG_SYSTEM_TRUSTED_KEYS=""
    CONFIG_SYSTEM_REVOCATION_KEYS=""
    CONFIG_SYSTEM_TRUSTED_KEYRING=n
    CONFIG_SECONDARY_TRUSTED_KEYRING=n

    CONFIG_MODULE_SIG=n
    CONFIG_MODULE_SIG_ALL=n
    CONFIG_MODULE_SIG_KEY=""

    Çekirdek imzalaması konusu daha ileride ele alınacaktır.

    Yukarıda da belirttiğimiz gibi derlenecek çekirdeklere yerel bir versiyon belirteci ve numarası da atanabilmektedir. 
    Bu işlem Bu "make menuconfig" menüsünde "General Setup/Local version - append custom release" seçeneği kullanılarak 
    ya da ".config" dosyasında "CONFIG_LOCALVERSION" satırı edit edilerek yapılabilir. Örneğin:

    CONFIG_LOCALVERSION="-custom"

    Artık çekirdek sürümüne "-custom" sonekini eklemiş olduk.

    5) Derleme işlemi için "make" komutu kullanılmaktadır. Örneğin:

    $ make

    Eğer derleme işleminin birden fazla CPU ya da çekirdek ile yapılmasını istiyorsanız "-j<cpu_sayısı>" seçeneğini 
    komuta dahil edebilirsiniz. Çalışılan sistemdeki CPU sayısının "nproc" komutuyla elde edildiğini anımsayınız. O halde 
    biz derleme için make komutunu şöyle kullanabiliriz:

    $ make -j$(nproc)

    Derleme işlemi bittiğinde ürün olarak biz "çekirdek imajını (yani çekirdek kodlarının bulunduğu dosyayı)", "çekirdek 
    tarafından yüklenecek olan modül dosyalarını (aygıt sürücü dosyalarını)" ve "diğer bazı dosyaları" elde etmiş oluruz. 
    Derleme işleminden sonra oluşturulan bu dosyalar ve onların yerleri şöyledir (buradaki <çekirdek_sürümü> "uname -r" 
    ile elde edilecek yazıyı belirtiyor):

    - Sıkıştırılmış Çekirdek Imajı: "arch/<platform>/boot" dizininde "bzImage" ismiyle oluşturulmaktadır. Denemeyi yaptığımız 
    Intel makinede dosyanın yol ifadesi "arch/x86_64/boot/bzImage" biçimindedir. (Ancak buradaki dosya x86_64 platformu için 
    "arch/x86/boot/bzImage" dosyasına sembolik link de yapılmış olabilir.)

    - Çekirdeğin Sıkıştırılmamış ELF İmajı: Kaynak kök dizininde "vmlinux" ismiyle oluşturulmaktadır.

    - Çekirdek Modülleri (Aygıt Sürücü Dosyaları): Çekirdek modülleri "drivers" dizininin altındaki dizinlerde, "fs" dizininin 
    altındaki dizinlerde ve "net" dizininin altındaki dizinlerde bulunur. Ancak "make modules_install" ile bunların hepsi 
    belirli bir dizine çekilebilir.

    - Çekirdek Sembol Tablosu: Kaynak kök dizininde "System.map" ismiyle bulunur. Çekirdek sembol tablosundan yalnızca çekirdek 
    debug edilirken faydalanılmaktadır. Bu dosya silinse bile sistemin çalışmasında bir sorun oluşmaz.

    Çekirdeğin derlemesi ne kadar zaman almaktadır? Şüphesiz bu derlemenin yapıldığı makineye göre değişebilir. Ancak derleme 
    süresinin uzamasına yol açan en önemli etken çekirdek konfigüre edilirken seçilen amodül (aygıt sürücü) sayısıdır. Pek 
    çok dağıtım "belki ileride lazım olur" gerekçesiyle konfigürasyon dosyalarında pek çok modülü dahil etmektedir. Bu 
    nedenle bir dağıtımın konfigürasyon dosyasını kullandığınız zaman çekirdek derlemesi uzayacaktır. Ayrıca çekirdek 
    konfigüre edilirken çok fazla modülün dahil edilmesi modüllerin çok fazla yer kaplamasına da yol açabilmektedir. Çekirdek 
    kodlarındaki platforma özgü default konfigürasyon dosyaları daha minimalist bir biçimde oluşturulmuş durumdadır. Derleminin 
    yapıldığı makine ve o makinedeki CPU sayısı da önemlidir. Örneğin sanal makineler genellikle düşük donanım konfigürasyonuyla 
    çalıştırıldığı için sanal makinelerde derleme süresi uzayacaktır. Tabii çekirdek bütünsel olarak bir kez derlendikten sonra 
    çekirdek kodlarında değişiklik yapıp çekirdeği yeniden derlemek istediğimizde artık derleme süresi bütünsel derleme kadar 
    uzun olmayacaktır. Çekirdek kodlarını değiştirdiğimizde ya da çekirdek kodlarına yeni bir dosya ya da dizin eklediğimizde 
    hangi dosyaların yeniden derleneceği yapılan değişikliğin ya da eklemelerin yerine göre değişebilmektedir. Temel dosyalardaki 
    değişiklikler çok fazla dosyanın yeniden derlenmesine yol açmaktadır.

    Bu tür durumlarda biz kursumuzda zamanı kısaltmak için ana makinenin Linux olduğu makine de kullanacağız. Bu makineye 
    dersin yapıldığı Windows host sisteminden uzak bağlantıyla bağlanacağız.

    6) Derleme sonrasında farklı dizinlerde oluşturulmuş olan aygıt sürücü dosyalarını (modülleri) belli bir dizine kopyalamak 
    için "make modules_install" komutu kullanılmaktadır. Bu komut seçeneksiz kullanılırsa default olarak "/lib/modules/<çekirdek_sürümü>" 
    dizinine kopyalama yapar. Her ne kadar bu komut pek çok ".ko" uzantılı aygıt sürücü dosyasını hedef dizine kopyalıyorsa 
    da bunlar çekirdek tarafından otomatik olarak yüklenmemektedir. Bu modüller kullanıcı tarafından yapılan birtakım işlemler 
    sonucunda başka bir deyişle ancak talep edildiğinde yüklenmektedir. Örneğin:

    $ sudo make modules_install

    Aslında "make modules_install" komutunun modül dosyalarını (aygıt sürücü dosyalarını) istediğimiz bir dizine kopyalamasını 
    da sağlayabiliriz. Bunun için INSTALL_MOD_PATH çevre değişkeni kullanılmaktadır. Örneğin:

    $ sudo INSTALL_MOD_PATH=modules make modules_install

    Burada aygıt sürücü dosyaları "/lib/modules/<çekirdek_sürümü>" dizinine değil bulunulan yerdeki "modules" dizinine 
    kopyalanacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi "make modules_install" komutu yalnızca modül dosyalarını mı hedef dizine kopyalamaktadır? Hayır aslında bu 
    komut modül dosyalarının kopyalanması dışında bazı dosyaları da oluşturup onları da hedef dizine kopyalamaktadır. 
    make modules_install komutu sırasıyla şunları yapmaktadır:

    - Modül dosyalarını "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.dep" isimli dosyayı oluşturur ve bunu "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.alias" isimli dosyayı oluşturur ve bunu "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.order" isimli dosyayı oluşturur ve "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.builtin" isimli dosyayı "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.

    Aslında burada oluşturulan dosyaların bazıları mutlak anlamda bulundurulmak zorunda değildir. Ancak sistemin öngörüldüğü 
    gibi işlev göstermesi için bu dosyaların ilgili dizinde bulundurulması uygundur.

    Bir aygıt sürücü başka aygıt sürücüleri de kullanıyor olabilir. Bu durumda bu aygıt sürücü yüklenirken onun kullandığı 
    tüm sürücülerin özyinelemeli olarak yüklenmesi gerekir. İşte "modules.dep" dosyası bir aygıt sürücünün yüklenmesi için 
    başka hangi aygıt sürücülerin yüklenmesi gerektiği bilgisini tutmaktadır. Aslında "modules.dep" bir text dosyadır. Bu 
    text dosya" satırlardan oluşmaktadır. Satırların içeriği şöyledir:

    <modül_yolu>: <bağımlılık1> <bağımlılık2> ...

    Dosyanın içeriğine şöyle örnek verebiliriz:
    ...
    kernel/arch/x86/crypto/nhpoly1305-sse2.ko.zst: kernel/crypto/nhpoly1305.ko.zst kernel/lib/crypto/libpoly1305.ko.zst
    kernel/arch/x86/crypto/nhpoly1305-avx2.ko.zst: kernel/crypto/nhpoly1305.ko.zst kernel/lib/crypto/libpoly1305.ko.zst
    kernel/arch/x86/crypto/curve25519-x86_64.ko.zst: kernel/lib/crypto/libcurve25519-generic.ko.zst
    ...

    Eğer bu "modules.dep" dosyası olmazsa bu durumda "modeprob" komutu çalışmaz ve çekirdek modülleri yüklenirken eksik 
    yükleme yapılabilir. Dolayısıyla sistem düzgün bir biçimde açılmayabilir. Eğer bu dosya elimizde yoksa ya da bir 
    biçimde silinmişse bu dosyayı yeniden oluşturabiliriz. Bunun için "depmod -a" komutu kullanılmaktadır. Komut doğrudan 
    kullanıldığında o anda çekirdek sürümü için "modules.dep" dosyasını oluşturmaktadır. Örneğin:

    $ sudo depmod -a

    Ancak siz yüklü olan başka bir çekirdek sürümü için "modules.dep" dosyasını oluşturmak istiyorsanız bu durumda çekirdek 
    sürümünü de komut satırı argümanı olarak aşağıdaki gibi komuta vermelisiniz:

    $ sudo depmod -a <çekirdek sürümü>

    Tabii depmod komutunun çalışabilmesi için "/lib/modules/<çekirdek_sürümü>" dizininde modül dosyalarının bulunuyor olması 
    gerekir. Çünkü bu komut bu dizindeki modül dosyalarını tek tek bulup ELF formatının ilgili bölümlerine bakarak modülün 
    hangi modülleri kullandığını tespit ederek "modules.dep" dosyasını oluşturmaktadır.

    "modules.alias" dosyası belli bir isim ya da id ile aygıt sürücü dosyasını eşleştiren bir text dosyadır. Bu dosyanın 
    bulunmaması bazı durumlarda sorunlara yol açmayabilir. Ancak örneğin USB port'a bir aygıt takıldığında bu aygıta ilişkin 
    aygıt sürücünün hangisi olduğu bilgisi bu dosyada tutulmaktadır. Bu durumda bu dosyanın olmayışı aygıt sürücünün yüklenememesine 
    neden olabilir. Dosyanın içeriği aşağıdaki formata uygun satırlardan oluşmaktadır:

    alias <tanımlayıcı> <modül_adı>

    Örnek bir içerik şöyle olabilir:

    ...
    alias usb:v05ACp*d*dc*dsc*dp*ic*isc*ip*in* apple_mfi_fastcharge
    alias usb:v8086p0B63d*dc*dsc*dp*ic*isc*ip*in* usb_ljca
    alias usb:v0681p0010d*dc*dsc*dp*ic*isc*ip*in* idmouse
    alias usb:v0681p0005d*dc*dsc*dp*ic*isc*ip*in* idmouse
    alias usb:v07C0p1506d*dc*dsc*dp*ic*isc*ip*in* iowarrior
    alias usb:v07C0p1505d*dc*dsc*dp*ic*isc*ip*in* iowarrior
    ...

    Bu dosya bir biçimde silinirse yine "depmod -a" komutu ile oluşturulabilir. (Yani "depmod" komutu yalnızca "modules.dep" 
    dosyasını değil bu dosyayı da oluşturmaktadır.)

    "modules.order" dosyası aygıt sürücü dosyalarının yüklenme sırasını barındıran bir text dosyadır. Bu dosyanın her 
    satırında bir çekirdek aygıt sürücüsünün dosya yol ifadesi bulunur. Daha önce yazılmış aygıt sürücüler daha sonra 
    yazılanlardan daha önce yüklenir. Bu dosyanın olmaması genellikle bir soruna yol açmaz. Ancak modüllerin belli sırada 
    yüklenmemesi bazı durumlarda bozukluklara da neden olabilmektedir. Bu dosyanın silinmesi durumunda yine bu dosya da 
    "depmod -a" komutuyla oluşturulabilmektedir.

    7) Eğer gömülü sistemler için derleme yapıyorsanız kaynak kod ağacında "arch/<platform>/boot/dts" dizini içerisindeki aygıt 
    ağacı kaynak dosyalarını da derlemelisiniz. Tabii elinizde zaten o versiyona "özgü aygıt (device tree blob)" dosyası 
    bulunuyor olabilir. Bu durumda bu işlemi hiç yapmayabilirsiniz. Aygıt ağacı kaynak dosyalarını derlemek için "make dtbs" 
    komutunu kullanabilirsiniz:

    $ make dtbs

    Derlenmiş aygıt ağacı dosyaları "arch/<platform>/boot/dts" dizininde ya da bu dizinin altındaki ilgili "vendor" dizininde 
    oluşturulacaktır. Yukarıda da belirttiğimiz gibi aygıt ağaçları gömülü sistemlerde kullanılmaktadır. Intel tabanlı PC'lerde 
    donanım birimlerinin tespit edilmesi otomatik olarak ACPI protokolü yoluyla yapıldığı için aygıt ağacı dosyaları bu platformda 
    kullanılmamaktadır. Ancak örneğin ARM platformunu kullanan gömülü sistemlerde ya da BBB ve Raspberry Pi gibi SBC'lerde aygıt 
    ağaçları kullanılmaktadır.

    8) Bizim çekirdek imajını, geçici kök dosya sistemine ilişkin dosyayı (bunu bizim oluşturmamız gerekir) ve aygıt ağacı 
    dosyasını "/boot" dizinine kopyalamamız gerekir. Ancak aslında bu işlem de "make install" komutuyla otomatik olarak 
    yapılabilmektedir. "make install" komutu bu dosyaları "/boot" dizinine kopyalamanın yanı sıra aynı zamanda GRUB önyükleyici 
    programın konfigürasyon dosyalarında da güncellemeler yapıp yeni çekirdeğin GRUB menüsü içerisinde görünmesini de 
    sağlamaktadır. Komut şöyle kullanılabilir:

    $ sudo make install

    Burada biz "geçici kök dosya sistemi ("initial ramdisk" ya da "initrd") diye bir terim kullandık. Geçici kök dosya sistemi 
    diskteki asıl kök dosya sistemi mount edilene kadar geçici bir süre sanki diskteki dosya sistemiymiş gibi işlev gören bir 
    RAM disk imajıdır. Tipik Linux sistemlerinde önce geçici kök dosya sistemi mount edilerek temel dosyalara oradan erişilir. 
    Sonra bu geçici dosya sistemi RAM'den atılıp diskteki gerçek kök dosya sistemi mount edilmektedir. Şimdi "geçici kök dosya 
    sistemine ne gerek var, doğrudan diskteki asıl kök dosya sistemi neden kullanılamıyor?" sorusu aklınıza gelebilir. Geçici 
    kök dosya sistemine gereksinimi basit bir örnekle anlayabiliriz. Diyelim ki diske erişmekte kullanılan aygıt sürücüsü 
    çekirdeğin içerisine yerleştirilmemiş olsun yani dışarıda "lib/modules/$(uame -r)" dizininde bir dosya biçiminde bulunyor 
    olsun. Şimdi çekirdeğin diske erişebilmesi için bu aygıt sürücüye ihtiyacı olacaktır. Ancak aygıt sürücü de disktedir. 
    İşte böyle bir durumda bu aygıt sürücüleri de barındıran bir RAM disk dosya sistemi oluşturulmakta ve önyükleyici 
    tarafından (örneğin GRUB önyükleyicisi) bu dosya da RAM'e yüklenmektedir. Böylece çekirdek artık diske erişebilir 
    hale gelir. Tabii bu gereklilik yalnızca diske erişim için söz konusu değildir. Diske erişmeden önce de başka aygıt 
    sürücülere gereksinim olabilmektedir. Ayrıca bazı kabuk komutlarının da çalıştırılabilmesi gerekebilmektedir. Pekiyi 
    bir Linux sistemi hiç geçici kök dosya sistemi olmadan da boot edilebilir mi? Evet teorik olarak bu mümkündür. Eğer 
    çekirdeğin gereksinim duyacağı bütün aygıt sürücü dosyaları konfigürasyon aşamasında çekirdeğin içerisine gömülmüşse 
    geçici kök dosya sistemi oluşturmadan da sistem boot edilebilir. Ancak bu sırada çözülmesi gereken problemlerle de 
    karşılaşılabilmektedir. Özellikle masaüstü sistemlerinde geçici kök dosya sistemi olmadan sistemi boot etmek oldukça 
    zahmetlidir. Geçici kök dosya sistemi aynı zamanda işletim sistemini "güvenli kipte (safe mode)" açmak için de 
    kullanılmaktadır. Geçici kök dosya sistemi sistem güncellemelerinde de kullanılmaktadır. Pek çok durumda çalışmakta 
    olan dosyalar diskte değiştirilemediği için mecburen sistem güncellemeleri geçici kök dosya sistemi yoluyla yapılmaktadır.

    "make install" komutu sırasıyla yapılanlar şunlardır:

    - Çekirdek imajı "arch/<platform>/boot/bzImage" dizininden alınarak hedef "/boot" dizinine "vmlinuz-<çekirdek_sürümü>" 
    ismiyle kopyalanır.
    - "System.map" dosyası hedef "/boot" dizinine "System.map-<çekirdek_sürümü>" ismiyle kopyalanır.
    - ".config" dosyası "/boot" dizinine "config-<çekirdek_sürümü>" ismiyle kopyalanır.
    - "Geçici kök dosya sistemine ilişkin dosyayı oluşturulur ve hedef "/boot" dizinine "initrd.img-<çekirdek_sürümü>" 
    ismiyle kopyalanır. (Aslında GRUB geçici kök dosya sistemini "update-initramfs" isimli programla oluşturmaktadır.)
    - Eğer GRUB önyükleyicisi kullanılıyorsa GRUB konfigürasyonu güncellenir ve GRUB menüsüne yeni girişler eklenir.

    Böylece sistemin otomatik olarak yeni çekirdekle açılması sağlanır.

    "make install" komutu uygulandığında eğer çekirdeğin versiyon bilgisi aynı ise "/boot" dizinindeki bir önceki kurulumun 
    dosyaları ".old" uzantısıyla saklanmaktadır. Böylece son "make install" komutundan önceki kuruluma manuel olarak geri 
    dönebilirsiniz. Örneğin yeniden aynı çekirdek versiyonunu "make install" yaptığımızda "/boot" dizininin içeriği şöyle 
    olacaktır:

    kaan@kaan-Huawei:~/Study/LinuxKernel/linux-6.9.2$ ls -l /boot
    total 655480
    -rw-r--r-- 1 root root    287375 Haz  7  2024 config-6.8.0-38-generic
    -rw-r--r-- 1 root root    287766 Ağu 15 11:57 config-6.9.2-custom
    -rw-r--r-- 1 root root    287766 Ağu 15 11:55 config-6.9.2-custom.old
    drwx------ 3 root root      4096 Oca  1  1970 efi
    drwxr-xr-x 6 root root      4096 Ağu 15 11:57 grub
    lrwxrwxrwx 1 root root        23 Ağu 10 11:20 initrd.img -> initrd.img-6.9.2-custom
    -rw-r--r-- 1 root root  73052139 Kas 19  2024 initrd.img-6.8.0-38-generic
    -rw-r--r-- 1 root root 526888758 Ağu 15 11:57 initrd.img-6.9.2-custom
    -rw------- 1 root root   9055262 Haz  7  2024 System.map-6.8.0-38-generic
    -rw-r--r-- 1 root root   8365115 Ağu 15 11:57 System.map-6.9.2-custom
    -rw-r--r-- 1 root root   8365115 Ağu 15 11:55 System.map-6.9.2-custom.old
    lrwxrwxrwx 1 root root        20 Ağu 15 11:57 vmlinuz -> vmlinuz-6.9.2-custom
    -rw-r--r-- 1 root root  14944648 Haz  7  2024 vmlinuz-6.8.0-38-generic
    -rw-r--r-- 1 root root  14819840 Ağu 15 11:57 vmlinuz-6.9.2-custom
    -rw-r--r-- 1 root root  14819840 Ağu 15 11:55 vmlinuz-6.9.2-custom.old
    lrwxrwxrwx 1 root root        24 Ağu 15 11:57 vmlinuz.old -> vmlinuz-6.9.2-custom.old
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										7. Ders 09/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz "make install" komutu ile yapılan işlemleri manuel olarak da yapabiliriz. Yukarıda da belirttiğimiz gibi derleme 
    işlemi sonucunda elde edilmiş olan dosyaların hedef sistemde bazı dizinlerde bulunuyor olması gerekir. Aslında çekirdek 
    imajı ve geçici kök dosya sistemi dosyaları default yerlerin dışında başka yerlerde de bulundurulabilir. Önyükleyiciye 
    bu konuda bilgi verilebilir. Ancak yukarıdaki dosyaların hedef sistemde bulundurulduğu default yerler şöyledir:

    - Çekirdek İmajı ---> "/boot" dizini
    - Çekirdek Sembol Tablosu ---> "/boot" dizini
    - Modül Dosyaları ---> "/lib/modules/<çekirdek_sürümü>/kernel" dizini
    - Geçici Kök Dosya Sistemi Dosyası ---> "/boot" dizinine

    Ancak yukarıdaki dosyalar dışında isteğe bağlı olarak aşağıdaki dosyalar da hedef sisteme konuşlandırılabilir:

    - Konfigürasyon Dosyası ---> "/boot" dizinine
    - Modüllere İlişkin Bazı Dosyalar ---> "/lib/modules/<çekirdek_sürümü>" dizinine

    Pekiyi yukarıda belirttiğimiz dosyalar hedef sistemdeki ilgili dizinlere hangi isimlerle kopyalanmalıdır? İşte tipik 
    isimlendirme şöyle olmalıdır (buradaki <çekirdek_sürümü> "uname -r" komutuyla elde edilecek olan yazıdır):

    - Çekirdek İmajı: "/boot/vmlinuz-<çekirdek_sürümü>". Örneğin "vmlinuz-6.9.2-custom" gibi.
    - Çekirdek Sembol Tablosu: "/boot/System.map-<çekirdek_sürümü>". Örneğin "System.map-6.9.2-custom" gibi.
    - Modüllere İlişkin Dosyalar: Bunlar yukarıda da belirttiğimiz gibi "/lib/modules/<çekirdek_sürümü>" dizininin içerisine 
    kopyalanmalıdır.
    - Konfigürasyon Dosyası: "/boot/config-<çekirdek_sürümü>". Örneğin "config-6.9.2-custom" gibi.
    - Geçici Kök Dosya Sistemine İlişkin Dosya: "/boot/initrd.img-<çekirdek_sürümü>". Örneğin "initrd.img-6.9.2-custom" gibi. 
    Bu dosyayı "update-initramfs" programıyla oluşturabilirsiniz.

    Ayrıca bazı dağıtımlarda "/boot" dizini içerisindeki "vmlinuz" dosyası default olan "vmlinuz-<çekirdek_sürümü>" dosyasına, 
    "initrd.img" dosyası da "/boot/initrd.img-<çekirdek_sürümü>" dosyasına sembolik link yapılmış durumda olabilir. Ancak bu 
    sembolik bağlantıları "GRUB" kullanmamaktadır. Aşağıda Intel sistemindeki 6.8.0 çekirdeğinin yüklü olduğu "/boot" dizininin 
    default içeriğini görüyorsunuz:

    $ ls -l /boot
    -rw-r--r-- 1 root root    287375 Haz  7  2024 config-6.8.0-38-generic
    drwx------ 3 root root      4096 Oca  1  1970 efi
    drwxr-xr-x 6 root root      4096 Ağu 15 11:57 grub
    lrwxrwxrwx 1 root root        23 Ağu 10 11:20 initrd.img -> initrd.img-6.8.0-38-generic
    -rw-r--r-- 1 root root  73052139 Kas 19  2024 initrd.img-6.8.0-38-generic
    -rw------- 1 root root   9055262 Haz  7  2024 System.map-6.8.0-38-generic
    lrwxrwxrwx 1 root root        20 Ağu 15 11:57 vmlinuz -> vmlinuz-6.8.0-38-generic
    -rw-r--r-- 1 root root  14944648 Haz  7  2024 vmlinuz-6.8.0-38-generic

    Geçici kök dosya sisteminin içerisindeki dosyalar "cpio" denilen bir arşiv formatıyla arşivlenmektedir. cpio arşivi 
    aslında tıpkı "tar" arşivinde olduğu gibi yalnızca dosyaların uç uca eklenmesiyle oluşturulmaktadır. İsterseniz geçiçi 
    kök dosya sistemine ilişkin "initrd-xxx" dosyalarını açabilirsiniz. Ancak bu dosyaların içeriği dağıtımların versiyonlarına 
    göre değişebilmektedir. Yeni dağıtımlarda bu "initrd-xxx" arşiv dosyası birkaç bölümden oluşmaktadır. Eğer biz bu dosyayı 
    "cpio" programıyla açmaya çalışırsanız bu program yalnızca ilk bölümü açacaktır. Tüm bölümleri açmak için "unmkinitramfs" 
    programından faydalanabilirsiniz. unmkinitramfs programı ile açım şöyle yapılabilir:

    unmkinitramfs /boot/initrd.img-6.9.2-custom initrd

    Bu komutla geçici kök dosya sistemine ilişkin arşiv dosyası "initrd" isimli dizinin altına açılacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi derleme sonucunda elde ettiğimiz dosyaları manuel isimlendirirken çekirdek sürüm yazısını nasıl bileceğiz? 
    Bunun için "uname -r" komutunu kullanamayız. Çünkü bu komut bize o anda çalışmakta olan çekirdeğin sürüm yazısını 
    verir. Biz yukarıdaki denemede Linux'un "6.9.2" sürümünü derledik. Bunun sonuna da "-custom" getirdik. Bu durumda 
    sürüm yazısının da "6.9.2-custom" olmasını bekleriz. Burada önemli bir uyarıda bulunmak istiyoruz. Bu sürüm yazısı 
    manuel olarak dosyaların isimlerinin değiştirilmesiyle değiştirilememektedir. Çünkü sürüm yazısı çekirdek imajının 
    içerisine de yazılmaktadır ve bizim bazı dosyalara verdiğimiz isimlerin çekirdek içerisindeki bu yazıyla uyumlu olması 
    gerekir. Default olarak "kernel.org" sitesinden indirilen kaynak kodlar derlendiğinde çekirdek sürümü "6.9.2" gibi üç 
    haneli bir sayılardan oluşmaktadır. Yani yazının sonunda "-generic" ya da "-custom" gibi sonekler yoktur. Tabii çekirdeği 
    derlemeden önce yukarıda da belirttiğimiz gibi ".config" dosyasında "CONFIG_LOCALVERSION" özelliğine bu sürüm numarasından 
    sonra eklenecek bilgiyi girebilirsiniz. Örneğin:

    CONFIG_LOCALVERSION="-custom"

    Anımsayacağınız gibi bu işlem "make menuconfig" menüsünde "General Setup/Local version - append custom release" seçeneği 
    kullanılarak da yapılabilmektedir. Biz buradaki örneğimizde bu işlemi yaparak çekirdeği derledik. Dolayısıyla bizim derlediğimiz 
    çekirdekte çekirdek imajı içerisinde yazan sürüm ismi "6.9.2-custom" biçimindedir. Pekiyi biz bu ismi unutsaydık nasıl 
    öğrenebilirdik? Bunun basit bir yolu sıkıştırılmamış çekirdek dosyası içerisindeki (kaynak kök dizindeki "vmlinux" dosyası) 
    string tablosunda "Linux version" yazısını aramaktır. Örneğin:

    $ strings vmlinux | grep "Linux version"
    Linux version 6.9.2-custom (kaan@kaan-virtual-machine) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for 
    Ubuntu) 2.38) # SMP PREEMPT_DYNAMIC
    Linux version 6.9.2-custom (kaan@kaan-virtual-machine) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for 
    Ubuntu) 2.38) #2 SMP PREEMPT_DYNAMIC Thu Dec 5 17:55:14 +03 2024

    Buradan sürüm yazısının "6.9.2-custom" olduğu görülmektedir. O halde derleme sonucunda elde ettiğimiz dosyaları manuel 
    biçimde kopyalarken sürüm bilgisi olarak "6.9.2-custom" yazısını kullanmamız gerekir. Çekirdek imajının "/boot" 
    dizinine manuel kopyalanması işlemi şöyle yapılabilir (kaynak kök dizinde bulunduğumuzu varsayıyoruz):

    $ sudo cp arch/x86_64/boot/bzImage /boot/vmlinuz-6.9.2-custom

    Konfigürasyon dosyasını da şöyle kopyalayabiliriz:

    $ sudo cp .config /boot/config-6.9.2-custom

    Tabii bizim çekirdek modüllerini de "/lib/modules/6.9.2-custom/kernel" dizinine, geçici kök dosya sistemine ilişkin 
    dosyayı da "/boot" dizinine kopyalamamız gerekir. Çekirdek modüllerinin kopyalanması biraz zahmetli bir işlemdir. 
    Çünkü bunlar derlediğimiz çekirdekte farklı dizinlerde bulunmaktadır. Bu kopyalamanın en etkin yolu "make modules_install" 
    komutunu kullanmaktır. Benzer biçimde çekirdek dosyalarının ve gerekli diğer dosyaların uygun yerlere kopyalanması 
    için de en etkin yöntem "make install" komutudur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Normal olarak biz "make install" yaptığımızda eğer sistemimizde GRUB önyükleyicisi varsa komut GRUB konfigürasyon 
    dosyalarında da güncellemeler yaparak sistemin yeni çekirdekle açılmasını sağlamaktadır. Böylece kullanıcı bir menü 
    yoluyla sistemin kendi istediği çekirdekle açılmasını da sağlayabilmektedir. GRUB menüsü otomatik olarak görüntülenmemektedir. 
    Boot işlemi sırasında ESC tuşuna basılırsa menü görüntülenir. Eğer GRUB menüsünün her zaman görüntülenmesi isteniyorsa 
    "/etc/default/grub" dosyasındaki iki satır aşağıdaki gibi değiştirilmelidir:

    GRUB_TIMEOUT_STYLE=menu
    GRUB_TIMEOUT=5

    Buradaki GRUB_TIMEOUT satırı eğer müdahale yapılmamışsa menünün en fazla 5 saniye görüntüleneceğini belirtmektedir.

    Bu işlemden sonra "update-grub" programı da çalıştırılmalıdır:

    $ sudo update-grub

    Bu tür denemeler yapılırken GRUB menüleri bozulabilmektedir. Düzeltme işlemleri bazı konfigürasyon dosyalarının edit 
    edilmesiyle manuel biçimde yapılabilir. Konfigürasyon dosyaları güncellendikten sonra "update-grub" programı mutlaka 
    çalıştırılmalıdır. Ancak eğer GRUB konfigürasyon dosyaları konusunda yeterli bilgiye sahip değilseniz GRUB işlemlerini 
    görsel bir biçimde "grub-customizer" isimli programla da yapabilirsiniz. Bu program "debian depolarında" olmadığı için 
    önce aşağıdaki gibi programın bulunduğu yerin "apt" kayıtlarına eklenmesi gerekmektedir:

    $ sudo add-apt-repository ppa:danielrichter2007/grub-customizer
    $ sudo apt-get update
    $ sudo apt-get install grub-customizer

    Bu işlemden sonra kurulum yapılabilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıda çekirdek derleme ve yeni çekirdeği kurma sürecini maddeler halinde açıkladık. Şimdi yukarıdaki adımları 
    özet haline getirelim:

    1) Çekirdek derlemesi için gerekli olan araçlar kurulur.

    2) Çekirdek kodları indirilir ve açılır.

    3) Zaten hazır olan konfigürasyon dosyası "/boot" dizininden alınarak ".config" ismiyle kaynak kök dizine kopyalanır.

    4) Konfigürasyon dosyası üzerinde "make menuconfig" komutu ile değişiklikler yapılır.

    5) Eğer çekirdeğin imzalanması istenmiyorsa yukarıda belirtildiği gibi ".config" dosyasındaki bazı satırlar üzerinde 
    değişiklikler yapılır.

    6) Çekirdek derlemesi "make -j$(nproc)" komutu ile gerçekleştirilir.

    7) Modüller ve ilgili dosyalar hedefe "sudo make modules_install" komutu ile konuşlandırılır.

    8) Çekirdek imajı ve ilgili dosyalar "sudo make install" komutu ile hedefe konuşlandırılır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi yeni çekirdeği derleyip sisteme dahil ettikten sonra nasıl onu sistemden tamamen çıkartabiliriz? Tabii yapılan 
    işlemlerin tersini yapmak gerekir. Kaldırma işlemi manuel biçimde şöyle yapılabilir:

    - "/lib/modules/<çekirdek_sürümü>" dizini tamamen silinir.
    - "/boot" dizinindeki çekirdek sürümüne ilişkin dosyalar silinir.
    - "/boot" dizininden çekirdek sürümüne ilişkin dosyalar silindikten sonra "update-grub" programı sudo ile çalıştırılmalıdır.
    Bu program "/boot" dizinini inceleyip otomatik olarak ilgili girişleri GRUB menüsünden siler. Yani aslında GRUB 
    konfigürasyon dosyaları üzerinde manuel değişiklik yapmaya gerek yoktur. GRUB işlemleri için diğer bir alternatif 
    ise "grub-customizer" programı ile görsel silme yapmaktır. Ancak bu program "/boot" dizini içerisindeki dosyaları 
    ve modül dosyalarını silmez. Yalnızca ilgili girişleri GRUB menüsünden çıkartmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										8. Ders 10/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeği yeniden derlemenin gerekçelerinden bahsetmiştik. Bunlardan biri de çekirdek kodları üzerinde değişikliklerin 
    yapılmış olmasıydı. Pekiyi çekirdek kodları üzerinde değişiklikler nasıl yapılabilir? Çekirdek kodları üzerinde değişiklikler 
    tipik olarak dört yolla yapılmaktadır:

    1) Çekirdek kodlarındaki bir dosya içerisinde bulunan fonksiyon kodlarında değişiklik yapılması.
    2) Çekirdek kodlarındaki bir dosya içerisine yeni bir fonksiyon eklenmesi.
    3) Çekirdek kodlarındaki bir dizin içerisine yeni bir C kaynak dosyası eklenmesi.
    4) Çekirdek kodlarındaki bir dizin içerisine yeni bir dizin ve bu dizinin içerisine de çok sayıda C kaynak dosyalarının 
    eklenmesi.

    Eğer biz birinci maddedeki ve ikinci maddedeki gibi çekirdek kodlarına yeni bir dosya eklemiyorsak çekirdeğin derlenmesini 
    sağlayan make dosyalarında bir değişiklik yapmamıza gerek yoktur. Ancak çekirdeğe yeni bir kaynak dosya ya da dizin 
    ekleyeceksek bu eklemeyi yaptığımız dizindeki make dosyasında izleyen paragraflarda açıklayacağımız biçimde bazı 
    güncellemelerin yapılması gerekir. Böylece çekirdek yeniden derlendiğinde bu dosyalar da çekirdek imajının içerisine 
    eklenmiş olacaktır. Eğer kaynak kod ağacında bir dizinin altına yeni bir dizin eklemek istiyorsak bu durumda o dizini 
    yine üst dizine ilişkin make dosyasında belirtmemiz ve o dizinde ayrı bir Makefile oluşturmamız gerekir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    GNU Make aracı oldukça ayrıntılı özelliklere sahip bir build aracıdır. Bu aracın ayrıntılarını öğrenmek ayrı bir 
    çabayı gerektirmektedir. Make dili aslında oldukça aşağı seviyeli bir build dilidir. Bu nedenle özellikle son yirmi 
    yıldır programcılar doğrudan GNU Make aracını kullanmak yerine daha üst düzey make araçlarını kullanmayı tercih 
    etmektedir. Bunlardan en yaygın olanlardan biri CMake denilen araçtır. Microsoft MSBuild isimli kendi tasarladığı 
    build aracını kullanmaktadır.

    Make dilinde değişkenler oluşturulabilmektedir. Örneğin:

    obj-y = a.o

    Burada obj-y isimli değişken a.o bilgisini tutmaktadır. Değişkenleri bir çeşit makro gibi düşünebilirsiniz. Bir 
    değişkene ekleme yapmak için Make dilinde += operatörü kullanılmaktadır. Örneğin:

    obj-y = a.o
    obj-y += b.o
    obj-y += c.o

    Burada artık obj-y değişkeni a.o b.o c.o biçiminde olacaktır.

    Linux çekirdeğinde özyinelemeli bir make yöntemi kullanılmaktadır. Her dizinde bir Makefile dosyası vardır. Bunun 
    içerisindeki obj-y gibi, obj-m gibi bazı değişkenler += operatörüyle eklenerek biriktirilmektedir. Bunlar da derleme 
    ve bağlama işlemine sokulmaktadır. Yukarıda da belirttiğimiz gibi Linux'taki bu build sistemine KBuild ya da KConfig 
    sistemi denilmektedir.

    Bizim Linux'ta Makefile dosyaları üzerinde gerekli güncellemeleri yapmak için çok fazla bilgiye sahip olmamız gerekmez. 
    Bazı yönergeleri uygun bir biçimde yerine getirirsek hedefimize ulaşabiliriz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux kaynak kod ağacında dizinlerin altında "Makefile" isimli make dosyaları bulunur. Eğer bir dizinin altına yeni 
    bir dosya eklenecekse o dizinin içerisinde bulunan Makefile dosyasının içerisine aşağıdaki gibi bir satırın eklenmesi 
    gerekir:

    obj-y += dosya_ismi.o

    Buradaki += operatörü obj-y isimli hedefe ekleme yapma anlamına gelmektedir. "obj" sözcüğünün yanındaki "-y" harfi 
    ilgili dosyanın çekirdeğin bir parçası biçiminde çekirdek imajının içerisine gömüleceğini belirtmektedir. Make dosyalarının 
    bazı satırlarında "obj-y" yerine "obj-m" de görebilirsiniz. Bu da ilgili dosyanın ayrı bir modül biçiminde derleneceği 
    anlamına gelmektedir. Eklemeler genellikle çekirdek imajının içine yapıldığı için biz de genellikle "obj-y" kullanırız. 
    Eğer bir dosyanın (aygıt sürücüler için bu durum söz konusudur) çekirdek imajının içine gömülmesi yerine ayrı bir çekirdek 
    modülü olarak derlenmesi isteniyorsa bu durumda dosyanın yerleştirildiği dizinin "Makefile" dosyasına aşağıdaki gibi bir 
    eklemenin yapılması gerekir:

    obj-m += dosya_ismi.o

    Eğer çekirdek kaynak kodlarına tümden bir dizin eklemek isteniyorsa bu durumda önce o dizininin oluşturulduğu dizindeki 
    "Makefile" dosyasına aşağıdaki gibi bir ekleme yapılmalıdır:

    obj-y += dizin_ismi/

    Burada dizin isminden sonra '/' karakterini unutmayınız. Tabii bu ekleme bir modül biçiminde de olabilirdi:

    obj-m += dizin_ismi/

    Fakat bu ekleme tek başına yetmemektedir. Bu ekleme yapıldıktan sonra ayrıca yaratılan dizinde "Makefile" isimli 
    bir dosyanın oluşturulması ve o dosyanın içerisinde o dizindeki kaynak dosyaların da belirtilmesi gerekmektedir. 
    Örneğin biz "drivers" dizininin altında "mydriver" isimli bir dizin oluşturup onun da içerisine "a.c" "b.c" ve "c.c" 
    dosyalarını eklemiş olalım. Bu durumda önce "drivers" dizini içerisindeki "Makefile" dosyasına aşağıdaki gibi bir 
    satır ekleriz:

    obj-y += mydriver/

    Sonra da "mydriver" dizini içerisinde "Makefile" isimli bir dosya oluşturup bu dosyanın içerisinde de dizin içerisindeki 
    dosyaları belirtiriz. Örneğin:

    obj-y += a.o
    obj-y += b.o
    obj-y += c.o
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kaynak kod ağacında Makefile dosyasının dışında build sistemiyle ilgili "Kconfig" isimli dosyalar da bulunmaktadır. 
    Bu dosyaların içerisinde ilgili dosyaların ya da dizinlerin "konfigürasyon dosyasına" yansıtılması için gerekli 
    bilgiler bulundurulmaktadır. Örneğin biz eklediğimiz "mydriver" dizinindeki dosyaların çekirdek kodlarına dahil edilip 
    edilmeyeceğini çekirdeği derleyenin konfigürasyon aşamasında belirlemesini sağlayabiliriz. Bunun için bu "Kconfig" 
    dosyasına bir giriş eklememiz gerekir. Böylece bu giriş de "menuconfig" yapıldığında bir seçenek olarak karşımıza 
    gelecektir. Tabii ekleyeceğimiz dosya ve dizinleri "Kconfig" dosyasında belirtmek zorunda değiliz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    "menuconfig" menüsünde seçenekler için birkaç seçme biçimi bulunmaktadır. Eğer seçenekte [] varsa bu seçenek "seçilebilir 
    ya da seçilmeyebilir" anlamına gelmektedir. Eğer bu seçenek seçilirse köşeli parantez içerisinde bir * karakteri 
    gösterilmektedir. Eğer ilgili seçenekte <> varsa bu açısal parantezlerin içersinde M karakteri ya da * getirilebilmekte 
    ya da bunun içi boş bırakılabilmektedir. Bu açısal parantezli seçeneklere "üç konumlu seçenekler" de denilmektedir. 
    Eğer ilgili seçenekte -*- varsa bu seçenek için seçilememezlik yapılamaz. Yani mutlaka çekirdek kodlarında bu seçeneğin 
    bulunması gerekir. Tabii menuconfig menüsünde yapılan her şey aslında ".config" dosyasına yansıtılmaktadır.

    []      ---> seçilebilir ya da seçilmeyebilir
    <?>     ---> üç konumlu, seçilebilir, seçilmeyebilir ya da 'M' olarak belirtilebilir
    -*-     ---> seçilememezlik yapılamaz
    değer   ---> ilgili özellik

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi çekirdeğin konfigüre edilmesi aşamasında "menuconfig" işleminde belirlediğimiz seçenekler kaynak kodlara nasıl 
    yansıtılmaktadır? Örneğin biz "menuconfig" işleminde bir modülün çekirdek kodlarına dahil edilmesini ilgili girişi "*" 
    ile seçerek sağlayabilmekteyiz. Benzer biçimde biz konfigürasyon aşamasında bazı çekirdek parametrelerini de değiştirebilmekteyiz. 
    Örneğin "timer tick" frekansı "menuconfig" menüsünde bir sayı biçiminde belirlenebilmektedir. Pekiyi buradaki belirlemeler 
    çekirdek kodlarına ve build sistemine nasıl yansıtılmaktadır?

    Anımsanacağı gibi "menuconfig" ve diğer config menülerinde yapılan seçimler daha önce de belirttiğimiz gibi ".config" 
    isimli bir text dosyaya save edilmektedir. Bu ".config" dosyası "özellik=değer" biçiminde satırlardan oluşmaktadır. 
    Aşağıda dosyanın birkaç satırını görüyorsunuz:

    ...
    CONFIG_CC_HAS_ASM_INLINE=y
    CONFIG_CC_HAS_NO_PROFILE_FN_ATTR=y
    CONFIG_PAHOLE_VERSION=125
    CONFIG_IRQ_WORK=y
    CONFIG_BUILDTIME_TABLE_SORT=y
    CONFIG_THREAD_INFO_IN_TASK=y
    ...

    Çekirdek derlenirken ilk aşamada bu ".config" dosyasının içeriği "include/generated/autoconf.h" dosyasının içerisine 
    #define önişlemci komutları biçiminde aktarılmaktadır. İşte eğer ilgili konfigürasyon dosyasındaki değer "y" ya da "m" 
    ise bu "autoconf.h" dosyası içerisinde buna ilişkin sembolik sabit 1 olarak görünür. (Başka bir deyişle "menuconfig" 
    menüsünde [*] ya da <*> ya da <M> seçenekleri için sembolik sabit 1 olur.) Eğer konfigürasyon dosyasında ilgili seçenek 
    gerçekten bir değer belirtiyorsa "autoconf.h" dosyası içerisinde bu sembolik sabit o değerde olur. Eğer konfigürasyon 
    dosyasında ilgili seçenek "n" biçiminde seçilmişse (yani "menuconfig" menüsünde ilgili seçenek [] ya da <> biçiminde 
    seçilmişse) bu durumda ilgili sembolik sabit hiç define edilmemiş hale gelir. Özetle aslında ".config" dosyası içerisindeki 
    satırlardan C dilinde anlamlı #define önişlemci komutları oluşturulmaktadır. Aşağıda üretilmiş olan "autoconf.h" dosyasının 
    birkaç satırını görüyorsunuz:

    ...
    #define CONFIG_IGB_HWMON 1
    #define CONFIG_ACPI_HOTPLUG_CPU 1
    #define CONFIG_DEV_DAX_KMEM_MODULE 1
    #define CONFIG_RIONET_RX_SIZE 128
    #define CONFIG_USB_SERIAL_KEYSPAN_PDA_MODULE 1
    #define CONFIG_BOOTTIME_TRACING 1
    ...

    Çekirdeğe ilişkin bir C kodu içerisinde "ilgili seçenek seçilmişse" bir kod parçasını derlemeye dahil etmek için 
    #ifdef önişlemci komutundan faydalanabilirsiniz. Örneğin:

    #ifdef CONFIG_XXX
    ...
    #endif

    Tabii üretilen bu "autoconf.h" dosyası çekirdek kaynak kodlarındaki çeşitli include dosyalarında doğrudan ya da 
    dolaylı bir biçimde include edilmiş durumdadır. Linux kaynak kodları da bu sembolik sabitleri kullanacak biçimde 
    yazılmıştır. Tabii Linux'un kaynak kodlarında "autoconf.h" dosyasını bulamazsınız. Çünkü bu dosya make işlemi 
    sırasında oluşturulmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki paragrafta ".config" dosyasındaki konfigürasyon parametrelerinin nasıl C'ye sembolik sabitler biçiminde 
    yansıtıldığını gördük. Pekiyi bu konfigürasyon seçenekleri "Makefile" dosyalarına nasıl yansıtılmaktadır? Aşağıda 
    çekirdeğin bir "Makefile" içeriğini görüyorsunuz:

    ...
    obj-$(CONFIG_I8254)             += i8254.o
    obj-$(CONFIG_104_QUAD_8)        += 104-quad-8.o
    obj-$(CONFIG_INTERRUPT_CNT)     += interrupt-cnt.o
    obj-$(CONFIG_RZ_MTU3_CNT)       += rz-mtu3-cnt.o
    obj-$(CONFIG_STM32_TIMER_CNT)   += stm32-timer-cnt.o
    ...

    Bu satırlar aslında "ilgili konfigürasyon seçenekleri seçilmişse ilgili dosyaların derlemeye dahil edileceğini" belirtmektedir.

    İşte "KBuild" sistemi aynı zamanda bu ".config" dosyasından hareketle make programı için anlamlı olan değişkenler de 
    oluşturmaktadır. Bu değişkenleri yukarıda açıkladığımız sembolik sabitlerle karıştırmayınız. Bu değişkenler make dili 
    için anlamlı olan make dilinin değişkenleridir. Örneğin eğer ".config" dosyasında bir seçenek "y" olarak belirtilmişse 
    (başka bir deyişle "menuconfig" menüsünde seçenek [*] ya <*> biçiminde seçilmişse bu konfigürasyon seçeneği için make 
    dilinde "y" değeri, eğer "m" olarak belirtilmişse de (başka bir deyişle "menuconfig" menüsünde seçenek <M> biçiminde 
    seçilmişse) "m" değeri oluşturulmaktadır. Böylece aslında yukarıdaki make satırları ilgili seçenek "y" olarak seçilmişse 
    "obj-y" biçimine, "m" olarak olarak seçilmişse "obj-m" biçimine dönüştürülmektedir. Örneğin:

    obj-$(CONFIG_I8254)         += i8254.o

    Burada CONFIG_I8254 seçeneği "y" ise ilgili dosya obj-y değişkenine dahil olacaktır. Yani çekirdeğin içerisinde bulunacaktır. 
    Ancak bu CONFIG_I8254 seçeneği "m" ise ilgili dosya obj-m değişkenine dahil olacaktır. Eğer bu seçenek "n" ise (yani hiç seçilmemişse) 
    çekirdek build sistemi ya bu CONFIG_I8254 değişkenini hiç tanımlamamakta ya da bunu "n" olarak tanımlamaktadır. Her iki 
    durumda da artık bu dosya herhangi bir biçimde derleyemeye dahil edilmeyecektir.

    Çekirdeğe birtakım kodlar ekleyenler eğer eklemeleri "Kconfig" dosyası yoluyla konfigürasyona yansıtmışlarsa bu durumda 
    kendi "Makefile" dosyasına bu eklemeleri yukarıdaki gibi girebilirler. Örneğin biz "mymodule" ile temsil ettiğimiz bir 
    modül dosyası oluşturup bu modül dosyasının çekirdek kodlarına eklenip eklenmeyeceğini konfigürasyonda "Kconfig" dosyası 
    yoluyla belirtebiliriz. Bu durumda "Makefile" içerisindeki girişi aşağıdaki gibi de oluşturabiliriz:

    obj-$(CONFIG_MYMODULE) += mymodule.o

    Görüldüğü gibi burada aslında konfigüre eden nasıl seçmişse biz onun seçimini yansıtmış olmaktayız. ".config" dosyasında 
    bir özellik "no" ise bu durumda ilgili "Makefile" satırı hale gelecektir:

    obj-n += mymodule.o

    obj-n biçiminde bir birikim yapılmadığı için zaten bu satır derleme aşamasında dikkate alınmayacaktır. Ancak bazen 
    sistem programcıları "y" durumu için aşağıdaki gibi bir kontrol ile modülü koşullu bir biçimde de derleme sürecine 
    ekleyebilmektedir:

    ifeq ($(CONFIG_MYSYSCALL), y)
        obj-y += mysyscall.o
    endif

    Burada eğer konfigürasyon yapılırken ilgili seçenek "y" biçiminde (yani [*] ya da <*> biçiminde) geçilmişse bu durumda 
    biz de ilgili dosyayı derlemeye dahil etmiş olduk.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										9. Ders 16/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir C dosyasını ya da dizini çekirdek kodlarına ekledikten sonra onun konfigürasyon sırasında (örneğin "make menuconfig" 
    işlemi sırasında) sırasında görünebilirliğini sağlamak için "Kconfig" dosyalarının kullanıldığını belirtmiştik. Yani 
    "Kconfig" dosyaları yaptığımız değişikliklerin konfigüre edilebilirliğini sağlamak için kullanılmaktadır. "Kconfig" 
    dosyalarının genel formatı için aşağıdaki bağlantıya başvurabilirsiniz:

    https://docs.kernel.org/kbuild/kconfig-language.html

    "Kconfig" dosyaları tıpkı "Makefile" dosyalarında olduğu gibi özyinelemeli biçimde işletilmektedir. Yani biz çekirdek 
    kaynak kod ağacında bir dizin yaratmayıp zaten var olan bir dizinin içerisine bir ".c" dosyası yerleştiriyorsak "Makefile" 
    ve "Kconfig" dosyaları oluşturmamıza gerek yoktur. Gerekli işlemleri zaten dizin içerisinde var olan bir "Makefile" ve 
    "Kconfig" dosyaları üzerinde yapabiliriz. Ancak eğer biz bir dizin oluşturup onun içerisine dosyalar yerleştireceksek 
    o dizin için bir tane "Makefile" ve bir tane de "Kconfig" dosyası oluşturmamız gerekir.

    Önceki paragrafta Linux kaynak kod ağacında bir dizin yaratıp onun içerisine dosyalar yerleştirirken o dizin için "Makefile" 
    ve "Kconfig" dosyalarının yazılması gerektiğini belirtmiştik. (Tabii aslında "Kconfig" dosyasının bulundurulması zorunlu 
    değildir. Ancak eklenen özelliğin konfigüre edilebilirliğinin sağlanması için gerekmektedir.) Bu dosyalar oluşturulduktan 
    sonra dış dizindeki "Makefile" ve "Kconfig" dosyalarında aşağıda belirtilen işlemler de yapılmalıdır:

    1) Daha önceden de belirttiğimiz gibi dış dizindeki "Makefile" dosyasında alt dizinin dikkate alınacağı aşağıdaki gibi 
    bir satırla belirtilmelidir:

    obj-y += <dizin_ismi>/

    2) Dış dizinin "Kconfig" dosyasında iç dizindeki "Kconfig" dosyasının dikkate alınması aşağıdaki gibi bir satırın 
    eklenmesiyle sağlanmaktadır:

    source "kaynak_kod_ağacının_köküne_göreli_yol_ifadesi"

    Örneğin:

    source "drivers/mydriver/Kconfig"

    Buradaki yol ifadesi çekirdek kodlarının kök dizinine göreli olmalıdır.

    Biz "Kconfig" dosyasına yukarıdaki gibi bir giriş yerleştirdiğimizde artık "make menuconfig" gibi konfigürasyon menülerinde 
    eklediğimiz "Kconfig" elemanı bir menü seçeneği biçiminde karşımıza çıkacaktır.

    Örneğin biz bir aygıt sürücümüzün dosyalarını çekirdeğin kaynak kod ağacında "drivers" dizininin altına "mydriver" dizinini 
    açarak eklemek isteyelim. Bu durumda şunları yapmamız gerekir:

    1) "drivers" dizini içerisinde "mydriver" dizinini yaratıp içerisine "mydriver.c" dosyasını (belki de "mydriver.h" gibi 
    bir başlık dosyasını da) yerleştirmeliyiz.

    2) "drivers/mydriver" dizininde aşağıdaki gibi bir "Kconfig" dosyasını oluşturmalıyız:

    config MYDRIVER
    tristate "My Character Device Driver"
    default y
    help
    Enable this option to include support for My Device Driver.
    It can either be built as a module or statically linked into the kernel.

    Buradaki "config MYDRIVER" satırı aslında make dilinde CONFIG_MYDRIVER değişkeninin oluşturulmasına yol açmaktadır.

    3) Üst dizindeki ("drivers" dizinindeki) "Kconfig" dosyasına aşağıdaki satırı yerleştirmeliyiz:

    source "drivers/mydriver/Kconfig"

    4) "drivers/mydriver" dizinindeki "Makefile" dosyası içerisine aşağıdaki gibi bir satır eklemeliyiz:

    obj-$(CONFIG_MYDRIVER) += mydriver.o

    5) Üst dizindeki (yani "drivers" dizinindeki) "Makefile" içerisine aşağıdaki gibi bir satır eklemeliyiz:

    obj-$(CONFIG_MyDRIVER) += mydriver/
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi çekirdeğe bazı kodlar ekleyip onu yeniden derleyerek bir deneme yapalım. Örneğin çekirdeğe yeni bir çekirdek 
    modülü ekleyelim ve çekirdeğin o modül gömülü olarak başlatılmasını sağlayalım. Ancak burada biz aynı zamanda bu 
    çekirdek modülünün "make menuconfig" ile seçilebilmesini de sağlayalım. Çekirdek modüllerinin nasıl yazılacağını 
    bilmediğinizi varsayıyoruz. Ancak biz yine de örneğimizde "hiçbir şey yapmayan iskelet bir çekirdek modülü" oluşturacağız. 
    Bu işlem şu adımlardan geçilerek yapılabilir (kaynak kod ağacının kök dizininde bulunduğumuzu varsayıyoruz):

    1) "drivers/mydriver" dizini yaratılır.

    2) İskelet bir çekirdek modülü "mydriver.c" biçiminde "drivers/mydriver" dizininde aşağıdaki gibi oluşturulur:

    /* mydriver.c */

    #include <linux/module.h>
    #include <linux/kernel.h>

    MODULE_LICENSE("GPL");
    MODULE_AUTHOR("Kaan Aslan");
    MODULE_DESCRIPTION("General Device Driver");

    static int __init mydriver_init(void)
    {
        printk(KERN_INFO "Hello World...\n");

        return 0;
    }

    static void __exit mydriver_exit(void)
    {
        printk(KERN_INFO "Goodbye World...\n");
    }

    module_init(mydriver_init);
    module_exit(mydriver_exit);

    3) "drivers/mydriver" dizininde "Kconfig" dosyası aşağıdaki gibi oluşturulmalıdır:

    config MYDRIVER
    tristate "My Character Device Driver"
    default y
    help
      Enable this option to include support for My Device Driver.
      It can either be built as a module or statically linked into the kernel.

    Burada konfigürasyon makrosunun ismi CONFIG_MYDRIVER biçiminde olacaktır.

    4) Üst dizinin (yani "drivers" dizininin) "Kconfig" dosyasına aşağıdaki ekleme yapılmalıdır:

    source "drivers/mydriver/Kconfig"

    5) "drivers/mydriver" dizininde aşağıdaki içeriğe sahip bir "Makefile" dosyası oluşturulmalıdır:

    obj-$(CONFIG_MYDRIVER) += my_driver.o

    6) Üst dizindeki ("drivers" dizinindeki) "Makefile" dosyasına aşağıdaki satır eklenmelidir:

    obj-$(CONFIG_MYDRIVER) += mydriver/

    Artık çekirdeği derleyebiliriz. "menuconfig" menüsünde kendi aygıt sürücümüze ilişkin seçenek de çıkacaktır.

    Çekirdek imzalamasını devre dışı bırakmak için konfigürasyon dosyasındaki satırlarda aşağıdaki değişiklikleri yapmayı 
    unutmayınız:

    CONFIG_SYSTEM_TRUSTED_KEYS=""
    CONFIG_SYSTEM_REVOCATION_KEYS=""
    CONFIG_SYSTEM_TRUSTED_KEYRING=n
    CONFIG_SECONDARY_TRUSTED_KEYRING=n

    CONFIG_MODULE_SIG=n
    CONFIG_MODULE_SIG_ALL=n
    CONFIG_MODULE_SIG_KEY=""

    Yeni çekirdeğimize "-custom" ismini de ekleyebiliriz. Daha önceden de belirttiğimiz gibi eğer çekirdeğin eski versiyonundan 
    konfigürasyon dosyası alınacaksa "make oldconfig" uygulanıp o versiyondan sonra eklenmiş olan özelliklerin gözden geçirilmesi 
    sağlanmalıdır. Ancak "make menuconfig" işlemi zaten "make oldconfig" işlemini de içermektedir.

    7) Artık çekirdek derlemesi aşağıdaki gibi yapılabilir:

    $ make -j$(nproc)

    8) Derleme işlemi bittikten sonra önce çekirdek modüllerini "sudo make modules_install" ile sonra da çekirdeğin kendisini 
    "sudo make install" ile install edebilirsiniz:

    $ sudo make modules_install
    ...
    $ sudo make install
    ...

    Anımsanacağı gibi "make install" komutu artık sistemin yeni çekirdekle açılmasını sağlayacaktır. "make install" aynı 
    zamanda geçici kök dosya sistemini "update-initramfs" komutu ile oluşturup "/boot" dizinine yerleştirmektedir. Tabii 
    "update-initramfs" programını siz de gerektiğinde kullanabilirsiniz. Programın tipik kullanımı şöyledir:

    $ sudo update-initramfs -c -k <çekirdek_sürümü>

    Buradaki "çekirdek_sürümü" yalnızca çekirdeğin numarasını değil ona verdiğiniz ekleri de içermelidir. (Örneğin 
    "6.9.2-custom" gibi.) Bu komut geçici kök dosya sistemini o anda çalışmakta olan sistemin konfigürasyonunu da dikkate 
    alarak oluşturur ve "/boot" dizinine kopyalar. Yukarıda da belirttiğimiz gibi "make install" zaten bu programı 
    çalıştırarak geçici kök dosya sistemini "/boot" dizininde oluşturmaktadır.

    Pekiyi çekirdeğin kaynak kodlarına yaptığımız eklemenin gerçekten yapılmış olduğunu nasıl anlayabiliriz? Bizim 
    yazdığımız iskelet aygıt sürücü kodlarında çekirdek aygıt sürücümüz yüklediğinde "mydriver_init" fonksiyonu çağrılacaktır. 
    Bu fonksiyonun içinde de printk isimli çekirdek fonksiyonu ile biz bir log mesajı yazdırdık. Bu log mesajları "kernel 
    ring buffer" denilen bir kuyruk sistemine yazılmaktadır. "dmesg" komutuyla bu kuyruk sistemi görüntülenebilir. Eğer 
    "dmesg" yaptığımızda biz bu mesajları görürsek aygıt sürücümüzün yüklenmiş olduğu sonucunu çıkartabiliriz. Örneğin:

    $ dmesg | grep "Hello"
    Hello World...

    Çekirdeğe gömülü olan modüller "/proc/modules" dosyasında görünmezler, dolayısıyla da "lsmod" komutu ile de bunları 
    göremeyiz. Bunlar için "/sys/module" dizininde de bir giriş oluşturulmamaktadır. "modinfo" komutu ise çekirdeğe 
    ilişkin bazı dosyalara da baktığı için bize bu konuda bilgi verebilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi çekirdek kodlarında küçük değişiklikler yaptıktan sonra yeniden "make modules_install" ve "make install" 
    işlemlerine gerek var mı? Aslında küçük değişiklikler için bu işlemler yapılmazsa genellikle bir sorun ortaya çıkmaz. 
    Yeni oluşturulan çekirdek imajı doğrudan eskisinin üzerine kopyalanabilir. Ancak değişikliğin yerine ve kapsamına göre 
    çekirdeğin sembol tabloları değişebileceği için genel olarak her derlemeden sonra "make install" yapabilirsiniz. 
    "drivers" dizininde "obj-m" biçiminde değişiklikler yapılmışsa "make modules_install" yapılmalıdır. Yukarıdaki örnekte 
    biz "drivers" dizininin içerisine "obj-y" ile eklemeler yaptık. Bu durumda aslında "make modules_install" yapmaya 
    gerek yoktur. Ancak aygıt sürücüler "obj-m" biçiminde ekleniyorsa make modules_install" komutu uygulanmalıdır. Çekirdeğin 
    modüllerle ilgili olmayan kısımlarında yapılan değişikler için "make modules_install" yapılmasına gerek olmadığını 
    bir kez daha belirtmek istiyoruz. "make modules_install" işleminden önce eski "/lib/modules/<çekirdek_sürümü>" dizinini 
    de "rm -r" komutu ile silmek daha güvenli bir yaklaşımdır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki çekirdek derlemesi sürecinde imzalama (signing) işlemlerini devre dışı bırakmıştık. Çekirdek kodları 
    ve özellikle de aygıt sürücüler belli imzalara sahip olacak biçimde derlenebilmektedir. Böylece onlar üzerinde birtakım 
    istenmeyen değişikliklerin yapılarak değiştirilmiş çekirdek ya da aygıt sürücülerin yüklenmesi engellenmiş olur. 
    Yukarıda da gördüğünüz gibi çekirdek kodları ve aygıt sürücülerde bu imzalama işlemi devre dışı da bırakılabilmektedir. 
    Ancak imzalama süreci sistem güvenliğini artırmaktadır. Bu tür imzalama işlemleri yalnızca Linux sistemlerinde değil 
    diğer UNIX türevi sistemlerde, Windows ve macOS sistemlerinde de bulunmaktadır.

    Çekirdeğin imza kontrolü temel olarak UEFI BIOS (eğer "secure boot" seçeneği aktif ise) ve önyükleyiciler (örneğin GRUB) 
    tarafından yapılmaktadır. Ancak Linux çekirdeği de aygıt sürücüler ve modüller yüklenirken imza kontrolü uygulayabilmektedir. 
    Biz burada çekirdek ve modül imzalamalarının nasıl yapılacağı üzerinde duracağız.

    İmzalama işlemi tipik olarak şu adımlardan geçilerek yapılmaktadır:

    1) İmzalama işlemi için öncelikle "openssl" kütüphanesinin yüklenmiş olması gerekir. Yükleme işlemi aşağıdaki gibi 
    yapılabilir:

    $ sudo apt-get install openssl

    Daha sonra "openssl" programı ile aşağıdaki gibi bir ".pem" dosyası üretilir:

    $ openssl req -new -x509 -newkey rsa:2048 -sha256 \
    -keyout signing_key.key -out certs/signing_key.crt \
    -nodes -days 36500 -subj "/CN=Local Kernel Module Key/"

    Üretilen ".pem" dosyası genellikle kaynak kod ağacında "certs" isimli bir dizine yerleştirilir:

    $ mkdir -p certs
    $ mv signing_key.pem /certs

    Daha sonra konfigürasyon dosyasında imzalama için aşağıdaki değişiklikler yapılmalıdır:

    CONFIG_MODULE_SIG=y
    CONFIG_MODULE_SIG_ALL=y
    CONFIG_MODULE_SIG_SHA256=y
    CONFIG_SYSTEM_TRUSTED_KEYRING=y
    CONFIG_MODULE_SIG_KEY="certs/signing_key.pem"
    CONFIG_SYSTEM_TRUSTED_KEYS="certs/signing_key.pem"

    Biz yukarıdaki işlemleri yaptığımızda yalnızca aygıt sürücüleri imzalamış oluruz. Çekirdeğin kendisinin imzalanması 
    ayrıca yapılmalıdır. Yukarıda da belirttiğimiz gibi UEFI BIOS'lar ve GRUB gibi önyükleyiciler çekirdek imajını 
    yüklemeden önce ayarları uygun biçime getirildiyse çekirdek imzasına bakmaktadır. Eğer çekirdek imzası yanlışsa 
    (çekirdek dışarıdan kasti ya da yanlışlıkla bozulmuş olabilir) çekirdeği hiç yüklememektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										10. Ders 17/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin imzalanması şöyle yapılabilir:

    - Oluşturmuş olduğumuz "signing_key.pem" dosyasından private ve public key ayrıştırılır:

    $ openssl x509 -in signing_key.pem -outform DER -out db.crt
    $ openssl rsa -in signing_key.pem -outform PEM -out db.key

    - Sonra da çekirdek sbsign programı ile aşağıdaki gibi imzalanır:

    $ sbsign --key db.key --cert db.crt --output vmlinuz.signed vmlinuz-6.9.2-custom

    Genellikle bu biçimde bir çekirdek imzalaması seyrek olarak yapılmaktadır. Önceki paragrafta biz aygıt sürücü 
    dosyalarının imzalandığını belirtmiştik. Aynı makinede aygıt sürücüyü derlerken (build ederken) oluşturulan imza 
    bilgisi de kullanılmaktadır. Yani biz aynı makinede bir aygıt sürücü derlediğimizde aygıt sürücümüz de zaten imzalanmış 
    olacaktır.

    Pekiyi aygıt sürücüler yukarıdaki gibi imzalandıktan sonra ya biz başka bir makinede oluşturulmuş olan ya da başka 
    bir uygulamanın oluşturmuş olduğu aygıt sürücüleri yüklemek istersek ne olacaktır? Normal olarak o aygıt sürücüler 
    bizim ürettiğimiz imza ile imzalanmadığı için onların yüklenmemesini bekleriz. Ancak bu durum kullanıcıları ek bir 
    çabaya sürükleyebilmektedir. İşte default durumda imzası uyuşmayan aygıt sürücüler sistem tarafından bir uyarı verilerek 
    yine de yüklenmektedir. Ancak bu tür durumlarda daha katı bir kontrolün uygulanması isteniyorsa ".config" dosyasında
    aşağıdaki gibi ekstra bir üst düzey güvenlik belirtilebilir:

    CONFIG_MODULE_SIG_FORCE=y

    Bu işlem "menuconfig" menüsünde "Enable loadable module support/Module signature verification/Require modules to 
    be validly signed" seçeneğinden de yapılabilir. Bu durumda çekirdek kendi oluşturduğumuz imzayla imzalanmamış olan 
    modülleri yüklemeyecektir.

    Eğer biz başkalarının yazdığı bir aygıt sürücüyü yüklerken çekirdeğin uyarı vermesini ya da yüklemeyi reddetmesini 
    istemiyorsak o aygıt sürücüyü de kendi ürettiğimiz anahtarla (ya da dağıtımın public anahtarıyla) imzalamalıyız. Bu 
    işlem çekirdek kodlarındaki "scripts" dizini içerisinde bulunan "sign-file" betiği (script) yapılmaktadır. Bu betiğin 
    tipik kullanımı şöyledir:

    scripts/sign-file <hash_alg> <private_key.pem> <public_cert.pem> <module.ko>

    Örneğin:

    $ scripts/sign-file sha256 signing_key.pem.pem signing_key.pem mydriver.ko

    Pekiyi Ubuntu, Mint gibi dağıtımlar çekirdek imzalaması uygulamakta mıdır? Evet genel olarak dağıtımlar kendi 
    private anahtarlarıyla (private keys) çekirdeği ve aygıt sürücüleri imzalamaktadır. Ancak aygıt sürücülerin yüklenmesinde 
    imza kontrolünü zorunlu hale getirmemektedir. İmzalama için kullanılacak public anahtarlar "/proc/keys" dosyasında 
    belirtilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir Linux sisteminin düzgün bir biçimde açılması için belli dizinlerin kök dosya sisteminde bulunuyor olması gerekir. 
    Biz bir dağıtımı kurduğumuzda zaten bu kök dosya sistemi de oluşturulmaktadır. Pekiyi sıfırdan dağıtımı tamamen 
    kurmadan kök dosya sistemini nasıl oluşturulabiliriz? Bu işlem tamamen manuel biçimde yapılabilir. Yani uygulamacı 
    kök dizin içerisindeki gerekli dizinleri elle yaratır. Sonra gerekli programları kaynak kodlarından hareketle hedef 
    makine için derler ve onları konuşlandırır. Sonra yine gerekli birtakım konfigürasyon dosyalarını elle oluşturur. 
    Ancak bu manuel yöntem oldukça zahmetlidir. Bunun yerine bu işlemi pratik bir biçimde yapan araçlar geliştirilmiştir. 
    Örneğin gömülü sistemlerde "BusyBox" denilen araç bu amaçla sıkça kullanılmaktadır. Kullanımı da oldukça kolaydır. 
    Gömülü sistemler için "Buildroot" ve "Yocto" gibi projeler daha genel amaçlar için gerçekleştirilmiştir ancak bunlarla 
    kök dosya sistemi de oluşturulabilmektedir. Bazı dağıtımların bu işi yapan özel utility programları da vardır. Örneğin 
    "debootstrap" programı Debian tabanlı kök dosya sistemini Internet'ten indirerek yerel makinede oluşturabilmektedir. 
    Ancak bu araçların bazıları esnek değildir. Özellikle gömülü sistemlerde düşük bir sistem kaynağının olduğu dikkate 
    alındığında bu araçların bazıları minimalist bir kurulum sağlayamamaktadır.

    debootstrap programı default olarak sisteminizde yüklü değildir. Bunu aşağıdaki gibi kurabilirsiniz:

    $ sudo apt-get install debootstrap

    debootstrap programının pek çok komut satırı argümanı vardır. Biz burada en önemli birkaç argüman üzerinde duracağız. 
    --arch komut satırı seçeneği hedef CPU mimarisini belirtmektedir. Bu argüman girilmezse o andaki platform temel 
    alınmaktadır. 64 bit Intel platformu için burada "amd64", BBB gibi 32 bit ARM platformu için "armhf", 64 bit ARM 
    platformu için bu seçeneğe "arm64" girilmelidir. Programın ilk seçeneksiz argümanı Debian sisteminin varyantını 
    belirtmektedir. Bu argüman için "buster" girebilirsiniz. İkinci komut satırı argümanı hedef kök dosya sisteminin 
    oluşturulacağı dizini, üçüncü komut satırı argümanı ise paketlerin indirileceği depoyu (repository) belirtmektedir. 
    Örneğin:

    $ sudo debootstrap --arch=amd64 --include=systemd bullseye myrootfs http://deb.debian.org/debian/

    Yukarıda da belirttiğimiz gibi "--arch" seçeneği girilmemişse programın çalıştırıldığı makine için kök dosya sistemi 
    indirilip kurulmaktadır.

    Default durumda "debootstrap" pek çok paketi kök dosya sistemine dahil ettiği için paketlerin indirilmesi ve kök dosya 
    sisteminin oluşturulması biraz zaman alacaktır.

    Uygulamacı isterse "--include" ve "--exclude" komut satırı seçenekleriyle birtakım paketleri dahil edebilir ya da 
    dışlayabilir. Ancak bu işlem biraz yorucudur. Örneğin biz "systemd" dışında "sudo" ve "gcc" paketlerini de aşağıdaki 
    gibi kuruluma dahil edebiliriz:

    $ sudo debootstrap --arch=amd64 --include=systemd,sudo,gcc bullseye myrootfs http://deb.debian.org/debian/

    debootstrap programı ile eğer host makineyle aynı platform için indirme işlemi yapılıyorsa deboostrap önce Internetten 
    gerekli paketleri indirip yerel makinede bir dizin içerisinde kök dosya sistemini oluşturur sonra da indirmenin yapıldığı 
    dizinde ikinci aşama işlemleri gerçekleştirir. Eğer host makineden farklı bir sistem için indirme yapılıyorsa (örneğin 
    host sistem Intel tabanlı bir makineyse ve ARM tabanlı bir Debian kök dosya sistemi oluşturulmak isteniyorsa) debootstrap 
    yüklemesini yapmadan önce aşağıdaki gibi "qemu" emülatör paketinin statik versiyonu ve "binfmt" destek paketi kurulmalıdır:

    $ sudo apt install qemu-user-static binfmt-support

    Bu işlemden sonra debootstrap programı yukarıda belirttiğimiz biçimde çalıştırılabilir:

    $ sudo debootstrap --include=systemd --arch armhf buster myrootfs http://deb.debian.org/debian/

    Bu komut hem birinci aşama hem de ikinci aşama işlemleri yapıp bitirecektir. Artık istenildiği zaman chroot işlemi 
    de yapılabilir:

    $ sudo chroot myrootfs
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    "debootstrap" programı ile biz Debian kök dosya sistemi için geçici kök dosya sistemi de oluşturabiliriz. Bunun en 
    pratik yolu kök dosya sistemini kurduktan sonra "chroot" yapıp "update-initramfs" programı ile geçici kök dosya 
    sistemini oluşturmaktır. Ancak bunun için "/boot" ve "/lib/modules" dizinlerinin uygun biçimde oluşturulmuş olması 
    gerekir. "update-initramfs" programı bu dizinlerdeki içerikten faydalanmaktadır. "update-initramfs" programı "initramfs-tools" 
    isimli pakettedir. chroot yaptıktan sonra öncelikle bu paketi aşağıdaki gibi kurmalısınız:

    $ sudo apt-get install initramfs-tools

    Bundan sonra geçici kök dosya sistemini aşağıdaki gibi oluşturabilirsiniz (Debian kök dosya sisteminin kökünde olduğumuzu 
    varsayıyoruz):

    $ update-initramfs -c -k 6.9.2-custom -b .

    Burada "6.9.2-custom" çekirdeğin sürüm ismidir. Geçici kök dosya sistemi "initrd.img-6.9.2-custom" ismiyle bulunulan 
    dizinde oluşturulacaktır. "-b ." seçeneği oluşturulacak dosyanın dizinini belirtmektedir.

    Biz bu tür konuların ayrıntılarına girmeyeceğiz. Bu konular daha çok "Gömülü Linux Sistemleri - Geliştirme ve Uygulama" 
    kursunun konularını oluşturmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aygıt sürücü geliştirirken çekirdeğin kaynak kodlarına gereksinim duyulmaz. Ancak çekirdeğin başlık dosyalarının 
    geliştirmenin yapıldığı bilgisayarda yüklü olması gerekir. Çekirdek kodlarının kendisini değil de yalnızca başlık 
    dosyalarını indirmek için aşağıdaki komut kullanılabilir:

    $ sudo apt install linux-headers-$(uname -r)

    Buradaki $(uname -r) çalışılmakta olan makinedeki çekirdek sürümünü belirtmektedir. Tabii biz istediğimiz 
    çekirdek sürümünün başlık dosyalarını da indirebiliriz. İndirilen dosyalar "/usr/src" dizininin altına $(uname -r) 
    isimli dizin içerisine yerleştirilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinin gerçekleştiriminde çeşitli veri yapılarından (data structures) faydalanılmıştır. Çekirdekte 
    en çok kullanılan veri yapıları şunlardır:

    - Çift Bağlı Listeler
    - Bitmap'ler
    - Hash Tabloları
    - Kuyruk Sistemleri
    - Dengelenmiş Ağaçlar ve Radix Ağaçları

    Çekirdek içerisinde bu veri yapıları türden bağımsız bir biçimde yani genelleştirilerek gerçekleştirilmiştir.

    Biz bu bölümde "bağlı listelerin (linked lists)" Linux çekirdeğindeki gerçekleştirimleri üzerinde duracağız. Diğer 
    veri yapılarını çeşitli konular içerisinde yeri geldikçe açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin en önemli veri yapılarından biri bağlı listelerdir. Genel olarak çekirdekteki neredeyse tüm bağlı listeler 
    "çift bağlı (doubly linked)" biçimde kullanılmaktadır. Önce bağlı listelerin Linux çekirdeğindeki gerçekleştirimleri 
    üzerinde duracağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aralarında öncelik-sonralık ilişkisi olan veri yapılarına "liste (list)" tarzı veri yapıları denilmektedir. Örneğin 
    bu tanıma göre diziler de "liste tarzı" veri yapılarıdır. Liste tarzı veri yapılarının en yaygın kullanılanlarından 
    biri "bağlı liste (linked list)" denilen veri yapısıdır. Önceki elemanın sonraki elemanın yerini gösterdiği dolayısıyla 
    elemanların ardışıl olma zorunluluğunun ortadan kaldırıldığı listelere "bağlı liste" denilmektedir. Dizi elemanlarının 
    bellekte fiziksel olarak ardışıl biçimde bulunduğunu anımsayınız. Bağlı listeler adeta "elemanları bellekte ardışıl 
    olmak zorunda olmayan diziler" gibidir.

    Bağlı listelerin her elemanına "düğüm (node)" denilmektedir. Bağlı listelerde her düğüm sonraki düğümün yerini tuttuğuna 
    göre ilk elemanın yeri biliniyorsa liste elemanlarının hepsine erişilebilmektedir. Örneğin:

    head ---> node ---> node ---> node ---> node (NULL)

    Her düğümün yalnızca sonraki düğümün yerini değil aynı zamanda önceki düğümün yerini de tuttuğu bağlı listelere 
    "çift bağlı listeler (double linked lists)" denilmektedir. Çift bağlı listelerde belli bir düğümün adresini biliyorsak 
    yalnızca ileriye doğru değil, geriye doğru da gidebiliriz.

    head <---> node <---> node <---> node <---> node (NULL)

    Çift bağlı listelere ilişkin bir düğümün bellekte daha fazla yer kaplayacağına dikkat ediniz. Çift bağlı listelerin 
    tek bağlı listelere göre en önemli özelliği "adresi bilinen bir düğümün" silinebilmesidir. Tek bağlı listelerde bu 
    durum mümkün değildir. Uygulamalarda ve özellikle çekirdek kodlarında buna çok sık gereksinim duyulmaktadır.

    Eğer bir bağlı listede son eleman da ilk elemanı gösteriyorsa bu tür bağlı listelere "döngüsel bağlı listeler 
    (circular linked lists)" denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi bağlı listelere neden gereksinim duyulmaktadır? Diziler varken bağlı listelere gerek var mıdır? Dizilerle 
    bağlı listeler arasındaki farklılıkları, benzerlikleri ve bağlı listelere neden gereksinim duyulduğunu birkaç maddede 
    açıklayabiliriz:

    1) Diziler ardışıl alana gereksinim duymaktadır. Ancak belleğin bölündüğü (fragmente olduğu) durumlarda bellekte yeteri 
    kadar küçük boş alanlar olduğu halde bunlar ardışıl olmadığı için dizi tahsisatı mümkün olamamaktadır. Bu tür durumlarda 
    ardışıllık gereksinimi olmayan bağlı listeler kullanılabilir. Özellikle heap gibi bir alanda çok sayıda dinamik dizi 
    bellek kullanımı bakımından verimsizliğe yol açabilmektedir. Bu dinamik diziler zamanla büyüdükçe birbirini engeller 
    hale gelebilmektedir. İşte uzunluğu baştan belli olmayan çok sayıda dizinin oluşturulacağı durumlarda dinamik dizi 
    yerine bağlı listeler toplamda daha iyi performans gösterebilmektedir. Dinamik dizilerde dinamik dizinin büyütülmesi 
    yavaş bir işlemdir. Çünkü büyütme sırasında bloklar yer değiştirebilmektedir.

    2) Dizilerde araya eleman ekleme (insert etme) ve aradaki bir elemanı silme dizinin kaydırılmasına ("expand" ve "shrink" 
    edilmesine) yol açacağından yavaş bir işlemdir. Teknik olarak dizilerde eleman insert etme ve eleman silme O(N) karmaşıklıkta 
    bir işlemdir. Halbuki bağlı listelerde eğer düğümün yeri biliniyorsa bu işlem O(1) karmaşıklıkta (yani döngü olmadan 
    tekil işlemlerle) yapılabilmektedir. O halde araya eleman eklemenin ve aradan eleman silmenin çok yapıldığı sistemlerde 
    diziler yerine bağlı listeler tercih edilebilmektedir. Çekirdek veri yapılarında araya eleman ekleme ve aradan eleman 
    silme gibi işlemler çok yoğun yapılmaktadır.

    3) Bağlı listelerde belli bir indeksteki elemana erişmek O(N) karmaşıklıkta bir işlemdir. Halbuki dizilerde elemana 
    erişim O(1) karmaşıklıkta yani çok hızlıdır. O halde belli bir indeks değeri ile elemana erişimin yoğun yapıldığı 
    durumlarda bağlı listeler yerine diziler tercih edilmelidir.

    4) Bağlı listeler toplamda bellekte daha fazla yer kaplama eğilimindedir. Çünkü bağlı listenin her düğümü sonraki 
    (ve duruma göre önceki) elemanın yerini de tutmaktadır.

    O halde bağlı listeler tipik olarak şu durumlarda dizilere tercih edilmelidir:

    - Eleman insert etmenin ve eleman silmenin sık yapıldığı durumlarda.
    - Uzunluğu baştan belli olmayan çok sayıda dizinin kullanıldığı durumlarda.
    - İndeks yoluyla erişimin az yapıldığı durumlarda.
    - Toplam bellek miktarının yeteri kadar fazla olduğu sistemlerde.

    İşte tüm yukarıdaki nedenlerden dolayı bağlı listeler çekirdek için en önemli veri yapılarından biridir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinde bağlı liste gerçekleştirimi "include/linux/list.h" dosyasında yapılmıştır. Bu dosya içerisindeki 
    fonksiyonlar static inline biçiminde tanımlanmıştır. Çekirdek derlemesi sırasında derleyicinin optimizasyon seçeneği 
    ayarlandığı için derleyici buradaki inline fonksiyonları sanki bir makro gibi koda açmaktadır.

    Linux çekirdeğindeki bağlı listelerde bağlı listelerin düğümleri list_head isimli bir yapıyla temsil edilmektedir:

    struct list_head {
        struct list_head *next;
        struct list_head *prev;
    };

    Aslında belli yapılar değil, bu list_head yapıları bağlı liste içerisinde birbirine bağlanmaktadır:

    list_head (kök) <---> list_head <---> list_head <---> list_head <---> list_head <---> list_head <---> list_head

    Tabii eğer bu list_head yapıları başka bir yapının (buna asıl yapı diyelim) içerisindeyse bu durumda aslında bir 
    list_head yapısının adresi asıl yapının bir elemanının adresi haline gelmektedir. Biz C'de bir yapının bir elemanının 
    adresini biliyorsak kolaylıkla o yapının başlangıç adresini elde edebiliriz. Çünkü yapı elemanları ardışıldır ve 
    standart "offsetof" makrosuyla belli bir yapı elemanının yapının başlangıcından itibaren hangi offset'te bulunduğu 
    bilgisi elde edilebilmektedir. "offsetof" makrosu <stddef.h> içerisinde aşağıdakine benzer biçimde tanımlanmıştır:

    #define offsetof(type, member)	((size_t)&(((type *)0)->member))

    "offsetof" makrosunun birinci parametresi yapının tür ismini, ikinci parametresi ilgili yapı elemanının ismini almaktadır. 
    Tabii "offsetof" makrosu yalnızca size_t türünden bir byte offset'i vermektedir. Elemanın adresinin makroyla elde 
    edilen bu değerden çıkartılarak tür dönüştürmesinin yapılması gerekir. İşte bunun için Linux çekirdeğinde "container_of" 
    isimli bir makro bulundurulmuştur. Bu makronun basit yazımı şöyle yapılabilir:

    #define container_of(ptr, type, member)	((type *)((char *)ptr - myoffsetof(type, member)))

    "container_of" makrosunun birinci parametresi yapı elemanın adresini, ikinci parametresi yapının tür ismini ve üçüncü 
    parametresi de ilgili yapı elemanının ismini almaktadır. Bu makro ikinci parametresine girilmiş olan yapı türünden 
    bir adres vermektedir.

    "container_of" makrosunu "list_entry" ismiyle de kullanabilirsiniz:

    #define list_entry(ptr, type, member) container_of(ptr, type, member)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										11. Ders 23/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğindeki bağlı listeler kullanılırken önce bir başlangıç düğümünün oluşturulması gerekir. Başlangıç 
    düğümünde next ve prev göstericilerinin başlangıç düğümünün kendisini göstermesi gerekir. Örneğin:

    struct list_head head = {&head, &head};

    Bu ilkdeğer verme C'de geçerlidir. Çünkü C'de bir değişken faaliyet alanına "=" atomu ile ilkdeğer verme kısmından 
    önce sokulmaktadır. "list.h" dosyasında bu işlemi yapan aşağıdaki gibi bir makro da bulundurulmuştur:

    #define LIST_HEAD_INIT(name) {&(name), &(name)}

    Bu durumda başlangıç düğümü bu makroyla şöyle de oluşturulabilir:

    struct list_head head = LIST_HEAD_INIT(head);

    Aslında bu tanımlamanın tamamını yapan aşağıdaki gibi bir makro da bulundurulmuştur:

    #define LIST_HEAD(name) \ 
        struct list_head name = LIST_HEAD_INIT(name)

    O halde biz başlangıç düğümünü basit bir biçimde şöyle oluşturabiliriz:

    LIST_HEAD(head);

    Linux'un bağlı liste gerçekleştiriminde next ve prev göstericilerinde NULL adres kullanılmamıştır. Son düğümün next 
    göstericisi NULL yerine başlangıç düğümünü, ilk düğümün prev göstericisi de NULL yerine son düğümü göstermektedir. 
    Yani aslında bağlı liste gerçekleştirimi döngüsel (circular) gibidir. Herhangi bir düğümden başlanarak başlangıç düğümü 
    de atlanırsa tam dolaşım sağlanabilir. Bağlı listenin başlangıç düğümünün prev göstericisi de son düğümü göstermektedir.

    Bağlı listenin başına list_head düğümünü eklemek için list_add fonksiyonu kullanılmaktadır:

    static inline void list_add(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head, head->next);
    }

    Fonksiyonun birinci parametresi eklenecek düğümün adresini, ikinci parametresi başlangıç düğümünün adresini almaktadır.

    Çekirdek kodlarında iki alt tire ile başlayan fonksiyonlar aşağı seviyeli fonksiyonlardır. Yani doğrudan çağrılmayan 
    ancak başka fonksiyonların içerisinden dolaylı biçimde çağrılan fonksiyonlardır. Buradaki __list_add fonksiyonu şöyle 
    yazılmıştır:

    static inline void __list_add(struct list_head *new,
                  struct list_head *prev,
                  struct list_head *next)
    {
        if (!__list_add_valid(new, prev, next))
            return;

        next->prev = new;
        new->next = next;
        new->prev = prev;
        WRITE_ONCE(prev->next, new);
    }

    Buradaki WRITE_ONCE makrosu çok işlemcili ya da çok çekirdekli sistemlerde bellek bariyeri oluşturarak atama yapmaktadır. 
    Siz WRITE_ONCE(a, b) çağrısını a = b gibi düşünebilirsiniz.

    Bağlı listenin sonuna düğüm eklemek için list_add_tail fonksiyonu kullanılmaktadır:

    static inline void list_add_tail(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head->prev, head);
    }

    Pekiyi biz bağlı listeyi nasıl dolaşabiliriz? Başlangıç düğümünü geçerek sürekli ileriye gidersek son düğüm de 
    başlangıç düğümünü gösterdiğine göre dolaşımı aşağıdaki gibi bir for döngüsüyle yapabiliriz:

    struct list_head *lh;
    /* ... */

    for (lh = head.next; lh != &head; lh = lh->next) {
        /* ... */
    }

    Tabii böyle bir döngüde dolaşım yapılırken aslında biz her yinelemede ilgili yapının başlangıç adresini değil 
    list_head yapısının başlangıç adresini elde ederiz. container_of makrosu ile bu adresin yapı adresine dönüştürülmesi 
    gerekir. Örneğin biz struct SAMPLE türünden yapı nesnelerini birbirine bağlamış olalım:

    LIST_HEAD(head);

    struct SAMPLE {
        int a;
        struct list_head link;
    };

    Eklemeleri şöyle yapmış olalım:

    struct SAMPLE *ps;

    for (int i = 0; i < 10; ++i) {
        if ((ps = (struct SAMPLE *)malloc(sizeof(struct SAMPLE))) == NULL) {
            fprintf(stderr, "cannot allocate memory!...\n");
            exit(EXIT_FAILURE);
        }
        ps->a = i;
        list_add_tail(&ps->link, &head);
    }

    Çekirdek kodlarında malloc gibi kullanıcı modu fonksiyonları kullanılamaz. Ancak biz burada yalnızca anlaşılır 
    bir test örneği vermek istiyoruz. struct SAMPLE yapımızdaki link elemanı list_head yapılarını birbirine bağlamak için 
    bulundurulmuştur. Örneğimizdeki list_add_tail fonksiyonunda ekleme yapılacak düğümün SAMPLE yapısının içerisindeki 
    link elemanının adresi olduğuna dikkat ediniz. Şimdi dolaşımı şöyle yapabiliriz:

    struct SAMPLE *ps;
    struct list_head *lh;
    /* ... */

    for (lh = head.next; lh != &head; lh = lh->next) {
        ps = container_of(lh, struct SAMPLE, link);
        /* ... */
    }

    Burada artık ps göstericisi list_head nesnesini değil struct SAMPLE nesnesini göstermektedir. Aslında "list.h" dosyası 
    içerisinde buradaki döngüyü oluşturan list_for_each isimli bir makro da bulundurulmuştur:

    static inline int list_is_head(const struct list_head *list, const struct list_head *head)
    {
        return list == head;
    }

    #define list_for_each(pos, head) \
	    for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)

    Makronun birinci parametresi list_head türünden bir göstericiyi, ikinci parametresi başlangıç düğümünün adresini 
    almaktadır. Döngünün her yinelenmesinde bu göstericiye sonraki düğümün list_head adresi atanmaktadır. Örneğin:

    struct list_head *lh;
    /* ... */

    list_for_each(lh, &head) {
        ps = container_of(lh, struct SAMPLE, link);
        printf("%d\n", ps->a);
    }

    Aslında çekirdekteki "list.h" dosyası içerisinde yukarıdaki dolaşımı tek hamlede yapan list_for_each_entry isimli bir 
    makro da bulunmaktadır:

    #define list_for_each_entry(pos, head, member)				\
	for (pos = list_first_entry(head, typeof(*pos), member);	\
	     !list_entry_is_head(pos, head, member);			    \
	     pos = list_next_entry(pos, member))

    Makronun birinci parametresi asıl yapı türünden bir göstericidir. Makro her yinelemede bu göstericiye sonraki düğümün 
    adresini yerleştirmektedir. Makronun ikinci parametresi başlangıç düğümünün adresini, üçüncü parametresi ise yapı 
    içerisindeki list_head elemanın ismini belirtmektedir. Bu makronun eşdeğeri bazı ayrıntılar ihmal edilerek şöyle de 
    yazılabilir:

    #define my_list_for_each_entry(pos, head, member)                                           \
        for (pos = container_of((head)->next, typeof(*pos), member);  &(pos)->member != head;   \
            pos = container_of((pos)->member.next, typeof(*pos), member))

    C standartlarında bir ifadenin türünü veren bir tür belirleyicisi yoktu. C++'a C++11 ile birlikte decltype ismi ile 
    böyle bir belirleyici eklendi. gcc derleyicileri uzun süredir bu işi yapan typeof isimli belirleyiciyi desteklemektedir. 
    Nihayet C'ye de C23 ile birlikte resmi olarak typeof belirleyicisi eklenmiştir. Makroda türün tür isminin typeof(*pos) 
    ifadesiyle elde edildiğine dikkat ediniz. Bu makro ile dolaşım şöyle sağlanabilir:

    struct SAMPLE *ps;
    /* ... */

    list_for_each_entry(ps, &head, link) {
        /* ... */
    }

    Bağlı liste makrolarının ve fonksiyonlarının isimlerinin sonunda entry sözcüğü varsa bu makro ya da fonksiyon asıl 
    yapıya ilişkin adres, yoksa list_head türünden adres verip almaktadır.

    Ayrıca "list.h" içerisinde list_for_each_entry_safe isimli bir döngü makrosu da bulundurulmuştur. Bu makro eğer liste 
    boşsa hiç dolaşım yapmamaktadır. Makro aşağıdaki gibi yazılmıştır:

    #define list_for_each_entry_safe(pos, n, head, member)			\
	for (pos = list_first_entry(head, typeof(*pos), member),	    \
		n = list_next_entry(pos, member);			                \
	     !list_entry_is_head(pos, head, member); 			        \
	     pos = n, n = list_next_entry(n, member))

    Makrounun birinci elemanı dolaşımda kullanılacak asıl yapı türünden göstericiyi almaktadır. Makronun ikinci parametresi 
    de asıl yapı göstericidir. Üçüncü ve dördüncü parametreler sırasıyla kök düğümün adresi ve bağ düğümünün ismini 
    almaktadır.

    Bağlı listeden bir düğümü silmek için list_del fonksiyonu kullanılmaktadır. Fonksiyon şöyle tanımlanmıştır:

    static inline void __list_del(struct list_head * prev, struct list_head * next)
    {
        next->prev = prev;
        WRITE_ONCE(prev->next, next);
    }

    static inline void __list_del_entry(struct list_head *entry)
    {
        if (!__list_del_entry_valid(entry))
            return;

        __list_del(entry->prev, entry->next);
    }

    static inline void list_del(struct list_head *entry)
    {
        __list_del_entry(entry);
        entry->next = LIST_POISON1;
        entry->prev = LIST_POISON2;
    }

    Fonksiyonun parametresi silinecek düğüme ilişkin list_head nesnesinin adresini almaktadır. Burada silinen düğümdeki 
    next ve prev göstericilerine özel bazı değerlerin atandığını görüyorsunuz. Bu değerler silinmiş düğümün kullanılması 
    durumunda "page fault" oluşmasına yol açmaktadır. Bu değerlerin debug mekanizması bir çeşit debug mekanizması oluşturmak 
    amacıyla atandığını söyleyebiliriz.

    Bağlı listenin arasına düğüm ekleyen insert isimli fonksiyonlar yoktur. Zaten list_add fonksiyonu araya ekleme 
    işlemini de yapmaktadır. Örneğin biz araya şöyle eklama yapabiliriz:

    list_add(&ps->link, &ps_insert->link);

    Burada ps eklenecek düğümün asıl yapı adresini, ps_insert ise önüne eklemenin yapılacağı asıl yapı adresini 
    belirtmektedir.

    Bir bağlı listeyi tümden serbest bırakmak için düğümlerin tek tek serbest bırakılması gerekmektedir. Buradaki 
    düğümler aslında başka yapıların içerisinde olduğuna göre liste içerisindeki o yapı nesnelerinin serbest bırakılması 
    gerekir.

    Aşağıda kullanıcı alanında çekirdekteki bağlı liste kullanımına bir örnek verilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

#include <stdio.h>
#include <stdlib.h>
#include <stddef.h>

struct list_head {
	struct list_head *next, *prev;
};

#define LIST_HEAD_INIT(name) { &(name), &(name) }

#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

#define container_of(ptr, type, member) ({				\
		void *__mptr = (void *)(ptr);					\
		((type *)(__mptr - offsetof(type, member))); })

#define list_entry(ptr, type, member) \
	container_of(ptr, type, member)

#define list_entry_is_head(pos, head, member)				\
	(&pos->member == (head))

#define list_first_entry(ptr, type, member) \
	list_entry((ptr)->next, type, member)

#define list_next_entry(pos, member) \
	list_entry((pos)->member.next, typeof(*(pos)), member)

#define list_for_each(pos, head) \
	for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)

#define list_for_each_entry(pos, head, member)				\
	for (pos = list_first_entry(head, typeof(*pos), member);	\
		!list_entry_is_head(pos, head, member);			\
		pos = list_next_entry(pos, member))

static inline int list_is_head(const struct list_head *list, const struct list_head *head)
{
	return list == head;
}

static inline void __list_add(struct list_head *new,
				struct list_head *prev,
				struct list_head *next)
{
	next->prev = new;
	new->next = next;
	new->prev = prev;
	prev->next = new;
}

static inline void list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
}

static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
}

static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
	prev->next = next;
}

static inline void __list_del_entry(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
}

static inline void list_del(struct list_head *entry)
{
	__list_del_entry(entry);
}

LIST_HEAD(head);

struct SAMPLE {
	int a;
	struct list_head link;
};

#define my_list_for_each_entry(pos, head, member)											\
	for (pos = container_of((head)->next, typeof(*pos), member); &(pos)->member != head;	\
			pos = container_of((pos)->member.next, typeof(*pos), member))

#define list_for_each_entry_safe(pos, n, head, member)			    \
	for (pos = list_first_entry(head, typeof(*pos), member),	    \
		n = list_next_entry(pos, member);			                \
	     !list_entry_is_head(pos, head, member); 			        \
	     pos = n, n = list_next_entry(n, member))

int main(void)
{
	struct SAMPLE *ps, *ps_node, *ps_temp, *ps_del, *ps_insert;
	struct list_head *lh;

	for (int i = 0; i < 10; ++i) {
		if ((ps = (struct SAMPLE *)malloc(sizeof(struct SAMPLE))) == NULL) {
			fprintf(stderr, "cannot allocate memory!...\n");
			exit(EXIT_FAILURE);
		}
		ps->a = i;
		list_add_tail(&ps->link, &head);

		if (i == 5)
			ps_del = ps;

		if (i == 7)
			ps_insert = ps;
	}

	list_for_each(lh, &head) {
		ps = container_of(lh, struct SAMPLE, link);
		printf("%d ", ps->a);
	}
	printf("\n");

	list_del(&ps_del->link);

	list_for_each(lh, &head) {
		ps = container_of(lh, struct SAMPLE, link);
		printf("%d ", ps->a);
	}
	printf("\n");

	if ((ps = (struct SAMPLE *)malloc(sizeof(struct SAMPLE))) == NULL) {
		fprintf(stderr, "cannot allocate memory!...\n");
			exit(EXIT_FAILURE);
	}
	ps->a = 100;

	list_add(&ps->link, &ps_insert->link);

	list_for_each(lh, &head) {
		ps = container_of(lh, struct SAMPLE, link);
		printf("%d ", ps->a);
	}
	printf("\n");

	ps_temp = NULL;
	list_for_each_entry_safe(ps, ps_node, &head, link) {
		free(ps_temp);
		ps_temp = ps;
	}

	return 0;
}

/*----------------------------------------------------------------------------------------------------------------------
    Belli bir süreden sonra "list.h" dosyasına RCU (Read-Copy_Update) mekanizmasını destekleyecek biçimde dolaşım yapan 
    list_for_each_rcu isimli makro da eklenmiştir. Bu makro bir yazıcı varsa okuyucuları bekletmeden işlem yapabilmeyi 
    sağlamaktadır. "Read-copy-update" mekanizması ileride ele alınacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin eski sürümlerinde "list.h" dosyası oldukça sade idi. Sonra bu dosyanın içeriğinde de bazı faydalı değişiklikler 
    yapıldı. Ancak bu değişiklikler veri yapısının anlaşılmasını da biraz zorlaştırmıştır. Örneğin 2.2.26 çekirdeğindeki 
    "list.h" dosyası oldukça sade bir biçimde aşağıdaki gibi oluşturulmuştur:

    /* 2.2.26 <list.h> dosyasının içeriği */

    #ifndef _LINUX_LIST_H
    #define _LINUX_LIST_H

    #ifdef __KERNEL__

    /*
    * Simple doubly linked list implementation.
    *
    * Some of the internal functions ("__xxx") are useful when
    * manipulating whole lists rather than single entries, as
    * sometimes we already know the next/prev entries and we can
    * generate better code by using them directly rather than
    * using the generic single-entry routines.
    */

    struct list_head {
        struct list_head *next, *prev;
    };

    #define LIST_HEAD_INIT(name) { &(name), &(name) }

    #define LIST_HEAD(name) \
        struct list_head name = { &name, &name }

    #define INIT_LIST_HEAD(ptr) do { \
        (ptr)->next = (ptr); (ptr)->prev = (ptr); \
    } while (0)

    /*
    * Insert a new entry between two known consecutive entries.
    *
    * This is only for internal list manipulation where we know
    * the prev/next entries already!
    */
    static __inline__ void __list_add(struct list_head * new,
        struct list_head * prev,
        struct list_head * next)
    {
        next->prev = new;
        new->next = next;
        new->prev = prev;
        prev->next = new;
    }

    /*
    * Insert a new entry after the specified head..
    */
    static __inline__ void list_add(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head, head->next);
    }

    /*
    * Insert a new entry at the tail
    */
    static __inline__ void list_add_tail(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head->prev, head);
    }

    /*
    * Delete a list entry by making the prev/next entries
    * point to each other.
    *
    * This is only for internal list manipulation where we know
    * the prev/next entries already!
    */
    static __inline__ void __list_del(struct list_head * prev,
                    struct list_head * next)
    {
        next->prev = prev;
        prev->next = next;
    }

    static __inline__ void list_del(struct list_head *entry)
    {
        __list_del(entry->prev, entry->next);
    }

    static __inline__ int list_empty(struct list_head *head)
    {
        return head->next == head;
    }

    /*
    * Splice in "list" into "head"
    */
    static __inline__ void list_splice(struct list_head *list, struct list_head *head)
    {
        struct list_head *first = list->next;

        if (first != list) {
            struct list_head *last = list->prev;
            struct list_head *at = head->next;

            first->prev = head;
            head->next = first;

            last->next = at;
            at->prev = last;
        }
    }

    #define list_entry(ptr, type, member) \
        ((type *)((char *)(ptr)-(unsigned long)(&((type *)0)->member)))

    #define list_for_each(pos, head) \
            for (pos = (head)->next; pos != (head); pos = pos->next)

    #endif /* __KERNEL__ */

    #endif
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										12. Ders 24/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinde "program" terimi çoğu kez "çalıştırılabilen bir dosyayı" ya da "bir kaynak dosyayı" belirtmektedir. 
    Çalışmakta olan programlara ise "proses" denilmektedir. Bir program çalıştırıldığında artık o bir proses haline gelmektedir. 
    Aynı programı birden fazla kez de çalıştırabiliriz. Bu durumda birbirinden bağımsız birden fazla proses oluşacaktır.

    İşletim sistemlerinin proses yönetimlerindeki en önemli veri yapısı "proses kontrol bloğu (process control block)" 
    denilen veri yapısıdır. İşletim sistemleri her proses için kavramsal olarak ismine "proses kontrol bloğu" denilen bir 
    yapı türünden nesne oluşturmaktadır. Proseslerin yönetimi proses kontrol bloğu denilen bu veri yapısına başvurularak 
    yapılmaktadır. Proses kontrol bloğu terimi yerine bazı kaynaklar "proses betimleyicisi (process descriptor)" terimini 
    de kullanmaktadır.

    Proses kontrol blokları içerisinde prosese ilişkin gerekli bütün bilgiler bulundurulmaktadır. Bu bilgilerden bazıları 
    şunlardır:

    - Prosesin kullanıcı id'si, grup id'si gibi hesap (credential) bilgileri
    - Proses id'si
    - Prosesin üst prosesinin, alt proseslerinin, kardeş proseslerinin hangi prosesler olduğu bilgisi
    - Prosesin durumsal bilgisi
    - Prosesin bağlamsal geçişini (context switch) sağlama için gereksinim duyulan alanlar
    - Prosesin bellekte nereye yüklü olduğu gibi bellekle ilgili bilgileri
    - Prosesin çizelgeleyici ile ilgili olan bilgileri (örneğin çizelgeleme politikası, önceliği gibi)
    - Prosesin CPU kullanım istatistiği
    - Prosesin sinyal bilgileri
    - Prosesin proseslerarası haberleşme için gerekli olan bilgileri
    - Prosesin çalışma dizini (current working directory)
    - Prosesin açmış olduğu dosyalara ilişkin bilgiler
    - ...
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinde proses kontrol bloğu <include/linux/sched.h> dosyası içerisinde bulunan task_struct isimli yapıyla
    temsil edilmiştir. Bu task_struct nesnesi eskiden daha az eleman içeriyordu. Sonra çekirdek gittikçe geliştirilince
    dev bir yapı haline geldi. Örneğin bu task_struct nesnesi Linus projeye ilk başlandığında (Linux 0.01) şu biçimdeydi:

    struct task_struct {
    /* these are hardcoded - don't touch */
        long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
        long counter;
        long priority;
        long signal;
        fn_ptr sig_restorer;
        fn_ptr sig_fn[32];
    /* various fields */
        int exit_code;
        unsigned long end_code,end_data,brk,start_stack;
        long pid,father,pgrp,session,leader;
        unsigned short uid,euid,suid;
        unsigned short gid,egid,sgid;
        long alarm;
        long utime,stime,cutime,cstime,start_time;
        unsigned short used_math;
    /* file system info */
        int tty;		/* -1 if no tty, so it must be signed */
        unsigned short umask;
        struct m_inode * pwd;
        struct m_inode * root;
        unsigned long close_on_exec;
        struct file * filp[NR_OPEN];
    /* ldt for this task 0 - zero 1 - cs 2 - ds&ss */
        struct desc_struct ldt[3];
    /* tss for this task */
        struct tss_struct tss;
    };

    Örneğin çekirdeğin 2.4'lü versiyonunda bu yapı şu hale gelmiştir:

    struct task_struct {
        /*
        * offsets of these are hardcoded elsewhere - touch with care
        */
        volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
        unsigned long flags;	/* per process flags, defined below */
        int sigpending;
        mm_segment_t addr_limit;	/* thread address space:
                            0-0xBFFFFFFF for user-thread
                            0-0xFFFFFFFF for kernel-thread
                        */
        struct exec_domain *exec_domain;
        volatile long need_resched;
        unsigned long ptrace;

        int lock_depth;		/* Lock depth */

    /*
    * offset 32 begins here on 32-bit platforms. We keep
    * all fields in a single cacheline that are needed for
    * the goodness() loop in schedule().
    */
        long counter;
        long nice;
        unsigned long policy;
        struct mm_struct *mm;
        int processor;
        /*
        * cpus_runnable is ~0 if the process is not running on any
        * CPU. It's (1 << cpu) if it's running on a CPU. This mask
        * is updated under the runqueue lock.
        *
        * To determine whether a process might run on a CPU, this
        * mask is AND-ed with cpus_allowed.
        */
        unsigned long cpus_runnable, cpus_allowed;
        /*
        * (only the 'next' pointer fits into the cacheline, but
        * that's just fine.)
        */
        struct list_head run_list;
        unsigned long sleep_time;

        struct task_struct *next_task, *prev_task;
        struct mm_struct *active_mm;
        struct list_head local_pages;
        unsigned int allocation_order, nr_local_pages;

    /* task state */
        struct linux_binfmt *binfmt;
        int exit_code, exit_signal;
        int pdeath_signal;  /*  The signal sent when the parent dies  */
        /* ??? */
        unsigned long personality;
        int did_exec:1;
        unsigned task_dumpable:1;
        pid_t pid;
        pid_t pgrp;
        pid_t tty_old_pgrp;
        pid_t session;
        pid_t tgid;
        /* boolean value for session group leader */
        int leader;
        /*
        * pointers to (original) parent process, youngest child, younger sibling,
        * older sibling, respectively.  (p->father can be replaced with
        * p->p_pptr->pid)
        */
        struct task_struct *p_opptr, *p_pptr, *p_cptr, *p_ysptr, *p_osptr;
        struct list_head thread_group;

        /* PID hash table linkage. */
        struct task_struct *pidhash_next;
        struct task_struct **pidhash_pprev;

        wait_queue_head_t wait_chldexit;	/* for wait4() */
        struct completion *vfork_done;		/* for vfork() */
        unsigned long rt_priority;
        unsigned long it_real_value, it_prof_value, it_virt_value;
        unsigned long it_real_incr, it_prof_incr, it_virt_incr;
        struct timer_list real_timer;
        struct tms times;
        unsigned long start_time;
        long per_cpu_utime[NR_CPUS], per_cpu_stime[NR_CPUS];
    /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
        unsigned long min_flt, maj_flt, nswap, cmin_flt, cmaj_flt, cnswap;
        int swappable:1;
    /* process credentials */
        uid_t uid,euid,suid,fsuid;
        gid_t gid,egid,sgid,fsgid;
        int ngroups;
        gid_t	groups[NGROUPS];
        kernel_cap_t   cap_effective, cap_inheritable, cap_permitted;
        int keep_capabilities:1;
        struct user_struct *user;
    /* limits */
        struct rlimit rlim[RLIM_NLIMITS];
        unsigned short used_math;
        char comm[16];
    /* file system info */
        int link_count, total_link_count;
        struct tty_struct *tty; /* NULL if no tty */
        unsigned int locks; /* How many file locks are being held */
    /* ipc stuff */
        struct sem_undo *semundo;
        struct sem_queue *semsleeping;
    /* CPU-specific state of this task */
        struct thread_struct thread;
    /* filesystem information */
        struct fs_struct *fs;
    /* open file information */
        struct files_struct *files;
    /* namespace */
        struct namespace *namespace;
    /* signal handlers */
        spinlock_t sigmask_lock;	/* Protects signal and blocked */
        struct signal_struct *sig;

        sigset_t blocked;
        struct sigpending pending;

        unsigned long sas_ss_sp;
        size_t sas_ss_size;
        int (*notifier)(void *priv);
        void *notifier_data;
        sigset_t *notifier_mask;

    /* Thread group tracking */
        u32 parent_exec_id;
        u32 self_exec_id;
    /* Protection of (de-)allocation: mm, files, fs, tty */
        spinlock_t alloc_lock;

    /* journalling filesystem info */
        void *journal_info;
    };

    Bugünkü 6'lı çekirdeklerde bu yapı birkaç sayfa uzunluğundadır. Eskiden yukarıda da gördüğünüz gibi yapının her 
    elemanı çekirdek derlemesine dahil ediliyordu. Ancak 2.6'lı versiyonlardan başlanarak artık yapı elemanlarının bazıları 
    konfigürasyona (yani "menuconfig" menüsündeki seçeneklere ya da doğrudan ".config" dosyasındaki seçeneklere) bağlı 
    olarak yapıya dahil edilmektedir. Bu nedenle gelişmiş çekirdeklerde yapıda aşağıdaki gibi #ifdef blokları görürseniz 
    şaşırmayınız:

    struct task_struct {
    #ifdef CONFIG_THREAD_INFO_IN_TASK
        /*
        * For reasons of header soup (see current_thread_info()), this
        * must be the first element of task_struct.
        */
        struct thread_info		thread_info;
    #endif
        unsigned int			__state;

        /* saved state for "spinlock sleepers" */
        unsigned int			saved_state;

        /*
        * This begins the randomizable portion of task_struct. Only
        * scheduling-critical items should be added above here.
        */
        randomized_struct_fields_start

        void				*stack;
        refcount_t			usage;
        /* Per task flags (PF_*), defined further below: */
        unsigned int			flags;
        ...
    };

    Bilindiği gibi Linux sistemlerinde yeni bir proses fork isimli POSIX fonksiyonuyla yaratılmaktadır. fork POSIX 
    fonksiyonu çekirdek içerisindeki sys_fork isimli sistem fonksiyonunu çağırmaktadır. Yeni çekirdeklerde bu fonksiyon 
    da kernel_clone isimli çekirdek fonksiyonu çağırmaktadır. İşte task_struct nesnesi bu fonksiyonlar tarafından çekirdeğin 
    heap alanında yaratılmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi UNIX/Linux sistemlerinde her prosesin (yani çalışmakta olan her programın) o anda sistem genelinde 
    tek (unique) olan bir "proses id (process id)" değeri vardır. Proses id değeri hem çekirdek tarafından hem de 
    kullanıcı modundan çağrılabilen sistem fonksiyonları tarafından kullanılan bir kavramdır. Proses id değeri bir 
    prosesi temsil etmekte kullanılan tamsayısal bir değerdir. ps komutuyla proseslere ilişkin proses id değerlerinin 
    görüntülendiğini anımsayınız. Kullanıcı modunda getpid POSIX fonksiyonu çalışmakta olan programın proses id değerini, 
    getppid POSIX fonksiyonu ise üst prosesin proses id değerini vermektedir. Tabii prosesin proses id değeri ve üst 
    prosesin proses id değeri proses kontrol bloğunda yani task_struct nesnesi içerisinde saklanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerine thread kavramı 90'lı yıllarda sokulmuştur. Ancak ilk thread denemeleri çeşitli çalışmalar 
    eşliğinde daha önceleri yapılmıştır. Linux işletim sistemine thread'ler ilk kez çekirdeğin 2.0 versiyonuyla eklenmiş 
    olsa da daha sonra çeşitli düzeltmelerle thread yapısı biraz değiştirilmiştir.

    Proses kavramı çalışmakta olan programın tüm bilgilerini temsil etmektedir. Halbuki thread yalnızca bir akış belirtmektedir. 
    Bir proses tek thread'le çalışmaya başlar. Prosesin çalışmaya başladığı thread'e "ana thread (main thread)" de 
    denilmektedir. Thread'ler de kullanıcı modundan sistem fonksiyonları (sys_clone sistem fonksiyonu) ile yaratılmaktadır.

    UNIX türevi sistemlere thread'ler ilk kez sokulduğunda farklı varyantlar farklı thread kütüphaneleri kullanıyordu. 
    Sonra thread kavramı POSIX standartlarına da POSIX.1c ile 1995 yılında sokuldu. Böylece POSIX taşınabilir thread 
    fonksiyonları oluşturdu. POSIX'in taşınabilir thread fonksiyonlarının gerçekleştirimi için Linux ve diğer UNIX 
    varyantları çekirdek içerisinde de değişiklikler yapmak zorunda kalmıştır.

    Thread'lerin yalnızca bir akış belirttiğini söylemiştik. Proses kavramı ise tüm thread'lerle birlikte çalışmakta 
    olan programın tüm bilgilerini temsil etmektedir. Pek çok bilgi thread'e özgü değildir, prosese özgüdür. Örneğin 
    bir thread'in "çalışma dizini (current working directory)" diye bir kavram yoktur. Prosesin çalışma dizini diye bir 
    kavram vardır. Örneğin bir thread'in kullanıcı id'si, grup id'si diye bir kavram yoktur. Prosesin kullanıcı id'si 
    ve grup id'si diye bir kavram vardır. Yani çalışmakta olan programa ilişkin pek çok bilgi programın tüm thread'leri 
    için de geçerlidir.

    Aslında thread'ler aynı bellek alanını ve ortak bilgileri kullanan prosesler gibi de düşünülebilir. Zaten Linux 
    işletim sisteminde thread yaratmakla proses yaratmak aslında aynı çekirdek fonksiyonuyla yapılmaktadır. Yani bir 
    prosesin thread'lerini biz aynı bellek alanını kullanan prosesler gibi de düşünebiliriz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux işletim sisteminde thread'ler de adeta aynı bellek alanı üzerinde çalışan birer proses gibi düşünülmüştür. 
    Bu nedenle yalnızca prosesler için değil thread'ler için de task_struct nesneleri oluşturulmaktadır. Tabii işletim 
    sistemi bir prosesin thread'lerini yani ana thread'in ve diğer thread'lerin task_struct nesnelerini izleyen paragraflarda 
    göreceğimiz biçimde bağlı listelerde tutmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    UNIX/Linux sistemlerinde prosesler arasında kuvvetli bir "altlık-üstlük (parent-child)" ilişkisi vardır. Bir proses 
    yaratıldığında prosesin task_struct bilgilerinin çoğu üst prosesten (parent process) alınmaktadır. Örneğin bir program 
    başka bir programı fork/exec ile çalıştırdığında çalıştırılan program çalıştıran programın pek çok özelliğini almaktadır. 
    Örneğin fork işlemi sırasında fork yapan programın kullanıcı id'si ve çalışma dizini neyse yeni yaratılan alt prosesin 
    (child process) kullanıcı id'si ve çalışma dizini de aynı olur. Daha teknik olarak ifade edersek fork işlemi sırasında 
    alt proses için yaratılan task_struct nesnesinin pek çok elemanı üst prosesin task_struct nesnesinden kopyalanmaktadır.

    POSIX thread sisteminde thread'ler arasında önemli bir altlık-üstlük ilişkisi yoktur. Yani birkaç özel durum dışında 
    bir thread'i hangi thread'in yarattığının önemi yoktur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    task_struct yapısının pek çok gösterici elemanı vardır. Bu gösterici elemanları başka yapıları göstermektedir. Hatta 
    o gösterici elemanların gösterdiği yapıların elemanları da başka yapıları gösterebilmektedir. O halde task_struct 
    aslında dallı budaklı bir yapıdır. Biz bir bilginin proses kontrol bloğunda olduğunu söylediğimiz zaman o bilginin 
    hemen task_struct içerisindeki bir elemanda olduğunu kastetmeyeceğiz. O bilgi task_struct içerisindeki bir elemanın 
    gösterdiği başka bir yapının içerisinde de olabilir. Önemli olan o bilgiye task_struct yapısından hareketle erişilebilmesidir.

    Bir task_struct yapı nesnesinin ana elemanları diğer bir task_struct nesnesine kopyalanırsa (C'de zaten aynı türden 
    iki yapı nesnesi birbirine atandığında yapının karşılıklı elemanları kopyalanmaktadır) aslında asıl yapı nesnesi ile 
    kopyalanmış olan yapı nesnesinin gösterici elemanları aynı yerleri gösterir duruma gelir. Programlamada bu tür kopyalamaya 
    "sığ kopyalama (shallow copy)" denildiğini anımsayınız. Aslında task_struct yapısının bu biçimdeki dallı budaklı tasarımı 
    üst proses-alt proses ilişkisinde alt prosesin üst prosesten birtakım özellikleri alması (inherit etmesi) sürecinde 
    faydalar sağlamaktadır. Böylece alt prosesin task_struct nesnesine daha az eleman kopyalanmaktadır. Örneğin güncel 
    çekirdeklerdeki task_struct yapısının aşağıdaki kısmına dikkat ediniz:

    struct task_struct {
        /* ... */

        /* Filesystem information: */
        struct fs_struct		*fs;

        /* Open file information: */
        struct files_struct		*files;

        /* ... */
    };

    Burada aslında fs göstericisi aşağıdaki gibi bir yapıyı göstermektedir:

    struct fs_struct {
        int users;
        spinlock_t lock;
        seqcount_spinlock_t seq;
        int umask;
        int in_exec;
        struct path root, pwd;
    } __randomize_layout;

    Prosesin kök dizininin ve çalışma dizinin burada tutulduğuna dikkat ediniz. files göstericisi ise aşağıdaki gibi 
    bir yapıyı göstermektedir:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    fork işlemi sırasında sığ kopyalama yapıldığından dolayı fork işleminden sonra üst ve alt prosesin task_struct 
    nesnelerinin fs ve files göstericileri aynı fs_struct ve files_struct nesnelerini gösteriyor olacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Thread'lerin de tıpkı prosesler gibi task_struct nesnelerine sahip olduğunu belirtmiştik. Pekiyi işletim sisteminin 
    çizelgeleyici (scheduler) alt sistemi neleri çizelgelemektedir? İşte çizelgeleyici alt sistem aslında yalnızca 
    thread'leri yani thread'lere ilişkin task_struct nesnelerini çizelgelemektedir. Yani çizelgeleyici alt sistemin 
    "çalışma kuyruğunda (run queue)" yalnızca task_struct nesnelerinin olduğunu varsayabilirsiniz. Bir proses yaratıldığında 
    zaten o proses için yaratılan task_struct nesnesi aynı zamanda prosesin ana thread'inin task_struct nesnesi gibidir. 
    Başka bir deyişle aslında bütün task_struct nesneleri birer thread belirtmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Eskiden Linux sistemleri yalnızca tek işlemciyi destekliyordu. Sonra 2.0 versiyonuyla birlikte çekirdek zamanla birden 
    fazla işlemciyi ya da çekirdeği (core) destekler hale geldi. İşte bir thread çalışırken task_struct * türünden 
    current isimli global değişken o anda çalışmakta olan thread'e ilişkin task_struct nesnesini gösterecek biçimde 
    ayarlanmaktadır. Yani bir çekirdek kodunda current göstericisini görürseniz bu current göstericisi o anda çalışmakta 
    olan thread'e ilişkin task_struct nesnesini gösteriyor durumda olacaktır. Tabii bu current göstericisinin o anda 
    çalışan thread'e ilişkin task_struct nesnesini göstermesini "bağlamsal geçişi (context switch)" gerçekleştiren çizelgeleyici 
    alt sistem sağlamaktadır. Eskiden Linux yalnızca tek işlemciyi desteklerken current global değişkeni de toplamda bir 
    taneydi. Daha sonra Linux birden fazla işlemciyi ya da çekirdeği desteklemeye başlayınca current global değişkeni de 
    biçim değiştirdi.

    Bir süredir birden fazla işlemciyi ya da çekirdeği destekleyen Linux çekirdeklerinde artık current değişkeni bir 
    gösterici değil fonksiyon belirtmektedir. (Eskiden doğrudan task_struct türünden bir gösterici belirtiyordu.) Mevcut 
    çekirdeklerde current aşağıdaki gibi bir makro biçiminde tanımlanmıştır:

    #define current get_current()

    Görüldüğü gibi artık biz current değişkenini kullandığımızda aslında get_current() fonksiyonunu çağırıp bu fonksiyonun 
    geri dönüş değerini kullanmış olmaktayız. Pekiyi bu fonksiyon o anda işlemcide ya da çekirdekte çalışmakta olan 
    thread'in task_struct nesnesinin adresini nasıl bulup geri döndürmektedir? İşte bunun için Linux çekirdeklerinde 
    zaman içerisinde platforma da (işlemciye de) bağlı olacak biçimde çeşitli teknikler kullanılmıştır. Bu konu thread'in 
    "çekirdek stack alanı (kernel stack)" ve "işlemciye özgü global alanlar" konusuyla ilgilidir. Biz bu konuya başka 
    bir bölümde değineceğiz. Ancak burada pratik bir açıklama yapmak gerekirse şunları söyleyebiliriz: O anda thread'i 
    çalıştırmakta olan işlemcinin ya da çekirdeğin bir yazmacı (register) özel bir alanı göstermektedir (tabii bu 
    gösterme bağlamsal geçiş sırasında ayarlanmaktadır), çalışmakta olan thread'in task_struct nesnesinin adresi de 
    buradan hareketle elde edilmektedir.

    Biz kursumuzda current için "current göstericisi" ya da "current makrosu" terimlerini kullanacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi UNIX/Linux sistemlerinde kullanıcı modunda yeni bir proses fork POSIX fonksiyonu ile, yeni bir thread 
    de pthread_create fonksiyonu ile yaratılmaktadır. Yukarıda da belirttiğimiz gibi aslında bu iki çağrı da belirli bir 
    noktadan sonra çekirdek içerisindeki aynı fonksiyonları çağırmaktadır. Kullanıcı modundaki fork fonksiyonunun çağrı 
    mekanizması şöyledir:

    fork (kullanıcı modu) ---> sys_fork (çekirdek modu) ---> kernel_clone ---> ...

    pthread_create fonksiyonunun çağrı mekanizması da tipik olarak şöyledir (ancak kütüphaneden kütüphaneye ön aşamalar 
    değişebilir):

    pthread_create (kullanıcı modu) ---> sys_clone (çekirdek modu) ---> kernel_clone ---> ...

    O halde aslında çekirdek gözüyle bakıldığında bir thread başka bir thread tarafından yaratılmaktadır. Yani bu 
    yaratımda iki thread söz konusudur: Yaratan thread ve yaratılan thread. Yaratan thread'e ilişkin zaten task_struct 
    nesnesi mevcuttur. O halde yaratılan thread için de bir task_struct nesnesi oluşturulup bir biçimde bu yapılar 
    ilişkilendirilecektir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										13. Ders 30/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de task_struct nesneleri arasındaki ilişkilerin bağlı listelerle nasıl oluşturulduğu üzerinde duracağız. 
    Çekirdeğin bir prosesin thread'lerine sonra da alt proseslerine nasıl eriştiğini açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir prosesin thread'lerine erişim için eskiden task_struct içerisindeki thread_group isimli döngüsel bağlı 
    liste bağı kullanılıyordu:

    struct task_struct {
        /* ... */

        struct list_head thread_group;

        /* ... */
    };

    Bu thread_group listesi herhangi bir thread'ten başlanarak dolaşılırsa liste döngüsel olduğu için prosesin tüm thread'leri 
    elde edilebiliyordu. Yani elimizde bir prosesin herhangi bir thread'inin task_struct nesnesi varsa biz o thread'in 
    ilişkin olduğu prosesin tüm thread'lerinin task_struct nesnelerini bu bağlı listeyi dolaşarak elde edebiliyorduk. Ancak 
    4.2 çekirdeği (2015) ile birlikte bu veri yapısında bir değişiklik yapılmıştır. Yeni sistemde prosesin thread'lerine 
    ilişkin bağlı listenin kök düğümü prosesin sinyal bilgilerinin bulunduğu yerde saklanmaktadır. Dolayısıyla artık thread 
    dolaşımına bu kök düğümden başlanmalıdır. Prosesin sinyal bilgileri task_struct içerisindeki signal göstericisinin 
    gösterdiği yerdeki signal_struct yapısı içerisinde tutulmaktadır. İşte signal_struct yapısının thread_head elemanı da 
    prosesin thread'lerine ilişkin bu kök düğümü belirtmektedir. Prosesin thread'lerinin bağlı listesi için ise task_struct 
    içerisindeki thread_node elemanı kullanılmaktadır. Thread listesine ilişkin ilgili elemanları şöyle betimleyebiliriz:

    struct signal_struct {
        /* ... */

        struct list_head thread_head;

        /* ... */
    };

    struct task_struct {
        /* ... */

        struct signal_struct *signal;
        struct list_head thread_node;

        /* ... */
    };

    signal_struct nesnesi içerisindeki thread_head kök düğümü prosesin ilk thread'ine ilişkin task_struct nesnesindeki 
    thread_node düğümünü göstermektedir. Prosesin thread'lerine ilişkin task_struct nesneleri de bu thread_node düğümüyle 
    birbirine bağlanmıştır.

    thread_node bağlı listesi çekirdeğe ilk eklendiğinde bir süre eski thread_group listesi de muhafaza edilmişti. Yani 
    her iki sistem birlikte kullanılabiliyordu. Sonra tamamen eski thread_group listesi task_struct içerisinden kaldırıldı. 
    Artık yeni çekirdeklerde prosesin thread'lerinin task_struct nesnelerini elde etmek için prosesin signal_struct 
    nesnesindeki thread_head kök düğümünden başlanarak bağlı listenin thread_node düğümlerinin dolaşılması gerekmektedir.

    thread_head <---> thread_node <---> thread_node <---> thread_node <---> thread_node <---> thread_head

    signal_struct içerisindeki list_head kök düğümünün hemen önündeki düğümün ana thread'e ilişkin olacağı varsayımında 
    bulunmayınız. Ancak çekirdek kodları incelendiğinde mevcut çekirdeklerde ilk düğümün ana thread'e ilişkin olduğu 
    görülmektedir. Çekirdek kodlarının bazı yerlerinde bu varsayıma dayanılarak da birtakım işlemler yapılmıştır.

    Aslında Linux çekirdeklerinde prosesin tüm thread'lerine ilişkin task_struct içerisinde bulunan signal göstericisi 
    aynı signal_struct nesnesini göstermektedir. Yani thread_head kök düğümüne biz prosesin ana thread'inden erişmek 
    zorunda değiliz. Proesin herhangi bir thread'ine ilişkin task_struct nesnesindeki signal göstericisi yoluyla bu 
    list_head kök düğümüne erişebiliriz.

    Çekirdek içerisinde bir task_struct yapısının prosesin ana thread'ine ilişkin olup olmadığını belirleyen 
    thread_group_leader isimli bir makro da vardır:

    #include <linux/sched/signal.h>

    static inline bool thread_group_leader(struct task_struct *p)
    {
        return p->exit_signal >= 0;
    }

    Tabii aslında bir task_struct nesnesinin ana thread'e ilişkin olup olmadığı p == p->group_leader ya da 
    p->pid == p->tgid işlemiyle anlaşılabilir. Ancak özel bir bilgi olarak task_struct içerisindeki exit_signal elemanı 
    da bu bilgiyi verebilmektedir. Bu eleman eğer thread ana thread değilse -1 değerinde, ana threda ise >= 0 değerinde 
    olmaktadır.

    Linux çekirdeğinde terminoloji bağlamında "prosesin ana thread'i" yerine "thread grup lideri (thread group leader)" 
    terimi tercih edilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi UNIX/linux sistemlerinde her prosesin sistem genelinde tek (unique) olan bir "proses id (pid)" değeri 
    vardır. Linux çekirdeği proses id değerini task_struct içerisindeki pid elemanında saklamaktadır:

    struct task_struct {
        /* ... */

        pid_t	pid;

        /* ... */
    };

    Linux'ta her thread'in ayrı bir task_struct nesnesi olduğuna göre ve pid değeri de task_struct içerisinde tutulduğuna 
    göre bu durumda her thread'in de ayrı bir pid değeri mi vardır? Bu konuya açıklık getirelim. POSIX standartlarına göre 
    thread'lerin pid değeri olmaz, pid değerleri proseslere özgüdür. getpid POSIX fonksiyonu prosesin hangi thread akışı 
    içerisinde çağrılırsa çağrılsın prosese ilişkin pid değerini vermektedir. İşte Linux çekirdeği bu uyumu korumak için 
    şöyle bir yöntem izlemiştir: Prosesin pid değeri aslında prosesin ana thread'ine ilişkin task_struct nesnesi içerisindeki 
    pid değeridir. Evet Linux'ta her thread'in task_struct nesnesinde ayrı bir pid değeri vardır fakat prosesin pid değeri 
    denildiğinde prosesin ana thread'inin pid değeri anlaşılmaktadır. Ana thread'in pid değeri aynı zamanda task_struct 
    içerisindeki tgid isimli bir eleman da tutulmaktadır. task_struct nesnesi hangi thread'e ilişkin olursa olsun tgid elemanı 
    her zaman prosesin ana thread'inin pid değerini tutmaktadır:

    struct task_struct {
        /* ... */

        pid_t   pid;        /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;       /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */

        /* ... */
    };

    Bu durumda biz prosesin thread'lerine ilişkin task_struct nesnelerini dolaştığımızda bu task_struct nesnelerinin 
    birinde pid == tgid olacaktır. İşte bu thread prosesin ana thread'idir. O halde aslında bazı yarıntıları da göz ardı 
    edersek getpid POSIX fonksiyonun çağırdığı sys_getpid sistem fonksiyonu prosesin proses id değerini doğrudan current 
    göstericisinin gösterdiği task_struct nesnesinin içerisindeki tgid değerinden alarak vermektedir:

    getpid (kullanıcı modu) ---> sys_getpid (öekirdek modu) ---> current tarafından gösterilen task_struct nesnesindeki 
    tgid değeri

    Şimdi aklınıza "neden her thread'te ayrıca ana thread'in pid değeri tgid ismiyle tutuluyor?" sorusu gelebilir. Bunun 
    iki nedeni vardır: Birincisi prosesin ana thread'i sonlanabilir, bu durumda ana thread'e ilişkin task_struct nesnesine 
    erişilemeyebilir. İkincisi ise bu yolla prosesin id değerine bu yolla daha hızlı erişimin sağlanabilmesidir.

    Anımsanacağı gibi POSIX thread kütüphanesinde (pthread kütüphanesini kastediyoruz) thread'lerin id değerleri sistem 
    genelinde tek değil proses genelinde tektir. Yani POSIX'teki pthread_t ile temsil edilen thread id değerinin Linux 
    çekirdeğindeki task_struct içerisinde bulunan pid değeri ile bir ilgisi yoktur. Pekiyi Linux'ta "her thread'in ayrı 
    bir pid değerinin olmasının" Linux için bir anlamı var mıdır? İşte Linux'ta thread'ler sanki birer proses gibi ele 
    alındığı için bu durum thread'lerin bazı ek yeteneklere sahip olmasını sağlamıştır. Örneğin bu sayede biz POSIX 
    standartlarında olmayan bazı işlemleri Linux'ta yapabilmekteyiz. Yani Linux çekirdeğinde aslında pid isteyen 
    fonksiyonlara biz thread'teki pid değerini geçirerek thread'e özgü işlemlerin yapılmasını da sağlayabilmekteyiz. 
    Kullanıcı modundan thread'e ilişkin pid değeri Linux'a özgü gettid fonksiyonu ile elde edilebilmektedir:

    #define _GNU_SOURCE
    #include <unistd.h>

    pid_t gettid(void);

    gettid fonksiyonun bir POSIX fonksiyonu olamdığına dikkat ediniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında Linux çekirdeği thread'lere ilişkin task_struct nesnelerinde yalnızca prosesin ana thread'ine ilişkin pid 
    değerini değil aynı zamanda ana thread'in task_struct nesnesinin adresini de tutmaktadır. Ana thread'in task_struct 
    nesnesinin adresi task_struct yapısının group_leader elemanında tutulmaktadır. Yani herhangi bir thread akışı 
    içerisinde o thread'in task_struct nesnesindeki group_leader göstericisi zaten prosesin ana thread'inin task_struct 
    nesnesinin adresini tutmaktadır:

     struct task_struct {
        /* ... */

        pid_t   pid;                        /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                       /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;   /* ana thread'e ilişkin task_struct nesnesinin adresi tutuluyor */

        /* ... */
    };

    Pekiyi prosesin ana thread'i sonlandığında group_leader göstericisinin ve signal_struct yapısı içerisindeki list_head 
    düğümünün durumu ne olacaktır? İşte Linux çekirdeği prosesin ana thread'i sonlandığında istisna olarak ana thread'e 
    ilişkin task_struct nesnesini yok etmemektedir (başka bir deyişle "hortlak (zombie)" olarak tutmaktadır). Yani bu 
    group_leader göstericisi her zaman geçerli bir task_struct nesnesini gösteriyor durumda olmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi aklınıza "mademki Linux'ta her thread'in task_struct nesnesinde zaten prosese ilişkin bilgiler tutuluyor bu 
    durumda prosesin ana thread'ine erişmenin ne gereği var?" sorusu gelebilir. Evet gerçekten de aslında bir prosesin 
    bilgilerinin çok büyük kısmı zaten thread'lere ilişkin task_struct nesnelerinde da bulunmaktadır. Ancak yine de bazı 
    bilgilere yalnızca ana thread'ten hareketle elde edilmektedir. Fakat çekirdeğin ana thread'in task_struct nesnesine 
    erişim gerekliliği çok az düzeydedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de Linux çekirdeğinde posesler arasndaki altlık-üstlük ilişkisinin nasıl sağlandığı üzerinde duralım. Bilindiği 
    gibi UNIX/Linux sistemlerinde her proses başka bir prosesin thread'i tarafından fork işlemi ile yaratılmaktadır. Bu 
    durumda prosesi yaratan thread'in ilişkin olduğu prosese üst proses (parent process) yeni yaratılan prosese de "alt 
    process (child process)" denilmektedir. UNIX/Linux sistemlerinde prosesler arasındaki altlık-üstlük ilişkisi çok 
    önemlidir. Örneğin wait fonksiyonları alt prosesler sonlanana kadar üst prosesi bekletip alt prosesin exit kodunu 
    almaktadır. Öte yandan Linux çekirdeğinde yalnızca prosseslerin değil her thread'in bir task_struct nesnesi vardır. 
    Pekiyi altlık-üstlük ilişkisi task_struct nesnelerinde nasıl sağlanmaktadır?
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Her thread'in task_struct nesnesi içerisindeki real_parent ve parent isimli göstericiler o thread'i yaratan thread'in 
    task_struct nesnesinin adresini tutmaktadır. fork işlemi sırasında alt prosesin ana thread'i için yeni bir task_struct 
    nesnesi yaratılmakta ve bu nesnenin real_parent ve parent elemanları fork işlemini yapan thread'in task_struct nesnesini 
    gösterir duruma getirilmektedir. Aynı durum tamamen pthread_create fonksiyonu tarafından yaratılan thread'ler için de 
    geçerlidir. Dolayısıyla fork işlemi ile artık yeni yaratılan alt prosesin ana thread'ine ilişkin tast_struct nesnesinin 
    real_parent ve parent elemanları üst prosese ilişkin thread'in task_struct nesnesini gösteriyor durumda olur.

    struct task_struct {
        /* ... */

        pid_t   pid;                                /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                               /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;           /* ana thread'e ilişkin task_struct nesnesini gösteriyor */

        struct task_struct __rcu	*real_parent;   /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */
        struct task_struct __rcu	*parent;        /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */

        /* ... */
    };

    Burada real_parent üst prosese ilişkin thread'in task_struct nesnesinin adresini tutmaktadır. Normalde real_parent 
    elemanı ile parent elemanı aynı task_struct nesnesini gösterir. Ancak seyrek durumlarda (örneğin debug işlemlerinde 
    ve ptracce işlemlerinde) geçici olarak parent başka bir task_struct nesnesini de (reparenting işlemi) gösteriyor durumda 
    olabilmektedir.

    Burada bir noktayı yeniden vurgulamak istiyoruz: fork işlemini yapan thread hangi thread olursa olsun yaratılan alt 
    prosesin ana thread'ine ilişkin task_struct nesnesindeki real_parent ve parent göstericileri üst prosesteki fork yapan 
    thread'in task_struct nesnesini göstermektedir. Anımsanacağı gibi getppid POSIX fonksiyonu üst prosesin pid değerini 
    vermektedir. İşte bu fonksiyonun çağırdığı sys_getppid sistem fonksiyonu real_parent elemanının gösterdiği task_struct 
    nesnesi içerisindeki tgid değerini geri döndürmektedir.

    Bir thread başka bir thread'i yarattığında da benzer biçimde yaratılan thread'in task_struct nesnesinin real_parent 
    ve parent göstericileri onu yaratan thread'in task_struct nesnesini göstermektedir. Thread'ler böyle birbirini 
    yarattığında bunlara ilişkin task_struct nesnelerinin tgid ve group_leader elemanlarının her zaman ana thread'e ilişkin 
    olarak kalacağına dikkat ediniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de bir prosesin alt proseslerinin nasıl bağlı listelerde tutulduğu üzerinde duralım.

    Örneğin bir proses üç kez fork yapmış olsun. Bu durumda bu prosesin üç tane alt prosesi olacaktır. Üst prosesleri aynı 
    olan proseslere "kardeş prosesler (sibling processes) de denilmektedir. İşte bir prosesin alt prosesleri children isimli 
    kök düğümle erişilen sibling bağları yoluyla bağlı listede tutulmaktadır:

    struct task_struct {
        /* ... */

        pid_t   pid;                                /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                               /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;           /* ana thread'e ilişkin task_struct nesnesini gösteriyor */

        struct task_struct __rcu	*real_parent;   /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */
        struct task_struct __rcu	*parent;        /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */

        struct list_head children;                  /* alt proses listesinin kök düğümü */
        struct list_head sibling;                   /* alt proseslerin bağlı liste bağı */

        /* ... */
    };

    Burada children bir kök düğüm biçimindedir. children aslında alt proses listesindeki ilk task_struct nesnesinin 
    sibling elemanını göstermektedir. Sonra her kardeş task_struct nesnesi bir sonraki kardeş task_struct nesnesini 
    gösterir. O halde bir prosesin alt prosesleri bu sibling düğümleri dolaşılarak elde edilmektedir. Proseslerin her 
    thread'inin ayrı bir task_struct ile temsil edildiğini belirtmiştik. Pekiyi bu durumda sibling bağları hangi task_struct 
    nesnelerini göstermektedir? İşte sibling bağları her zaman prosesin ana thread'ine ilişkin (grup liderine ilişkin) 
    task_struct nesnelerini göstermektedir. Örneğin bir prosesin bir thread'i (buna tp1 thread'i diyelim) fork yapsın 
    sonra diğer bir thread'i de (buna da tp2 diyelim) fork yapsın. İlk fork işleminden elde edilen ana thread'e tc1 
    ikinci fork işleminden elde edilen ana thread'e de tc2 diyelim. İşte burada sibling bağlı listesinde tc1 ve tc2 
    thread'lerine ilişkin task_struct nesneleri tutulmaktadır. tc1'in parent ve real_parent göstericileri tp1 thread'inin 
    task_struct nesnesini, tc2'nin parent ve real_parent götericileri ise tp2'nin task_struct nesnesini göstermektedir. 
    (Tabii aslında bu göstericiler doğrudan task_struct nesnesini göstermemektedir, asıl nesnenin adresi container_of 
    makrosuyla elde edilmektedir. Biz pratik bir anlatım için durumu böyle ifade ediyoruz.)

    Bir thread akışında yeni bir thread yaratıldığı zaman yaratılan thread bu children/sibling listesinde bulunmaz. 
    prosesin thread'leri yalnızca list_head/list_node listesinde tutulmaktadır. children/sibling listesinde her zaman 
    yalnızca alt proseslere ilişkin ana thread'lerin task_struct nesneleri tutulmaktadır.

    Pekiyi bir prosesin tüm thread'lerine ilişkin task_struct nesnelerindeki children elemanları aynı listeyi mi 
    göstermektedir? Yani ben alt proseslerin ana thread'lerine ilişkin task_struct nesnelerini dolaşmak istesem bunu 
    kendi ana thread'imin children kök düğümünden itibaren mi yapmak zorundayım? İşte Linux'ta alt proses listesi 
    ana thread'in children kök düğümünden hareketle dolaşılmak zorundadır. Alt prosesler yaratıldığında yalnızca ana 
    thread'in children kök düğümü güncellenmektedir. Yani prosesin diğer thread'lerinin children düğümleri bu işlemlerde 
    kullanılmamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek ayrıca proseslerin ana thread'lerine ilişkin task_struct nesnelerini de task_struct içerisindeki tasks 
    isimli bir düğüm ile birbirine bağlamaktadır. Yani eğer sistemdeki tüm proseslerin ana thread'lerine ilişkin task_struct 
    nesnelerinin dolaşılması için bu tasks düğümü kullanılabilir:

        struct task_struct {
        /* ... */

        pid_t   pid;                                /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                               /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;           /* ana thread'e ilişkin task_struct nesnesini gösteriyor */

        struct task_struct __rcu	*real_parent;   /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */
        struct task_struct __rcu	*parent;        /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */

        struct list_head children;                  /* alt proses listesinin kök düğümü */
        struct list_head sibling;                   /* alt proseslerin tutulduğu liste bağı */
        struct struct list_head	tasks;              /* ana thread'lere ilişkin task_struct nesnelerinin tutulduğu liste bağı */

        /* ... */
    };

    Pekiyi bu tasks düğümlerine ilişkin bağlı listenin kök düğümü nerededir? İşte Linux'ta boot işlemi sırasında çekirdek 
    yüklenip birtakım ilk işlemler yapıldıktan sonra ilk oluşturulan prosese init_task denilmektedir. Bu init_task 
    prosesine ilişkin task_struct nesnesi statik bir biçimde yeni çekirdeklerde "init/init_task.c" dosyasında bulunmaktadır:

    struct task_struct init_task __aligned(L1_CACHE_BYTES) = {
        /* ... */

        .ptraced	= LIST_HEAD_INIT(init_task.ptraced),
        .ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
        .real_parent	= &init_task,
        .parent		= &init_task,
        .children	= LIST_HEAD_INIT(init_task.children),
        .sibling	= LIST_HEAD_INIT(init_task.sibling),
        .group_leader	= &init_task,
        RCU_POINTER_INITIALIZER(real_cred, &init_cred),
        RCU_POINTER_INITIALIZER(cred, &init_cred),
        .comm		= INIT_TASK_COMM,
        .thread		= INIT_THREAD,
        .fs		= &init_fs,
        .files		= &init_files,
    #ifdef CONFIG_IO_URING
        .io_uring	= NULL,
    #endif
        .signal		= &init_signals,
        .sighand	= &init_sighand,

        /* ... */
    };

    init_task prosesinin pid değeri 0'dır. Anımsanacağı gibi pid için 0 değeri POSIX sistemlerinde geçerli bir değer 
    değildir. Geleneksel olarak UNIX/Linux sistemlerinde sistem boot edildiğinde yaratılan bu tür proseslere "swapper" 
    ya da "pager" da deniliyordu. init_task prosesi init prosesini yarattıktan sonra işlevini sonlandırarak durdurulmaktadır. 
    Ancak bu prosese ilişkin task_struct nesnesi gerçek anlamda hiçbir zaman yok edilmemektedir. İşte init_task prosesinin 
    task_struct nesnesi proseslerin ana thread'lerine ilişkin task_struct nesnelerini dolaşmak için bir kök düğüm olarak 
    kullanılmaktadır. O halde tasks düğümlerinin kök düğümü init_task.tasks elemanıdır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeklerinde 2.6'lı versiyonlara kadar proses grubundaki prosesler ve oturuma ilişkin prosesler bağlı 
    listelerde tutulmuyordu. Sistem bir id'ye ilişkin proses grubuna ve oturuma ilişkin task_struct nesnelerini sistemdeki 
    tüm task_struct nesnelerini dolaşarak buluyordu. Ancak 2.6'lı çekirdeklerle birlikte artık bu arama işlemi için de 
    bağlı liste zincirleri kullanılmaya başlanmıştır. Bu işlemin nasıl yürütüldüğünü izleyen paragraflarda açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi yukarıda açıkladığımız konuyu madde madde özetleyelim:

    - Her thread'in ayrı bir pid değeri vardır, ancak POSIX'teki getpid fonksiyonu ana thread'in (thread grup liderinin) 
    pid değerini vermektedir.

    - Bir prosesin tüm thread'lerini dolaşmak için signal göstericisinin gösterdiği signal_struct yapısı içerisindeki 
    thread_head bağlı listesi thread_node düğümleriyle dolaşılır.

    - task_struct içerisindeki real_parent ve parent göstericileri o prosesi ya da thread'i yaratan thread'in task_struct
    nesnesini göstermektedir.

    - task_struct içerisindeki tgid ana thread'in pid değerini, thread_group göstericisi ise ana thread'in task_struct 
    nesnesinin adresini tutmaktadır. Ana thread sonlansa bile onun task_struct nesnesi proses sonlanana kadar muhafaza 
    edilmektedir.

    - Bir prosesin bütün alt proseslerine ilişkin ana thread'lerinin task_struct nesneleri children/sibling bağlı 
    listesinde tutulmaktadır.

    - Prosesin alt proseslerini elde etmek için prosesin ana thread'inin children kök düğümünden yola çıkmak gerekir.

    - Çekirdek yaratılmış olan tüm proseslerin ana thread'lerine ilişkin task_struct nesnelerini ayrıca task_struct 
    içerisindeki tasks düğümüyle birbirine bağlamaktadır. Bu düğümüm kökü de init_task.tasks elemanıdır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										14. Ders 31/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda konuyla ilgili bazı işlemlerin bu bağlı listeler kullanılarak nasıl gerçekleştirileceğine ilişkin tipik
    soruları ve bunların yanıtlarını veriyoruz:

    Soru: Bir prosesin tüm thread'leri nasıl elde edilebilir?
    Yanıt: Bir prosesin tüm threadleri prosesin signal yapısı içerisinde bulunan thread_head elemanı kök yapılarak 
    thread_node düğümlerinin dolaşılmasıyla elde edilebilir.

    Soru: Bir prosesin bütün alt prosesleri nasıl elde edilebilir?
    Yanıt: Prosesin ana thread'indeki children düğümü kök yapılarak sibling düğümlerinin dolaşılmasıyla prosesin tüm alt 
    proesleri elde edilebilir.

    Soru: Bir task_struct nesnesinin prosesin ana thread'ine ilişkin olduğu nasıl anlaşılır?
    Yanıt: Eğer task_struct nesnesinde pid == tgid ise bu task_struct ilgili prosesin ana thread'ine ilişkin task_struct 
    nesnesidir. Zaten prosesin tüm thread'lerine ilişkin task_struct nesnelerinde group_leader göstericisi ana thread'e 
    ilişkin bu task_struct nesnesini göstermektedir. O anda çalışmakta olan thread eğer prosesin ana thread'i ise 
    current == group_leader koşulunun sağlanacağına da dikkat ediniz.

    Soru: Sistemdeki tüm proseslerin ana thread'lerine ilişkin task_struct nesneleri nasıl dolaşılır?
    Yanıt: init_task prosesindeki tasks düğümü kök düğüm yapılıp tasks düğümleri dolaşılırsa tüm proseslerin ana 
    thread'lerine ilişkin task_struct nesneleri elde edilebilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de yukarıda gördüğümüz bağlı listelerin nasıl dolaşıldığına yönelik açıklamalar eşliğinde küçük örnekler 
    yapacağız. Ancak örnekleri yapmadan önce pratik bir ortam oluşturmamız gerekiyor. Bu biçimdeki küçük testler için 
    çekirdeği yeniden derlememize gerek yoktur. Basit bir karakter aygıt sürücüsü yazarak da bu işlemleri yapabiliriz. 
    Aygıt sürücülerinin yazımı kursumuzun konusu içerisinde değildir. Bu konu "UNIX/Linux Sistem Programlama" ve "Gömülü 
    Linux Sistemleri - Geliştirme ve Uygulama" kursunda ele alınmaktadır. Ancak biz burada bir karakter aygıt sürücüsünün 
    nasıl derlenip yükleneceği ve boşaltılacağı konusunda hedefe yönelik açıklamalar yapmakla yetineceğiz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Genellikle bir Linux sistemini yüklediğimizde zaten çekirdek modüllerini ve aygıt sürücüleri oluşturabilmek için 
    gereken başlık dosyaları ve diğer gerekli öğeler "/usr/src" dizini içerisindeki "linux-headers-$(uname -r)" dizininde 
    yüklü biçimde bulunmaktadır. Ancak bunlar yüklü değilse Debian tabanlı sistemlerde bunları şöyle yükleyebilirsiniz:

    $ sudo apt-get install linux-headers-$(uname -r)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Karakter aygıt sürücüleri bir ya da birden fazla kaynak ".c" dosyası oluşturularak yazılabilmektedir. Tabii bunların 
    yanı sıra programcı "*.h" uzantılı bazı başlık dosyaları da oluşturabilmektedir. Aygıt sürücülerin derlenmesi oldukça 
    karmaşık bir süreçle gerçekleşmektedir. Bu nedenle derleme işlemi için hazır "make dosyaları" bulundurulmuştur. Programcı 
    kendisi bir "Makefile" dosyası oluşturup asıl make dosyalarını buradan devreye sokar. Aygıt sürücüler için en basit 
    "Makefile" dosyası aşağıdaki gibi oluşturulabilir:

    obj-m += ${file}.o

    all:
        make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
    clean:
        make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

    Bu "Makefile" dışarıdan "file" isimli bir argüman almaktadır. Aygıt sürücünün derlenmesi şöyle yapılır:

    make file=<aygıt_sürücü_dosyasının_ismi>

    Örneğin:

    make file=mydriver

    Kaynak dosya için uzantı belirtilmediğine dikkat ediniz.

    Aygıt sürücüler derlendikten sonra ".ko" uzantılı bir dosya elde edilecektir. Bu dosya çekirdeğe sisteme yüklenmelidir. 
    Aygıt sürücü dosyalarını (".ko" dosyalarını) çekirdeğe yüklemek için "insmod" ya da "depmod" komutları kullanılmaktadır. 
    "Ancak "depmod" komutu birtakım bağımlılıklara da bakmaktadır. Biz kursumuzda aygıt sürücüleri "insmod" komutuyla 
    yükleyeceğiz. Tabii aygıt sürücüleri yükleyebilmek için "root" önceliğine sahip olmak gerekir. Modern UNIX/Linux 
    sistemlerinde bu işlem "sudo" komutuyla yapılmaktadır. Örneğin:

    $ sudo insmod mydriver.ko

    "insmod" ile yüklenmiş olan aygıt sürücü çekirdekten "rmmod" komutuyla çıkartılır. Komut şöyle kullanılır:

    $ sudo rmmod mydriver.ko

    Burada ".ko" uzantısı belirtilmeyebilir.

    Aygıt sürücünün yüklenip yüklenmediğini anlayabilmek için "/proc/devices" dosyasına başvurabilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aygıt sürücüler çekirdek modunda çekirdeğin bir parçasıymış gibi çalışan modüllerdir. Nasıl masaüstü bilgisayarların 
    genişleme yuvalarına kart takıldığında o kart donanımın bir parçası haline geliyorsa aygıt sürücüler de benzer biçimde 
    adeta çekirdeğin bir parçası haline gelmektedir. Aygıt sürücüler içerisinde kullanıcı modundaki standart C fonksiyonları 
    ya da POSIX fonksiyonları kullanılamaz. Aygıt sürücüler yalnızca çekirdek içerisinde "export" edilmiş fonksiyonları 
    kullanabilirler. (Yani çekirdekteki her fonksiyonu aygıt sürücü kullanamaz, yalnızca export edilmiş olan başka bir 
    deyişle "kullanılmasına izin verilmiş olan" fonksiyonları kullanabilir.) Bir fonksiyon ya da nesne çekirdek kodlarında 
    EXPORT_SYMBOLS makrosu ile export edilmektedir. Örneğin:

    ...
    void clear_nlink(struct inode *inode)
    {
        if (inode->i_nlink) {
            inode->__i_nlink = 0;
            atomic_long_inc(&inode->i_sb->s_remove_count);
        }
    }
    EXPORT_SYMBOL(clear_nlink);
    ...
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında Linux sistemlerinde bu bağlamda birbirleriyle ilişkili iki kavram vardır: "Çekirdek modülleri (kernel modules)" 
    ve "aygıt sürücüler (device drivers)". Çekirdeğin içerisine yüklenebilen modüllere "çekirdek modülleri" denilmektedir. 
    Eğer bir çekirdek modülüne kullanıcı modundan bir dosya (buna "aygıt dosyası (device file)" denilmektedir) yoluyla 
    erişilip aygıt sürücüye işlemler yaptırılabiliyorsa bu tür çekirdek modüllerine "aygıt sürücü (device drivers)" de 
    denilmektedir. Yani her aygıt sürücü bir çekirdek modülü belirtir ancak her çekirdek modülü bir aygıt sürücü beirtmek 
    zorunda değildir. Sistemdeki yüklü olan çekirdek modülleri "/proc/modules" dosyası yoluyla, yüklü olan aygıt sürücüler 
    ise "/proc/devices" dosyası yoluyla görüntülenebilmektedir.

    Aşağıda iskelet bir çekirdek modülü görüyorsunuz:

    #include <linux/module.h>
    #include <linux/kernel.h>

    MODULE_LICENSE("GPL");

    static int helloworld_init(void);
    static void helloworld_exit(void);

    static int helloworld_init(void)
    {
        printk(KERN_INFO "Hello World...\n");

        return 0;
    }

    static void helloworld_exit(void)
    {
        printk(KERN_INFO "Goodbye World...\n");
    }

    module_init(helloworld_init);
    module_exit(helloworld_exit);

    Bir çekirdek modülü "insmod" komutuylaile yüklendiğinde onun module_init makrosuyla belirtilen fonksiyonu çağrılmaktadır. 
    Çekirdek modülü içerisinde ilk işlemler bu fonksiyonda yapılmaktadır. Çekirdek modülü "rmmod" ile sistemden çıkartılırken 
    de çekirdek modülündeki module_exit makrosunda belirtilen fonksiyonu çağrılmaktadır. Birtakım son işlemler (örneğin geri 
    bırakma işlemleri) bu fonksiyonda yapılmaktadır.

    Çekirdek modülleri içerisinde ekrana birşeyler yazamayız (mümkün olsa da uygun değildir). Eğer çekirdek modülleri 
    içerisinde birtakım mesajlar iletilmek isteniyorsa bu mesajlar bir "log" sistemine "yazdırılmaktadır. Bu log sistemine 
    "kernel ring buffer" da denilmektedir. Bu log sistemine yazdırılan yazılar "dmesg" komutuyla ya da "/var/log/syslog" 
    dosyası ile görüntülenebilmektedir. Çekirdek modülü içerisinde bu log sistemine mesajlar printk isimli çekirdek 
    fonksiyonuyla yazılmaktadır. Örneğin:

    printk(KERN_INFO "Goodbye World...\n");

    Burada KERN_INFO mesajın türünü belirtmektedir. KERN_INFO ile diğer argüman arasında virgül atomunun bulunmadığına 
    dikkat ediniz. KERN_INFO aslında bir bir sembolik sabittir ve bir string açımı yapmaktadır. (C'de yana yana iki 
    string'in birleştirildiğini anımsayınız.) KERN_INFO mesaj türü ile mesaj yazdırmanın daha basit yolu pr_info 
    makrosunu kullanmaktadır. Zaten bu makro da printk açılımı yapmaktadır. Örneğin:

    pr_info("Goodbye World...\n");

    printk fonksiyonun kullanımı büyük ölçüde printf fonksiyonuna benzemektedir. Yani bu fonksiyonla değişkenler 
    içerisindeki değerler de yazdırılabilmektedir.

    Çekirdek modüllerini ve aygıt sürücüleri yazarken build sistemi include dosyaları için aşağıdaki dizini kök dizin 
    kabul etmektedir:

    "/lib/modules/$(shell uname -r)/build/include"

    Burada build dizini aslında bir sembolik bağlantı belirtmektedir. Bu sembolik bağlantı başlık dosyalarının (ya da 
    çekirdek kodlarının bulunduğu) ana dizine referans etmektedir. Dolayısıyla include işlemlerinde kök dizin aslında 
    çekirdek kodlarındaki "include" dizini olmaktadır. Çekirdek modüllerinde ve aygıt sürücülerde bir başlık dosyasını 
    include ederken bu çekirdeğin kaynak kod ağacındaki "include" dizininden itibaren yol belirtilmesi gerekir. Örneğin:

    #include <linux/module.h>

    Burada aslında Linux kaynak kod ağacındaki include dizininin itibaren bir yol belirtilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi aygıt sürücülere bir dosya yoluyla kullanıcı modundan erişilebilmektedir. Aygıt 
    sürücülere erişmekte kullanılan bu özel dosyalara "aygıt dosyaları (device files)" denilmektedir. Aygıt dosyaları 
    bir disk dosyası değildir. Yalnızca bir dizin girişi belirtmektedir. Aygıt dosyası open POSIX fonksiyonuyla açıldığında 
    çekirdek diske değil yüklü olan aygıt sürücüye referans etmektedir. Kullanıcı modundaki programcı aygıt dosyası 
    üzerinde open, read, write, lseek, close gibi klasik dosya işlemlerini yaptığında aslında programın akışı çekirdek 
    moduna (kernel mode) geçerek aygıt sürücü içerisindeki ilgili fonksiyonlar çalıştırılmaktadır. Yani biz bu mekanizmayla 
    aslında normalde kullanıcı modunda yapamayacağımız birtakım işlemleri dolaylı bir biçimde yapabilir duruma gelmekteyiz.

    Linux sistemlerinde aygıt sürücüler (ve dolayısıyla aygıt dosyaları) iki kısma ayrılmaktadır:

    - Karakter Aygıt Sürücüleri (Character Device Drivers)
    - Blok Aygıt Sürücüleri (Block Device Drivers)

    En çok kullanılan aygıt sürücüler karakter aygıt sürücüleridir. Blok aygıt sürücüleri "hard disk, SSD, CDROM, ramdisk" 
    gibi blok blok transferlerin yapıldığı aygıtlar söz konusu olduğunda kullanılmaktadır. Karakter aygıt sürücülerine 
    ilişkin aygıt dosyaları UNIX/Linux sistemlerinde 'c' dosya türü ile , blok aygıt sürücülerine ilişkin aygıt dosyaları 
    ise 'b' dosya türü ile temsil edilmektedir. Örneğin:

    ...
    crw-rw----   1 root disk     21,   0 Ağu 31 10:49 sg0
    crw-rw----+  1 root cdrom    21,   1 Ağu 31 10:49 sg1
    drwxrwxrwt   2 root root          40 Ağu 31 12:30 shm
    crw-------   1 root root     10, 231 Ağu 31 10:49 snapshot
    drwxr-xr-x   3 root root         200 Ağu 31 10:49 snd
    brw-rw----+  1 root cdrom    11,   0 Ağu 31 10:52 sr0
    lrwxrwxrwx   1 root root          15 Ağu 31 10:49 stderr -> /proc/self/fd/2
    lrwxrwxrwx   1 root root          15 Ağu 31 10:49 stdin -> /proc/self/fd/0
    lrwxrwxrwx   1 root root          15 Ağu 31 10:49 stdout -> /proc/self/fd/1
    crw-rw-rw-   1 root tty       5,   0 Ağu 31 11:45 tty
    crw--w----   1 root tty       4,   0 Ağu 31 10:49 tty0
    crw--w----   1 root tty       4,   1 Ağu 31 10:49 tty1
    crw--w----   1 root tty       4,  10 Ağu 31 10:49 tty10
    ...

    UNIX/Linux sistemlerinde aygıt dosyaları genel olarak "/dev" dizininde bulunmaktadır. Ancak böyle bir zorunluluk 
    yoktur. Eskiden bu "/dev" dizini disk tabanlı bir dosya sistemi içerisinde bulunuyordu. Sonra zamanla bu dosya 
    sistemi ramdisk tabanlı hale getirildi. Linux sistemlerinde "udev" denilen bir servis (daemon) ile bu dizinde aygıt 
    dosyalarının bu dizinde yaratılması mümkün hale getirilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Her aygıt sürücünün "majör" ve "minör" numarası vardır. Bu majör ve minör numaralar çekirdeğin aygıt sürücüye erişebilmesi 
    için bir adres görevi görmektedir. Bir aygıt dosyası yaratılırken aygıt dosyasına da bu majör ve minör numara iliştirilmektedir. 
    (Normal (regular) dosyalarda böyle bir majör ve minör numara olmadığına dikkat ediniz.) Örneğin:

    crw--w----   1 root tty       4,   0 Ağu 31 10:49 tty0

    Buradaki karakter aygıt dosyasının majör numarası 4, minör numarası 0'dır. Majör numara aygıt sürücünün ana kodunu temsil 
    eder. Minör numara ise onun örneklerini (instances) temsil etmektedir. Örneğin bir seri port aygıt sürücüsünün tek 
    bir majör numarası vardır. Eğer sistemimizde 4 tane seri port varsa aynı majör numaraya sahip minör numaraları farklı 
    4 aygıt dosyasının bulunması gerekir. Aygıt sürücünün aslında toplamda bir tane kodunun bulunduğuna dikkat ediniz. Aygıt 
    sürücüleri yazanlar zaten birden fazla minör numarayı destekleyecek biçimde kodlarını yazmaktadır.

    Aygıt dosyaları komut satırında "mknod" isimli programla yaratılmaktadır. Tabii bu program da aslında mknod isimli 
    POSIX fonksiyonunu çağırarak yazılmıştır. mknod POSIX fonksiyonu da sys_mknod isimli sistem fonksiyonunu çağırmaktadır. 
    mknod programı ile aygıt dosyası şöyle yaratılmaktadır:

    sudo mknod [-m ya da --mode <erişim_hakları>] <dosya_ismi> <c ya da b> <majör_numara> <minör_numara>

    Örneğin:

    $ sudo mknod -m 666 mydriver c 250 0

    O halde aygıt sürücüyü yazanlar belli bir majör ve minör numara belirleyerek belirledikleri bu majör ve minör numaraya 
    dayalı olarak aygıt sürücülerini yazarlar ve aygıt sürücülerinin kullanıcı modundan kullanılabilmesini sağlamak için 
    de aynı majör ve minör numaraya ilişkin aygıt dosyalarını yaratırlar. (Tabii aygıt sürücüler birden fazla minör numarayı 
    da destekleyebilirler.) Ancak aynı majör ve minör numaraya ilişkin birden fazla aygıt sürücü yüklenememektedir. Bu 
    nedenle aygıt sürücüleri yazanlar "kullanılmayan bir majör numarayı" çekirdekten isteyip aygıt sürücülerine o majör 
    numarayı atayabilmektedir. Tabii bu durumda aygıt sürücünün majör numarası önceden bilinmediği için aygıt dosyası da 
    ancak aygıt sürücü yüklendikten sonra yaratılabilmektedir. İşte tipik olarak sistem programcısı bir "kabuk betiği 
    (shell script)" yazarak bu işlemi otomatize etmeye çalışmaktadır. Bu betik önce "insmod" ile aygıt sürücüyü yükler, 
    "/proc/devices" dosyasına başvurup onun majör numarasını elde eder ve aygıt dosyasını "mknod" komutuyla dinamik bir 
    biçimde bu majör numarayı kullanacak biçimde oluşturur. Biz de denemelerimizde böyle yapacağız. Bunun için aşağıdaki 
    gibi bir "load" betiği kullanacağız:

    #!/bin/bash

    module=$1
    mode=666

    /sbin/insmod ./${module}.ko ${@:2} || exit 1
    major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
    rm -f $module
    mknod -m $mode $module c $major 0

    Bu betik dosyasını oluşturduktan sonra dosyaya "x" hakkını vermeyi unutmayınız:

    $ chmod +x load

    Bu betik aşağıdaki gibi çalıştırılmalıdır:

    $ sudo ./load mydriver

    Bu betik hem aygıt sürücüyü yükleyecek hem de ilgili majör numarayı kullanarak minör 0 olacak biçimde karakter 
    aygıt dosyasını oluşturacaktır. Bu biçimde yüklenen aygıt sürücüyü sistemden çıkardıktan sonra aygıt dosyası da 
    silinmelidir. Bu işlemde bir kabuk betik dosyası ile yapılabilir. Aygıt sürücüyü "rmmod" ile çıkartıp ilgili aygıt 
    dosyasını silen "unload" isimli bir kabuk betiği aşağıdaki gibi yazılabilir:

    #!/bin/bash

    module=$1

    /sbin/rmmod ./${module}.ko || exit 1
    rm -f $module

    Tabii yine bu "unload" betiğine de "x" hakkının verilmesi gerekir:

    $ chmod +x unload

    Bu betik şöyle kullanılır:

    sudo ./unload mydriver
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi aygıt sürücülere neden gereksinim duyulmaktadır? İşte kullanıcı modunda sistemin çökmesine yol açabilecek 
    işlemler "işlemcilerin sağladığı koruma mekanizması" yoluyla engellenmektedir. Bu durumda kullanıcı modundaki programlar 
    aslında yapamayacakaları işlemleri aygıt sürücülere dolaylı olarak yaptırabilir hale gelmektedir. Örneğin ekrana 
    bir yazının yazdırılması aslında doğrudan kullanıcı modundaki programlar tarafından sağlanamamaktadır. Bunun için 
    işletim sistemlerinde "terminal aygıt sürücüsü" denilen aygıt sürücüler bulundurulmaktadır. C'deki stdin ve stdout 
    dosyaları aslında bu aygıt sürücüye ilişkin aygıt dosyalarını temsil etmektedir. Programcı bu stdout dosyasına birşeyler 
    yazdırmak istediğinde programın akışı çekirdek moduna geçer ve aygıt sürücü içerisindeki ilgili fonksiyon çalıştırılır, 
    bu fonksiyon koruma engeli ortadan kalktığı için ekrana yazıları basabilmektedir. Aygıt sürücüler tipik olarak "donanım 
    aygıtlarıyla kullanıcı modundaki programlar arasında arayüz" oluşturma görevini üstlenirler. Tabii zamanla "aygıt 
    sürücü" kavramı daha genelleştirilmiştir. Linux sistemlerinde değişik mekanizmayla çalışan aygıt sürücüler bulunmaktadır. 
    İşlemcilerin "koruma mekanizması (protection mechanism)" kursumuzda izleyen bölümlerde ele alınacaktır.

    Pekiyi çekirdek üzerindeki manipülasyonların hepsi aygıt sürücü yazarak sağlanabilir mi? Hayır, aygıt sürücü çekirdekteki 
    olanakları kullanmaktadır ancak çekirdek kodlarında değişiklik yapamamaktadır. Dolayısıyla bazı çekirdek davranışları 
    basit bir biçimde aygıt sürücü yazarak gözlemlenebilir ve test edilebilir. Ancak aygıt sürücüler çekirdeğin davranışında 
    önemli değişikliklere neden olamamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda aygıt sürücülerin bir dosya gibi kullanıldığını belirtmiştik. Gerçekten de aygıt sürücü üzerinde dosya 
    işlemleri yapıldığında aslında aygıt sürücüyü yazan sistem programcısının belirlediği fonksiyonlar çağrılmaktadır. 
    Örneğin:

    open ---> aygıt sürücüdeki open fonksiyonu çağrılır
    close ---> aygıt sürücüdeki closed fonksiyonu çağrılır
    read ---> aygıt sürücüdeki read fonksiyonu çağrılır
    write ---> aygıt sürücüdeki write fonksiyonu çağrılır
    lseek ---> aygıt sürücüdeki lseek fonksiyonu çağrılır

    Ancak her türlü işlem bir dosya işlemi yoluyla sağlanamamaktadır. Klasik dosya işlemleriyle alakası olmayan aygıt 
    sürücüdeki spesifik bir kod ioctl denilen mekanizmayla da çağrılabilmektedir. Aygıt sürücüyü yazanlar aygıt dosyasını 
    açarlar, ioctl isimli POSIX fonksiyonu bir kod numarasıyla çağırırlar. (Tabii bu POSIX fonksiyonu da aslında arka 
    planda sys_ioctl isimli sistem fonksiyonunu çağırmaktadır.) Böylece aygıt sürücü içerisinde belirlenmiş bir fonksiyon 
    kullanıcı modundan çağrılabilmektedir. Tabii ioctl çağrısı yapıldığında çağrıyı yapan thread kullanıcı modundan 
    çekirdek moduna geçmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda iskelet bir karakter aygıt sürücüsü oluşturulmuştur. Bu karakter aygıt sürücüsü için bir ioctl kodu da 
    hazırlanmıştır. Aygıt sürücü kodu şöyledir:

    /* test-driver.h */

    #ifndef TEST_DRIVER_H_
    #define TEST_DRIVER_H_

    #include <linux/stddef.h>
    #include <linux/ioctl.h>

    #define TEST_DRIVER_MAGIC		't'
    #define IOC_TEST		        _IO(TEST_DRIVER_MAGIC, 0)

    #endif

    /* test-driver.c */

    #include <linux/module.h>
    #include <linux/kernel.h>
    #include <linux/fs.h>
    #include <linux/cdev.h>
    #include "test-driver.h"

    MODULE_LICENSE("GPL");
    MODULE_AUTHOR("Kaan Aslan");
    MODULE_DESCRIPTION("test-driver");

    static int test_driver_open(struct inode *inodep, struct file *filp);
    static int test_driver_release(struct inode *inodep, struct file *filp);
    static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
    static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
    static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

    static long ioctl_test(struct file *filp, unsigned long arg);

    static dev_t g_dev;
    static struct cdev g_cdev;
    static struct file_operations g_fops = {
        .owner = THIS_MODULE,
        .open = test_driver_open,
        .read = test_driver_read,
        .write = test_driver_write,
        .release = test_driver_release,
        .unlocked_ioctl = test_driver_ioctl
    };

    static int __init test_driver_init(void)
    {
        int result;

        printk(KERN_INFO "test-driver module initialization...\n");

        if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
            printk(KERN_INFO "cannot alloc char driver!...\n");
            return result;
        }
        cdev_init(&g_cdev, &g_fops);
        if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
            unregister_chrdev_region(g_dev, 1);
            printk(KERN_ERR "cannot add device!...\n");
            return result;
        }

        return 0;
    }

    static void __exit test_driver_exit(void)
    {
        cdev_del(&g_cdev);
        unregister_chrdev_region(g_dev, 1);

        printk(KERN_INFO "test-driver module exit...\n");
    }

    static int test_driver_open(struct inode *inodep, struct file *filp)
    {
        printk(KERN_INFO "test-driver opened...\n");

        return 0;
    }

    static int test_driver_release(struct inode *inodep, struct file *filp)
    {
        printk(KERN_INFO "test-driver closed...\n");

        return 0;
    }

    static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
    {
        printk(KERN_INFO "test-driver read...\n");

        return 0;
    }

    static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
    {
        printk(KERN_INFO "test-driver write...\n");

        return 0;
    }

    static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
    {
        long result;

        printk(KERN_INFO "test_driver_ioctl...\n");

        switch (cmd) {
            case IOC_TEST:
                result = ioctl_test(filp, arg);
                break;
            default:
                result = -ENOTTY;
        }

        return result;
    }

    long ioctl_test(struct file *filp, unsigned long arg)
    {
        printk(KERN_INFO "ioctl_test...\n");

        return 0;
    }

    module_init(test_driver_init);
    module_exit(test_driver_exit);

    Burada kullanıcı modundan aygıt dosyası açıldığında aygıt sürücünün test_driver_open fonksiyonu, kapatıldığında 
    test_driver_release fonksiyonu, aygıt dosyasından okuma yapılmak istendiğinde test_driver_read fonksiyonu ve aygıt 
    sürücüye yazma yapılmak istendiğinde ise test_driver_write fonksiyonu çağrılacaktır. Bu fonksiyonlar çağrıldığında 
    log mesajları yazdırılmaktadır. Ayrıca örneğimizde bir ioctl kodu da oluşturulmuştur. Bu ioctl kodu hem aygıt sürücüden 
    hem de kullanıcı modundan kullanıldığı için bir başlık dosyasında define edilmiştir:

    /* test-driver.h */

    #ifndef TEST_DRIVER_H_
    #define TEST_DRIVER_H_

    #include <linux/stddef.h>
    #include <linux/ioctl.h>

    #define TEST_DRIVER_MAGIC		't'
    #define IOC_TEST		        _IO(TEST_DRIVER_MAGIC, 0)

    #endif

    İşte kullanıcı modundan biz aşağıdaki getirilebilmekte ioctl işlemi yaptığımızda aygıt sürücümüzdeki ioctl_test 
    fonksiyonu çağrılacaktır. Bu aygıt sürücüyü kullanan örnek bir kullanıcı modu programı şöyle yazılabilir:

    /* app.c */

    #include <stdio.h>
    #include <stdlib.h>
    #include <fcntl.h>
    #include <unistd.h>
    #include <sys/ioctl.h>
    #include "test-driver.h"

    void exit_sys(const char *msg);

    int main(void)
    {
        int fd;

        if ((fd = open("test-driver", O_RDONLY)) == -1)
            exit_sys("open");

        if (ioctl(fd, IOC_TEST) == -1)
            exit_sys("ioctl");

        close(fd);

        return 0;
    }

    void exit_sys(const char *msg)
    {
        perror(msg);

        exit(EXIT_FAILURE);
    }

    Aygıt sürücümüzü şöyle derleyebilirsiniz:

    $ make file=test-driver

    Yükleme işlemini"load" betiği ile şöyle yapabilirsiniz:

    $ sudo ./load test-driver

    Bu işlemle dizin içerisinde "test-driver" isimli bir aygıt dosyası da oluşturulacaktır.

    İlgili C programını derleyerek aşağıdaki gibi çalıştırabilirsiniz:

    $ gcc -Wall -o app app.c
    $ ./app

    "app" programını çalıştırdıktan sonra "dmesg" komutunu kullandığımızda printk fonksiyonu ile yazdırılan log mesajlarının
    aşağıdaki gibi görülmesi gerekir:

    ...
    [  431.787375] test-driver module initialization...
    [  450.631940] test-driver opened...
    [  450.631953] test_driver_ioctl...
    [  450.631958] IOC_test_driver_TEST1
    [  450.631967] test-driver closed...

    Aygıt sürücüyü sistemden kaldırma işlemini de "unload" betiği ile şöyle yapabilirsiniz:

    $ sudo ./unload test-driver

    Bu işlemden sonra "dmesg" komutunu uyguladığımızda aşağıdakine benzer bir çıktı görüntüleceketir:

    ...
    204.225618] test-driver module initialization...
    [ 6220.150945] test-driver opened...
    [ 6220.150971] test_driver_ioctl...
    [ 6220.150985] IOC_test_driver_TEST1
    [ 6220.151003] test-driver closed...
    [ 6240.648236] test-driver module exit...
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST				_IO(TEST_DRIVER_MAGIC, 0)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test(struct file *filp, unsigned long arg);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
	.unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	long result;

	printk(KERN_INFO "test_driver_ioctl...\n");

	switch (cmd) {
		case IOC_TEST:
			result = ioctl_test(filp, arg);
			break;
		default:
			result = -ENOTTY;
	}

	return result;
}

long ioctl_test(struct file *filp, unsigned long arg)
{
	printk(KERN_INFO "IOC_test_driver_TEST1\n");

	return 0;
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

obj-m += ${file}.o

all:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include "test-driver.h"

void exit_sys(const char *msg);

int main(void)
{
	int fd;

	if ((fd = open("test-driver", O_RDONLY)) == -1)
		exit_sys("open");

	if (ioctl(fd, IOC_TEST) == -1)
		exit_sys("ioctl");

	close(fd);

	return 0;
}

void exit_sys(const char *msg)
{
	perror(msg);

	exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
										15. Ders 06/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de görmüş olduğumuz task_struct yapısına ilişkin bağlı listeler üzerinde gezinme işlemlerine örnekler verelim. 
    Daha önceden de belirttiğimiz gibi böyle testler için çekirdeği yeniden derlememize gerek yoktur. Basit bir çekirdek 
    modülü ya da aygıt sürücü yazarak da bu tür testleri yapabiliriz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz task_struct listelerini dolaşırken çekirdek bu listelere eleman eklerse ya da bu listelerden eleman silerse bizim 
    dolaşım yapan kodumuzda "tanımsız davranışlar (undefined behaviours)" oluşabilir. Bu da tüm sistemin çökmesine yol 
    açabilir. Bu tür durumlarda dolaşımın senkronizasyon nesneleri kullanılarak yapılması gerekmektedir. Biz kursumuzda 
    çekirdekte kullanılan senkronizasyon nesnelerini ileride ayrı başlık halinde ele alacağız. Birden fazla thread'in 
    beklemeden okuma işlemi yapabildiği ancak bir yazan thread varsa okuyan ve yazan thread'lerin bekletildiği senkronizasyon 
    nesnelerine "okuma yazma kilitleri (readers-writer locks)" denilmektedir. Eskiden Linux çekirdekleri task_struct 
    nesnelerini dolaşırken tasklist_lock isimli bir okuma yazma kilidini kullanılıyordu. Biz de dolaşımları aynı kilidi 
    kullanarak güvenli bir biçimde yapabiliyorduk. Ancak daha sonraları Linux çekirdeklerinde "kilitsiz (lock-free) 
    senkronizasyon nesneleri kullanılmaya başlandı. Artık belli bir süredir çekirdek task_struct listeleri üzerinde kilitsiz 
    bir biçimde "RCU (Read-Copy-Update)" mekanizmasıyla işlemler yapmaktadır. Dolayısıyla bizim de kendimizi çekirdeğin 
    kullandığı bu RCU mekanizmasına uydurarak dolaşım yapmamız gerekir. tasklist_lock kilidinin dışarıya export edilmesine 
    çekirdeğin 2.6.18 versiyonu ile son verilmiştir. Dolayısıyla bu kilit güncel çekirdeklerde artık çekirdek modülleri 
    ve aygıt sürücüler tarafından kullanılamamaktadır. (Tabii çekirdeğin içerisine gömeceğimiz kodlar ve aygıt sürücüler 
    bu kilidi kullanılabilirler. export işlemi yalnızca çekirdek modülleri ve aygıt sürücüleri etkilemektedir.) tasklist_lock 
    kilidi çekirdek tarafından eş zamanlı yazmaları senkronize etmek için halen kullanılmaktadır.

    RCU mekanizmasının ayrıntılarını ileride ayrı başlıkta ele alacağız. Ancak bu noktada RCU mekanizması ile task_struct 
    listelerinin dolaşılması için bazı temel bilgileri de vereceğiz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    RCU mekanizmasıyla bağlı listeleri dolaşırken işlemin başında rcu_read_lock fonksiyonun, işlemin sonunda da rcu_read_unlock 
    fonksiyonun çağrılması gerekmektedir. Bu fonksiyonların prototipleri şöyledir:

    #include <linux/rcupdate.h>

    void rcu_read_lock(void);
    void rcu_read_unlock(void);

    Bizim dolaşım kodunu bu iki çağrı arasına yerleştirmemiz gerekir:

    rcu_read_lock();
    ...
    rcu_read_unlock();

    RCU mekanizması altında bağlı listelerin bağlarına erişilirken rcu_dereference makrosu kullanılmalıdır. Bu makroya 
    parametre olarak erişeceğiniz bağlı listenin prev ya da next düğümlerini vermelisiniz. Bu durumda örneğin sistemdeki 
    bütün proseslerin ana thread'lerine ilişkin task_struct nesneleri aşağıdaki gibi güvenli bir biçimde dolaşılabilir:

    static void walk_processes(void)
    {
        struct task_struct *ts;
        struct list_head *lh;

        rcu_read_lock();

        lh = rcu_dereference(init_task.tasks.next);
        while (lh != &init_task.tasks) {
            ts = container_of(lh, struct task_struct, tasks);      /* ts = list_entry((lh, struct task_struct, tasks) */
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
            lh = rcu_dereference(ts->tasks.next);
        }

        rcu_read_unlock();
    }

    container_of makrosunun list_entry makrosunun tamamen eşdeğer olduğunu anımsayınız. printk fonksiyonu ile task_struct 
    içerisindeki pid değerinin yanı sıra comm elemanın da yazıdırıldığına dikkat ediniz. Bu comm elemanında en fazla 
    16 karakterlik task_struct nesnesini betimleyen bir yazı bulunmaktadır. fork işlemi sırasında ya da thread yaratımı 
    sırasında comm elemanındaki yazı bu işlemi yapan thread'in task_struct nesnesindeki comm elemanından kopyalanmaktadır. 
    exec işlemleri sırasında ise bu yazı çalıştırılan programın dosya isminden hareketle değiştirilmektedir. comm elemanındaki 
    yazı istenirse prctl isimli kütüphane fonksiyonu ile istenildiği zaman da değiştirilebilir. Tabii bu fonksiyon da 
    sys_prctl isimli sistem fonksiyonunu çağırmaktadır. Linux sistemlerinde thread'lere pthread_setname_np fonksiyonu ile 
    isimlerde de verilebilmektedir. Bu fonksiyonla thread'e bir isim verildiğinde thread'e ilişkin task_struct nesnesinin 
    comm elemanındaki yazı da değiştirilmektedir. (pthread_setname_np bir POSIX fonksiyonu değildir. libc kütüphanesinde 
    bulunmaktadır.)

    <linux/sched/signal.h> dosyası içerisinde next_task isimli bir makro da vardır. Bu makro bir task_struct nesnesinin 
    adresini parametre olarak alıp RCU mekanizmasına uygun olarak onun tasks.next göstericisinin gösterdiği yerdeki 
    task_struct nesnesininin adresini vermektedir:

    #include <linux/sched/signal.h>

    next_task(p)

    Bu makro kullanılarak walk_process fonksiyonu şöyle de yazılabilir:

    static void walk_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        ts = next_task(&init_task);
        while (ts != &init_task) {
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
            ts = next_task(ts);
        }

        rcu_read_unlock();
    }

    Aşağıda tüm proseslerin ana thread'lerini dolaşan örnek bir çekirdek modülü verilmiştir. Modulü aşağıdaki derleyip 
    yüklediğiniz zaman modülün init fonksiyonu içerisinde yukarıdaki walk_processes fonksiyonu çağrılacaktır. Yükleme 
    işleminden sonda "dmesg" komutunu kullandığınızda proseslerin ana thread'lerine ilişkin task_struct nesnelerini 
    "çekirdeğin ring tamponunda" görüntüleyebilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/* test-module.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/rcupdate.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);
static void walk_processes(void);

static int test_module_init(void)
{
	printk(KERN_INFO "test_module_init...\n");

	walk_processes();

	return 0;
}

static void walk_processes(void)
{
	struct task_struct *ts;
	struct list_head *lh;

	rcu_read_lock();

	ts = next_task(&init_task);
	while (ts != &init_task) {
		printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
		ts = next_task(ts);
	}

	rcu_read_unlock();
}

static void test_module_exit(void)
{
	printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/*----------------------------------------------------------------------------------------------------------------------
    Aslında tasks listesini daha kolay dolaşmak için çekirdek kodlarında <linux/sched/signal.h> başlık dosyasında bulunan 
    for_each_process isimli bir döngü makrosu da kullanılmaktadır. Bu makroya parametre olarak task_struct türünden bir 
    gösterici verilir. Bu makro da her dolaşımda sıradaki task_struct nesnesinin adresini bu göstericiye yerleştirir. 
    for_each_process makrosu kendi içerisinde rcu_dereference işlemini de yapmaktadır. Bu makroyla dolaşım tipik olarak 
    şöyle yapılmaktadır:

    rcu_read_lock();

    for_each_prosess(ts) {
        /* ... */
    }

    rcu_read_unlock();

    Aslında çoğu kez <linux/sched/signal.h> başlık dosyasının include edilmesine gerek kalmamaktadır. Çünkü bu dosya çok 
    temel bir dosya olduğu için zaten <linux/kernel.h> başlık dosyasında include edilmektedir.

    Dolaşımı yapan fonksiyonu şöyle yazabiliriz:

    static void walk_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        for_each_process(ts) {
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Aşağıda örneğin tamamı verilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/* test-module.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);
static void walk_processes(void);

static int test_module_init(void)
{
	printk(KERN_INFO "test_module_init...\n");

	walk_processes();

	return 0;
}

static void walk_processes(void)
{
	struct task_struct *ts;

	rcu_read_lock();

	for_each_process(ts) {
		printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
	}

	rcu_read_unlock();
}

static void test_module_exit(void)
{
	printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/*----------------------------------------------------------------------------------------------------------------------
    Bağlı listeleri RCU mekanizmasına uygun biçimde dolaşan list_for_each_entry_rcu isimli bir döngü makrosu da vardır. 
    Bu makro <linux/rculist.h> dosyasında bulunmaktadır. Bu makrosunun parametrik yapısı şöyledir:

    #include <linux/rculist.h>

    list_for_each_entry_rcu(pos, head, member)

    Makronun birinci parametresi bağın içinde bulunduğu nesnenin adresinin yerleştirileceği göstericiyi, ikinci parametresi 
    kök bağ düğümünün adresini ve üçüncü parametresi de kök bağ düğümün ismini almaktadır. Tüm proseslerin ana thread'lerini 
    dolaşan fonksiyonu bu makroyu kullanarak aşağıdaki gibi de yazabilirdik:

    static void walk_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        list_for_each_entry_rcu(ts, &init_task.tasks, tasks) {
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Aynı başlık dosyasındaki list_entry_rcu makrosu bir bağın adresini alarak RCU mekanizmasına uygun bir biçimde 
    bağın bulunduğu nesnesin adresi vermektedir. Bu makronun parametrik yapısı da şöyledir:

    <linux/rculist.h>

    list_entry_rcu(ptr, type, member)

    Makronun birinci parametresi bağ düğümünün list_head adresini, ikinci parametresi bağın içinde bulunduğu yapının 
    tür ismini, üçüncü parametresi ise bağ ismini almaktadır. Makro bağın içinde bulunduğu nesnenin adresini vermektedir. 
    Aslında önceki paragraflarda gördüğümüz next_task makrosu da bu makro kullanılarak yazılmıştır:

    #define next_task(p) \
        list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										16. Ders 07/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de thread'lerle ilişkin task_struct nesnelerinin dolaşılmasına örnekler verelim. Anımsanacağı gibi yeni çekirdeklerde 
    belli bir prosesin thread'lerine ilişkin task_struct nesneleri signal nesnesindeki thread_head düğümü kök alınarak 
    thread_node düğümlerinin dolaşılmasıyla elde ediliyordu.

    Belli bir prosesin thread'lerinin dolaşılabilmesi için çekirdek modülü yetersiz kalmaktadır. Çünkü çekirdek modüllerindeki 
    fonksiyonlar dışarıdan prosesler tarafından (yani proseslerin thread'leri tarafından) çağrılamamaktadır. Biz bir çekirdek 
    modülünü yüklediğimizde onun init fonksiyonu çalıştırılır. Biz bu init fonksiyonunda current makrosunu kullanırsak onu 
    yükleyen "insmod" prosesinin ilgili thread'inin (muhtemelen ana thread'inin) task_struct adresine erişiriz. Benzer biçimde 
    çekirdek modülünün exit fonksiyonunda current makrosunu kullanırsak "rmmod" prosesine ilişkin thread'in (muhtemelen ana 
    thread'inin) task_struct nesnesine erişiriz. O halde biz thread dolaşım testlerimizi ancak aygıt sürücü yazarak oluşturabiliriz. 
    Aygıt sürücülerdeki fonksiyonlar kullanıcı modundaki thread'ler tarafından çağrılabilmektedir. Bu fonksiyonlarda current 
    makrosu kullanıldığında ilgili fonksiyonu çağıran thread'in task_struct nesnesinin adresi elde edilecektir.

    Biz daha önce iskelet bir aygıt sürücü yazmıştık. Bu aygıt sürücümüz için bir ioctl kodu da oluşturmuştuk. O halde 
    testimizi bu ioctl fonksiyonu içerisinde yapabiliriz:

    long ioctl_test(struct file *filp, unsigned long arg)
    {
        walk_process_thread();

        return 0;
    }

    void walk_process_thread(void)
    {
        /* ... */
    }

    Şimdi artık walk_process_thread fonksiyonunda current makrosunu kullanırsak ioctl işlemini yapan thread'e ilişkin 
    task_struct nesnesine erişebiliriz.

    Prosesin thread'leri aşağıdaki gibi dolaşılabilir:

    static void walk_process_thread(void)
    {
        struct list_head *lh;
        struct task_struct *ts;

        rcu_read_lock();

        lh = rcu_dereference(current->signal->thread_head.next);

        while (lh != &current->signal->thread_head) {
            ts = container_of(lh, struct task_struct, thread_node);		/* ts = list_entry((lh, struct task_struct, thread_node) */
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
            lh = rcu_dereference(ts->thread_node.next);
        }

        rcu_read_unlock();
    }

    Tabii aynı işlemi daha önce görmüş olduğumuz list_for_each_entry_rcu makrosuyla da yapabilirdik:

    static void walk_process_thread(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        list_for_each_entry_rcu(ts, &current->signal->thread_head, thread_node) {
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Bir task_struct adresini alıp onun thread_node.next göstericisinin gösterdiği yerdeki task_struct adresini veren 
    next_thread isimli bir fonksiyon da vardır. next_thread fonksiyonu şöyle tanımlanmıştır:

    #include <linux/sched/signal.h>

    static inline struct task_struct *next_thread(struct task_struct *p)
    {
        return __next_thread(p) ?: p->group_leader;
    }

    Fonksiyondaki ?: operatöründe gcc eklentisi kullanılmıştır. Biz burada bu fonksiyonun içini açıklamayacağız. Ancak 
    bu fonksiyon ile biz prosesin herhangi bir thread'inden başlayarak tüm thread'lerini dolaşabilmekteyiz. Bu fonksiyon 
    dolaşım sırasında signal yapısındaki thread_head düğümüne gelindiğinde onu atlayabilmektedir. Bu durumda biz prosesin 
    thread'lerini bu fonksiyonu kullanarak aşağıdaki gibi de yapabiliriz:

    static void walk_process_thread(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        ts = current;

        do {
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
            ts = next_thread(ts);
        } while (ts != current);

        rcu_read_unlock();
    }

    Fonksiyonun yazımına dikkat edildiğinde, dolaşım sırasında yeniden signal yapısı içerisindeki thread_head düğümüne 
    gelindiğinde onu atlamak için group_leader düğümüne geçildiği görülmektedir. Buradaki kod bağlı listenin ilk elemanının 
    group_leader olduğu garantisi ile yazılmıştır. (Fonksiyon bu garantiye dayalı olmadan daha anlaşılır biçimde de 
    yazılabilirdi. Böyle yazılmasının bir gerekçesi de olabilir. Bunun için daha fazla incelemenin yapılması gerekir.)

    Aslında prosesin thread'lerini basit bir biçimde dolaşmak için for_each_thread döngü makrosu tercih edilmektedir:

    #include <linux/sched/signal.h>

    for_each_thread(p, t)

    Makronun birinci parametresi prosesin herhangi bir thread'inin task_struct adresini almaktadır. Döngü her yinelendikçe 
    yeni bir thread'e ilişkin task_struct yapının adresi ikinci parametreyle belirtilen task_struct türünden göstericinin 
    içerisine yerleştirilmektedir. Dolaşım her zaman signal yapısındaki thread_head düğümden başlatılmaktadır. (Bu bağlı 
    listenin ilk elemanı ana thread olduğu için aslında dolaşım ana thread'ten başlatılmaktadır.)

    for_each_thread döngü makrosu kullanılarak dolaşım aşağıdaki gibi basit bir biçimde yapılabilmektedir:

    static void walk_process_thread(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        for_each_thread(current, ts) {
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Aşağıda prosesin thread'lerini dolaşan örnek aygıt sürücü kodlarının tamamı verilmiştir. Aygıt sürücüyü yükledikten 
    sonra "app" programı çalıştırmalısınız. Bu program önce 10 tane thread yaratıp sonra aygıt sürücüdeki ioctl kodunu 
    çalıştırmaktadır. Ekrana çıkan thread'lere ilişkin pid değerleriyle "dmesg" komutunun çıktısını karşılaştırınız.
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST				_IO(TEST_DRIVER_MAGIC, 0)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test(struct file *filp, unsigned long arg);
static void walk_process_thread(void);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
	.unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	long result;

	printk(KERN_INFO "test_driver_ioctl...\n");

	switch (cmd) {
		case IOC_TEST:
			result = ioctl_test(filp, arg);
			break;
		default:
			result = -ENOTTY;
	}

	return result;
}

long ioctl_test(struct file *filp, unsigned long arg)
{
	walk_process_thread();

	return 0;
}

static void walk_process_thread(void)
{
	struct task_struct *ts;

	rcu_read_lock();

	for_each_thread(current, ts) {
		printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
	}

	rcu_read_unlock();
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#define _GNU_SOURCE

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <fcntl.h>
#include <unistd.h>
#include <pthread.h>
#include <sys/ioctl.h>
#include "test-driver.h"

#define NTHREADS		10

void exit_sys(const char *msg);
void *thread_proc(void *param);

int main(void)
{
	int fd;
	int result;
	pthread_t tids[NTHREADS];

	for (int i = 0; i < NTHREADS; ++i) {
		if ((result = pthread_create(&tids[i], NULL, thread_proc, NULL)) != 0) {
			fprintf(stderr, "pthread_create: %s\n", strerror(result));
			exit(EXIT_FAILURE);
		}
	}

	if ((fd = open("test-driver", O_RDONLY)) == -1)
		exit_sys("open");

	if (ioctl(fd, IOC_TEST) == -1)
		exit_sys("ioctl");

	close(fd);

	for (int i = 0; i < NTHREADS; ++i)
		pthread_join(tids[i], NULL);

	return 0;
}

void *thread_proc(void *param)
{
	printf("Thread PID: %jd\n", (intmax_t)gettid());

	sleep(120);

	return NULL;
}

void exit_sys(const char *msg)
{
	perror(msg);

	exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi sistemdeki bütün task_struct nesnelerini dolaşabilir miyiz? Bunu yapmanın en pratik yolu init_task nesnesinden 
    hareketle tüm proseslerin ana thread'lerine ilişkin task_struct nesnelerini elde edip o ana thread'lerden faydalanarak 
    onların diğer thread'lerine ilişkin task_struct nesnelerini elde etmektir. Aslında bunu iç içe iki döngü oluşturarak 
    RCU mekanizması eşliğinde yapan for_each_process_thread isimli bir makro vardır:

    #include <linux/sched/signal.h>

    #define for_each_process_thread(p, t)	        \
        for_each_process(p) for_each_thread(p, t)

    Makroya biz iki task_struct göstericisi veririz. Makro her yinelemede prosesin ana thread'ine ilişkin task_struct 
    nesnesinin adresini birinci parametreyle verilen göstericinin içerisine, o prosesin thread'lerine ilişkin task_struct 
    nesnelerinin adresleri de ikinci parametreyle verilen göstericinin içerisine yerleştirmektedir. O halde tüm thread'lere 
    ilişkin task_struct nesneleri aşağıdaki gibi elde edilebilir:

    static void walk_all_threads(void)
    {
        struct task_struct *tsp;
        struct task_struct *tst;

        rcu_read_lock();

        for_each_process_thread(tsp, tst) {
            printk(KERN_INFO "Process Main Thread PID = %d, Thread PID == %d COMM = %s\n", tsp->pid, tst->pid, tst->comm);
        }

        rcu_read_unlock();
    }

    Tabii proseslerin thread'lerini girinti biçimde göstermek için kodu aşağıdaki gibi de düzenleyebiliriz:

    static void walk_all_threads(void)
    {
        struct task_struct *tsp;
        struct task_struct *tst;

        rcu_read_lock();

        for_each_process_thread(tsp, tst) {
            if (tsp == tst) {
                printk(KERN_INFO "Process Main Thread PID = %d, COMM = %s\n", tsp->pid, tst->comm);
            }
            else {
                printk(KERN_INFO "Thread PID == %d COMM = %s\n", tst->pid, tst->comm);
            }
        }

        rcu_read_unlock();
    }

    "dmesg" çıktısı aşağıdakine benzer biçimde elde edilecektir:

    ...
    [ 6022.928026] Process Main Thread PID = 3494, COMM = kworker/u257:2
    [ 6022.928027] Process Main Thread PID = 3997, COMM = kworker/1:0
    [ 6022.928042] Process Main Thread PID = 4271, COMM = kworker/0:0
    [ 6022.928044] Process Main Thread PID = 4279, COMM = kworker/u259:0
    [ 6022.928045] Process Main Thread PID = 4543, COMM = kworker/3:1
    [ 6022.928048] Process Main Thread PID = 4546, COMM = kworker/5:2
    [ 6022.928049] Process Main Thread PID = 4547, COMM = kworker/u264:2
    [ 6022.928051] Process Main Thread PID = 4565, COMM = bash
    [ 6022.928053] Process Main Thread PID = 4599, COMM = app
    [ 6022.928054]     Thread PID == 4600 COMM = app
    [ 6022.928056]     Thread PID == 4601 COMM = app
    [ 6022.928057]     Thread PID == 4602 COMM = app
    [ 6022.928059]     Thread PID == 4603 COMM = app
    [ 6022.928060]     Thread PID == 4604 COMM = app
    [ 6022.928062]     Thread PID == 4605 COMM = app
    [ 6022.928064]     Thread PID == 4606 COMM = app
    [ 6022.928081]     Thread PID == 4607 COMM = app
    [ 6022.928083]     Thread PID == 4608 COMM = app
    [ 6022.928084]     Thread PID == 4609 COMM = app
    [ 6022.928087] Process Main Thread PID = 4610, COMM = sudo
    [ 6022.928089] Process Main Thread PID = 4611, COMM = sudo
    [ 6022.928090] Process Main Thread PID = 4612, COMM = insmod
    ...

    Aşağıda bu işlemi yapan çekirde modülünün tüm kodları verilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/* test-module.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);
static void walk_all_threads(void);

static int test_module_init(void)
{
	printk(KERN_INFO "test_module_init...\n");

	walk_all_threads();

	return 0;
}

static void walk_all_threads(void)
{
	struct task_struct *tsp;
	struct task_struct *tst;

	rcu_read_lock();

	for_each_process_thread(tsp, tst) {
		if (tsp == tst) {
			printk(KERN_INFO "Process Main Thread PID = %d, COMM = %s\n", tsp->pid, tst->comm);
		}
		else {
			printk(KERN_INFO "Thread PID == %d COMM = %s\n", tst->pid, tst->comm);
		}
	}

	rcu_read_unlock();
}

static void test_module_exit(void)
{
	printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de son olarak bir prosesin alt proseslerini dolaşalım. Anımsanacağı gibi prosesin alt proses listesinin kök 
    düğümü ana prosesin ana thread'ine ilişkin task_struct nesnesinin children elemanında tutuluyordu. Bu children elemanı 
    task_struct içerisindeki sibling düğümlerini dolaşmakta kullanılıyordu. children/sibling listesinde yalnızca alt 
    proseslerin ana thread'lerine ilişkin task_struct nesnelerinin bulunduğunu da anımsayınız.

    Alt prosesleri dolaşan hazır bir döngü makrosu yoktur. Ancak biz yukarıdaki tekniklerle dolaşımı yapabiliriz. Örneğin 
    bunun için RCU mekanizmasıyla bağlı listeyi dolaşan list_for_each_entry_rcu döngü makrosundan faydalanabiliriz:

    static void walk_child_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        list_for_each_entry_rcu(ts, &current->children, sibling) {
            printk(KERN_INFO "Child Process Main Thread PID = %d, COMM = %s\n", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Bu kodda current makrosunun belirttiği prosesin alt proses listesi dolaşılmaktadır. Tabii daha önce de belirttiğimiz 
    gibi alt proseslerin dolaşılması prosesin ana thread'inden hareketle yapılmalıdır. Yukarıdaki fonksiyonda current 
    makrosu kullanıldığı için biz dolaşımı basit bir çekirdek modülü ile yapamayız, ancak bir aygıt sürücü yoluyla yapabiliriz. 
    Bir karakter aygıt sürücüsü oluşturarak yukarıdaki fonksiyonu onun ioctl kodu içerisinden çağırabiliriz:

    static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
    {
        long result;

        printk(KERN_INFO "test_driver_ioctl...\n");

        switch (cmd) {
            case IOC_TEST:
                result = ioctl_test(filp, arg);
                break;
            default:
                result = -ENOTTY;
        }

        return result;
    }

    long ioctl_test(struct file *filp, unsigned long arg)
    {
        walk_child_processes();

        return 0;
    }

    Kodu önce çeşitli alt prosesler yarattıktan sonra aygıt sürücü üzerinde ioctl çağrısı yapan bir programla test edebilirsiniz. 
    Biz test işlemini yapan "app.c" programında 10 tane alt proses ve her alt proseste 3 tane thread yarattık. Aygıt 
    sürücüyü yükleyip "app.c" programını derleyip çalıştırdıktan sonra "dmesg" komutunu uygulayarak çıktıyı incelemelisiniz.

    Aşağıda tüm kodları bütünsel olarak veriyoruz.
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST				_IO(TEST_DRIVER_MAGIC, 0)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test(struct file *filp, unsigned long arg);
static void walk_child_processes(void);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
	.unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	long result;

	printk(KERN_INFO "test_driver_ioctl...\n");

	switch (cmd) {
		case IOC_TEST:
			result = ioctl_test(filp, arg);
			break;
		default:
			result = -ENOTTY;
	}

	return result;
}

long ioctl_test(struct file *filp, unsigned long arg)
{
	walk_child_processes();

	return 0;
}

static void walk_child_processes(void)
{
	struct task_struct *ts;

	rcu_read_lock();

	list_for_each_entry_rcu(ts, &current->children, sibling) {
		printk(KERN_INFO "Child Process Main Thread PID = %d, COMM = %s\n", ts->pid, ts->comm);
	}

	rcu_read_unlock();
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#define _GNU_SOURCE

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/wait.h>
#include <pthread.h>
#include <sys/ioctl.h>
#include "test-driver.h"

#define NCHILDS		10
#define NTHREADS	3

void exit_sys(const char *msg);
void *thread_proc(void *param);

int main(void)
{
	int fd;
	int result;
	pid_t pid;

	for (int i = 0; i < NCHILDS; ++i) {
		if ((pid = fork()) == -1) {
			perror("fork");
			exit(EXIT_FAILURE);
		}

		if (pid == 0) {
			pthread_t tids[NTHREADS];


			for (int k = 0; k < NTHREADS; ++k) {
				if ((result = pthread_create(&tids[k], NULL, thread_proc, NULL)) != 0) {
					fprintf(stderr, "pthread_create: %s\n", strerror(result));
					exit(EXIT_FAILURE);
				}
			}

			for (int k = 0; k < NTHREADS; ++k)
				pthread_join(tids[k], NULL);

			_exit(0);
		}
		else {
			printf("Child PID = %jd\n", (intmax_t)pid);
		}
	}

	if ((fd = open("test-driver", O_RDONLY)) == -1)
		exit_sys("open");

	if (ioctl(fd, IOC_TEST) == -1)
		exit_sys("ioctl");

	close(fd);

	for (int i = 0; i < NCHILDS; ++i)
		if (wait(NULL) == -1) {
			perror("fork");
			exit(EXIT_FAILURE);
		}

	return 0;
}

void *thread_proc(void *param)
{
	sleep(60);

	return NULL;
}

void exit_sys(const char *msg)
{
	perror(msg);

	exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin uyguladığı RCU mekanizması tek bir yazan ve birden fazla okuyan akışın bulunduğu durumda beklemeyi ortadan 
    kaldırmaktadır. Ancak veri yapısına birden fazla yazanın (güncellemeyi kastediyoruz) olması durumunda yazan tarafların 
    bir kilit mekanizmasıyla (tipik olarak okuma yazma kilitleri yoluyla) senkronize edilmesi gerekir. İşte güncel 
    çekirdekler hala task_struct bağlı listelerine yazma için daha önce sözünü etmiş olduğumuz tasklist_lock kilidini 
    kullanmaktadır. Daha önceden de belirttiğimiz tasklist_lock okuma yazma kilidi artık çekirdek modülleri ve aygıt 
    sürücüler için export edilmemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										17. Ders 13/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de pid değerleriyle task_struct nesneleri arasındaki ilişkiyi ele alalım. Bilindiği gibi kullanıcı modunda 
    prosesler pid değerleriyle temsil edilmektedir. Bazı POSIX fonksiyonlarının pid parametresi aldığını anımsayınız. 
    pid parametresine sahip tipik POSIX fonksiyonları şunlardır:

    kill
    waitpid
    getpgis
    setpgid
    getsid
    getpriority
    setpriority

    Bu POSIX fonksiyonları genellikle Linux sistemlerinde doğrudan çekirdek içerisindeki sistem fonksiyonlarını çağırmaktadır. 
    Çekirdek de pid değeri ile belirtilen prosesin (ya da thread'in) task_struct nesnesine erişerek işlemlerini bu nesne 
    içerisindeki bilgileri kullanarak yapmaktadır. Örneğin programcı kill POSIX fonksiyonu ile bir prosese sinyal gönderecek 
    olsun. Programcı sinyal göndereceği prosesin pid değerini fonksiyona argüman olarak geçecektir. kill POSIX fonksiyonu 
    Linux çekirdeğindeki sys_kill sistem fonksiyonu çağırmaktadır. Çekirdek de bu pid değerinden hareketle prosese ilişkin 
    task_struct nesnesini elde edip işlemlerini buradaki bilgileri kullanarak yapacaktır. Linux sistemlerinde pid değerleriyle 
    işlem yapan ancak POSIX standartlarında bir karşılığı bulunmayan başka sistem fonksiyonları da vardır. Bu sistem 
    fonksiyonlarının çoğu "libc" kütüphanesi tarafından da sarmalanmıştır.

    O halde çekirdeğin pid değerinden hareketle o pid değerine ilişkin task_struct nesnesini hızlı bir biçimde elde 
    etmesi gerekmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bizim çekirdek geliştiricisi olarak pid değerleriyle ilgili aşağıdaki iki sorunun yanıtını biliyor olmamız gerekir:

    1) Yeni bir proses ya da thread yaratıldığında çekirdek o proses ya da thread'e ilişkin pid değerini nasıl üretmektedir?
    2) Çekirdek belli bir pid değerine ilişkin proses ya da thread'in task_struct nesnesine nasıl ulaşmaktadır?

    Bir task_struct nesnesi çekirdek tarafından yaratıldığında ona atanacak pid değeri eskiden oldukça basit bir biçimde 
    belirleniyordu. Çekirdek last_pid isminde global bir değişken tutuyordu. Boş pid araması bu değerden itibaren yapılıyordu. 
    Tabii henüz bu değer üst limite varmadığında pid tahsisatı hızlı bir biçimde yapılabiliyordu. Ancak bu değer üst limite 
    varıp da yeniden başa döndüğünde boş pid değerinin aranması zaman alıyordu. Yani arama işlemi doğrusal arama (O(N) 
    karmaşıklıkta arama) haline geliyordu. Daha sonra Linux çekirdekleri boş pid değerlerini bir bit dizisinde (buna 
    Linux'ta bitmap veri yapısı deniyor) tutmaya başladı ve işlemcilerin özel makine komutları sayesinde aramaların eskisine 
    göre daha hızlı yapılması sağlandı. Ancak Linux sistemlerinde proses limitlerinin gittikçe yükselmesiyle bu bitmap 
    yöntemi da yetersiz kalmaya başlamıştır. Özellikle container teknolojilerinin desteklenmesi amacıyla çekirdeğe "isim 
    alanı (namespace)" kavramının eklenmesiyle birlikte pid değerinin hızlı elde edilmesi daha da önem kazanmıştır. Bugün 
    mevcut çekirdeklerde boş pid değerleri radix ağaçlarının özelleştirilmiş bir biçimi olan XArray denilen veri yapısı 
    kullanılarak biraz karmaşık bir biçimde elde edilmektedir. Mevcut çekirdeklerde boş pid değerini bu karmaşık yöntemle 
    elde eden alloc_pid isimli yüksek seviyeli bir fonksiyon bulunmaktadır. Bu fonksiyonun prototipi güncel çekirdeklerde 
    şöyledir:

    struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid, size_t set_tid_size);

    Az daha geriye gittiğimizde bu fonksiyonun prototipi şöyleydi:

    struct pid *alloc_pid(struct pid_namespace *ns)

    Bu fonksiyonlar dışarıya export edilmemiştir. Bu fonksiyonların doğrudan bir pid değeri vermediğine dikkat ediniz. 
    Bu fonksiyonlar pid isimli bir yapı nesnesi tahsis edip onun adresini geri döndürmektedir. Bu yapıda da çekirdeğin 
    çeşitli versiyonlarında değişiklikler yapılmıştır. Güncel versiyonlardaki pid yapısı şöyledir:

    struct pid {
        refcount_t count;
        unsigned int level;
        spinlock_t lock;
        struct dentry *stashed;
        u64 ino;
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct hlist_head inodes;
        /* wait queue for pidfd notifications */
        wait_queue_head_t wait_pidfd;
        struct rcu_head rcu;
        struct upid numbers[];
    };

    Linux çekirdekleri belli bir versiyondan sonra her pid değeri için bir pid yapı nesnesi tutmaya başlamıştır. pid 
    üretim mekanizması ve pid değerinden hareketle task_struct nesnesinin elde edilme mekanizması zaman içerisinde 
    karmaşıklaşmıştır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi eski çekirdeklerde boş pid değerinin elde edilmesi oldukça basit bir biçimde yapılıyordu. 
    Linux'un öğrenci ödevi gibi olan ilk 0.01 versiyonu bu bakımdan çok ilkeldi:

    int find_empty_process(void)
    {
        int i;

        repeat:
            if ((++last_pid)<0) last_pid=1;
            for(i=0 ; i<NR_TASKS ; i++)
                if (task[i] && task[i]->pid == last_pid) goto repeat;
        for(i=1 ; i<NR_TASKS ; i++)
            if (!task[i])
                return i;
        return -EAGAIN;
    }

    Bu ilk versiyonda gördüğünüz gibi sistemde tahsis edilebilecek en fazla task_struct nesnesi NR_TASK kadardı:

    #define NR_TASKS 64

    Zaten bu versiyonda 64 tane task_struct yapısı baştan statik olarak tahsis edilmişti:

    struct task_struct * task[NR_TASKS] = {&(init_task.task), };

    Buradaki find_empty_process fonksiyonu tasks dizisi içerisinde boş bir task_struct indeksi ile geri dönmektedir. Bu 
    task_struct indeksi için kullanılacak pid değeri ise last_pid değişkeninde saklanmıştır. task_struct nesnelerinin 
    64 ile sınırlı olduğu olduğu halde pid değerinin 64 ile sınırlı olmadığına dikkat ediniz.

    2.4 çekirdeğinde pid numarasının belirlenmesi işlemi belli bir değerden itibaren aday olan pid değerlerinin herhangi 
    bir task_struct nesnesi tarafından kullanılıp kullanılmadığına bakılarak yapılmıştır:

    static int get_pid(unsigned long flags)
    {
        static int next_safe = PID_MAX;
        struct task_struct *p;
        int pid, beginpid;

        if (flags & CLONE_PID)
            return current->pid;

        spin_lock(&lastpid_lock);
        beginpid = last_pid;
        if((++last_pid) & 0xffff8000) {
            last_pid = 300;		/* Skip daemons etc. */
            goto inside;
        }
        if(last_pid >= next_safe) {
    inside:
            next_safe = PID_MAX;
            read_lock(&tasklist_lock);
        repeat:
            for_each_task(p) {
                if(p->pid == last_pid	||
                p->pgrp == last_pid	||
                p->tgid == last_pid	||
                p->session == last_pid) {
                    if(++last_pid >= next_safe) {
                        if(last_pid & 0xffff8000)
                            last_pid = 300;
                        next_safe = PID_MAX;
                    }
                    if(unlikely(last_pid == beginpid)) {
                        next_safe = 0;
                        goto nomorepids;
                    }
                    goto repeat;
                }
                if(p->pid > last_pid && next_safe > p->pid)
                    next_safe = p->pid;
                if(p->pgrp > last_pid && next_safe > p->pgrp)
                    next_safe = p->pgrp;
                if(p->tgid > last_pid && next_safe > p->tgid)
                    next_safe = p->tgid;
                if(p->session > last_pid && next_safe > p->session)
                    next_safe = p->session;
            }
            read_unlock(&tasklist_lock);
        }
        pid = last_pid;
        spin_unlock(&lastpid_lock);

        return pid;

    nomorepids:
        read_unlock(&tasklist_lock);
        spin_unlock(&lastpid_lock);
        return 0;
    }

    Tabii bu versiyonlarda artık çekirdeğin bir heap sistemi vardır ve task_struct yapıları dinamik olarak bu heap 
    sisteminden tahsis edilmektedir. Bu fonksiyonda last_pid değerinden başlanarak sistemdeki bütün task_struct nesneleri 
    dolaşılmış (o zamanlar bunun için for_each_task makrosu kullanılıyordu) ve boş bir pid değeri doğrusal aramayla 
    tespit edilmeye çalışılmıştır. Ancak pid araması 2.6 çekirdekleriyle birlikte bitmap veri yapısı kullanılarak 
    elde edilmeye başlanmıştır. Bitmap veri yapısı bitleri tutan bir veri yapısıdır. Modern işlemcilerde belli bir 
    adresten itibaren 0 olan ilk bitin yerini veren makine komutları bulunmaktadır. 2.6 çekirdeklerinde bu veri yapısı 
    kullanılarak boş bir pid değeri elde eden fonksiyon şöyle yazılmıştır:

    static int alloc_pidmap(struct pid_namespace *pid_ns)
    {
        int i, offset, max_scan, pid, last = pid_ns->last_pid;
        struct pidmap *map;

        pid = last + 1;
        if (pid >= pid_max)
            pid = RESERVED_PIDS;
        offset = pid & BITS_PER_PAGE_MASK;
        map = &pid_ns->pidmap[pid/BITS_PER_PAGE];
        /*
        * If last_pid points into the middle of the map->page we
        * want to scan this bitmap block twice, the second time
        * we start with offset == 0 (or RESERVED_PIDS).
        */
        max_scan = DIV_ROUND_UP(pid_max, BITS_PER_PAGE) - !offset;
        for (i = 0; i <= max_scan; ++i) {
            if (unlikely(!map->page)) {
                void *page = kzalloc(PAGE_SIZE, GFP_KERNEL);
                /*
                * Free the page if someone raced with us
                * installing it:
                */
                spin_lock_irq(&pidmap_lock);
                if (!map->page) {
                    map->page = page;
                    page = NULL;
                }
                spin_unlock_irq(&pidmap_lock);
                kfree(page);
                if (unlikely(!map->page))
                    break;
            }
            if (likely(atomic_read(&map->nr_free))) {
                do {
                    if (!test_and_set_bit(offset, map->page)) {
                        atomic_dec(&map->nr_free);
                        set_last_pid(pid_ns, last, pid);
                        return pid;
                    }
                    offset = find_next_offset(map, offset);
                    pid = mk_pid(pid_ns, map, offset);
                } while (offset < BITS_PER_PAGE && pid < pid_max);
            }
            if (map < &pid_ns->pidmap[(pid_max-1)/BITS_PER_PAGE]) {
                ++map;
                offset = 0;
            } else {
                map = &pid_ns->pidmap[0];
                offset = RESERVED_PIDS;
                if (unlikely(last == offset))
                    break;
            }
            pid = mk_pid(pid_ns, map, offset);
        }
        return -1;
    }

    Fonksiyon biraz karmaşık olsa da boş pid değerinin elde edildiği yer şurasıdır:

    offset = find_next_offset(map, offset);

    find_next_offset fonksiyonu bitmap'teki ilk boş olan 0 bitinin offset değerini bulan özel makine komutu kullanılarak 
    yazılmıştır. Kursumuzda bitmap veri yapısını ileride ayrı bir başlık altında inceleyeceğiz.

    Çekirdeğin 4.15 versiyonu ile pid tahsisatında bitmap kullanımı bırakılarak ve radix ağaçları kullanılmaya başlanmıştır. 
    güncel çekirdekler ise boş pid değerinin üretimi için artık radix ağaçlarınının özel bir biçimi olan XArray veri 
    yapısını kullanmaktadır. Biz bu konuyu izleyen paragraflarda ele alacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdekleri maksimum pid değeri için bir üst limit kullanmaktadır. Sistemdeki hiçbir pid değeri bu üst limitten 
    daha büyük olamamaktadır. Linux'un öğrenci ödevi gibi olan 0.01 versiyonunda böyle bir üst limit kullanılmamıştır. 
    Yani bu ilk versiyonda pid değeri long türünün sınırları içerisinde herhangi bir değerde olabiliyordu. Çekirdeğin 
    2.2 ve 2.4 versiyonlarında pid değerininin üst sınırı PID_MAX sembolik sabiti ile belirlenmiştir. Bu sistemlerde 
    PID_MAX sembolik sabiti 32768 olarak define edilmişti. Bu sistemlerde en büyük pid değeri bu değerden bir eksik 
    değer olan 32767 olabiliyordu. Daha sonra 2.6 çekirdekleriyle birlikte PID_MAX sembolik sabitinin ismi PID_MAX_LIMIT 
    olarak değiştirildi. 2.6'lı çekirdeklerden itibaren günümüze kadarki çekirdeklerde PID_MAX_LIMIT sembolik sabiti şöyle 
    bildirilmiştir:

    #define PID_MAX_LIMIT (IS_ENABLED(CONFIG_BASE_SMALL) ? PAGE_SIZE * 8 : \
            (sizeof(long) > 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))

    Görüldüğü gibi eğer CONFIG_BASE_SMALL konfigürasyon parametresi 'y' yapılmışsa bu limit PAGE_SIZE * 8 yani 32768 
    değerini, CONFIG_BASE_SMALL konfigürasyon parametresi 'n' yapılmışsa (normal kullanımda bu konfigürasyon parametresi 
    'n' biçimindedir) ve sizeof(long) 4 byte'tan büyükse (64 bit sistemlerde böyledir) bu limit 4 * 1024 * 1024 = 4194304 
    değerini belirtmektedir. Eğer sizeof(long) 4 değerinden büyük değilse (32 bit sistemler böyledir) bu durumda bu 
    PID_MAX_LIMIT değeri PID_MAX_DEFAULT biçimindedir. Bu sembolik sabit de şöyle tanımlanmıştır:

    #define PID_MAX_DEFAULT (IS_ENABLED(CONFIG_BASE_SMALL) ? 0x1000 : 0x8000)

    Buradan 32 bir Linux sistemlerinde PID_MAX_LIMIT değerinin eğer CONFIG_BASE_SMALL konfigürasyon parametresi 'y' 
    yapılmışsa 4096, 'n' yapılmışsa 32768 olduğu görülmektedir.

    PID_MAX_LIMIT kullanan 2.6 ve günümüze kadarki çekirdeklerde bir prosesin ya da thread'in pid değeri en fazla 
    bu PID_MAX_LIMIT değerlerinden bir eksik olabilmektedir.

    Bugün kullandığımız 64 bitlik işlemcilerdeki Linux sistemlerinde maksimum pid değeri 4194304, 32 bit sistemlerde ise 
    32767 biçimindedir. Bu değer proc dosya sisteminde "/proc/sys/kernel/pid_max" dosyasında da belirtilmektedir. 
    Programcı isterse sysctl komutu yoluyla sistem çalışırken bu değeri düşürebilir ancak yükseltemez.

    Pekiyi maksimum pid değeri için neden bir kısıtlama getirilmiştir? İşte bunun en temel nedeni pid aramaları için 
    gereken hash tablosu ya da bitmap'ler gibi veri yapılarının daha iyi performans göstermesini sağlamaktır. Tabii 
    sistemde hiçbir zaman bu maksimum pid değerinden daha fazla sayıda task_struct nesnesi bulunamayacaktır. Aslında 
    maksimum task_struct sayısı (başka bir deyişle maksimum thread sayısı) için başka önemli kısıtlar da vardır. pid 
    kısıtı yalnızca bunlardan biridir. Sisteminizde tahsis edilebilecek maksimum task_struct nesnelerinin sayısını 
    proc dosya sistemindeki "/proc/sys/kernel/threads-max" dosyasından öğrenebilirsiniz. Bu "threads-max" dosyasındaki 
    değer sistem çalışırken de değiştirilebilir. Bu değiştirme işlemi daha resmi olarak "sysctl" komutuyla da yapılabilmektedir. 
    Örneğin:

    $ echo 100000 | sudo tee /proc/sys/kernel/threads-max

    Bu sayıyı ayarlamak için doğrudan bir çekirdek konfigürasyon parametresi yoktur. (Bazı değerler başka değerlere 
    bağlı olabilmektedir. Bu ilişki konfigürasyon parametrelerinde olduğu gibi belli bir sembolik sabite değer vermekle 
    sağlanamamaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarda çekirdeğin yeni bir pid değerini nasıl elde ettiğine yönelik bilgiler verdik. Ancak yeni çekirdeklerde 
    bu süreç karmaşık hale geldiği için henüz bunun ayrıntıları üzerinde durmadık. Şimdi de ikinci soru üzerinde duralım: 
    "Çekirdek belli bir pid değerine ilişkin task_struct adresini nasıl hızlı bir biçimde elde etmektedir?"

    Bir pid değerine ilişkin task_struct nesnesi düz mantıkla sistemdeki bütün task_struct nesneleri dolaşılarak elde 
    edilebilir. Çünkü pid değerleri zaten task_struct nesnelerinin içerisinde bulunmaktadır. Ancak böyle bir arama 
    çekirdek için çok yavaştır. Sistemde binlerce proses ve thread bulunuyor olabilir. Bu biçimdeki sıralı arama çekirdeğin 
    performansını düşürebilir. Bu tür hızlı aramalar için Linux çekirdeğinde genel olarak hash tabloları bazen de arama 
    ağaçları kullanılmaktadır. Linux'un 2.6.24 versiyonuna kadar bunun için hash tabloları kullanılıyordu. Ancak bu 
    versiyondan itibaren bu amaçla "radix ağaçları" da işin içine sokulmuştur. Güncel çekirdeklerdeki arama sistemi 
    radix ağaçlarının özel bir biçimi olan XArray veri yapısı ve hash tablolarının hibrit bir biçimine benzemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										18. Ders 14/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Arama işlemleri bir anahtar (key) eşliğinde yapılmaktadır. Anahtar bulunması istenen varlığı temsil etmektedir. 
    Örneğin pek çok kişinin bilgileri bir veri yapısında tutuluyor olabilir. Biz de ismini bildiğimiz bir kişiyi bu veri 
    yapısında arayabiliriz. Burada anahtar isimdir. Veri yapıları dünyasında anahtar-değer çiftlerini tutan ve anahtar 
    verildiğinde ona karşı gelen değerin elde edilmesini sağlayan veri yapılarına genel olarak "sözlük (dictionary)" tarzı 
    veri yapıları denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Elemanların sıralı olmadığı liste tarzı veri yapılarında elemanların tek tek gözden geçirilmesi yoluyla yapılan 
    aramalara "sıralı arama (sequential search)" denilmektedir. Sıralı arama oldukça yavaş bir yöntemdir. Sıralı aramada 
    listedeki eleman sayısı N olmak üzere anahtarın bulunması için ortalama N/2 kez karşılaştırmanın yapılması gerekmektedir. 
    Bu tür algoritmaların karmaşıklığına "Big O" notasyonunda "O(N) karmaşıklık" denildiğini anımsayınız. Ancak ne 
    olursa olsun eleman sayısının makul olduğu bir durumda sıralı arama en iyi yöntem haline de gelebilmektedir. Örneğin 
    en fazla 20 civarında elemanın bulunduğu bir durumda bu elemanları diziye yerleştirip sıralı bir biçimde aramak en 
    etkin yöntem haline gelebilmektedir.

    Eğer elemanlar sıralıysa ve herhangi bir elemana erişim çok hızlı (buna rastgele erişim de denilmektedir) yapılabiliyorsa 
    "ikili arama (binary search)" en iyi yöntemdir. İkili aramanın algoritmik karmaşıklığı O(log N) biçimindedir. Aslında 
    ikili aramanın daha genel bir biçimine "enterpolasyon araması (interpolation search)" denilmektedir. Enterpolasyon 
    aramasında bölme ortadan değil daha uygun yerlerden yapılmaktadır. Ancak enterpolasyon araması dizi dağılımının 
    bilindiği ve özellikle de dizinin düzgün dağıldığı durumlarda faydalı bir etki oluşturmaktadır. Diziyi önce sıraya 
    dizip sonra ikili arama uygulamak ise genellikle iyi bir fikir değildir. Çünkü sırayı korumak için araya eleman ekleme 
    ve aradan eleman silme gibi işlemlerde O(N) karmaşıklıkta kaydırmaların yapılması gerekmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi ideal bir anahtar-değer araması nasıl olabilir? Şüphesiz ideal durumda aramanın O(1) karmaşıklıkta yapılması 
    istenir. Anahtarın bir int değer olduğunu ve kişinin numarasını belirttiğini düşünelim. Biz de numarasını bildiğimiz 
    kişinin bilgilerini elde etmek isteyelim. Örneğimizdeki kişilerin bilgileri PERSON isimli bir yapıyla temsil edilmiş 
    olsun:

    struct PERSON {
        ...
    };

    struct PERSON türünden büyük bir dizi açabiliriz:

    struct PERSON people[MAX_SIZE];

    Sonra da kişilerin numaralarını indeks yaparak bu diziye yerleştirebiliriz. Örneğin numarası 123 olan kişinin bilgilerini 
    diziye şöyle yerleştirilebilir:

    people[123] = person_info;

    Artık numarası 123 olan bu kişinin bilgilerini O(1) karmaşıklıkta aşağıdaki gibi elde edebiliriz:

    person_info = people[123];

    Bu yöntem ilk bakışta çok iyi bir yöntem gibi gözükse de genellikle kullanılabilir bir yöntem değildir. Çünkü burada 
    anahtar int türdendir. Ancak uygulamalarda anahtarlar farklı türlerden olabilmektedir. Örneğin anahtar kişinin adı 
    soyadı olabilir. Ad ve soyad gibi yazısal bilgiler indeks belirtmemektedir. Bu yöntemin diğer bir sakıncası da 
    anahtarların yüksek basamaklı sayılardan oluşabildiği durumlarda dizilerin çok fazla yer kaplamasıdır. Örneğin TC 
    kimlik numarasının anahtar yapılarak kişilerin bilgilerinin elde edilmesinin istendiği bir durumu düşünelim. TC 
    kimlik numarası 11 basamaklı bir sayıdır. Yani skalası 100 milyarlık sınırdadır. 100 milyarlık bir yapı dizisini bu 
    amaçla oluşturmak mümkün olmayabilir, mümkün olsa da etkin olmayabilir. Bu yönteme "indeksli arama (index search)" 
    denilmektedir. Ancak çok özel durumlarda bu yöntem uygun bir yöntem olarak kullanılabilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Algoritmik aramalarda en çok kullanılan yöntemlerden biri "hash tabloları (hash tables)" denilen yöntemdir. Hash 
    tabloları aslında yukarıda belirttiğimiz indeksli arama ile sıralı aramanın hibrit bir biçimi gibidir. Yöntemde ismine 
    "hash tablosu (hash table)" denilen makul uzunlukta bir dizi oluşturulur. Sonra anahtarlar ismine "hash fonksiyonu 
    (hash function)" denilen bir fonksiyona sokularak dizi indeksine dönüştürülür. Sonra da dizinin o indeksteki elemanına 
    başvurulur. Örneğin kişilerin bilgilerini TC kimlik numaralarına göre saklayıp geri almak isteyelim. Hash tablomuzun 
    uzunluğu da 1000 olsun. Hash fonksiyonumuzun da "1000'e bölümden elde edilen kalan" değerini veren fonksiyon olduğunu 
    varsayalım. Bu durumda örneğin 2566198712 TC kimlik numarasına sahip kişinin bilgileri hash tablosunun 712'nci 
    indeksteki elemanında saklanacaktır. 72484926820 TC kimlik numarasına sahip kişinin bilgileri de dizinin 820'nci 
    indeksteki elemanında saklanacaktır. Ancak farklı kişilerin TC kimlik numaraları hash fonksiyonuna sokulduğunda aynı 
    indeks değerleri de elde edilebilecektir. Örneğin 6238517712 TC numarasına sahip kişi de dizinin 712'nci indeksteki 
    elemanına yerleşmek isteyecektir. İşte hash tablosu yönteminde bu duruma "çakışma (collison)" denilmektedir. Hash 
    tablosu yöntemi çakışma durumunda izlenecek stratejiye göre çeşitli alt kollara ayrılmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Hash tabloları yönteminde çakışma durumunda bu sorunu çözmek için temel olarak iki alt yöntem grubu kullanılmaktadır: 
    "Ayrı zincir oluşturma (separate chaining)" yöntemi ve "açık adresleme (open addressing)" yöntemi. Açık adresleme 
    yöntemi de kendi aralarında "doğrusal yoklama (linear probing)", "karesel yoklama (quadratic probing)", "çift hash'leme 
    (double hashing)" gibi alt yöntemlere ayrılmaktadır. Ayrı zincir oluşturma ve açık adresleme yöntemlerinin dışında başka 
    çakışma çözümleme stratejileri de vardır. Ancak ağırlıklı olarak bu strateji tercih edilmektedir.

    Linux çekirdeklerindeki tüm hash tablolarında "ayrı zincir oluşturma (separate chaining)" alt yöntemi tercih edilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Ayrı zincir oluşturma (separate chaining) yönteminde hash tablosu aslında bir bağlı liste dizisi gibi oluşturulur. 
    Yani hash tablosunun her elemanı bağlı listenin ilk elemanını (head pointer) gösteren bir gösterici durumundadır. 
    Anahtar hash fonksiyonuna sokulur, bir indeks elde edilir ve o indeksteki bağlı listenin hemen önüne (ya da duruma 
    göre arkasına) eklenir. Eleman aranırken anahtar yine aynı hash fonksiyonuna sokulur ve dizinin ilgili indeksindeki 
    bağlı listede sıralı arama yapılır.

    Hash tablolarına eleman insert etmek O(1) karmaşıklıktadır. Tabii burada kullanılacak hash fonksiyonu da önemlidir. 
    Küçük döngüler içeren hash fonksiyonları O(1) karmaşıklığı yükseltmemektedir. Elemanın silinmesi de O(1) karmaşıklıkta 
    yapılabilmektedir. Eleman aramanın O(1) karmaşıklıkta yapılabilmesi için bağlı listelerdeki zincir uzunluklarının 
    uzun olmaması gerekir. Örneğin yukarıda 10 kadar eleman için en hızlı arama yönteminin sıralı arama olduğunu söylemiştik. 
    Bu koşul sağlandığında sıralı aramanın O(1) karmaşıkta olduğu söylenebilir. O halde eğer zincirlerdeki ortalama 
    eleman makul bir düzeyde tutulursa arama işlemi de O(1) karmaşıklıkta yapılabilecektir. Ancak hash tablosu küçük 
    fakat tabloya eklenecek eleman fazla ise bu durumda hash tablosu yöntemi artık sıralı arama yöntemine benzer hale 
    gelir. Yani bu yöntemin "en kötü durumdaki (worst case)" karmaşıklığının O(N) olduğu söylenebili . Tabii hash tablosu 
    yöntemini kullanan kişiler sistem hakkında bazı ön bilgilere sahip olursa tabloyu uygun bir büyüklükte oluşturabilirler. 
    İşletim sistemi gibi yüksek performans isteyen sistemlerde hash tablolarının zincirlerinin ortalama 1 civarında tutulması 
    uygun olabilmektedir. Hash tabloları da duruma göre büyütülebilmektedir. Ancak büyütmenin önemli bir zaman maliyeti 
    vardır. Yeni bir hash tablosunun tahsis edilmesi eski tablodaki elemanların yeniden hash'lenerek yeni tabloya yerleştirilmesi 
    uzun zaman alan bir işlemdir. İşletim sistemlerinin çekirdeklerinde bu biçimde uzun zaman alacak işlemler tercih edilmez. 
    Bu nedenle Linux çekirdeğindeki hash tabloları büyütülmemektedir. Hash tablolarında tablo elemanlarına (zincirlere değil) 
    İngilizce "bucket (kova)", tablodaki toplam eleman sayısının bucket sayısına bölümüne de "yükleme faktörü (load factor)" 
    denilmektedir. İdeal yükleme faktörünün <= 1 olduğunu söyleyebiliriz.

    Sözlük tarzı veri yapılarında genel olarak aynı anahtara ilişkin birden fazla anahtar-değer çifti veri yapısına 
    yerleştirilememektedir. (Bazı kütüphanelerde buna izin verilebilmektedir.) Eğer aynı anahtara ilişkin yeni bir değer 
    insert edilmeye çalışılırsa eski değer yeni değerle yer değiştirilmektedir. Bazı tasarımlar ise aynı anahtara 
    ilişkin insert yapmayı engellemektedir. Yani bu tasarımlarda yalnızca olmayan elemanlar tabloya insert edilebilmektedir. 
    Linux çekirdeğindeki hash tablolarında anahtarlar birbirinden farklı olmak zorundadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi hash tablolarında kullanılacak iyi bir hash fonksiyonu nasıl olmalıdır? İyi bir hash fonksiyonunun "hızlı" 
    olması gerekir. Çünkü insert gibi arama gibi işlemlerde hash fonksiyonu kullanılacaktır. İyi bir hash fonksiyonunun 
    "anahtarlar yanlı bile olsa" tabloya onları iyi bir biçimde yaydırabilmesi gerekir. Örneğin aslında sayısal anahtarlar 
    için "bölümden elde edilen kalan" iyi bir hash fonksiyonu değildir. Hash tablolarında tablonun asal sayı uzunluğunda 
    olması hash fonksiyonlarının daha iyi yaydırmasına yardımcı olmaktadır. (Örneğin tablo uzunluğu için 100 yerine 101 
    değeri tercih edilebilmektedir.) Hash fonksiyonları "sayıyı indekse dönüştüren" ve "yazıyı indekse" dönüştüren 
    fonksiyonlar biçiminde oluşturulabilir. Hash tablolarının 2'nin kuvveti uzunluğunda alınması hash fonksiyonlarının 
    daha hızlı çalışmasına da yol açabilmektedir. (Örneğin bölümden elde edilen kalan yerine bit düzeyinde öteleme 
    işlemleri hız kazancı sağlayabilmektedir.) Linux çekirdeklerinde hash tabloları için genellikle sayfa katlarında 
    (yani 4096'nın katlarında) alanlar tahsis edilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Her ne kadar Linux çekirdeğindeki hash tablolarında "ayrı zincir oluşturma (separate chaining)" stratejisi kullanılıyorsa 
    da burada "açık adresleme (open addressing)" stratejisi hakkında da küçük bir açıklama yapmak istiyoruz.

    Açık adresleme yöntemi de kendi arasında "yoklama (probing)" biçimine göre çeşitli alt yöntemlere ayrılmaktadır. Açık 
    adreslemenin en yaygın ve basit biçimi "doğrusal yoklama (linear probing)" denilen biçimidir.

    Doğrusal yoklama (linear probing) oldukça basit bir fikre dayanmaktadır. Bu yöntemde yine bir hash tablosu oluşturulur. 
    Ancak hash tablosunda bağlı listelerin adresleri tutulmaz bizzat değerlerin kendisi tutulur. Tabloya eleman ekleneceği 
    zaman yine anahtardan bir hash değeri elde edilir. Doğrudan değer tablonun hash ile elde edilen indeksine yerleştirilir. 
    Başka bir anahtar aynı hash değerini verdiğinde (yani çakışma durumu oluştuğunda) o indeksten itibaren boş yer bulunana 
    kadar yan yana indekslere sırasıyla bakılır. Örneğin hash olarak 123 değerini elde etmiş olalım. Tablonun 123'üncü elemanın 
    dolu olduğunu düşünelim. Bu durumda 124'üncü elemanına bakarız. O da doluysa 125'inci elemanına bakarız. Ta ki boş bir 
    indeks bulana kadar. Değeri ilk boş indekse yerleştiririz. Tabii bu durumda nasıl başka bir değer bizim indeksimize 
    yerleşmişse biz de aslında başka bir değerin indeksine yerleşmiş oluruz. Ancak bizim yerleştiğimiz indeks için hash'e 
    sahip olan değer de bizim yaptığımız gibi ilk boş yer bulunana kadar ilerleyecektir. Bu yöntemde arama işlemi de benzer 
    biçimde yapılmaktadır. Yani aranacak elemanın hash değeri elde edilir. O indekse başvurulur. Anahtar o indekste değilse 
    anahtar bulunana kadar ya da boş bir kova (bucket) görülene kadar yan yana diğer indekslere bakılır.

    Anahtara dayalı eleman silme de benzer biçimde yapılmaktadır. Ancak eleman silindiğinde ilgili kovanın (bucket) 
    boşaltılması arama işlemlerinde sorunlara yol açabilecektir. Burada yöntemlerden biri silinen elemana ilişkin kovanın 
    boş yapmayıp silinmenin özel bir değerle belirtilmesidir. Örneğin her kova için bir durum bayrağı tutulabilir. Bu 
    durum bayrağı ilgili kovanın "dolu" olduğunu", "boş" olduğunu ya da "silinmiş" olduğunu belirtebilir. Böylece arama 
    sırasında "silinmiş" kovalar görüldüğünde durulmaz. İlk boş kova görüldüğünde durulur. Tabii silinmiş kovalara yeni 
    elemanlar eklenebilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de Linux çekirdeğindeki hash tablosu gerçekleştirimi üzerinde duralım. Linux kaynak kodlarında hash tablosuna 
    ilişkin yapılar ve fonksiyonlar bağlı listelere ilişkin yapıların ve fonksiyonların bulunduğu başlık dosyasında 
    tanımlanmıştır. Yani hash tabloları için ayrı bir başlık dosyası oluşturulmamıştır. RCU'suz hash tablolarının gerçekleştirimi 
    "include/linux/list.h" dosyası içerisinde RCU'lu hash tablolarının gerçekleştirimi ise "include/linux/rculist.h" dosyası 
    içerisinde bulunmaktadır. Ancak her iki gerçekleştirim de aynı yapıları kullanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Ayrı zincir oluşturma yönteminde hash tablosundaki slotların (buckets) aslında bağlı listelerin ilk düğümünün yerini 
    tutan göstericilerden oluştuğunu belirtmiştik. Linux çekirdeklerinde bağlı listenin kök düğümü de eleman düğümleri 
    de list_head yapısıyla temsil ediliyordu. Ancak ayrı zincir oluşturmalı hash tablolarındaki kovalarda (buckets) bağlı 
    listenin son düğümünün yerinin tutulmasına gerek yoktur. Elemanlar hemen listenin başına eklenebilir. Arama da listenin 
    başından itibaren yapılabilir. Tabii performans bakımından bağlı liste düğümlerinin yine çift bağlı (doubly linked) 
    olması gerekir. Çünkü adresini bildiğimiz bir düğümü hash tablosundan kolaylıkla çıkartabiliriz. O halde çekirdek 
    tablolarında "kök düğümde tek gösterici, eleman düğümlerinde ise çift gösterici" bulunmalıdır.

    Çekirdekte kullanılan hash tablosu gerçekleştiriminde kök düğüm hlist_head yapısıyla temsil edilmiştir:

    struct hlist_head {
        struct hlist_node *first;
    };

    Görüldüğü gibi yapıda tek bir gösterici vardır. O da zincirdeki ilk elemanı göstermektedir. Zincirdeki bağlı listenin 
    düğümleri de hlist_node yapısıyla temsil edilmiştir:

    struct hlist_node {
        struct hlist_node *next, **pprev;
    };

    Buradaki düğüm yapısını listelerdeki düğüm yapısı ile karşılaştırınız:

    struct list_head {
        struct list_head *next, *prev;
    };

    Hash tablosundaki düğümlerin pprev elemanı önceki düğümün adresini değil önceki düğümdeki next elemanın adresini 
    göstermektedir. Bu nedenle pprev göstericiyi gösteren göstericidir. Hash tablolarındaki düğümlerde geri gitmek 
    için bir neden yoktur. Ancak eleman silme durumunda bu tasarım önceki elemanın next göstericisinin daha kolay 
    güncellenmesine yol açmaktadır. Örneğin p göstericisi bir hlist_node düğümününü gösteriyor olsun. Biz de bu düğümü 
    silecek olalım. Burada önceki düğümün next elemanının sileceğimiz düğümün next elemanındaki düğümü göstermesi 
    sağlanmalıdır. Bu işlem de pratik olarak şöyle yapılabilmektedir:

    *p->pprev = p->next;

    Halbuki düğümler list_head yapısıyla temsil ediliyor olsaydı bu işlem ancak şöyle yapılabilirdi:

    p->prev->next = p->next;

    Görüldüğü gibi bu güncellemede fazladan bir işlem yapılmaktadır. hlist_node tasarımının diğer önemli bir faydası 
    da by tasarımda kök düğüm için özel bir işlemin yapılmasına gerek kalmamasıdır. Yani listeye ilk kez eleman eklerken 
    *p->pprev kök düğümün next göstericisi haline gelecektir. Heterojen yapılarda bu tür güncellemelerin yapılması 
    daha fazla çabayı gerektirmektedir.

    Pekiyi neden bütün çift bağlı listelerde bu teknik kullanılmıyor? hlist_node yapısında olduğu gibi prev göstericisi 
    önceki düğümün başlangıç adresini göstermek yerine neden önceki düğümün next göstericisini göstermiyor? İşte geriye 
    doğru ilerlemenin gereli olabildiği durumlarda bu tasarımda geriye gidiş zorlaşmaktadır. Ancak yukarıda da belirttiğimiz 
    gibi hash tablolarındaki zincirlerde zaten geriye gitmenin bir anlamı yoktur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi biz yukarıdaki hlist_head ve hlist_node yapılarını kullanarak hash tablosunu nasıl oluşturabiliriz? İşte 
    yapılacak ilk şey hlist_head yapısı türünden bir dizi tahsis etmektir. Örneğin biz bu işlemi kullanıcı modunda 
    aşağıdaki gibi simüle edebiliriz:

    #include <stdio.h>
    #include <stdlib.h>

    #define TABLE_SIZE		4096

    struct hlist_head {
        struct hlist_node *first;
    };

    struct hlist_node {
        struct hlist_node *next, **pprev;
    };

    int main(void)
    {
        struct hlist_head *hash_table;

        if ((hash_table = (struct hlist_head *)malloc(sizeof(struct hlist_head) * TABLE_SIZE)) == NULL) {
            fprintf(stderr, "cannot allocate memory!...\n");
            exit(EXIT_FAILURE);
        }

        /* ... */

        free(hash_table);

        return 0;
    }

    Çekirdekteki "list.h" dosyası içerisinde hash tablosuna eleman eklemek için çeşitli basit fonksiyonlar bulundurulmuştur. 
    Tabii hlist_node düğümleri de aslında başka yapıların elemanı durumunda olacaktır. Yine asıl yapı nesnesinin adresi 
    container_of makrosuyla elde edilecektir.

    Başlangıçta hlist_head yapısındaki first elemanı NULL değerinde olmalıdır. Bunun için "list.h" içerisinde makrolar 
    bulundurulmuştur:

    #define HLIST_HEAD_INIT { .first = NULL }
    #define HLIST_HEAD(name) struct hlist_head name = { .first = NULL }
    #define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)

    Örneğin:

    for (int i = 0; i < TABLE_SIZE; ++i)
        INIT_HLIST_HEAD(&hash_table[i]);

    Zincirlerdeki son düğümün next elemanı da NULL değerindedir. Böylece arama NULL görmeyene kadar ilerlenerek yapılabilmektedir.

    Tablodaki zincirlerin önüne eleman eklemek için hlist_add_head fonksiyonu bulundurulmuştur:

    static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
    {
        struct hlist_node *first = h->first;
        WRITE_ONCE(n->next, first);
        if (first)
            WRITE_ONCE(first->pprev, &n->next);
        WRITE_ONCE(h->first, n);
        WRITE_ONCE(n->pprev, &h->first);
    }

    Burada fonksiyonun birinci parametresi kök düğüm nesnesinin adresini, ikinci parametresi yeni düğümün adresini 
    belirtmektedir. Buradaki WRITE_ONCE makrosu çok işlemcili ya da çok çekirdekli sistemlerde bellek bariyeri oluşturarak 
    atama yapmaktadır. Siz WRITE_ONCE(a, b) çağrısını a = b gibi düşünebilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										19. Ders 20/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Hash tablosundan bir düğüm silmek için hlist_del fonksiyonu kullanılmaktadır. Fonksiyon "list.h" içerisinde şöyle 
    tanımlanmıştır:

    static inline void hlist_del(struct hlist_node *n)
    {
        __hlist_del(n);
        n->next = LIST_POISON1;
        n->pprev = LIST_POISON2;
    }

    Burada asıl silme işlemi yapan fonksiyon __hlist_del fonksiyonudur. Düğüm silindikten sonra silinen düğümün next 
    ve pprev göstericilerine güvenlik amacıyla özel değerler atandığını görüyorsunuz. Bu özel değerler geçerli adresler 
    belirtmemektedir. Bu özel değerler kullanılmayan adres değerleri olduğu için eğer silinen düğüm yanlışlıkla kullanılırsa 
    "page fault" oluşmasına yol açacaktır. __hlist_del fonksiyonu da şöyle tanımlanmıştır:

    static inline void __hlist_del(struct hlist_node *n)
    {
        struct hlist_node *next = n->next;
        struct hlist_node **pprev = n->pprev;

        WRITE_ONCE(*pprev, next);
        if (next)
            WRITE_ONCE(next->pprev, pprev);
    }

    Zincirdeki son düğümün next göstericisinde NULL adres bulunmalıdır. Fonksiyonun içerisinde bu durumun kontrol edildiğine 
    ve ilk düğümün silinmesinin özel bir durum oluşturmadığına dikkat ediniz. Çok fazla gereksinim olmasa da zincirdeki belli 
    bir düğümün önüne ve arkasına düğüm insert eden hlist_add_before ve hlist_add_behind fonksiyonları da bulundurulmuştur:

    static inline void hlist_add_before(struct hlist_node *n, struct hlist_node *next)
    {
        WRITE_ONCE(n->pprev, next->pprev);
        WRITE_ONCE(n->next, next);
        WRITE_ONCE(next->pprev, &n->next);
        WRITE_ONCE(*(n->pprev), n);
    }

    static inline void hlist_add_behind(struct hlist_node *n, struct hlist_node *prev)
    {
        WRITE_ONCE(n->next, prev->next);
        WRITE_ONCE(prev->next, n);
        WRITE_ONCE(n->pprev, &prev->next);

        if (n->next)
            WRITE_ONCE(n->next->pprev, &n->next);
    }

    Hash tablosundaki bir hlist_node adresi bilindiğinde onun içinde bulunduğu asıl yapının adresinin container_of 
    makrosu ile elde edilebildiğini biliyorsunuz. Ancak tıpkı bağlı listelerde olduğu gibi hash tablolarında da bunun 
    için container_of makrosu ile aynı işlemi yapana bir entry makrosu bulundurulmuştur:

    #define hlist_entry(ptr, type, member) container_of(ptr,type,member)

    Hash tablosundaki bir zinciri dolaşmak için hlist_for_each döngü makrosu kullanılmaktadır:

    #define hlist_for_each(pos, head) \
        for (pos = (head)->first; pos ; pos = pos->next)

    Makronun ilk parametresi hlist_node türünden bir göstericiyi, ikinci parametresi ise bağlı liste zincirinin başlangıç 
    düğümüne ilişkin hlist_head nesnesinin adresini almaktadır. Bu döngü makrosunun next elemanı NULL olmayana kadar 
    ilerleme sağladığına dikkat ediniz. Döngü her yinelendikçe birinci parametreye girilen göstericinin içerisine zincirdeki 
    sıraki düğümün adresi yerleştirilmektedir. Bu döngü makrosuyla zincir dolaşılırken döngünün her yinelenmesinde asıl 
    yapı nesnesinin değil onun içerisindeki hlist_node nesnesinin adresinin elde edildiğine de dikkat ediniz. Bu adresten 
    hareketle container_of ya da hlist_entry makrolarıyla asıl nesnenin başlangıç adresininin elde edilmesi gerekir. İşte 
    bu iki işlemi aynı anda yapan hlist_for_each_entry isimli bir döngü makrosu da bulundurulmuştur:

    #define hlist_for_each_entry(pos, head, member)				                    \
    for (pos = hlist_entry((head)->first, typeof(*(pos)), member);                  \
        pos;							                                            \
        pos = hlist_entry((pos)->member.next, typeof(*(pos)), member))

    Bu makronun artık birinci parametresi doğrudan asıl yapı türünden bir göstericiyi, ikinci ve üçüncü parametreler 
    sırasıyla bağlı listenin hlist_head adresini ve asıl yapıdaki bağ elemanın ismini almaktadır.

    Yukarıdaki makroda bir noktaya dikkatini çekmek istiyoruz. Hash tablosundaki hlist_head kök düğümünün first elemanında 
    NULL adres varsa yukarıdaki hlist_for_each_entry makrosu tanımsız davranışa yol açar. İşte eğer first elemanı NULL ise 
    bu durumu ele alıp dolaşımı sonlandıran hlist_for_each_entry döngü makrosu da bulundurulmuştur:

    #define hlist_for_each_entry_safe(pos, n, head, member) 		    \
    for (pos = hlist_entry_safe((head)->first, typeof(*pos), member);   \
        pos && ({ n = pos->member.next; 1; });			                \
        pos = hlist_entry_safe(n, typeof(*pos), member))

    Makronun birinci parametresi asıl yapı nesnesi türünden göstericiyi, ikinci parametresi hlist_node türünden göstericiyi, 
    üçüncü parametresi hlist_head türünden kök düğümün adresini, dördüncü parametresi de asıl yapıdaki bağ düğümünün ismini 
    almaktadır. Bu makronun hlist_for_each_entry makrosundan tek farkı zincir boşsa bir soruna yol açmadan dönünün sonlanmasını 
    sağlamasıdır. Döngü makrosunun içerisinde hlist_entry_safe makrosunun kullanıldığına dikkat ediniz. Bu makro zincirin 
    sonundaki NULL adresi de dikkate almaktadır:

    #define hlist_entry_safe(ptr, type, member)                 \
	({ typeof(ptr) ____ptr = (ptr);                             \
	     ____ptr ? hlist_entry(____ptr, type, member) : NULL;     \
	})

    "list.h" dosyası içerisinde hash tablolarına ilişkin başka yararlı fonksiyonlar da vardır. Bu fonksiyonları ileride 
    gerektiğinde açıklayacağız. Bunları siz de inceleyebilirsiniz.

    Bir hash tablosunun tamamen yok edilmesi için yalnızca hash tablosunun değil onun tüm zincirlerdeki elemanlarının da 
    serbest bırakılması gerekir. Çekirdekte genel olarak böyle bir gereksinim yoktur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda "list.h" içerisindeki hash tablosu işlemleri için kullanıcı modunda çalıştırılabilecek bir örnek verilmiştir. 
    Örnekte TABLE_SIZE uzunluğunda her bir elemanı hlist_head türünden olan bir dizi oluşturulmuştur:

    if ((hash_table = (struct hlist_head *)malloc(sizeof(struct hlist_head) * TABLE_SIZE)) == NULL) {
        fprintf(stderr, "cannot allocate memory!...\n");
        exit(EXIT_FAILURE);
    }

    Sonra bu hlist_head elemanlarına ilkdeğer verilmiştir. (Yani onların first göstericilerine NULL adres yerleştirilmiştir):

    for (int i = 0; i < TABLE_SIZE; ++i)
        INIT_HLIST_HEAD(&hash_table[i]);

    Ondan sonra hash tablosuna rastgele 100 eleman eklenmiştir. Ancak bunun yanı sıra belli bir eleman da listeye 
    ayrıca eklenmiştir:

    for (int i = 0; i < 100; ++i) {
        if ((per = (struct PERSON *)malloc(sizeof(struct PERSON))) == NULL) {
             fprintf(stderr, "cannot allocate memory!...\n");
            exit(EXIT_FAILURE);
        }
        if (i == 50) {
            strcpy(per->name, "ALI SERCE");
            per->no = 12345678;
        }
        else
            set_random_person(per);
        hash = hash_func(per->no);
        hlist_add_head(&per->hlink, &hash_table[hash]);
    }

    Sonra liste dolaşılmıştır. Program biterken de tüm hash tablosu zincirleriyle birlikte serbest bırakılmıştır:

    {
        struct PERSON *per, *temp;
        struct hlist_node *node;

        for (int i = 0; i < TABLE_SIZE; ++i) {
            temp = NULL;
            hlist_for_each_entry_safe(per, node, &hash_table[i], hlink) {
                free(temp);
                temp = per;
            }
            free(temp);

        }
        free(hash_table);
    }

    Bağlı liste düğümleri serbest bırakılırken sonraki düğümün adresini saklamak gerekir. NULL adrese free uygulamanın 
    bir soruna yol açmayacağını anımsayınız.
----------------------------------------------------------------------------------------------------------------------*/

#include <stdio.h>
#include <stddef.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#define TABLE_SIZE		4096

struct hlist_head {
	struct hlist_node *first;
};

struct hlist_node {
	struct hlist_node *next, **pprev;
};

#define HLIST_HEAD_INIT { .first = NULL }
#define HLIST_HEAD(name) struct hlist_head name = { .first = NULL }
#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)

#define WRITE_ONCE(a, b)		((a) = (b))		/* bize özgü */
#define LIST_POISON1			(struct hlist_node *)0x00100100
#define LIST_POISON2			(struct hlist_node **)0x00200200

static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
{
	struct hlist_node *first = h->first;

	WRITE_ONCE(n->next, first);
	if (first)
		WRITE_ONCE(first->pprev, &n->next);
	WRITE_ONCE(h->first, n);
	WRITE_ONCE(n->pprev, &h->first);
}

static inline void __hlist_del(struct hlist_node *n)
{
	struct hlist_node *next = n->next;
	struct hlist_node **pprev = n->pprev;

	WRITE_ONCE(*pprev, next);
	if (next)
		WRITE_ONCE(next->pprev, pprev);
}

static inline void hlist_del(struct hlist_node *n)
{
	__hlist_del(n);
	n->next = LIST_POISON1;
	n->pprev = LIST_POISON2;
}

#define container_of(ptr, type, member) ({				\
		void *__mptr = (void *)(ptr);					\
		((type *)(__mptr - offsetof(type, member))); })

#define hlist_entry(ptr, type, member) container_of(ptr,type,member)

#define hlist_entry_safe(ptr, type, member) \
	({ typeof(ptr) ____ptr = (ptr); \
		____ptr ? hlist_entry(____ptr, type, member) : NULL; \
	})

#define hlist_for_each(pos, head) \
		for (pos = (head)->first; pos ; pos = pos->next)

#define hlist_for_each_safe(pos, n, head) \
	for (pos = (head)->first; pos && ({ n = pos->next; 1; }); \
		pos = n)

#define hlist_for_each_entry(pos, head, member)							\
	for (pos = hlist_entry((head)->first, typeof(*(pos)), member);		\
		pos;															\
		pos = hlist_entry((pos)->member.next, typeof(*(pos)), member))

#define hlist_for_each_entry_safe(pos, n, head, member) 		\
	for (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\
		pos && ({ n = pos->member.next; 1; });			\
		pos = hlist_entry_safe(n, typeof(*pos), member))

/* Test code */

struct PERSON {
	char name[32];
	int no;
	struct hlist_node hlink;
};

void set_random_person(struct PERSON *per);
unsigned int hash_func(unsigned int key);

int main(void)
{
	struct hlist_head *hash_table;
	struct PERSON *per;
	unsigned int hash;
	int no;

	srand(time(NULL));

	if ((hash_table = (struct hlist_head *)malloc(sizeof(struct hlist_head) * TABLE_SIZE)) == NULL) {
		fprintf(stderr, "cannot allocate memory!...\n");
		exit(EXIT_FAILURE);
	}

	for (int i = 0; i < TABLE_SIZE; ++i)
		INIT_HLIST_HEAD(&hash_table[i]);

	for (int i = 0; i < 100; ++i) {
		if ((per = (struct PERSON *)malloc(sizeof(struct PERSON))) == NULL) {
			 fprintf(stderr, "cannot allocate memory!...\n");
			exit(EXIT_FAILURE);
		}
		if (i == 50) {
			strcpy(per->name, "ALI SERCE");
			per->no = 12345678;
		}
		else
			set_random_person(per);
		hash = hash_func(per->no);
		hlist_add_head(&per->hlink, &hash_table[hash]);
	}

	{
		struct hlist_node *node;
		struct PERSON *per_find;

		printf("Person no:");
		scanf("%d", &no);

		hash = hash_func(no);

		hlist_for_each(node, &hash_table[hash]) {
			per_find = hlist_entry(node, struct PERSON, hlink);
			if (per_find->no == no) {
				printf("Found: %s, %d\n", per_find->name, per_find->no);
				break;
			}
		}
		if (node == NULL)
			printf("cannot find...\n");
	}

	{
		struct PERSON *per_find;
		struct hlist_node *node;

		hash = hash_func(no);

		hlist_for_each_entry_safe(per_find, node, &hash_table[hash], hlink) {
			if (per_find->no == no) {
				printf("Found: %s, %d\n", per_find->name, per_find->no);
				break;
			}
		}
		if (per_find == NULL)
			printf("cannot find...\n");
	}

	{
		struct PERSON *per, *temp;
		struct hlist_node *node;

		for (int i = 0; i < TABLE_SIZE; ++i) {
			temp = NULL;
			hlist_for_each_entry_safe(per, node, &hash_table[i], hlink) {
				free(temp);
				temp = per;
			}
			free(temp);

		}
		free(hash_table);
	}

	return 0;
}

void set_random_person(struct PERSON *per)
{
	int i;

	for (i = 0; i < 31; ++i)
		per->name[i] = rand() % 26 + 'A';
	per->name[i] = '\0';

	per->no = rand() % 1000000;
}

unsigned int hash_func(unsigned int key)
{
	key = (key ^ 61) ^ (key >> 16);
	key = key + (key << 3);
	key = key ^ (key >> 4);
	key = key * 0x27d4eb2d;
	key = key ^ (key >> 15);

	return key % TABLE_SIZE;
}

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğine kilitsiz (lock-free) RCU mekanizması eklendiğinde tıpkı bağlı listelerde olduğu gibi hash tablolarına 
    da yukarıdaki fonksiyonların sonu _rcu ile biten RCU uyumlu versiyonları eklenmiştir. Bu fonksiyonlar "include/linux/rculist.h" 
    dosyası içerisindedir. Bu dosyada yukarıda gördüğümüz hash tablosu fonksiyonlarının RCU mekanizmalı versiyonları 
    bulunmaktadır. Biz buradaki fonksiyonların yalnızca isimlerini vereceğiz. RCU mekanizması daha önce de belirttiğimiz 
    gibi başka bir başlık altında ele alınacaktır:

    hlist_add_head_rcu
    hlist_del_rcu
    hlist_add_before_rcu
    hlist_add_behind_rcu
    hlist_for_each_entry_rcu
    hlist_for_each_entry_srcu	(safe version)

    hlist_first_rcu(head)
    hlist_next_rcu(node)
    hlist_pprev_rcu(node)

    Bu RCU'lu fonksiyonlar tıpkı bağlı listelerde olduğu gibi birden fazla okuyan ancak tek bir yazan taraf varsa beklemeye 
    yol açmamaktadır. Tabii birden fazla yazan tarafın ayrıca bir senkronizasyon nesnesiyle korunması gerekir. Bu 
    makrolarla dolaşım yapılırken yine bağlı listelerde olduğu gibi ilgili kod bloğunun başına ve sonuna rcu_read_lock 
    ve rcu_read_unlock çağrılarının yerleştirilmesi gerekmektedir.

    Çekirdek tıpkı listelerde olduğu gibi pek çok yerde (ancak her yerde değil) hash tablolarıyla RCU mekanizması eşliğinde 
    işlem yapmaktadır. Çekirdeğin RCU mekanizması eşliğinde işlem yaptığı yerlerde sizin de bu RCU'lu fonksiyonları 
    kullanmanız gerekir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Hash tablolarını da görmüş olduk. Şimdi bir pid değeri verildiğinde çekirdeğin ona ilişkin task_struct nesnesini 
    nasıl bulunduğu üzerinde duralım. pid değerinden hareketle task_struct nesnesinin bulunmasına ilişkin algoritmalar 
    zaman içerisinde çekirdeğin çeşitli versiyonlarında değiştirilmiştir.

    Çekirdeğin öğrenci ödevi gibi olan ilk 0.01 versiyonunda zaten en fazla 64 tane task_struct nesnesi oluşturulabiliyordu. 
    Bu versiyonda pid değerine ilişkin task_struct nesnesinin bulunması için bu task_struct nesnelerinin tutulduğu task 
    isimli global dizide sıralı arama yapılmıştır.

    Çekirdeğin 2.2 versiyonlarında pid değerinden hareketle task_struct nesnesinin bulunması için hash tablosu kullanılmıştır. 
    Bu versiyonlarda bu işi yapan find_task_by_pid fonksiyonu aşağıdaki gibi yazılmıştır:

    extern __inline__ struct task_struct *find_task_by_pid(int pid)
    {
        struct task_struct *p, **htable = &pidhash[pid_hashfn(pid)];

        for(p = *htable; p && p->pid != pid; p = p->pidhash_next)
            ;

        return p;
    }

    Bu versiyonlarda henüz yukarıda açıkladığımız hash tablosu fonksiyonları çekirdekte bulunmuyordu. find_task_by_pid 
    fonksiyonunda önce pid değeri pid_hashfn isimli bir hash fonksiyonuna sokularak bir index değeri elde edilmiş sonra 
    da pidhash dizisinin bu indexteki bağlı liste zincirinde arama yapılmıştır. Buradaki pidhash dizisi şöyle 
    tanımlanmıştır:

    struct task_struct *pidhash[PIDHASH_SZ];

    Görüldüğü gibi hash tablosundaki zincirler doğrudan task_struct nesnelerini tutmaktadır. Bu versiyonlarda bu zincirler 
    için task_struct içerisinde iki link elemanı bulunduruluyordu:

    struct task_struct {
        ...
        struct task_struct *pidhash_next;
        struct task_struct **pidhash_pprev;
        ...
    };

    Çekirdeğin 2.4 versiyonunda da algoritmalarda ve yukarıdaki fonksiyonda bir değişiklik yapılmamıştır. Yani yine bir 
    hash tablosu eşliğinde arama yapılmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										20. Ders 21/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin 2.6'lı versiyonlarında artık her farklı pid değeri için o pid değerine ilişkin bir pid nesnesi (struct pid 
    nesnesi) oluşturulmaya başlanmıştır. Bu versiyonlarda çekirdek her farklı pid değeri için bir pid nesnesi oluşturup 
    bu pid nesnelerini de hash tablolarında saklamaktadır. Her pid nesnesi de izleyen paragraflarda açıklayacağımız gibi 
    bir grup bağlı listenin kök düğümlerini tutmaktadır. Bu versiyonlarda bir pid değerine ilişkin task_struct nesnesini 
    bulmak için çekirdek önce hash tablosundan o pid değerine ilişkin pid nesnesini elde etmekte,sonra o pid nesnesinin 
    içerisindeki bağlı listelerde arama yapmaktadır.

    2.6'lı versiyonlardaki pid yapısı şöyleydi:

    struct pid
    {
        atomic_t count;
        unsigned int level;
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct rcu_head rcu;
        struct upid numbers[1];
    };

    Yukarıda da belirttiğimiz gibi bu versiyondan itibaren çekirdek önce pid değerinden hareketle o pid değerine 
    ilişkin pid yapı nesnesini elde edip daha sonra bu pid nesnesinin içerisindeki ilgili bağlı listeyi dolaşmaktadır. 
    pid yapısında bir tane bağlı liste değil PIDTYPE_MAX kadar bağlı liste bulunmaktadır. Yapının tasks elemanı bu bağlı 
    listelerin kök düğümlerini belirtmektedir. Bu versiyonlardaki durumu şöyle özetleyebiliriz:

    1) Çekirdek her pid için bir pid nesnesi (struct pid nesnesi) oluşturup onu bir hash tablosunda saklamaktadır. Yani 
    pid nesneleri için bir hash tablosu kullanılmıştır.

    2) pid yapısının içerisindeki tasks elemanı o pid değerine ilişkin izleyen paragraflarda açıklayacağımız bağlı 
    liste zincirlerinin kök düğümlerini tutmaktadır.

    3) Bu durumda sistem bir pid değerine ilişkin task_struct nesnesini elde etmek için önce o pid değerine ilişkin pid 
    nesnesini hash tablosundan bulmakta sonra onun içerisindeki ilgili bağlı listede arama yapmaktadır.

    pid değeri ---> pid nesnesi ---> pid nesnesi içerisindeki ilgili bağlı listede arama

    2.6 çekirdekleriyle birlikte Linux'a "isim alanları (name spaces)" kavramı da sokulmaya başlanmıştır. Bu versiyonlar 
    artık pid değerleri için bir isim alanı da kullanmaktadır. pid isim alanı sayesinde sanki çekirdek birden fazla pid 
    dünyasına sahip gibi bir etki oluşturulmaktadır. Bu sayede bir pid isim alanı yaratılıp bu isim alanın diğerlerinden 
    bağımsız bir biçimde kullanılabilmesi sağlanmıştır. Pid isim alanları iç içe de oluşturulabilmektedir. Yani artık bu 
    versiyonlarla birlikte her pid değeri bir pid isim alanı içerisindedir. Tabii sistem açıldığında zaten yaratılmış hazır
    default bir pid isim alanı bulunmaktadır. Artık 2.6 versiyonlarıyla birlikte pid değerleri sistem genelinde tek değil 
    isim alanı genelinde tektir. Linux çekirdeğine eklenen bu isim alanları özelliği "docker" gibi container teknolojilerinin 
    gelişmesine katkı sağlamıştır. Linux üzerinde bu ortamlar işletim sisteminin kendi çekirdek kaynaklarını kullanarak 
    sanallaştırmayı daha kolay bir biçimde gerçekleştirilebilmektedir.

    2.6'lı çekirdeklerden itibaren pid işlemleri "kernel/pid.c" dosyası içerisinde gerçekleştirilmiştir. 2.6'lı çekirdeklerde 
    pid değeri verildiğinde task_struct nesnesinin elde edilmesi için "kernel/pid.c" dosyası içerisinde aşağıdaki fonksiyonlar 
    yazılmıştır:

    struct task_struct *find_task_by_vpid(pid_t vnr)
    {
        return find_task_by_pid_ns(vnr, current->nsproxy->pid_ns);
    }

    struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns)
    {
        rcu_lockdep_assert(rcu_read_lock_held());
        return pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);
    }

    struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
    {
        struct hlist_node *elem;
        struct upid *pnr;

        hlist_for_each_entry_rcu(pnr, elem,
                &pid_hash[pid_hashfn(nr, ns)], pid_chain)
            if (pnr->nr == nr && pnr->ns == ns)
                return container_of(pnr, struct pid,
                        numbers[ns->level]);

        return NULL;
    }

    struct task_struct *pid_task(struct pid *pid, enum pid_type type)
    {
        struct task_struct *result = NULL;
        if (pid) {
            struct hlist_node *first;
            first = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),
                            rcu_read_lock_held() ||
                            lockdep_tasklist_lock_is_held());
            if (first)
                result = hlist_entry(first, struct task_struct, pids[(type)].node);
        }
        return result;
    }

    Burada en ana fonksiyonun find_task_by_vpid (buradaki "v" harfi "virtual" sözcüğünden gelmektedir) olduğunu görüyorsunuz. 
    Bu fonksiyon o anda prosesin çalıştığı pid isim alanında arama yapmaktadır. Fonksiyonların çağırma grafı şöyledir:

    find_task_by_vpid ---> find_task_by_pid_ns ---> find_pid_ns ---> pid_task ---> task_struct

    Burada find_task_by_vpid fonksiyonu çağrıyı yapan prosesin pid isim alanında aramayı başlatmaktadır. find_task_by_pid_ns 
    fonksiyonu yalnızca belli bir isim alanında aramayı sağlar. find_pid_ns fonksiyonu ise ilgili isim alanında pid yapı nesnesini 
    bulur. pid_task fonksiyonu da o pid nesnesinin içerisindeki bağlı zincirinde arama yaparak task_struct nesnesinin adresini 
    elde eder. findd_task_vpid fonksiyonu çağrıyı yapana prosesin pid isim alanına task_struct yapısı içerisindeki nsproxy->pid_ns 
    elemanı yoluyla erişmektedir.

    2.6 çekirdeklerinde pid ve isim alanından hareketle pid nesnelerinin bulunması için tek bir hash tablosu oluşturulmuştur. 
    Yani bütün isim alanlarındaki pid nesneleri aynı hash tablosu içerisindedir. Hash tablosunda pid nesnesini arayan 
    find_pid_ns fonksiyonu şöyle yazılmıştır:

    struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
    {
        struct hlist_node *elem;
        struct upid *pnr;

        hlist_for_each_entry_rcu(pnr, elem,
                &pid_hash[pid_hashfn(nr, ns)], pid_chain)
            if (pnr->nr == nr && pnr->ns == ns)
                return container_of(pnr, struct pid,
                        numbers[ns->level]);

        return NULL;
    }

    Burada hash zincirlerindeki düğümler upid isimli yapıyla temsil edilmiştir. Gördüğünüz gibi arama yapılırken hem 
    pid değerine hem de pid isim alanına birlikte bakılmıştır. Buradaki hash tablosu üzerinde bir açıklama yapalım. 
    Ana hash tablosu pid_hash dizisi biçimindedir. Bu dizinin tahsisatı çekirdek ilkdeğerlenirken (initialize edilirken) 
    yapılmaktadır. Burada hlist_node elemanı upid yapısının içerisinde, upid yapısı da pid yapısının içerisindedir. 
    Bunlar arasındaki içerme ilişkisini daha iyi anlayabilmeniz için bu iki yapıyı alt alta veriyoruz:

    static struct hlist_head *pid_hash;
    ...

    struct upid {
        /* Try to keep pid_chain in the same cacheline as nr for find_vpid */
        int nr;
        struct pid_namespace *ns;
        struct hlist_node pid_chain;
    };

    struct pid
    {
        atomic_t count;
        unsigned int level;
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct rcu_head rcu;
        struct upid numbers[1];
    };

    Burada hlist_node bağının uid nesnesinin içerisinde olduğuna, uid nesnesinin de pid nesnesinin içerisinde olduğuna 
    dikkat ediniz.

    2.6'lı çekirdek versiyonlarında hash tablosundan pid nesnesi elde edildikten sonra artık task_struct nesnesi bu pid 
    yapısının içerisindeki bağlı listelerde aranmaktadır. pid yapısının tasks elemanının bir grup bağlı liste zincirinin 
    kök düğümlerini tuttuğunu belirtmiştik. Buradaki tasks dizisinin PIDTYPE_MAX kadar elemana sahip olduğunu görüyorsunuz. 
    Aslında PIDTYPE_MAX aşağıdaki gibi bir enum türünün elemanıdır:

    enum pid_type
    {
        PIDTYPE_PID,
        PIDTYPE_PGID,
        PIDTYPE_SID,
        PIDTYPE_MAX
    };

    Burada PIDTYPE_MAX değerinin 3 olduğunu görüyorsunuz. Ancak güncel çekirdeklerde bu değer 4'tür. Güncel çekirdeklerdeki
    pid_type enum türü şöyledir:

    enum pid_type {
        PIDTYPE_PID,
        PIDTYPE_TGID,
        PIDTYPE_PGID,
        PIDTYPE_SID,
        PIDTYPE_MAX,
    };

    Bu diziye sonraları PIDTYPE_TGID ile temsil edilen bir bağlı listenin daha eklendiğine dikkat ediniz. Pekiyi pid 
    değerini saklamak için neden birden fazla bağlı liste kullanılmaktadır? Bir pid nesnesinin tek bir pid değeri belirtmesi 
    gerekmez mi? İşte 2.6'lı çekirdeklerle birlikte bu tarz aramalarda hız kazancı sağlamak için tasarım değiştirilmiştir. 
    Buradaki bağlı listelerin hangi task_struct nesnelerini tuttuğunu tek tek açıklayalım:

    - PIDTYPE_PGID bağlı listesi aynı proses grubuna ilişkin proseslerin ana thread'lerinin task_struct nesnelerini tutmaktadır. 
    Yani aranan pid değeri bir proses grup liderine ilişkin pid belirtiyorsa bu zincirde o proses grubundaki tüm proseslerin 
    ana thread'lerinin task_struct nesnelerinin adresleri bulunmaktadır. Çekirdeğe biz bir proses grup id değeri verdiğimizde 
    çekirdek o proses grubunun içerisindeki tüm proseslerin (proseslerin ana thread'lerinin) task_struct adreslerini bu 
    zinciri dolaşarak elde edebilmektedir.

    - PIDTYPE_SID bağlı listesinde belli bir oturuma (session) ilişkin tüm proseslerin (onların ana thread'lerinin) task_struct 
    nesnelerinin adresleri tutulmaktadır. Yani biz çekirdeğe bir oturum liderinin pid değerini verdiğimizde çekirdek o 
    oturumdaki tüm proseslerin task_struct adreslerini bu listeyi dolaşarak elde edebilmektedir.

    - PIDTYPE_TGID bağlı listesi ise belli bir prosesin thread'lerini dolaşmak için kullanılmaktadır. Bu bağlı listedeki 
    tüm task_struct nesneleri aynı prosesin thread'lerini oluşturmaktadır. Yani biz çekirdeğe bir prosesin ana thread'ine 
    ilişkin bir pid değeri verdiğimizde çekirdek bu bağlı liste zincirini dolaşarak o prosesin tüm thread'lerinin 
    task_struct adreslerini elde edebilmektedir.

    - PIDTYPE_PID bağlı liste zincirinde aslında tek bir eleman vardır. Amacımız yalnızca belli bir pid değerine ilişkin 
    task_struct nesnesinin adresinin bulunmasıysa hemen bu listenin ilk elemanını alabiliriz.

    Yukarıda sözünü ettiğimiz pid_task fonksiyonu pid yapısının içerisindeki bir bağlı liste zincirinin tamamını değil 
    yalnızca ilk elemanını vermektedir:

    struct task_struct *pid_task(struct pid *pid, enum pid_type type)
    {
        struct task_struct *result = NULL;
        if (pid) {
            struct hlist_node *first;
            first = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),
                            rcu_read_lock_held() ||
                            lockdep_tasklist_lock_is_held());
            if (first)
                result = hlist_entry(first, struct task_struct, pids[(type)].node);
        }
        return result;
    }

    2.6 versiyonlarında ilgili task_struct pid değerine ilişkin pid nesnesinin adresi ayrıca task_struct yapısının 
    içerisinde tutulmuyordu. Ancak çekirdeğin 4.20 versiyonuyla birlikte artık task_struct pid değeri doğrudan task_struct 
    yapısının thread_pid elemanında da saklanmaya başlamıştır.

    Pekiyi 2.6 öncesindeki yukarıda sözünü ettiğimiz işlemler nasıl yapılıyordu? Örneğin bir proses grubunun pid değeri 
    verildiğinde çekirdek o gruptaki tüm proseslerin (onların ana thread'lerinin) task_struct adreslerini nasıl elde 
    ediyordu? İşte o çekirdeklerde tek bir hash tablosu vardı. Bu nedenle proses grubundaki proseslerin task_struct 
    nesneleri ancak tüm task_struct nesneleri dolaşılarak elde edilebiliyordu. O versiyonlarda bir prosesin thread'lerine 
    ilişkin task_struct nesneleri için de önce ana thread'in task_struct nesnesi, sonra o ana thread'in task_struct 
    nesnesinden hareketle prosesin thread'lerine ilişkin task_struct nesneleri elde ediliyordu. Halbuki 2.6'daki bu 
    tasarımla bu biçimdeki dolaşımlar bu hash tablosu ve bağlı listeler yoluyla daha hızlı yapılabilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek versiyonu 4.20'lere geldiğinde yukarıdaki mekanizmada bazı değişiklikler yapılmıştır. En önemli değişiklik 
    pid ve isim alanından hareketle pid nesnesinin bulunması işleminde hash tablosu yerine radix ağacının kullanılmaya 
    başlanmasıdır. Aslında 2.5 ile birlikte çekirdekte başka alt sistemlerde radix ağacı kullanılmaya başlanmıştı. Sonra 
    bu radix ağaçlarının 4.20 versiyonuyla birlikte XArray isimli yeni bir gerçekleştirimi yapılmıştır. İşte çekirdeğin 
    4.20 versiyonuna gelindiğinde bu radix ağacının XArray gerçekleştirimi pid alt yapısında da kullanılmıştır.

    4.20 ve sonrasındaki pid yapısındaki (struct pid içerisindeki) bağlı listelerde bir farklılık yoktur. Yalnızca pid 
    ve isim alanı bilgisinden hareketle pid yapı nesnesinin elde edilmesinde hash tablosu yerine radix ağacı (XArray 
    gerçekleştirimi) kullanılmaktadır. Bu bağlamda pid mekanizmasında radix ağacının hash tablosuna tercih edilmesinin 
    tipik nedenleri şunlardır:

    - Radix ağacındaki arama hash tablolarından yavaş gözükse bile aslında pratikte durum tam böyle değildir. 
    Buradaki radix ağacı seyrek tutulmaktadır. Bu yüzden arama hızlı yapılır.

    - pid’leri artan sırayla gezmek, boş pid bulmak (en küçük kullanılmayan PID’i seçmek) çok daha kolaydır.

    - Ağaçta yalnızca kullanılan düğümler tutulmaktadır. Bu da bellek kazancı sağlamaktadır.

    - pid sayısı arttıkça hash tablosunun performansı düşme eğilimi gösterdiği halde radix ağacının performansı hash 
    tablosuna kıyasla düşmez.

    - Radix ağacı yalnızca pid araması için değil, aynı zamanda yeni yaratılan task_struct nesneleri için pid tahsis 
    edilmesinde de kullanılabilmektedir. Bu tasarımda yeni bir pid numarası eskisine göre daha hızlı elde edilebilmektedir.

    Özetle yalnızca pid araması için hash tablosu tasarımına devam edilebilirdi. Ancak pid tahsisatı gibi diğer işlemler 
    için bu radix ağacı tasarımı toplamda daha iyi performans sağlamaktadır. Güncel çekirdeklerde hem pid aramaları hem 
    de boş pid değerlerinin tespit edilmesi işlemleri bu XArray gerçekleştirimi ile sağlanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz radix arama ağacını ve XArray gerçekleştirimini daha sonra başka bir bölümde ele alacağız. Burada yalnızca radix 
    arama ağacının temel mekanizması üzerinde açıklamalar yapacağız. Radix arama ağaçları için "dijital arama ağaçları 
    (digital search tree)", "önek arama ağacı (prefix search tree)", "sıkıştırılmış arama ağaçları (compresses search 
    trie)" gibi isimler de kullanılmaktadır. Bu arama ağaçlarında düğümlerde anahtar olarak anahtarın hepsi değil onun 
    ön kısımları kullanılmaktadır. Böylece ağaç bir leksigrafik sıralama ağacı gibi de kullanılabilmektedir. Tipik 
    anlatım sayısal değerlerden oluşan ve öneklerin bit belirttiği anahtarlar üzerinde yapılmaktadır. Ağacın kökünden 
    girilir. Her kademede bir bit daha anahtara eklenir. Örneğin 4 bitlik sayıların radix ağacına yerleştirilmek istendiğini 
    düşünelim. Bu sayılar şunlar olsun:

    1100
    1010
    0101
    1011
    0010
    0111
    0000
    1000

    Ağacın kökünün boş olduğunu düşünelim. Aslında ağaca yerleştirilecek ilk değer de kök yapılabilir. İlk değer için 
    ilk bit bite bakılır. 0 olan bitler için sola, 1 olan bitler için sağa yerleştirme yapılmaktadır:

                                Kök
                                        1100

    Şimdi ikinci değeri ağaca yerleştirelim. Bu durumda 1010 değeri kökün sağına ancak 1100 deüğümn soluna yerleştirilir:

                                Kök
                                             1100
                                      1010

    Üçüncü değer olan 0101 kökün soluna yerleşecektir:

                                Kök
                   0101                           1100
                                      1010

    Dördüncü değer olan 1011'i yerleştirdikten sonra ağaç şu görünümde olacaktır:

                                 Kök
                   0101                            1100
                                      1010
                                            1011

    Şimdi de 0010 değerini yerleştirelim:

                                    Kök
                        0101                          1100
                0010                1010
                                          1011

    Şimdi de 0111 değerini yerleştirelim:

                                    Kök
                         0101                          1100
                   0010        0111       1010
                                                1011

    Şimdi de 0000 değerini yerleştirelim:

                                      Kök
                        0101                         1100
                0010          0111        1010
         0000                                   1011

    Nihayet 1000 değerini de ağaca yerleştirelim:

                                      Kök
                        0101                            1100
                0010         0111            1010
         0000                          1000       1011

    Pekiyi bu ağaçta arama nasıl yapılır? İşte arama için kökten girilir. Her bit pozisyonu için bir aşağıya inilir. 
    Örneğin 0000 değerini arayacak olalım. İlk bit 9 olduğu için kökten sola ineriz. Oradaki değer 0101'dir. Bulamadığızdan 
    dolayı ikinci bite bakarız. İkinci bit de 0 olduğu için oradan da sola gideriz. Oradaki değer 0010'dır. Bulamadığımız 
    için üçüncü bite bakarız. Üçüncü bit de 0 olduğu için sola gideriz. Artık değeri buluruz.

    Burada biz yüksek anlamlı bitten hareketle dallanmaları yaptık. Ancak bunun tersini de yapabilirdik. Radix ağaçlarını 
    enlemesine (breadt-first gibi) dolaştığımızda anahtarların sıralı bir biçimde elde edilebildiğine dikkat ediniz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										21. Ders 27/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki anlatımda düğümlerde anahtarların turulduğunu varsaydık. Aslında düğümlerde anahtarların tutulmasına 
    da gerek yoktur. Zaten her kademede bir bit ilerlendiğinde göre arama yaprağa gelindiğinde sonlandırılabilir. Bir 
    yaprağa gelindiğinde ya ilgili değer bulunmuştur ya da ilgili değer bulunamamıştır. Eğer bütün bitler tüketilerek 
    yaprağa gelinmişse ilgili anahtar bulunmuş, bütün bitler tüketilmeden yaprağa gelinmişse ilgili değer bulunamamıştır.

    Kök
    ├── 0
    │    ├── 1
    │    │    ├── 0
    │    │    │    └── 1 (0101 son)
    │    │    └── 1
    │    │         └── 1 (0111 son)
    │    └── 0
    │         ├── 1
    │         │    └── 0 (0010 son)
    │         └── 0
    │              └── 0 (0000 son)
    └── 1
        ├── 1
        │    └── 0
        │         └── 0 (1100 son)
        └── 0
            ├── 1
            │    ├── 0 (1010 son)
            │    └── 1 (1011 son)
            └── 0
                    └── 0 (1000 son)

    Aynı ağacı şöyle de gösterebiliriz:

                     ┌───────────────────────────┐
                     |           Kök             |
                     └───────────────────────────┘
                    /                             \
                 ┌───┐                            ┌───┐
                 | 0 |                            | 1 |
                 └───┘                            └───┘
              /        \                       /         \
        ┌───┐           ┌───┐             ┌───┐           ┌───┐
        | 0 |           | 1 |             | 0 |           | 1 |
        └───┘           └───┘             └───┘           └───┘
       /    \          /      \          /     \            |
   ┌───┐    ┌───┐    ┌───┐    ┌───┐    ┌───┐   ┌───┐      ┌───┐
   | 0 |    | 1 |    | 0 |    | 1 |    | 0 |   | 1 |      | 0 |
   └───┘    └───┘    └───┘    └───┘    └───┘   └───┘      └───┘
     |        |        |        |       |      /   \        |
   ┌───┐    ┌───┐    ┌───┐    ┌───┐   ┌───┐  ┌───┐ ┌───┐  ┌───┐
   | 0 |    | 0 |    | 1 |    | 1 |   | 0 |  | 0 | | 1 |  | 0 |
   └───┘    └───┘    └───┘    └───┘   └───┘  └───┘ └───┘  └───┘

    Görüldüğü gibi bu tasarımda hiç anahtarlar saklanmadan yaprak görülene kadar ilerlenirse ya ilgili anahtar bulunmuş 
    olur ya da bulunmamış olur. Pekiyi her düğümde anahtarı tutmakla yalnızca yapraklarda değeri (anahtarı değil) tutmanın 
    hangisi daha iyi bir yöntemdir? İşte her iki yöntemin de avantajları ve dezavantajları vardır:

    1. Her Düğümde Anahtar Saklama Yöntemi

    Avantajları:
    • Erken durabilme: Sorgu sırasında ara bir düğümde tam eşleşme varsa yaprağa kadar inmek zorunda kalınmaz.
    • Prefix aramalar daha kolay: “Bu prefix ile başlayan anahtarlar var mı?” soruları doğal olarak çözülebilir.

    Dezavantajları:
    • Veri yapının karmaşıklığı artar; hem iç düğümlerde hem de yapraklarda anahtar tutmak gerekir.
    • Bellek kullanımı artar.

    2. Yalnızca Yapraklarda Değer Saklama Yöntemi

    Avantajları:
    • Basit tasarım: İç düğümler yalnızca yönlendirme (ayrıştırma) bilgisi tutar.
    • Bellek daha düzenli kullanılır, çünkü anahtar yalnızca tek yerde saklanır.
    • Arama, ekleme, silme algoritmaları daha etkin yapılabilir.

    Dezavantajları:
    • Her zaman yaprağa kadar inmek gerekir, dolayısıyla tam eşleşme ara düğümde olsa bile ek adımlar atılır.
    • Prefix tabanlı aramalarda ek iş yapılması gerekir.

    Linux'un klasik radix ve XArray gerçekleştirimlerinde değerler her zaman yapraklarda tutulmaktadır. Ara düğümlerde 
    anahtarlar tutulmamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Radix ağacınının ikili ağaç olması zorunlu değildir. Dallanma bit bit yapıldığı zaman yukarıdaki örnekte olduğu gibi 
    radix ağacı ikili ağaç durumunda olur. Ancak dallanma n bit n bit de yapılabilir. Bu durumda ağaç da n'li ağaç olacaktır. 
    Örneğin 32 bitlik pid değerlerini böyle bir ağaçta tutmak isteyelim. Ama dallanmayı bit bit değil de 6 bit 6 bit yapalım. 
    Ağacın kademelerini de düşük anlamlı bitlerden yüksek anlamlı bitlere doğru oluşturalım. 6 bit 2^6 = 64 farklı dallanmaya 
    yol açacaktır. Bu durumda ağacımızın her düğümünde 64 gösterici bulunacaktır:

                                                 Kök
    000000   000001  000010  000011  ...                ...  111100  111101  111110 111111

    İşte bu kökten itibaren her düğümde 64 dallanma olacaktır. Yapraklara varabilmek için en fazla 6 kademe ilerlenecektir. 
    Güncel sürümlerde Linux'un pid araması için kullandığı XArray ağacı bu örnekteki gibi altışar bitlidir.

    Bu biçimdeki XArray ağacının performansını 2.6'lı versiyonlardan itibaren kullanılmaya başlanmış olan hash tablolarının 
    performansıyla kıyasladığımızda ilk bakışta hash tablolarının daha iyi performans gösterebileceğini sanabilirsiniz. Ancak 
    boş pid değerlerinin aranmasında da aynı ağaçtan faydalanıldığı göz önüne alındığında XArray ağacının toplamda daha yüksek 
    performans sağladığı görülebilmektedir. Aynı zamanda bu ağaç yapısı iç içe pid isim alanlarında da hızlı aramaya olanak 
    sağlamaktadır. XArray ağacının kullanıldığı çekirdeklerde boş pid değerinin elde edilmesi için yine son pid değerini 
    tutan bir değişkenden başlanarak ağaçta arama yapılır. Ancak pid değerleri doluysa ilk boş pid değerinin bulunması bu veri 
    yapısında daha hızlı gerçekleşebilmektedir. Boş bir pid değerinin bulunmasının amacı zaten yeni bir pid nesnesinin (struct 
    pid nesnesinin) tahsis edilerek ağaca iliştirilmesidir. XArray ağaçlarında bu iki işlem birlikte yapılmaktadır. 
    Bu işlemler güncel çekirdeklerde çu çağrılar eşliğinde yapılmaktadır:

    alloc_pid ---> idr_alloc_cyclic ---> idr_alloc_u32 ---> idr_get_free

    pid değerinden pid yapı nesnesinin elde edilmesi sürecinde hash tablosu ile XArray gerçekleştimi arasındaki avantaj ve 
    dezavantajlar şöyle sıralanabilir:

    Hash Tablosu İle PID Arama

    - Arama karmaşıklığı: Ortalama O(1), yani sabit zamanlı.
    - Bellek kullanımı: Tablo önceden belli bir boyutta ayrılır. pid üst lmiti yüksekse ama kullanılan pid sayısı göreli olarak 
    azsa, büyük miktarda alan boşa gider.
    - Çakışmalar (Collisions): Birden fazla pid aynı kovaya düşerse arama yavaşlayabilir en kötü durumda O(n) haline gelebilir.
    - Ölçeklenebilirlik: Tablo boyutunu başlangıçta iyi seçmek gerekir. Aksi takdirde performans düşebilir.
    - pid numaralarına göre sıralı gezinme: Tüm pid’leri sırayla dolaşmak yavaştır. Çünkü hash tablosunda doğal bir sıralama 
    yoktur.

    XArray/Radix Ağacı ile PID Arama

    - Arama karmaşıklığı: O(logₖ(n)), burada k = dallanma sayısıdır (tipik olarak Linux'ta 64). pid aramasında bit gruplarıyla 
    seviye seviye aşağıya inilir. Tipik derinlik çok küçüktür (ör. 32-bit PID için ortalama 2–3 seviye).
    - Bellek kullanımı: Yalnızca kullanılan pid aralıkları için düğümler açılır (sparse allocation). pid alanı seyrek 
    kullanılıyorsa çok verimlidir.
    - Çakışma sorunu yoktur: Her pid tek bir yolu takip eder, hash gibi aynı kovaya düşme ihtimali yok.
    - Ölçeklenebilirlik: pid alanı büyüse bile ağaç yapısı uyum sağlar; yeniden boyutlandırma gerekmez.
    - XArray/Radix Ağacı doğası gereği indeks sırasını korur. Tüm pid'leri sıralı biçimde dolaşmak çok hızlıdır.

    Özet olarak da şunları söyleyebiliriz:

    - Tekil arama için hash tablosu genellikle biraz daha hızlıdır.
    - Büyük ve seyrek pid isim alanlarında XArray çok daha verimli bellek kullanır.
    - Sıralı gezinme gerektiğinde XArray üstün gelir, hash tablosunda bu durum mümkün değildir.
    - Çakışmasız, deterministik performans açısından XArray daha öngörülebilirdir, hash tablosunda ise çakışma durumunda 
    kötüleşmeler yaşanabilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz çekirdeğin 2.6 versiyonlarıyla birlikte pid'ler için "pid isim alanlarının" da oluşturulduğunu söylemiştik. 
    İşte çekirdeğin 4.20 versiyonlarıyla hash tablosu bırakılıp XArray ağacının kullanılmaya başlanmasıyla birlikte her 
    pid isim alanı için ayrı bir XArray ağacı oluşturulmaya başlanmıştır. Anımsanacağı gibi 2.6'lı versiyonlarda hash 
    tablolarına geçildiğinde tüm pid isim alanları aynı hash tablosunda bulunuyordu. Halbuki güncel çekirdeklerde bir pid 
    nesnesi aranacaksa o nesne belli bir pid isim alanındaki XArray ağacında aranmaktadır. 
 
    Güncel çekirdeklerde pid araması yapan yüksek seviyeli fonksiyonlardan biri find_vpid fonksonudur. Fonkiyon şöyle
    tanımlanmıştır:

    struct pid *find_vpid(int nr)
    {
        return find_pid_ns(nr, task_active_pid_ns(current));
    }
    EXPORT_SYMBOL_GPL(find_vpid);

    Bu fonksiyon belli bir pid değerine ilişkin pid nesnesini çağrıyı yapan prosesin bulunduğu pid isim alanı içerisindeki 
    XArray ağacında aramaktadır. Pekiyi bu fonksiyon o anda çalışmakta olan prosesin pid isim alanını nasıl bulmaktadır? 
    2.6'lı versiyonlarda prosesin ilişkin olduğu pid isim alanının bilgilerine task_struct yapısının nsproxy elemanının 
    gösterdiği nsproxy yapısının pid_ns elemanı yoluyla erişiliyordu. Ancak güncel versiyonlarda artık prosesin ilişkin olduğu
    pid isim alanının bilgilerine pid nesnesi yoluyla erişilmektedir. 4.20 versiyonlarından itibaren task_struct pid değerinin 
    artık doğurdan task_struct yapısının thread_pid elemanında da tutulduğunu önceki paragraflarda belirtmiştik:

    struct task_struct {
        /* ... */

        pid_t pid;
        struct pid *thread_pid;     

        /* ... */
    };

    İşte güncel versiyonlarda ilgili task_struct pid değerine ilişkin pid nesnesine bu thread_pid elemanı ile erişilmektedir. 
    Güncel versiyonlardaki pid yapısı şöyledir: 

    struct pid {
        refcount_t count;
        unsigned int level;
        spinlock_t lock;
        struct {
            u64 ino;
            struct rb_node pidfs_node;
            struct dentry *stashed;
            struct pidfs_attr *attr;
        };
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct hlist_head inodes;
        /* wait queue for pidfd notifications */
        wait_queue_head_t wait_pidfd;
        struct rcu_head rcu;
        struct upid numbers[];
    };

    Çekirdeğin 4.20 versiyonundan itibaren prosesin ilişkin olduğu pid isim alanına pid yapısı içerisindeki numbers dizisinin 
    ilgili elemanından erişilmektedir. numbers dizisinin elemanlarının upid isimli yapı türünden olduğuna dikkat ediniz. upid 
    yapısı da şöyle bildirilmiştir:

    struct upid {
        int nr;
        struct pid_namespace *ns;
    };  

    Şimdi find_vpid fonksiyonuna geri dönelim:

    struct pid *find_vpid(int nr)
    {
        return find_pid_ns(nr, task_active_pid_ns(current));
    }

    Bu fonksiyon önce task_active_pid_ns fonksiyonu ile yukarıda belirttiğimiz biçimde prosesin içerisinde bulunduğu pid
    isim alanınının bilgilerini aşağıdaki çağrı zinciriyle elde etmektedir:

    task_active_pid_ns ---> ns_of_pid --> task_pid 

    Çağrı zincirindeki fonksiyonların tanımlamalarını veriyoruz:

    struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)
    {
        return ns_of_pid(task_pid(tsk));
    }
    EXPORT_SYMBOL_GPL(task_active_pid_ns);

    static inline struct pid *task_pid(struct task_struct *task)
    {
        return task->thread_pid;
    }

    static inline struct pid_namespace *ns_of_pid(struct pid *pid)
    {
        struct pid_namespace *ns = NULL;
        if (pid)
            ns = pid->numbers[pid->level].ns;
        return ns;
    }

    İşte find_vpid fonksiyonu prosesin içinde bulunduğu pid isim alanının bilgilerini elde ettikten sonra daha genel olan, 
    belli bir pid isim alanındaki XArray ağacında arama yapan find_pid_ns fonksiyonunu çağırmaktadır:

    struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
    {
        return idr_find(&ns->idr, nr);
    }
    EXPORT_SYMBOL_GPL(find_pid_ns);

    Ağaçta arama yapan asıl fonksiyon idr_find isimli fonksiyondur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    XArray ağaçları üzerinde ekleme, silme arama gibi işlemler için aşağı seviyeli fonksiyonlar bulundurulmuştur. XArray 
    veri yapısının aşağı seviyeli gerçekleştirimi "include/linux/xarray.h" dosyası içerisinde yapılmıştır. XArray ağacı 
    bu dosyadaki xarray isimli yapıyla temsil edilmiştir:

    struct xarray {
        spinlock_t	xa_lock;
        /* private: The rest of the data structure is not to be used directly. */
        gfp_t		xa_flags;
        void __rcu *	xa_head;
    };

    Bu ağaç üzerinde işlem yapan xa_ öneki ile başlayan bir grup fonksiyon vardır:

    void *xa_load(struct xarray *, unsigned long index);
    void *xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);
    void *xa_erase(struct xarray *, unsigned long index);
    void *xa_store_range(struct xarray *, unsigned long first, unsigned long last, void *entry, gfp_t);
    bool xa_get_mark(struct xarray *, unsigned long index, xa_mark_t);
    void xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);
    void xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);
    void *xa_find(struct xarray *xa, unsigned long *index, unsigned long max, xa_mark_t) __attribute__((nonnull(2)));
    void *xa_find_after(struct xarray *xa, unsigned long *index, unsigned long max, xa_mark_t) __attribute__((nonnull(2)));
    unsigned int xa_extract(struct xarray *, void **dst, unsigned long start, unsigned long max, unsigned int n, xa_mark_t);
    void xa_destroy(struct xarray *);

    Bu fonksiyonların tanımlamaları "lib/xarray.c" dosyasında yapılmıştır.

    Güncel çekirdeklerin yukarıda belirttiğimiz aşağı seviyeli fonksiyonlarının yanı sıra aynı zamanda bunları kullanan 
    yüksek seviyeli IDR fonksiyonları da bulunmaktadır. Çekirdek kodları incelendiğinde genellikle bu yüksek seviyeli idr_ 
    önekli fonksiyonların kullanıldığı görülmektedir. Yukarıda da belirttiğimiz gibi bu fonksiyonlar xa_ önekli aşağı 
    seviyeli XArray fonksiyonlarını çağırmaktadır:

    +-------------------------------------------------------+
    |          Cekirdek Kodları ve Aygıt Sürücüler          |
    |   (sürücüler, alt sistemler, idr_* çağrıları yapan)   |
    +-------------------------------------------------------+
                            │
                            ▼
    +-------------------------------------------------------+
    |                  IDR Katmanı (idr_*)                  |
    |   Basit integer→pointer eşlemeleri için sarma fonk.   |
    |                                                       |
    |   Örnekler:                                           |
    |     idr_alloc()   → çağırır xa_alloc()                |
    |     idr_find()    → çağırır xa_load()                 |
    |     idr_remove()  → çağırır xa_erase()                |
    |     idr_replace() → çağırır xa_store()                |
    |     idr_for_each  → çağırır xa_for_each()             |
    +-------------------------------------------------------+
                            │
                            ▼
    +-------------------------------------------------------+
    |                XArray Katmanı (xa_*)                  |
    |   Gerçek veri yapısı: radix ağaç tabanlı,             |
    |   concurrency-aware, lock/RCU destekli.               |
    |                                                       |
    |   Fonksiyonlar: xa_alloc, xa_load, xa_store, xa_erase |
    |                 xa_find, xa_for_each, vb.             |
    +-------------------------------------------------------+
                            │
                            ▼
    +-------------------------------------------------------+
    |         Çekirdek Alt Veri Yapısı / Radix Tree         |
    |   (XArray'in düşük seviye radix implementasyonu)      |
    +-------------------------------------------------------+

    XArray gerçekleştirimi hakkında ileride başka konularda ek bilgiler vereceğiz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz son derslerde task_struct nesnelerinin birbirleriyle bağlantısını ve pid değerleri ile task_struct nesneleri 
    arasındaki ilişkileri gördük. Şimdi dikkatimizi bir süre dosya sistemine ilişkin çekirdek veri yapıları üzerine 
    yönelteceğiz. Bilindiği UNIX/Linux sistemlerinde pek çok kavram kullanıcıya bir dosya gibi gösterilmektedir. Biz bu 
    bölümde belli bir derinliğe kadar çekirdeğin dosya işlemleri için oluşturduğu organizasyon üzerinde duracağız. Daha 
    sonra başka bir bölümde dosya sistemine ilişkin aşağı seviyeli ayrıntıları ele alacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinin "dosya sistemi (file system)" denilen alt sistemlerinin iki tarafı vardır:

    1) Disk tarafı
    2) Bellek Tarafı

    Dosya bilgileri disk üzerindeki bloklarda tutulmaktadır. (Bu bloklara Microsoft dünyasında "cluster" da denilmektedir.) 
    Hangi dosyaların diskin hangi bloklarında tutulduğu, dosyaların meta-data bilgilerinin diskte nasıl saklandığı gibi 
    belirlemeler dosya sisteminin disk tarafını, diskteki dosya sisteminin çekirdekteki temsilinin oluşturulması ve 
    işletim sisteminin açılan dosyalar için yaptığı düzenlemeler ise dosya sisteminin bellek tarafını oluşturmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Disk terimi genel bir terimdir. Bir süre önceye kadar disk olarak ağırlıklı biçimde "hard disk" denilen elektromekanik 
    birimler kullanılıyordu. Ancak bir süredir artık disk olarak yarı iletken teknolojiler kullanılarak oluşturulmuş SSD 
    (Solid State Disk) denilen diskler kullanılmaktadır. Bugün ağırlıklı olarak kullandığımız SSD disklerin herhangi 
    bir mekanik parçası yoktur. SSD'ler "NAND Flash" denilen bellek teknolojisini kullanmaktadır. SSD'ler hard disklere 
    göre oldukça hızlıdır. Ancak onların en önemli handikapları belli bir yazma ömrünün olmasıdır. SSD'lerde aynı bölgeye 
    belli sayıdan daha fazla yazma yapıldığında artık SSD'nin o bölgesi bozulabilmektedir. Tabii teknoloji bu bakımdan 
    da ilerleme içerisindedir. SSD teknolojisi ile USB yuvalarına taktığımız flash belleklerin teknolojisi birbirine 
    benzemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kullandığımız disk birimi ister "hard disk" olsun isterse SSD olsun disk ile bilgisayarımızın RAM'i arasındaki 
    transferler sektör (sector) denilen byte grupları ile yapılabilmektedir. Sektör bir diskten okunabilecek ya da 
    bir diske yazılabilecek en küçük birimdir. Bir sektör genellikle 512 byte'tır. Diskte byte düzeyinde erişim yoktur. 
    Sektörel erişim vardır. Örneğin diskteki bir sektörde bulunan bir byte üzerinde değişiklik ancak şöyle yapılabilmektedir:
    Önce o byte'ın içinde bulunduğu sektör RAM'e okunur. Sonra byte RAM üzerinde değiştirilir. Sonra aynı sektör yeniden 
    diske yazılır.

    Diskteki her sektörün ilk sektör 0 olmak üzere bir mantıksal numarası vardır. Hard disklerde ardışıl numaralı mantıksal 
    sektörler disk üzerinde de fiziksel olarak peşi sıra bulunmaktadır. Mekanik hard disklerde bilgiler "track" denilen 
    yollara yazılmaktadır. Ardışıl sektörler aynı track'te bulunurlar. Dolayısıyla hard disklerde diskin kafası bir 
    kez konumlandırıldığında ardışıl sektörlere daha hızlı okuma yazma yapılabilmektedir. SSD'ler mekanik öğe barındırmadığı
    için rastgele erişimlidir. Yani her sektörden okuma eşit hızda yapılma ve her sektöre yazma eşit hızda yapılmaktadır.

    Modern bilgisayar sistemlerinde disk birimine doğrudan erişilmez. Disk erişimlerinde bu işleme aracılık eden ismine 
    "disk denetleyicisi (disk controller)" denen yerel bir işlemcinden faydalanılmaktadır. Yani sistem programcısı ya da 
    işletim sistemlerini yazanlar disk denetleyicisini programlar, disk denetleyicisi isteği elektriksel olarak disk 
    birimine iletir, okuma yazma işlemleri de disk birimi tarafından yapılır:

    ┌─────────────────┐
    │ İşletim Sistemi │
    └───────┬─────────┘
            │
            ▼
    ┌───────────────┐
    │   CPU / RAM   │
    └───────┬───────┘
            │
            ▼
    ┌─────────────────┐
    │ Disk Denetleyici│
    │  (Controller)   │
    └───────┬─────────┘
            │
            ▼
    ┌───────────────┐
    │     Disk      │
    │ (HDD / SSD)   │
    └───────────────┘

    Bugün PC'lerde SATA ve NVMe denetleyicileri en çok kullanılan disk denetleyicileridir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										22. Ders 28/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi disk denetleyicisi işletim sistemi tarafından "falanca sektörleri oku" ya da "falanca sektörlere yaz" biçiminde 
    programlandıktan sonra aktarım nasıl yapılmaktadır? Aktarım CPU tarafından tek tek byte'ların denetleyiciden alınarak 
    RAM'e yerleştirilmesi yoluyla yapılmamaktadır. (Çok eskiden ilk PC mimarisinde aktarım böyle de yapılabiliyordu.) Çünkü 
    CPU'nun bu işle meşgul olması önemli bir zaman kaybı oluşturmaktadır. Bu tür disk ile RAM arasındaki aktarımlar için 
    "DMA (Direct Memory Access)" denilen yardımcı denetleyiciler kullanılmaktadır. Tipik olarak CPU'da çalışan kod (yani 
    işletim sistemi) disk denetleyicisine transfer isteğini ve aktarımda kullanılacak bellek alanlarının aresini bildirir. 
    disk denetleyicisi de disk birimini ve DMA'yı elektirksel düzeyde programlayarak aktarımın DMA üzerinden doğrudan RAM'e 
    yapılmasını sağlar. Aktarım sırasında artık CPU bu işle meşgul olmaz, işletim sistemi de CPU'yu başka bir thread'i 
    çalıştırması için bağlamsal geçişe (task switch) sokar. Tabii aktarım işlemi bittiğinde disk denetleyicisi CPU'yu bir 
    donanım kesmesi yoluyla durumdan haberdar etmektedir. Yani disk ile RAM arasındaki aktarım işlemleri tipik olarak şöyle 
    yapılmaktadır:

    1) İşletim sistemi disk denetleyicisine aktarılacak sektörlere ilişkin bilgileri ve transfer adreslerini CPU yoluyla 
    elektriksel olarak iletir.
    2) Okuma söz konusuysa disk denetleyicisi disk birimine elektriksel düzeyde komutlar göndererek sektörlerin okunmasını 
    ve DMA yoluyla bunların RAM'de uygun yerlere aktarılmasını sağlar. Eğer yazma söz konusuysa RAM'de belirtilen adresteki 
    bilgiler yine DMA yoluyla disk birimine iletilerek yazma gerçekleştirilir.
    3) Aktarım işlemi bittiğinde disk denetleyicisi bir donanım kesmesi yoluyla CPU'yu durumdan haberdar eder.
    4) CPU aktarım için gereken kodları çalıştırdıktan sonra aktarım bitene kadar meşgul bir döngüde beklemez.
    Başka thread'ler çalıştırılabiliyorsa "bağlamsal geçiş (context switch)" yapılarak CPU'nun boşta kalması engellenir.

    Yukarıdaki süreci bir diyagramla aşağıdaki gibi özetleyebiliriz:

     ┌──────────────┐        ┌──────────────────┐        ┌─────────────────────┐
     │   Uygulama   │        │  İşletim Sistemi │        │ Blok Aygıt Sürücüsü │
     └──────┬───────┘        └─────────┬────────┘        └────────┬────────────┘
            │                          │                          │
            │   read()/write() isteği  │                          │
            └────────────────────────► │                          │
                                       │   I/O isteği hazırla     │
                                       └────────────────────────► │
                                                                  │
                                                                  │ Komutu
                                                                  │ denetleyiciye
                                                                  │ gönder
                                                                  ▼
                                                        ┌────────────────────┐
                                                        │ Disk Denetleyicisi │
                                                        └───┬───────────┬────┘
                                                            │           │
                                        DMA tabloları ◄───┘             │
                                                            │           │
                                                Kesme (IRQ) ◄───────────┘
                                                            │
                                            ┌───────────────┴────────────────┐
                                            │         Disk Donanımı          │
                                            │ (HDD: kafa hareketi, SSD: FTL) │
                                            └────────────────────────────────┘

    Kullandığımız masaüstü bilgisayarlarda zaman içerisinde DMA denetleyicileri de geliştirilmiştir. Eskiden Intel 
    tabanlı PC mimarisinde ISA bus kullanıldığı zamanlarda tek bir merkezi DMA denetleyicisi (Intel 8237) vardı. 
    Ancak daha sonra Intel tabanlı PC mimarisinde PCI bus kullanılmaya başlanmasıyla birlikte artık transfer 
    yapabilen her donanım birimi kendi DMA denetleyicisini de içermeye başladı. Bugün Intel tabanlı ve Apple Silicon 
    tabanlı bilgisayar mimarilerinde disk denetleyicisi kendi içerisindeki DMA denetleyicisini programlayarak transferi 
    gerçekleştirmektedir. Disk denetleyicilerinin programlanması ise artık uzunca bir süredir "bellekten tabanlı IO 
    (memory-mapped IO)" tekniği ile yapılmaktadır.

    Hard disklerde disk birimi içerisinde bir önbellek (cache) de bulundurulmaktadır. Böylece disk denetleyicisi aynı 
    sektörleri disk biriminden istediği zaman disk birimi eğer daha önce ilgili sektörler önbellek içerisindeyse kafa 
    hareketleri yapmadan onları doğrudan kendi önbelleğinden vermektedir. Bugün örneğin 1 TB'lık hard disklerde 64MB, 
    128, 256 MB civarında önbellek kullanılmaktadır. SSD'ler de bir önbellek sistemi vardır. Bu önbellek sistemi 
    özellikle yazma işlemlerinde hız kazancı sağlamakta ve aynı sektörlere sürekli yazım yapıldığında o bölgenin 
    yıpranmamasını (wear leveling) sağlamaktadır. Tabii bu ö önbellek stemleri tamamen disk birimleri tarafından içsel 
    olarak işletilmektedir. Bu önbellek sistemleri işletim sistemleri tarafından erişilebilir değildir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi bir disk biriminde transfer edilecek en küçük birime sektör denilmektedir. Bir sektör 
    tipik olarak 512 byte uzunluğundadır. Ancak aslında sektör uzunlukları da disk üreticilerine bağlı olarak değişebilir 
    bir niceliktir. 512 byte sektör uzunlukları bugün için standart bir uzunluktur. Tabii zaman geçtikçe diskler büyüdüğü 
    için sektör uzunluklarının da büyüyebileceğini söylemek istiyoruz. Nitekim 4K uzunluğunda sektörlere sahip olan diskler 
    özellikle büyük sistemlerde gittikçe yaygınlaşmaktadır. Disk birimi her sektöre ilk sektör 0 olmak üzere mantıksal 
    bir numara vermektedir. Yani adeta disk üzeründeki her sektörün bir adresi vardır. Disk denetleyicisi disk birimine 
    transfer edilecek sektörlerin numaralarını elektriksel düzyde iletmektedir. (Bu biçimde mantıksal sektör numaraları 
    kullanılmadan önce 80'lerde ve 90'ların ilk yarısında sektörlerin yerleri "fiziksel koordinat sistemi" denilen "hangi 
    yüz (head)", "hangi track", "hangi sektör dilimi" biçiminde üç parametreyle belirtiliyordu.)

    Sektör kavramı aslında dosya sistemleri için küçük bir depolama birimidir. İşletim sistemleri bir dosyanın parçası 
    olabilecek minimum disk alanı için sektör yerine "blok (block)" ya da "cluster" denilen daha büyük birimleri kullanmaktadır. 
    Blok terimi daha çok UNIX/Linux sistemlerinde kullanılmaktadır. Microsoft ise blok yerine "cluster" terimini kullanmaktadır. 
    Bir blok ardışıl n tane sektörden oluşmaktadır. Uygulamada bu n değeri 2'nin bir kuvveti olur. Ardışıllık hard disklerde 
    önemli bir unsurdur. Çünkü hard disklerde en önemli zaman kaybı mekanik bir birim olan disk kafasının track hizasına 
    çekilmesinde yaşanmaktadır. Disk kafası track hizasına çekildiğinde disk dönerken artık ardışıl sektörler hiç kafa 
    hareketi yapılmadan okunup yazılabilmektedir. Pekiyi neden işletim sistemi dosyalar söz konusu olduğunda bir dosyanın 
    parçası olabilecek en küçük birim için sektör değil de ardışıl n tane sektör kullanmaktadır? İşte bunun birkaç nedeni 
    vardır:

    1) Dosyaların parçaları disk üzerinde ardışıl yerlerde olmak zorunda değildir. Dosyaların parçaları üzerinde yayılmış 
    bir biçimde bulunabilmektedir. Eğer dosyalar çok fazla parçadan oluşursa hard disklerde (ve kısmen de olsa SSD'lerde 
    de) bu parçalar disk üzerinde daha fazla yayılmış olur, bunlara erişmek için gereken zaman artar.

    2) Eğer dosyanın parçaları sektör gibi küçük birimlerden oluşsaydı bu parçaların diskteki yerlerine ilişkin "meta 
    data" tabloları büyürdü. Bu da hem disk alanını hem de işletim sisteminin bellekte yaptığı düzenlemede alan verimsizliği 
    oluştururdu.

    3) CPU'ların kullandığı sayfalama mekanizmasında genellikle 4K uzunluklar kullanılmaktadır. Dosya parçalarının 4K 
    uzunluğun katlarında olması dosya sistemi ile sayfalama sistemi arasında daha iyi bir uyumun ortaya çıkmasına yol 
    açmaktadır. Bu uyum da dolaylı biçimde performans artışı sağlamaktadır.

    Pekiyi bu durumda işletim sistemleri blok denilen dosyanın parçası olabilecek en küçük birim için hangi uzunluğu 
    kullanmaktadır? İşte genellikle bu karar disk formatlanırken diskin (disk bölümünün) büyüklüğüne bakılarak verilmektedir. 
    Dosyaların son bloklarında kalan kullanılmayan alanların oluşmasına "içsel bölünme (internal fragmentation)" 
    denilmektedir. Küçük disklerde (disk bölümlerinde) içsel bölünmenin etkisi daha büyük olacağından blokların 1K gibi 
    küçük uzunluklarda alınması uygun olabilir. Ancak orta büyüklükte disklerde içsel bölünmenin etkisi göreli olarak 
    azalacağı için bloklar 4k gibi bir değerde seçilebilmektedir. Büyük disklerde ise 8K, 16K blok büyüklükleri tercih 
    edilmektedir. Aslında blok büyüklükleri ilgili disk bölümü formatlanırken (Linux sistemlerinde mkfs.xxx programlarıyla 
    formatlama yapılmaktadır) belirlenmektedir. Yani kullanıcı isterse kendisi bu programda kendi tercih ettiği blok 
    uzunluğunu kullanabilir. Ancak kullanıcılar genellikle böyle bir belirleme yapmazlar. Bu durumda bu programlar disk 
    bölümünün büyüklüğüne bağlı olarak yukarıda açıkladığımız gibi uygun bir blok büyüklüğünü seçerler.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    UNIX/Linux sistemlerinde dosya sistemi için tek bir kök vardır. Blok aygıtları (örneğin hard diskler, flash bellekler 
    vs) belli bir dizine mount edilmektedir. Mount işlemi bir dizin üzerine uygulanır. Mount işlemi sonucunda o dizinin 
    içeriği görünmez, artık mount edilen dosya sisteminin kök dizini mount dizininde gözükür. Dolayısıyla bu sistemlerde 
    aslında farklı dizinler farklı blok büyüklüklerine ilişkin dosya sistemlerinin içerisinde olabilmektedir. Anımsanacağı 
    gibi stat POSIX fonksiyonu ya da komut satırından uygulanan "stat" komutu belli bir dosyanın bilgilerini verirken 
    o dosyanın içinde bulunduğu dosya sisteminin blok uzunluğunu da vermektedir. Linux sistemlerinde bu bilgi doğrudan 
    dosya sistemine ilişkin blok aygıtı üzerinde dumpe2fs programıyla da elde edilebilmektedir. Windows sistemlerinde 
    de "cluster" adı altında blok sistemi kullanılmaktadır. O sistemlerde blok uzunluklarını "chkdsk" programı ile ya 
    da "fsutil" programı ile komut satırından elde edebilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri işlemlerini kolaylaştırmak için her bloğa bir numara da vermektedir. Örneğin bir bloğun 4K (tipik 
    olarak 8 sektör) olduğunu düşünelim. Artık işletim sistemi için ilgili disk (aslında disk bölümü ya da genel olarak 
    blok aygıtı da diyebiliriz) bloklardan oluşmaktadır. Örneğin diskin (disk bölümünün) ilk 8 sektörü artık 0'ıncı bloktur. 
    Sonraki 8 sektöre 1'inci bloktur. İşletim sistemi içsel olarak artık ilgili diski bloklardan oluşan ve her bloğun bir 
    numarasının olduğu mantıksal bir depolama alanı gibi ele almaktadır. Yani işletim sistemi için yalnızca sektörlerin 
    değil aynı zamanda dosya sistemine ilişkin blokların da numaraları vardır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri son okunan ya da yazılan disk bloklarını RAM'de bir önbellek sisteminde saklamaktadır. Bu önbellek 
    (cache) sisteminde genel olarak "işletim sisteminin disk önbellek sistemi" denilmektedir. Linux dünyasında eskiden bu 
    önbellek sistemine "buffer cache" deniliyordu. Sonra bu önbellek sistemi iyileştirildi ismi de "page cache" olarak 
    değiştirildi. İşletim sistemlerinin bu disk önbellek sistemleri disk erişimini ciddi boyutta azaltmakta ve sistem 
    performansı üzerinde en önemli olumlu etkilerden birini oluşturmaktadır. Eğer işletim sistemlerinde böyle bir disk 
    önbellek sistemi olmasaydı sistemler çok yavaş çalışırdı.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz UNIX/Linux sistemlerinde bir dosyadan okuma yapmak için ya da bir dosyaya yazma yapmak için read/write POSIX 
    fonksiyonlarını kullanmış olalım. Bu fonksiyonlar çekirdek içerisindeki sys_read ve sys_write sistem fonksiyonlarını 
    çalıştırmaktadır. İşletim sistemi bellek tarafında yaptığı organizasyonla okunacak ya da yazılacak bilginin ilgili 
    blok aygıtının (disk bölümünün) kaç numaralı bloğuna ve sektörüne ilişkin olduğunu belirleyebilmektedir. Ancak işletim 
    sisteminin sys_read ve sys_write gibi sistem fonksiyonları hemen diske yönelmez. Bu fonksiyonlar önce dosyanın ilgili 
    bölümünün RAM'de oluşturulmuş bir önbellek sistemi içerisinde olup olmadığına bakmaktadır. Eğer ilgili bölüm bu önbellek 
    sisteminin içerisinde varsa bu fonksiyonlar diske hiç erişmeden dolayısıyla da hiç bloke olmadan bu okuma yazma işlemini 
    gerçekleştirmektedir. Eğer dosyadan okunacak ya da dosyaya yazılacak kısım RAM'de oluşturulmuş olan bu önbellek sisteminde 
    yoksa bu durumda gerçek disk okuması ya da yazması izleyen paragrafta açıklayacağımız biçimde yürütülmektedir. Linux 
    sistemlerinde bu önbellek sistemine eskiden "buffer cache" dendiğini ancak sonraları isminin "page cache" olarak 
    değiştirildiğini belirtmiştik. Biz artık Linux temelli anlatımlarımızda bu önbellek sistemine "sayfa önbelleği 
    diyeceğiz. Çizimlerde doğrudan İngilizce "page cache" ismini de kullanacağız.

    read/write POSIX fonksiyonları çağrıldığında yapılan işlemleri aşağıdaki şekille özetleyebiliriz:

    ┌─────────────────────────────────────┐
    │    read/write POSIX Fonksiyonları   │
    └──────────────┬──────────────────────┘
                   │
                   ▼
    ┌──────────────────────────────┐
    │    sys_read / sys_write      │
    └──────────────┬───────────────┘
                   │
                   ▼
    ┌──────────────────────────────┐
    │   Page Cache'te bilgi var?   │
    └───────┬─────────────┬────────┘
            │             │
        Evet           Hayır
            │             │
            ▼             ▼
    ┌──────────────┐  ┌─────────────────────────┐
    │  Veriyi      │  │ Gerçek disk I/O başlat  │
    │  kopyala     │  │  (okuma / yazma)        │
    └──────────────┘  └───────────┬─────────────┘
                                  │
                                  ▼
                        ┌────────────────────────┐
                        │  Page Cache güncellenir│
                        └────────────────────────┘
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi dosyadaki okunacak ya da yazılacak kısım sayfa önbelleğinde (page caceh) yoksa gerçek transfer nasıl yapılmaktadır? 
    İşte işletim sistemlerinde bu tür transferler aslında başka bir birime devredilmektedir. Linux sistemlerinde bu 
    transferlerin yapıldığı birime "blok aygıt sürücüleri (block device drivers)" denilmektedir. Bir Linux sistemi 
    kurulduğunda zaten temel disk denetleyicileri üzerinden transfer yapabilen blok aygıt sürücüleri çekirdeğin içerisine 
    gömülmüş durumda olur. Ancak sistem programcısının kendisi de blok aygıt sürücüleri yazabilir. Örneğin bir 
    gömülü Linux sisteminde yeni bir SD kart birimi için bir blok aygıt sürücüsü yazmak zorunda kalabilirsiniz. Blok 
    aygıt sürücülerinin yazımı aygıt sürücülerinin yazılmasına ilişkin konuların bir bölümünü oluşturmaktadır.

    Aslında işletim sistemi sayfa öneblleğinde (page cache) olmayan kısımların transfer edilmesini blok aygıt sürücülerinden 
    istemektedir. Disklere ilişkin (bunlara genel olarak blok aygıtları denilmektedir) birtakım okuma yazma işlemleri 
    aslında işletim sistemleri tarafından çizelgelemeye sokulmaktadır. Çünkü çok sayıda farklı proses aynı disk sektörlerini 
    okuyacak ya da o sektörlere yazacak olabilir. İşletim sistemi bu nedenle hemen IO isteğini blok aygıt sürücüsüne 
    göndermez. Önce onları sıraya dizer, mümkünse birleştirir, bu biçimdeki iyileştirme işleminden sonra istekleri blok 
    aygıt sürücüsüne gönderir. Bu sürece işletim sistemlerinin "IO çizelgelemesi (IO scheduling)" denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                    23. Ders 04/10/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    O halde bir dosya okuması ya da yazması sonucunda gelişen olayları şöyle özetleyebiliriz:

    1) Kullanıcı modunda çalışan program (yani proses) read ya da write POSIX fonksiyonlarını çağırır. (UNIX/Linux 
    sistemlerindeki C derleyicilerinin standart dosya fonksiyonları da eğer okunacak ya da yazılacak kısım kendi tamponlarında 
    yoksa zaten bu POSIX fonksiyonlarını çağırmaktadır.)
    
    2) read ve write POSIX fonksiyonları Linux'ta sys_read ve sys_write isimli sistem fonksiyonlarını çağırır. Artık 
    akış kullanıcı modundan (user mode) çekirdek moduna (kernel mode) geçmiştir. Çekirdekteki kodlar çalışmaktadır.

    3) sys_read ve sys_write fonksiyonları önce okunacak ya da yazılacak yerin Linux'un' disk önbellek sistemi olan "sayfa 
    önbelleğinde ("page cache", eski ismiyle "buffer cache") sisteminde olup olmadığına bakar. Eğer ilgili disk blokları 
    sayfa önbelleğinde varsa akış hiç bloke olmadan sayfa önbelleği içerisinden karşılanır. Ancak eğer söz konusu bloklar 
    sayfa önbelleğinde yoksa bu durumda Linux çekirdeği isteği "IO çizelgeleyicisi (IO scheduler)" denilen çekirdek birimine 
    iletir. read/write çağrısını yapan thread bloke edilir.

    4) IO çizelgeleyicisi istekleri çizelgeler. Okuma işlemi söz konusuyse sayfa önbelleğinde transfer edilecek önbellek 
    bloklarını tahsis eder. Yazma işlemi söz konusuysa diske transfer edilecek önbellek bloklarını belirler. (Bu konuda 
    bazı ayrıntılar da vardır.) Blok aygıt sürücüsüne transfer edilecek sektörleri iletir. 

    5) Gerçek transfer işlemi blok aygıt sürücüsü tarafından yapılmaktadır. İşletim sistemi blok aygıt sürücüsüne "hangi 
    sektörlerin sayfa önbelleğindeki hangi adreslere transfer edileceğini ya da sayfa önbelleği içerisindeki hangi adresteki
    bilgilerin diskteki hangi sektörlere transfer edileceğini" bir kuyruk sistemi yardımıyla iletmektedir. 
    
    6) Blok aygıt sürücüsü diskten istenen sektörleri sayfa önbelleği içerisinde belirtilen adrese ya da sayfa önbelleğinde
    belirtilen adresteki bilgileri diskin belirtilen sektörlerine transfer eder. 

    7) Artık okuma söz konusuysa okunan bilgi sayfa önbelleği içerisindedir. İşlemi başlatan thread'in blokesi çözülür. 
    sys_read sistem fonksiyonu bunu sayfa önbelleği içerisinden programcının kullanıcı modundaki adresine kopyalar.

    Tabii bugün kullandığımız Linux sistemlerinde aslında transfleri yapan blok aygıt sürücüleri zaten çekirdek imajı 
    içerisine gömülmüş bir biçimde bulunmaktadır. Ancak nadiren de olsa sistem programcısının yeni birtakım aygıtlar için 
    blok aygıt sürücüleri yazması gerekebilmektedir. 

    Linux sistemlerinde yukarıda özetlediğimiz olaylar silsilesi zaman içerisinde değişikliklere uğratılarak ve sürekli
    geliştirilerek bugünkü durumuna getirilmiştir. 

    Yukarıda maddeler halinde açıkladığımız süreci bir şekille de özetleyebiliriz:

    ┌───────────────────────────────────────────────────┐
    │ Kullanıcı Modu (User Mode)                        │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 1) Program read() / write() çağırır               │
    │ (veya stdio tamponları dolduysa/boşaldıysa)       │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 2) POSIX fonksiyonları sys_read / sys_write       │
    │    sistem çağrılarını çağırır                     │
    │    → Geçiş: User Mode ➜ Kernel Mode              │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 3) sys_read / sys_write, Page Cache'i             │
    │    (disk cache) kontrol eder:                     │
    │  - Veri Page Cache'te varsa:                      │
    │    → Bloke olmaz, doğrudan bellekten okunur veya  │
    │      yazılır                                      │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 4) Bilgi Page Cache'te yoksa:                     │
    │    → İstek I/O çizelgeleyicisine gönderilir,      │
    │     thread bloke edilir                           │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 5) I/O Çizelgeleyicisi isteği çizelgeler          │
    │    - Page Cache içinde yer ayırır                 │
    │    - Blok aygıt sürücüsüne iletir                 │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 6) Blok aygıt sürücüsü                            │
    │    - Hangi sektörlerin okunacağını/               │
    │      yazılacağını bilir                           │
    │    - Page Cache adresleriyle eşleştirir           │
    │    - İsteği donanıma iletir                       │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 7) Gerçek transfer donanım tarafından yapılır     │
    │ (ör. disk → page cache veya tersi)                │
    │ - Tamamlanınca çekirdeğe kesme gönderir           │
    │ - Bilgi artık page cache içindedir                │
    │ - İşlemi başlatan thread'in blokesi çözülür       │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 8) Kernel Mode → User Mode dönüşü                 │
    │ - Okuma tamamlandıysa veri kullanıcıya kopyalanır │
    │ - Yazma tamamlandıysa geri dönüş yapılır          │
    └───────────────────────────────────────────────────┘
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Buradaki süreçte yazma olayı söz konusu olduğunda bazı ayrıntılar da devreye girmektedir. Kullanıcı modundaki 
    program write POSIX fonksiyonunu çağırıp bu fonksiyon da sys_write fonksiyonunu çağırdığında bu sistem fonksiyonu 
    yazılmak istenen bilgiler diske yazılana kadar write işlemini yapan thread'i bloke etmez. Yazma işlemi her zaman 
    Linux'un RAM'deki sayfa önbelleğine yapılmaktadır. sys_write fonksiyonu yazmayı sayfa önbelleği içerisine yaptıktan 
    sonra hemen "başarılı" olarak geri dönmektedir. Yani gerçek transferi beklememektedir. Bu bakımdan write işlemi
    aslında seknron yapılmamaktadır. Sistem programlama terminolojisinde IO işlemlerinde "senkron" terimi "fonksiyon geri 
    döndüğünde tüm işlemlerin yapılıp bitmiş olması" anlamına gelmektedir. Asenkron terimi ise "işlemin başlatılması, 
    fonksiyonun geri dönmesi ancak işlemin aslında arka planda devam etmesi" anlamına gelmektedir. Görüldüğü gibi 
    modern işletim sistemlerinde diske yazma işlemi aslında senkron bir işlem değildir. Çünkü gerçek aktarım yapılmadan 
    yazan fonksiyon hemen başarıyla geri dönmektedir. 

    Pekiyi yazma işleminde sayfa önbelleğine yazılan bilgiler çekirdek tarafından ne zaman gerçek aygıta aktarılmakatdır? 
    İşte işletim sistemi kasten bu tür durumlarda araya belli bir gecikme koymaktadır. Böylece peşi sıra yapılan write 
    işlemlerinin tek tek gereksiz biçimde aygıta yapılması engellenir, bunlar biriktirilerek ve çizelgelenerek blok aygıt 
    sürücüsüne aktarılır. Bu biçimdeki aktarmaya işletim sistemleri dünyasında "gecikmeli yazım (delayed write)" da denilmektedir. 
    Tabii işletim sisteminin IO çizelgleyicisi çizelgelemeyi yaptıktan sonra yukarıda da belirttiğimiz gibi transferi kendisi 
    yapmamaktadır. Transfer blok aygıt sürücüsüne istek olarak gönderilir. Transferden asıl sorumlu birim blok aygıt 
    sürücüleridir. 

    Buradaki süreci aşağıdaki gibi özetleyebiliriz:

    Kullanıcı Modundayız
            │
            ▼
            write()
            │
            ▼
            sys_write()  (Çekirdek moduna geçiliyor) ───────────────┐
            │                                                       │
            ▼                                                       │
            Page Cache'e kopyala                                    │
            (sayfa "kirlenmiş (dirty)" olarak işaretlenir)          │
            │                                                       │
            ▼                                                       │
            Geri dönüş (user mode) ◄────────────────────────────────┘
            │
            ▼
            (Asenkron olarak)
            Flusher thread → I/O cizelgeleyici → Blok aygıt sürücüsü → Disk denetleyicisi → Disk donanımı

    Pekiyi işletim sistemi transfer işlemlerini ne kadar süre bekletmektedir? Eğer transfer çok uzun süre bekletilirse 
    elektrik kesilmesi gibi durumlarda kayıplar fazlalaşır. İşte modern işletim sistemlerinde kirlenmiş sayfaların flush 
    edilmesi "çekirdek thread'leri (kernel threads)" tarafından yapılmaktadır. Örneğin Linux sistemlerinde bu işlemlerden 
    "flush" isimli çekirdek thread'leri sorumludur. Eskiden Linux çekirdeklerinin 2.6.32 versiyonuna kadar bu işlemler 
    "pdflush" isimli tek bir çekirdek thread tarafından yapılıyordu. Bu versiyondan sonra artık her blok aygıt sürücüsü 
    için (disk bölümü için değil, diskin bütünü için) ayrı bir flush thread'i oluşturulmaya başlandı. Bu thread'leri 
    komut satırında aşağıdaki gibi görüntüleyebilirsiniz:

    $ s -aux | grep flush
    
    flush thread'leri arka planda sürekli olarak sayfa önbelleğini izler. Orada "kirlenmiş (dirty)" olan sektörleri 
    ilgili blok aygıt sürücüsüne gönderir. Pekiyi bu işleyişte yazma gecikmesi takriben kaç saniye civarında olmaktadır? 
    Aslında bu gecikme süresi başka faktörlere de bağlı olarak değişebilmketedir Burada fikir vermek amacıyla istersek 
    modern Linux sistemleri için bu sürenin ortalama 5 saniye civarında olduğunu söyleyebiliriz. Ancak bu değerler de 
    değiştirilebilmektedir. flush thread'lerinin parametreleri hakkında aşağıda tabloda özet bir bilgi veriyoruz:

    ┌────────────────────────────────┬────────────────────┬─────────────────────────────────────────────────────────────┐
    │ Parametre                      │ Varsayılan değer   │ Anlamı                                                      │
    ├────────────────────────────────┼────────────────────┼─────────────────────────────────────────────────────────────┤
    │ dirty_writeback_centisecs      │ 500                │ Flusher thread’in periyodik olarak çalıştığı aralık         │
    │                                │                    │ (santi saniye cinsinden). 500 cs = 5 saniye.                │
    │                                │                    │ Bu aralıkta çekirdek "dirty" sayfaları kontrol eder.        │
    ├────────────────────────────────┼────────────────────┼─────────────────────────────────────────────────────────────┤
    │ dirty_expire_centisecs         │ 3000               │ Bir "dirty" sayfa, en fazla bu kadar süre (santi saniye     │
    │                                │                    │ cinsinden) RAM’de kalabilir. 3000 cs = 30 saniye sonra      │
    │                                │                    │ “süresi dolmuş” sayılır ve flush edilir.                    │
    ├────────────────────────────────┼────────────────────┼─────────────────────────────────────────────────────────────┤
    │ dirty_ratio /                  │ %20 / %10 civarı   │ RAM’in ne kadarı "dirty" sayfalarla dolarsa flush işleminin │
    │ dirty_background_ratio         │                    │ başlatılacağını belirler (bellek baskısı durumunda          │
    │                                │                    │ zaman beklenmez).                                           │
    └────────────────────────────────┴────────────────────┴─────────────────────────────────────────────────────────────┘

    Bu değerler proc dosya sisteminden görüntülenebilmektedir. Örneğin:

    $ cat /proc/sys/vm/dirty_writeback_centisecs
    $ cat /proc/sys/vm/dirty_expire_centisecs

    "sysctl" komutu ile de bu değerler değerler aşağıdaki örnekte gibi de değiştirilebilmektedir:

    $ sudo sysctl -w vm.dirty_writeback_centisecs=100 

    "sysctl" komutu zaten kendi içerisinde "/proc/sys" dizinindeki dosyalar üzerinde güncelleme işlemleri yamaktadır.

    flush thread'lerinin çalışması daha aytıntılı olarak "sayfa önbelleği (page cache)" konusunun ele alındığı bölümde 
    açıklanacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi "gecikmeli yazım (delayed write)" işleminin gerekçeleri nelerdir? Yukarıda da belirttiğimiz gibi en önemli
    gerekçe peşi sıra yapılan yazma işlemlerinin tek hamlede aygıta yansıtılmasıdır. Bu sayede yazma işlemini yapan 
    thread bloke olmaz ve toplamda bu işlemler paralel yürütüldüğü içim sistem performasnı yükselir. Aynı zamanda flash 
    belleklerde ve SSD'lerde bu gecikme aynı zamanda sürekli yazım sonucunda belleğin eskimesini de kısmen engellemektedir.
    (Tabii "bu eskime" sorunu aslında asıl olarak flash belleklerdeki ve SSD içerisindeki önbellekler yardımıyla
    azaltılmaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında sayfa önbelleğinde "kirlenmiş (dirty)" hale gelmiş olan sayfalar bazı durumlarda işletim sisteminin "tazaleyici
    ("flusher") thread'lerini beklemeden de diske aktarılabilmektedir. Örneğin bir dosya kapatıldığında artık bu işlem 
    arka planda dosyanın kirlenmiş sayfalarının da diske yazılmasına yol açmaktadır. UNIX/Linux sistemlerinin çoğunda bazı 
    özel sistem fonksiyonları yoluyla ya da open fonksiyonundaki bayraklarla da bu duruma müdahale edilebilmektedir. Örneğin 
    sync isimli POSIX fonksiyonu çağrıldığında o anda dosya sistemine ilişkin "kirlenmiş (dirty olan)" olan sayfaların 
    hepsinin flush edilmesini sağlamaktadır:

    #include <unistd.h>

    void sync(void); 

    sync fonksiyonu asenkron biçimde çalışmaktadır. Yani fonksiyon geri döndüğünde tüm blokların flush edilmiş olma garantisi
    yoktur. fsync POSIX fonksiyonu ise belli bir dosyaya ilişkin kirlenmiş sayfaların flush edilmesi için kullanılmaktadır:

    #include <unistd.h>

    int fsync(int fildes); 

    fsync fonksiyonu senkronize (synchronous) çalışmaktadır. Yani fonksiyon geri döndüğünde sayfa önbelleğindeki kirlenmiş 
    sayfaların flush edilmiş olması garanti edilmektedir. 

    Bir dosya açılırken open POSIX fonksiyonunda kullanılan konuyla ilgili üç bayrak vardır. Konuyla ilgili olduğu için 
    bu bayrakların da işlevlerini burada açıklamak istiyoruz.

    O_DSYNC Bayrağı: Bu bayrak POSIX'in "Base Definitions" bölümündeki "Synchronized I/O Data Integrity Completion" 
    başlığında açıklanan yazma koşullarının sağlanacağını belirtmektedir. Bu bayrak kullanıldığında aşağıdaki iki durumun 
    çekirdek tarafından sağlanması garanti edilmektedir:

    - Dosyaya yazdırılan bilgilerin write fonksiyonu geri döndüğünde hedefe transfer edilmiş olması.
    - Yazılan bilginin dosyadan okunabilmesi için gereken meta data bilgilerinin hedefe transfer edilmiş olması. 
    (Tüm meta data bilgilerinin hedefe transfer edilmiş olması gerekmemektedir. Yazılanların okunabilmesi için gerekli 
    meta data bilgilerinin transfer edilmesi yeterlidir.)

    O_SYNC Bayrağı: Bu bayrak POSIX'in "Base Definitions" bölümündeki "Synchronized I/O File Integrity Completion" 
    başlığında açıklanan koşulların sağlanacağını belirtmektedir. O_SYNC bayrağı O_DSYNC bağrağını kapsamaktadır. Fakat 
    bu bayrak write fonksiyonu geri dönmeden önce tüm meta data bilgilerinin hedefe transfer edilmiş olmasını zorunlu 
    tutmaktadır.
    
    O_RSYNC: Bu okuma işlemi ile ilgilidir. Tek başına değil O_DSYNC ya da O_SYNC bayraklarıyla birlikte kullanılır. 
    Eğer O_RSYNC bayrağı O_DSYNC bayrağı ile birlikte kullanılırsa read işlemini etkileyecek olan daha önce yapılmış 
    write işlemleri varsa read fonksiyonu geri dönmeden önce bu write işlemleri için O_DSYNC bayrağında belirtilen semantik 
    uygulanmaktadır. Eğer bu bayrak O_SYNC ile birlikte kullanılırsa read işlemini etkileyecek olan daha önce yapılmış 
    write işlemleri varsa read fonksiyonu geri dönmeden önce bu write işlemleri için O_SYNC bayrağında belirtilen semantik 
    uygulanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi C'nin standart dosya fonksiyonları bu süreçte nerede yer almaktadır? C'nin dosya fonksiyonları aslında neticede
    POSIX fonksiyonlarını çağırmaktadır. Yukarıda da belirtitğimiz gibi POSIX fonksiyonları da sistem fonksiyonlarını 
    çağırıp çekirdek içerisindeki kodların çalışmasını sağlamaktadır. Ancak diğer kurslardan da anımsayacağınız gibi C'nin 
    standart dosya fonksiyonları işletim sisteminin okuma yazma fonksiyonlarını daha az çağırmak için "kullanıcı alanında 
    (user space)" her dosya için bir önbellek de oluşuturmaktadır. Bu önbellek sistemine genellikle önbellek yerine 
    "tamponlama (buffering)" sistemi, burada kullanılan önbelleğe de "tampon (buffer)" denilmektedir. Örneğin Linux'ta 
    biz C'nin getc gibi dosya fonksiyonunu çağırmış olalım. Standart C kütüphanesi fgetc ile 1 byte okumak istediğimizde 
    read POSIX fonksiyonu ile 1 byte okumamaktadır. read POSIX fonksiyonu ile <stdio.h> dosyasında belirtilen BUFSIZ kadar
    byte'ı bir tampona okumakta ve oradan 1 byte'ı programcıya vermektedir. Böylece sonradan okunanacak byte'lar için hiç 
    read fonksiyonu çağrılmayacak ve istek hemen bu tampondan karşılanacaktır. Aynı durum yazma için de söz konusudur. 
    Bu nedenle C'nin standart fonksiyonlarına "tamponlı IO (buffered IO)" fonksiyonları da denilmektedir. Buradaki önbellek 
    sisteminin POSIX fonksiyonlarını dolayısıyla da sistem fonksiyonlarını daha az çağırmak için oluşturulduğuna dikkat 
    ediniz. O halde C'nin standart dosya fonksiyonlarıyla yapılan tipik bir okuma işlemi şöyle gerçekleşmektedir:

    C'deki okuma fonksiyonu ---> read POSIX fonksiyonu ---> sys_read sistem fonksiyonu ---> ....

    Tabii biz kursumuzda okuma ve yazma süreçlri üzerinde dururken olaylar silsilesini standart C fonksiyonlarından 
    başlatmayacağız. POSIX fonksiyonlarından ta da sistem fonksiyonlarından başlatacağız. 

    C'deki bu tamponlama yani önbellek mekanizması C'ye özgü değildir. Diğer prgramlama dillerinin de standart kütüphanelerinde
    benzer biçimde tamponlamalar yapılmaktadır. Örneğin C++'taki <iostream> kütüphanesi, C#'ta kullanılan .NET kütüphaneleri
    Java'da kullanılan temel kütüphanelerde hep kullanıcı alanında C'de olduğu gibi tamponlama yapmaktadır. Ancak bunların 
    hepsi neticede Linux sistemlerinde POSIX fonksiyonlarını, onlar da sistem fonksiyonlarını çağırmaktadır. 

    POSIX dosya fonksiyonlarının Linux'taki işletim sisteminin sistem fonksiyonlarını çağırdığını belirtmiştik. İşletim 
    sisteminin sistem fonksiyonları bilgi önbellekte olsa bile belli bir yavaşlık oluşturmaktadır. Programın akışının 
    kullanıcı modundan çekirdek moduna geçirilmesi ve akışın ilgili sistem fonksiyonuna aktarılması göreli bir zaman 
    kaybına yol açmaktadır. Bu nedenle ayrıca bu kütüphanelerin kullanıcı modunda tamponlama yapması önemli olmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz şimdiye kadar blok ve sektör düzeyinde okuma yazmaların kabaca nasıl gerçekleştirildiğini açıkladık. Ancak 
    çekirdeğin açık dosyalar için oluşturuğu organizasyon hakkında bilgi vermedik. Şimdi sürecin bu yönü üzerinde 
    duracağız. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi UNIX/Linux sistemlerinde dosyalar open isimli POSIX fonksiyonuyla açılmaktadır. open POSIX fonksiyonu
    başarı durumunda ismine "dosya betileyicisi (file descriptor)" denilen bir handle değeri vermektedir. read, write, 
    lseek, close gibi POSIX'in diğer dosya fonksiyonları bu dosya betimleyicisini parametre olarak alıp hangi dosya 
    üzerinde işlem yapılacağını bu betimleyiciden hareketle belirlemektedir. 

    open, read, write, lseek, close gibi POSIX'in temel dosya fonksiyonları Linux sistemlerinde aslında neredeyse doğrudan 
    Linux'un ilgili sistem fonksiyonlarını çağırmakatadır:

    open ---> sys_open
    read ---> sys_read
    write ---> sys_write
    lseek ---> sys_lseek
    close ---> sys_close

    Bu nedenle birtakım ayrıntıları da göz ardı edersek biz Linux sistemlerinde dosya işlemlerini yapan temel POSIX 
    fonksiyonlarının aslında doğrudan sys_xxx sistem fonksiyonlarını çağırdığını varsayabiliriz. Bir sistem fonksiyonu 
    çağrıldığında ileride de ele alacağımız gibi akış otomatik olarak kullanıcı modundan (user mode) çekirdek moduna 
    (kernel mode) geçirilmektedir. Çekirdek içerisindeki sistem fonksiyonları çalıştığı sürece artık koruma mekanizması 
    (yani bellek ve komut koruması) ortadan kalkmakta bu çekirdek kodları her makine komutlarını kullanabilmektedir. 

    UNIX/Linux sistemlerinde kullanılan standart C kütüphaneleri aynı zamanda POSIX fonksiyonlarını da içermektedir. 
    Bilindiği gibi bugün masaüstü Linux sistemlerinde en fazla kullanılan standart C kütüphanesi GNU'nun "libc" kütüphanesidir. 
    Bu kütüphane pek çok sistem için ortak kodlar içerdiğinden dolayı biraz karmalık bir kod yapısına sahiptir. Eğer standart 
    C fonksiyonlarının ve POSIX fonksiyonlarının nasıl yazılıdığını merak ediyorsanız gömülü Linux sistemleri için daha 
    minimalist biçimde yazılmış olan kütüphanelerin kaynak kodlarını inceleyebilirsiniz. Bunun için iki alternatif "musl" 
    ve "uclibc" kütüphaneleridir. "uclibc" kütüphanesine "Mikro C kütüphanesi de denilmektedir. Bu kütüphanelerin kaynak 
    kodlarını yine "elixir.bootlin.com" sitesinden inceleyebilirsiniz. Bu site yalnızca Linux çekirdekleri için değil 
    başka projeler için de kodlar üzerinde gezinme olanağı de sunmaktadır. Biz sadeliği nedeniyle "musl" kütüphanesini
    incelemenizi salık veriririz. Kütüphanenin kodları üzerinde gezinebilmek için aşağıdaki bağlantıdan faydalanabilirsiniz:

    https://elixir.bootlin.com/musl/v1.2.5/source
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    task_struct yapısı (proses kontrol blok) içerisinde proseslere ilişkin dosya işlemleri için kullanılan iki önemli 
    eleman bulunmaktadır:

    struct task_struct {
        ...
        
        /* Filesystem information: */
        struct fs_struct		*fs;

        /* Open file information: */
        struct files_struct		*files;
        
        ...
    };

    Bu iki eleman çok uzun süredir task_struct yapısı içerisinde bulunmaktadır. Ancak buradaki fs_struct ve files_struct 
    yapılarının içeriğinde çekirdeğin versiyonları ilerledikçe çeşitli değişiklikler de yapılmıştır. Yuradaki fs_struct 
    yapısı açık dosyalara ilişkin yapılan organizasyonla ilgili değildir. Prosesin kök dizini ve çalışma dizini gibi dosya 
    sistemine ilişkin proses bilgileri burada tutulmaktadır. Mevcut çekirdeklerde fs_struct yapısı  "/include/linux/fs_struct.h"
    dosyası içerisinde şöyle bildirilmiştir:

    struct fs_struct {
        int users;
        spinlock_t lock;
        seqcount_spinlock_t seq;
        int umask;
        int in_exec;
        struct path root, pwd;
    } __randomize_layout;

    Buradaki root ve pwd elemanları sırasıyla prosesin kök dizinini ve çalışma dizinini (current working directory)
    tutmaktadır. umask elemanı ise prosesin umask değerini tutmaktadır. Buradaki path yapısı da şöyle bildirilmiştir:

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;
        
    vfsmount ve dentry yapıları çeşitli başka bölümlerde ele alınacaktır. Eskiden bu yapı biraz daha küçüktü. Örneğin 
    çekirdeğin 2.2'li versiyonlarında şöyleydi:

    struct fs_struct {
        atomic_t count;
        int umask;
        struct dentry * root, * pwd;
    };

    Çekirdeğin 2.4'te şu hale getirildi:

    struct fs_struct {
        atomic_t count;
        rwlock_t lock;
        int umask;
        struct dentry * root, * pwd, * altroot;
        struct vfsmount * rootmnt, * pwdmnt, * altrootmnt;
    };

    2.6'da da şu ise hale getirildi:

    struct fs_struct {
        int users;
        spinlock_t lock;
        seqcount_t seq;
        int umask;
        int in_exec;
        struct path root, pwd;
    };

    Çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu yapı yoktu. Bu yapıdaki bilgiler doğrudan task_struct 
    içerisinde bulunmaktaydı:

    struct task_struct {
        /* ... */

        unsigned short umask;
        struct m_inode * pwd;
        struct m_inode * root;
        unsigned long close_on_exec;

        /* ... */
    };

    Ayrıntıları göz ardı edersek bu fs_struct yapısındaki en önemli elemanlar "prosesin kök dizinin yeri", "prosesin 
    çalışma dizinin yeri" ve "prosesin umask değeri" dir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        24. Ders 05/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    task_struct içerisindeki files isimli gösterici prosesin açmış olduğu dosyalara ilişkin bilgilerin tutulduğu files_struct 
    türünden yapı nesnesini göstermektedir. files_struct yapısı da zaman içerisinde değişiklere uğratılmıştır. Güncel 
    çekirdeklerde bu yapı "include/linuc/fdtable.h" dosyası içerisinde şöyle bildirilmiştir:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    2.6'lı çekirdeklerde bu yapı şöyleydi:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        int next_fd;
        struct embedded_fd_set close_on_exec_init;
        struct embedded_fd_set open_fds_init;
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    2.4'lü çekirdeklerde şöyleydi:

    struct files_struct {
        atomic_t count;
        rwlock_t file_lock;	/* Protects all the below members.  Nests inside tsk->alloc_lock */
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;	/* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
    };

    2.2 versiyonunda yapı bir eleman dışında aşağı yukarı aynıydı:

    struct files_struct {
        atomic_t count;
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;	/* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
    };

    Çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu bilgiler doğrudan task_struct içerisinde bulunuyordu:

    struct task_struct {
        /* ... */

        unsigned short umask;
        struct m_inode * pwd;
        struct m_inode * root;
        unsigned long close_on_exec;

        /* ... */
    };
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux sistemlerinde ne zaman open POSIX fonksiyonuyla bir dosya açılsa sys_open sistem fonksiyonu açılan dosya 
    için file isimli (struct file türünden) bir yapı türünden nesne tahsis edip dosya işlemleri için gereken bilgileri 
    bu yapı nesnesinin içerisine yerleştirmektedir. sys_read, sys_write, sys_lseek, sys_close gibi sistem fonksiyonları
    da dosya üzerinde işlem yapabilmek için bu file yapısındaki bilgileri kullanmaktadır. İşletim sistemlerinde bu amaçla 
    kullanılan nesnelere "dosya nesnesi (file object)" de denilmektedir. Biz de kursumuzda file yapısı türünden nesneye 
    "dosya nesnesi" de diyeceğiz. Tabii sistem fonksiyonları ve çekirdek bu dosya nesnelerine task_struct nesnesinden 
    hareketle erişmektedir. İşte zaten files_struct yapısı bu erişme ilişkin bilgileri de içermektedir. Biz aşağıdaki 
    gibi bir dosya açmış ollaım:

    fd = open(...);

    sys_open sistem fonksiyonu açılmak istenen dosyanın diskteki yerini ve meta data bilgilerini bulur O bilgilerden 
    hareketle bir dosya nesnesi (file object) oluşturur. O dosya nesnesinin adresini de izleyen paragrafta açıklayacağımız
    gibi files_struct nesnesinin içerisine yerleştirir. Böylece sys_read, sys_write, sys_lseek, sy_close gibi sistem 
    fonksiyonları task_struct nesnesindne hareketle bu dosya nesnesine erişebilmektedir. Güncel çekirdeklere file yapısı 
    "include/linuc/fs.h" dosyasının içerisinde şöyle bildirilmiştir:

    struct file {
        spinlock_t			            f_lock;
        fmode_t				            f_mode;
        const struct file_operations	*f_op;
        struct address_space		    *f_mapping;
        void				            *private_data;
        struct inode			        *f_inode;
        unsigned int			        f_flags;
        unsigned int			        f_iocb_flags;
        const struct cred		        *f_cred;
        struct fown_struct		        *f_owner;
        /* --- cacheline 1 boundary (64 bytes) --- */
        struct path			            f_path;
        union {
            /* regular files (with FMODE_ATOMIC_POS) and directories */
            struct mutex		        f_pos_lock;
            /* pipes */
            u64			                f_pipe;
        };
        loff_t				            f_pos;
    #ifdef CONFIG_SECURITY
        void				            *f_security;
    #endif
        /* --- cacheline 2 boundary (128 bytes) --- */
        errseq_t			            f_wb_err;
        errseq_t			            f_sb_err;
    #ifdef CONFIG_EPOLL
        struct hlist_head		        *f_ep;
    #endif
        union {
            struct callback_head	    f_task_work;
            struct llist_node	        f_llist;
            struct file_ra_state	    f_ra;
            freeptr_t		            f_freeptr;
        };
        file_ref_t			            f_ref;
        /* --- cacheline 3 boundary (192 bytes) --- */
    } __randomize_layout
    __attribute__((aligned(4)));	/* lest something weird decides that 2 is OK */

    Eskiden bu yapının içeriği daha küçüktü. Zaman içerisinde bu yapıda da değişilikler ve eklemeler yapılmıştır.  
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi bir dosya sys_open sistem fonksiyonuyla açıldığında kabaca dosya nesnesinin ve dosya betimleycisinin (file 
    decriptor) nasıl saklandığına ilişkin bilgileri edinelim. Güncel çekirdeklerde bu veri yapısı biraz ayrıntılıdır. 
    Biz bu ayrıntılardan bahsedeceğiz. Ancak önce çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu süreci açıklayalım. 
    Bu ilkel versiyonda henüz struct_files biçiminde bir yapı yoktu. Açık dosya bilgileri doğrudan task_struct içerisinde 
    bulunan aşağıdaki elemalarda saklanıyordu:

    struct task_struct {
        /* ... */

        unsigned long close_on_exec;
	    struct file *filp[NR_OPEN];

        /* ... */
    };

    Burada filp isimli dizinin struct file * türünden olduğuna dikkat ediniz. Yani filp diisi file nesneleriin adreslerini 
    tutan bir gösterici dizisidir. Uzunluğu da NR_OPEN kadardır. Bu versiyonda NR_OPEN şöyle define edilmiştir:

    #define NR_OPEN 20

    İzleyen paragraflarda da anlayacağınız üzere bu ilkel versiyonda bir proses en fazla 20 dosyayı açık durumda tutabiliyordu. 
    UNIX/Linux dünyasında dosya nesnelerinin adreslerini tutan bu gösterici dizilerine "dosya betimleyici tablosu (file 
    descriptor table)" denilmektedir. Yukarıda da belirttiğimiz gibi dosya betimleyici tablosu dosya nesnelerinin adreslerini 
    tutan bir gösterici dizisi biçimindedir. Bunu 0.01 çekirdeği için şöyle bir şekille şöyle gösterebiliriz:
    
    Dosya Betimleyici Tablosu 
    ┌───┐
    │ 0 │ ───────────────► Dosya Nesnesi 
    │ 1 │ ───────────────► Dosya Nesnesi 
    │ 2 │ ───────────────► Dosya Nesnesi 
    │ 3 │ ───────────────► Dosya Nesnesi 
    │ . │                         
    │ . │                         
    │ . │                         
    │18 │ (boş)                   
    │19 │ (boş)                   
    └───┴

    Buradaki sayılar dizinin indekslerini belirtmektedir. Tabii zamanla dosyalar kapanınca bu dizinin elemanları boşa
    düşecektir. Boş elemanlara NULL adres yerleştirilmektedir. İşte open POSIX fonksiyonunun (yani sys_open sistem 
    fonksiyonun) verdiği "dosya betimleyicisi (file descriptor)" aslında dosya betimleyici tablosundaki dizide bir 
    indeks belirtmektedir. open POSIX fonksiyonun (dolayısıyla sys_open sistem fonksiyonunun) dosya betimleyici tablosundaki 
    en düşük boş indeksi vereceği POSIX standartlarında garanti altına alımıştır. Dosya betimleyici tablosunun (yani 
    struct file *) dizisinin uzunluğunun "aynı anda açık tutulabilecek" dosya sayısını da belirttiğine dikkat ediniz. 

    Linux çekirdeğindeki sys_read, sys_write, sys_lseek, sys_close gibi sistem fonksiyonları önce task_struct yapından 
    hareketle dosya betimleyici tablosuna erişmekte, parametre olarak aldıkları betimleyici değerini bu diziye indeks 
    yaparak asıl dosya nesnesine erişmektedir. 

    Dosya betimelyici tablosunun "prosese özgü" olduğuna dikkat ediniz. Bir proseste açılmış olan dosyaya ilişkin dosya 
    nesnesinin adresi o prosesteki dosya betimleyici tablosuna yazılmaktadır. Dosya betimleyicileri sistem genelinde 
    bir değer belirtmemektedir, dosya betimleyici değerleri yalnızca ilgili proses için anlamlıdır. Örneğin 12 numaralı 
    betimleyici bir proseste bir dosyayı belirtirken diğer bir proseste başka bir dosyayı belirtiyor olabilir. Dolayısıyla 
    biz bir proseste bir dosya açıp elde ettiğimiz dosya betimleyicisini başka bir prosese prosesler arası haberleşme 
    yöntemleriyle iletsek o proseste o betimleyicinin hiçbir anlamı olmaz. Ancak anımsanacağı gibi özel bir durum olarak 
    üst proses fork işlemi yaptığında üst prosesin dosya betimleyici tablosu alt prosese sığ kopyalanmaktadır. Böylece 
    üst proses ile alt proses aynı dosya üzerinde işlem yapabilmektedir. (trace işlemlerinde Linux sistemlerine özgü 
    kullanılmak üzere bulundurulmuş olan sys_pidfd_getfd isimli bir sistem fonksiyonu vardır. Ayrıca başka bir prosesin
    açmış olduğu dosyanın başka bir proseste açılmasına olanak sağlayan sys_pidfd_open isimli bir sistem fonksiyonu da 
    Linux çekirdeğinde bulunmaktadır.)

    Yukarıdaki 0.01 versiyonunda konuyla ilgili unsigned long türden close_on_exec isimli bir elemanın da bulunduğunu
    görüyorsunuz. Bu elemanın her biti bir betimleyicinin "close-on-exec" drumunu belirtmektedir. Söz konusu bit 1 ise 
    ilgili betimleyici exec işlemleri sırasında close edilir, 0 ise close edilmez. POSIX standartlarında bir dosya 
    açıldığında close-on-exec bayrağının default durumda 0 olduğu belirtilmiştir. (Yani default durumda exec işlemlerinde 
    dosya kapatılmamaktadır.) Bu ilkel versiyonda zaten bir prosesin maksimum açık tutacağı dosya sayısı 20'dir. O 
    zamanlarda long türü 32 bitti. Yani bu unsigned long eleman bütün dosya betimleyicilerinin close-on-exec bayraklarını 
    tutmak için yeterliydi. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    sys_open sistem fonksiyonu öncelikle dosya betimleyici tablosundaki ilk boş betimleyiciyi bulmaya çalışır. Çünkü 
    dosya betimleyici tablosu tamamen doluysa zaten bir dosya nesnesinin oluşturulup işlemlere devam edilmesinin de bir 
    anlamı olmayacaktır. Pekiyi dosya betimleyici tablosundaki ilk boş betimleyici nasıl bulunmaktadır? Düz mantıkla 
    "mademki dosya betimleyici tablosundaki boş indekslerde NULL adres var o zaman ilk NULL adres görülene kadar bir 
    döngü ile sıralı arama yapılabilir" diye düşünebilirsiniz. Eğer dosya betimleyici tabloları 0.01 versiyonundaki gibi 
    çok küçük olsaydı sıralı arama yapmanın önemli bir sakıncası olmayabilirdi. Gerçekten de 0.01 versiyonunda boş 
    betilmeyici şöyle bulunmuştur:

    int sys_open(const char * filename,int flag,int mode) {
        /* ... */

    	for(fd=0 ; fd<NR_OPEN ; fd++)
            if (!current->filp[fd]) 
                break;
        
        /* ... */
    }

    Görüldüğü gibi bu ilkel versiyonda dosya betimleyici tablosu üzerinde tek tek sıralı arama yapılmış, ilk boş 
    betimleyici (yani NULL adres içeren ilk dizi elemanının indeksi) elde edilmiştir. Ancak uzunca bir süredir proseslerin 
    default dosya betimleyici tablolarının default uzunlukları 1024'tür ve bu uzunluk da büyütülebilmektedir. 1024 elemanlı 
    bir tabloda sıralı arama ile ilk NULL olan dizi elemanının indeksinin bulunması yavaş bir işlemdir. İşte bir süre 
    sonra Linux çekirdeklerinde bu arama işlemi bit düzeyinde aramayla hızlandırılmıştır. Bit düzeyinde arama yönteminde 
    dosya betimleyici tablosunun uzunluğu kadar bit dizisi oluşturulur. Sonra o bit dizisindeki ilk 0 olan bitin indeksi 
    bulunmaya çalışılır. Bu bit dizisindeki 0 olan bitler betimleyici tablosundaki boş elemanları 1 olan bitler dolu olan 
    elemanları belirtmektedir. İşlemcilerde belli bir yazmaçtaki (ya da Intel işlemcileri söz konusuysa bellek adresindeki) 
    "ilk 0 olan bitin indeksini veren özel makine komutları" bulunmaktadır. Tabii işlemci 32 bit ise bu makine komutları 
    32 bitlik yani 4 byte'lık bir veri üzerinde, 64 bit ise 64 bitlik yani 8 byte'lık bir veri üzerinde işlem yapabilmektedir. 
    Örneğin elimizdeki işlemcinin 64 bit olduğunu düşünelim. Bu işlemcilerdeki C derleyicilerinde unsigned long türü 
    8 byte yani 64 bittir. Bu durumda örneğin 1024 eleman uzunluğundaki dosya betimleyici tablosu için 16 elemanlı bir 
    unsigned long dizi bitmap olarak kullanılabilir. Tabii bu sistemlerde ilk 0 bitini bulan makine komutları zaten 64 
    bitlik bir bilgi üzerinde bu işi yapabilmektedir. O halde çekirdek tasarımcısı 16 elemanlı bir döngü kullanıp dizinin 
    her elemanı için bu özel makine komutunu kullanarak işlemleri hızlandırabilir. Ancak belli bir süreden sonra bu 
    yöntem de biraz daha geliştirilerek arama işlemi biraz daha hızlandırılmıştır. Bu ikinci hızlandırma yönteminde ikinci 
    bir bit dizisi kullanılmaktadır. Ancak ikinci bir dizisinin her biti ilk bit dizisindeki unsigned long elemanın tüm 
    bitlerinin 0 olup olmadığını tutmaktadır. Bu durumda güncel çekirdeklerde önce bu ikinci bit dizisindeki ilk 1 olan 
    bit bulunur. Sonra bu bitin indeksi birinci bit dizisine indeks yapılarak oradaki unsigned long değer içerisinde ilk 
    0 olan bit elde edilir. Bu yöntemde örneğin ilk bit dizisinin aşağıdaki gibi olduğunu varsayalım:
    
    1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 - 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 -
    1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 - 1111 1101 1111 1111 1111 1111 1111 1111 1111 1111 - ....

    Burada ilk bit dizisi unsigned long dizi biçimindedir. Görüldüğü gibi bu dizinin ilk üç elemanında hiç 0 olan bit 
    yoktur. İlk 0 olan bit 3'üncü indekstedir. Bu durumda ikinci bir dizisi de aşağıdaki gibi olacaktır:

    0001.....

    Bu hızlandırma mantığında önce kinci bit dizisindeki ilk 1 olan bitin indeksi elde edilir. Örneğimizde bu 3'tür. 
    Sonra birinci bit disizinin 3'üncü indeksteki unsigned long elemanında ilk 0 olan bitin indeksi bulunur. Bu yöntemde 
    birkaç makine komutuyla istenen bilginin elde edilebildiğine dikkat ediniz. 

    Çekirdek dokümantasyonunda her dosya betimleyicisinin boş mu dolu mu olduğunu tutan bitmap'e "birinci düzey bitmap"
    bu bitmap'teki ilk boş unsigned long elemanın dizi indeksini veren ikinci bitmap'e ise "ikinci düzey bitmap" 
    denilmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    2.6 çekirdeğine kadar (bu çekirdek de dahil) bit dizileri için fd_set isimli bir yapı kullanılıyordu. Sonraları bu 
    fd_set yapısı bırakıldı. Örneğin çekirdeğin 2.2 ve 2.4 versiyonundaki "include/linux/sched.h" içerisindeki files_struct 
    yapısı şöyleydi (2.2 versiyonunda file_lock elemanı yoktu):

    struct files_struct {
        atomic_t count;
        rwlock_t file_lock;	        /* Protects all the below members.  Nests inside tsk->alloc_lock */
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;	/* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
    };

    Burada görmüş olduğunuz fd_set yapısı "bit dizilerini" temsil etmektedir. Nu yapı şöyle bildirilmiştir:

    typedef __kernel_fd_set		fd_set;

    typedef struct {
        unsigned long fds_bits [__FDSET_LONGS];
    } __kernel_fd_set;

    Buradaki __FDSET_LONGS sembolik sabiti 32 bit sistemlerde 32 değerini 64 bit sistemlerde 16 değerini vermektedir. 
    Yani bu yapının içerisindeki fds_bits elemanı toplamda 1024 biti tutan unsigned long türünden dizidir. 2.6 çekirdeği 
    dahil olmak üzere bit dizisi anlamında çekirdekte bu fd_set yapısı kullanılmıştır. Ancak bu fd_set temsilinin tasarımında
    da aslında kusurlar vardır. Bu temsilde bit dizisi büyütülmek istendiğinde artık bu fd_set temsili işe yaramaz 
    hale gelmektedir. Bu nedenle artık güncel çekirdeklerde fd_set yerine doğrudan unsigned long * türünden bir gösterici 
    tutulup bu göstericinin gösterdiği yer için belli uzunlukta unsşigned long dizi tahsis edilmektedir. Aslında uzun 
    süre kulanılmış olan bu fd_set temsilinden vazgeçilmesi iyi olmuştur. Yukarıdaki çekirdeğin 2.4 versiyonundaki 
    files_struct yapısında dosya betimleyici tablosunun uzunluğu yapının max_fds elemanında tutulmaktadır. Çünkü işin 
    başında bu tablo 1024 elemanlık olsa da daha sonra büyütülebilmektedir. Bu versiyonda dosya betimelyeici tablosunun 
    adresinin de fd elemanında tutulduğuna dikkat ediniz. Dosyaların close-on-exec bayrakları da yine yapının close_on_exec 
    elemanında tutulmaktadır. 
    
    Yukarıdaki files_struct yapısı biraz kafanızı karıştırabilir. Sanki size bu yapıda aynı amaçla kullanılan birden 
    fazla eleman varmış gibi gelebilir. Konuya açıklık getirmek amacıyla bu versiyondaki yapı elemanlarının hepsinin 
    işlevlerini tek tek açıklayalım:

    - Bir proses yaratıldığında işin başında dosya betimleyici tablosu için, boş betimleyici tespit etmek için ve 
    close-on-exec bayrakları için files_struct yapısı içerisinde alanlar ayrılmıştır:

    struct files_struct {
        /* ... */

        fd_set close_on_exec_init;                  /* close-on-exec bayrakları için kullanılan static bitmap */
        fd_set open_fds_init;                       /* açık dosya betimleyicilerini tutan statik bitmap */
        struct file * fd_array[NR_OPEN_DEFAULT];    /* dosya betimleyici tablosu için ayrılmış statik dizi */

        /* ... */
    };

    Burada NR_OPEN_DEFAULT 32 bit sistemlerde 32, 64 bit sistemlerde 64 değerini vermektedir. Eğer proses dosya betimleyici 
    tablosunu genişletmezse zaten bu tablolar ve bitmap'ler files_struct yapısı içerisinde hazır bir biçimde tutulmaktadır. 
   
    - Çekirdek her zaman dosya tablosunun yerini fd göstericisinin gösterdiği yerde, açık dosya betimleyicilerinin bitmap'ini 
    open_fds göstericisinin gösterdiği yerde, close-on-exec bayraklarına ilişkin bitmap'i ise close_on_exec göstericisinin 
    gösterdiği yerde aramaktadır. Yapının bu elemanalrına dikkat ediniz:

    struct files_struct {
        /* ... */

        struct file ** fd;	                    /* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;

        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];

        /* ... */
    };

    İşin başında default durumda fd göstericisi fd_array elemanını, close_on_exec göstericisi close_on_exec_init elemanını
    ve open_fds göstericisi de open_fds_init elemanını göstermektedir. 

    current göstericisinden hareketle fdx betimeleyicisinin gösterdiği yerdeki dosya nesnesine (struct file) 
    current->files->fd[fdx] ifadesiyle erişilebilir. Bu erişimi kolaylaştırmak için 2.2 ve 2.4 çekirdeklerinde fcheck
    isimli çekirdek fonksiyonu bulundurulmuştur:

    static inline struct file * fcheck(unsigned int fd)
    {
        struct file * file = NULL;
        struct files_struct *files = current->files;

        if (fd < files->max_fds)
            file = files->fd[fd];
        return file;
    }

    Ancak bu fonksiyon export edilmemiştir. Yani aygıt sürücüler tarafından kullanılamamaktadır. Aslında çekirdekte bir 
    dosya betimleyicisinden hareketle dosya nesnesini elde etmek için daha yüksek seviyeli fget fonksiyonu kullanılmaktadır.
    Bu fonksiyon 2.4 ve 2.6 versiyonlarında aşağıdaki yazılmıştır:

    struct file fastcall *fget(unsigned int fd)
    {
        struct file * file;
        struct files_struct *files = current->files;

        read_lock(&files->file_lock);
        file = fcheck(fd);
        if (file)
            get_file(file);
        read_unlock(&files->file_lock);
        return file;
    }

    Bu fonksiyonun fcheck fonksiyonu kullanılarak yazıldığını görüyorsunuz. Ancak bu fonksiyon ileride göreceğimiz gibi 
    dosya nesnesi içerisindeki (struct file yapısındaki) sayacı da güvenli bir biçimde artırmaktadır. fget fınksşyonu 
    da bu versiyonlarda export edilmemiştir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        25. Ders 11/10/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin 2.6 versiyonlarına gelindiğinde files_struct yapısının içerisi fdtable isimli bir yapı ile biraz daha 
    derli toplu fakat biraz daha karmaşık hale getirilmiştir. 2.6'lı versiyonlardaki files_struct yapısı şöyledir:
  
    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        int next_fd;
        struct embedded_fd_set close_on_exec_init;
        struct embedded_fd_set open_fds_init;
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    fdtable yapısı da şöyledir:

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd;      /* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        struct rcu_head rcu;
        struct fdtable *next;
    };

    Artık bu yapılar da "include/linux/fdtable.h" isimli dosya oluşturularak oraya taşınmıştır. By versiyonlarda 
    çekirdek her zaman fdt gösterisicinin gösteriği yerden işlemine başlamaktadır. fdt göstericisi işin başında yapı 
    içerisindeki fdtab yapı nesnesini göstermektedir. fdtab yapı nesnesinin içerisinde de önceki versiyonlarda olduğu
    gibi fd, close_on_exec, open_fds göstericileri vardır. Bu göstericiler de işin başında files_struct içerisindeki 
    fd_array, close_on_exec_init ve open_fds_init elemanlaırnı göstermektedir. Ancak ileride aslında files_struct 
    içerisindeki fdt göstericisi başka bir fdtable nesnesini, fdtable nesnesinin içerisindeki göstericiler de büyütülmüş
    başka nesneleri gösterir hale gelebilmektedir. 

    Bu versiyonlarda current göstericisinden hareketle fdx betimelyicisinin gösterdiği yerdeki dosya nesnesine (struct file) 
    current->files->fdt->fd[fdx] ifadesiyle erişilebilir. Bu versiyonalarda da bu erişimi bazı kontrollerle sağlayan 
    ayrı fonksiyonlar ve makrolar da bulundurulmuştur. Örneğin fcheck_files fonksiyonu şöyle tanımlanmıştır:

    static inline struct file * fcheck_files(struct files_struct *files, unsigned int fd)
    {
        struct file * file = NULL;
        struct fdtable *fdt = files_fdtable(files);

        if (fd < fdt->max_fds)
            file = rcu_dereference_check_fdtable(files, fdt->fd[fd]);
        return file;
    }

    #define files_fdtable(files)        \
		    (rcu_dereference_check_fdtable((files), (files)->fdt))

    #define fcheck(fd)	fcheck_files(current->files, fd)

    Yani çekirdek içerisinde fcheck makrosuyla fd numaralı betimleyiciye ilişkin dosya nesnesi elde edilebilmektedir. 
    Ancak fcheck_files fonksiyonu da export edilmemiştir. Yine 2.6'lı çekirdeklerde de dosya betimleyicisinden hareketle 
    dosya nesnesi içerisindeki sayacı artırarak dosya nesnesini elde eden daha yüksek seviyeli fget isimli bir fonksiyon 
    da bulunmaktadır:

    struct file *fget(unsigned int fd)
    {
        return __fget(fd, FMODE_PATH);
    }
    EXPORT_SYMBOL(fget);

    Biz burada bu fonksiyonun çağırdığı fonksiyonları gözden geçirmeyeceğiz. Ancak bu fonksiyonun artık export edildiğine
    dikkat ediniz. Yani bu versiyondan itibaren aygıt sürücüler de dosya betimleyicisinden hareketle dosya nesnesine bu 
    fonksiyon yoluyla erişebilmektedir. Çekidekteki nesnenin sayacını artırarak erişim sağlayan fonksiyonlar genel olarak 
    get soneki ile sayacı eksilten fonksiyonlar da put soneki ile isimlendirilmiştir. fget fonksiyonuyla elde edilen 
    dosya nesnesi fput fonksiyonuyla geri bırakılmaktadır:

    void fput(struct file *file)
    {
        if (atomic_long_dec_and_test(&file->f_count))
            __fput(file);
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Belli bir zamandan sonra artık bit dizisi oluşturmak için fd_set yapısının kullanılmasından vazgeçilmiştir. Güncel 
    çekirdeklerdeki açık dosyalara ilişkin veri yapısı 2.6 ile çok benzerdir. Ancak yukarıda da belirttiğimiz gibi artık
    fd_set yapısı kullanılmamaktadır. Güncel çekirdeklerdeki files_struct yapısı şöyledir:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    fdtable yapısı da şöyledir:

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd;      /* current fd array */
        unsigned long *close_on_exec;
        unsigned long *open_fds;
        unsigned long *full_fds_bits;
        struct rcu_head rcu;
    };

    Görüldüğü gibi artık bit dizileri fd_set yerine doğrudan unsigned long türden bir dizi biçiminde oluşturulmaktadır. 
    Yine bu versiyonlarda da fdx numaralı dosya onlar betimleyicisinin gösterdiği yerdeki dosya nesnesine current->files->fdt->fd[fdx]
    ifadesiyle erişilmektedir. Fakat artık güncel versiyonlarda fcheck biçiminde bir makro ve fcheck_files isimli bir 
    fonksiyon yoktur. Ancak yine güncel versiyonlarda dosya betimleyicisi yoluyla dosya nesnesine erişimi referans sayacını 
    artırarak yapan fget fonksiyonu bulunmaktadır:
    
    struct file *fget(unsigned int fd)
    {
        return __fget(fd, FMODE_PATH);
    }
    EXPORT_SYMBOL(fget);

    Yine referans sayacını eksliterek nesneyi bırakmak için fput fonksiyonu kullanılmaktadır:

    void fput(struct file *file)
    {
        if (unlikely(file_ref_put(&file->f_ref)))
            __fput_deferred(file);
    }
    EXPORT_SYMBOL(fput);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Mevcut çekirdeklerde dosya betimleyici tablosundaki ilk boş betimleyicinin bulunmasının bit dizilerinde "ilk 0 olan 
    bitin bulunması" problemi biçiminde ele alındığını belirtmiştik. Bunun için güncel çekirdeklerde iki düzey bitmap 
    kullanılıyordu. Güncel çekirdeklerdeki fdtable yapısının içerisinde bulunan open_fds birinci düzey bitmap'i full_fds_bits  
    ise ikinci düzey bitmap'i belirtmektedir. Tüm dosya betimleyicilerinin dolu boş mu olduğu open_fds bitmap'inde tutulmaktadır. 
    full_fds_bits bitmap'i ise open_fds bitmap'indeki tüm bitleri 0 olmayan ilk unsigned long elemanın indeksinin bulunmasında 
    kullanılmaktadır. files_struct ve fdtable yapılarını aşağıda yeniden veriyoruz:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd;      /* current fd array */
        unsigned long *close_on_exec;
        unsigned long *open_fds;
        unsigned long *full_fds_bits;
        struct rcu_head rcu;
    };
    
    Uzun süredir bir bit dizisi içerisindeki ilk 0 olan bitin indeksini elde etmek için find_next_zero_bit isimli 
    bir çekirdek fonksiyonu kullanılmaktadır. Tabii bu fonksiyon nihayetinde yukarıda da bahsettiğimiz gibi işlemciye 
    özgü makine komutlarını kullanmaktadır. Çekirdeğin güncel versiyonlarında sys_open sistem fonksiyonundan başlanarak
    ilk boş dosya betimleyicisinin bulunması için yapılan çağrılar şöyledir:

    sys_open --> do_sys_open ---> do_sys_openat2 ---> __get_unused_fd_flags ---> alloc_fd ---> find_next_fd ---> 
    find_next_zero_bit

    Bu çağrı zincirinde bir dizisi içerisinde ilk 0 olan bitin bulunması işlemini find_next_zero_bit fonksiyonu yapmaktadır. 
    İlk 0 olan bitin bulunması aslında baştan başlanarak yapılmamaktadır. files_struct yapısı içerisindeki next_fd elemanı 
    aramanın başlatılacağı yeri belirtmektedir. Yani buradaki next_fds elemanının belirttiği değerden küçük tüm dosya 
    betimleyicileri doludur. Dolayısıyla arama full_fds_bits dizisinin hemen başından başlatılmamaktadır. Tabii eğer bu 
    next_fds elemanın belirttiği dosya betimeleyicisinden daha küçük bir betimleyici kapatılırsa çekirdek zaten bu next_fds 
    elemanını güncellemektedir. 

    Güncel çekirdeklerdeki find_next_fd fonksiyonu şöyle yazılmıştır:

    static unsigned int find_next_fd(struct fdtable *fdt, unsigned int start)
    {
        unsigned int maxfd = fdt->max_fds; /* always multiple of BITS_PER_LONG */
        unsigned int maxbit = maxfd / BITS_PER_LONG;
        unsigned int bitbit = start / BITS_PER_LONG;
        unsigned int bit;

        /*
        * Try to avoid looking at the second level bitmap
        */
        bit = find_next_zero_bit(&fdt->open_fds[bitbit], BITS_PER_LONG,
                    start & (BITS_PER_LONG - 1));
        if (bit < BITS_PER_LONG)
            return bit + bitbit * BITS_PER_LONG;

        bitbit = find_next_zero_bit(fdt->full_fds_bits, maxbit, bitbit) * BITS_PER_LONG;
        if (bitbit >= maxfd)
            return maxfd;
        if (bitbit > start)
            start = bitbit;
        return find_next_zero_bit(fdt->open_fds, maxfd, start);
    }

    Fonksiyonun birinci parametresi fdtable nesnesinin adresini, ikinci parametresi ise aramanın başlatılacağı betimleyicinin 
    numarasını belirtmektedir. Fonksiyon önce ikinci düzey birtmap'te arama yapmadan ilk düzey bitmap'te, dizinin hemen 
    aramanın yapılacağı indeksinde hızlı bir arama yapar. Eğer bu aramadan sonuç elde edilemeze önce ikinci düzey bitmap'te 
    birinci düzey bitmap için dizi indeksini elde eder, sonra birinci düzey bitmap'te arama yapar. find_next_zero_bit 
    fonksiyonu da güncel çekirdeklerde şöyle tanımlanmıştır:

    unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
				 unsigned long offset)
    {
        if (small_const_nbits(size)) {
            unsigned long val;

            if (unlikely(offset >= size))
                return size;

            val = *addr | ~GENMASK(size - 1, offset);
            return val == ~0UL ? size : ffz(val);
        }

        return _find_next_zero_bit(addr, size, offset);
    }

    Buradaki small_const_nbits fonksiyonu find_next_fd fonksiyonundaki ilk hızlı aramanın ve birinci düzey bitmap'taki 
    aramanın yapılabilmesi için kontrol sağlamaktadır. Yani arama tek bir dizi elemanı üzerinde yapılacaksa bu if deyiminin 
    doğruysa kısmı çalıştırılacaktır. Eğer arama birden fazla dizi elemanı üzerinde yapılacaksa bu durumda arama 
    _find_next_zero_bit fonksiyonuna yaptırılmaktadır. Bu fonksiyon da şöyle tanımlanmıştır:

    unsigned long _find_next_zero_bit(const unsigned long *addr, unsigned long nbits,
					 unsigned long start)
    {
        return FIND_NEXT_BIT(~addr[idx], /* nop */, nbits, start);
    }

    #define FIND_NEXT_BIT(FETCH, MUNGE, size, start)				        \
    ({										                                \
        unsigned long mask, idx, tmp, sz = (size), __start = (start);		\
                                                                            \
        if (unlikely(__start >= sz))						                \
            goto out;							                            \
                                                                            \
        mask = MUNGE(BITMAP_FIRST_WORD_MASK(__start));				        \
        idx = __start / BITS_PER_LONG;						                \
                                                                            \
        for (tmp = (FETCH) & mask; !tmp; tmp = (FETCH)) {			        \
            if ((idx + 1) * BITS_PER_LONG >= sz)				            \
                goto out;						                            \
            idx++;								                            \
        }									                                \
                                                                            \
        sz = min(idx * BITS_PER_LONG + __ffs(MUNGE(tmp)), sz);			    \
    out:										                            \
        sz;									                                \
    })

    Burada dizi elemanlarında arama gördüğünüz gibi FIND_NEXT_BIT makrosuyla yapılmıştır. Tabii bu makro içerisindeki 
    döngü ancak birinci düzey bitmap aramasında çalıştırılacaktır. 

    Burada şöyle bir özet yapmak istiyoruz:

    1) Çekirdek hemen ikinci düzey bitmap'e (full_fds_bits) yönelmez. Önce open_fds dizisinde tek bir unsigned long 
    elemanda hızlı bir arama yapar.

    2) Eğer yukarıdaki arama başarısız olursa bu durumda önce ikinci düzey bitmap'te (full_fds_bits) ilk 0 olan bitin 
    indeksi elde edilir. Birinci düzey bitmap'te (open_fds) yalnızca bu indeksteki unsigned long dizi elemanında arama 
    yapılır. 

    3) find_next_zero_bit fonksiyonu unsigned long dizisinin yalnızca tek bir elemanında mı yoksa belli bir elemandan 
    itibaren dizinin geri kalan tüm elemanlarında mı aramaya yapılacağına small_const_nbits çağrısıyla karar 
    vermektedir. 

    Pekiyi yukarıdaki kodlarda bitmap dizisinin belli bir unsigned long elemanında işlemcinin özel makine komutlarıyla
    arama işlemi tam nerede yapılmaktadır? İşte yukarıdaki kodlar incelnirse makine dili düzeyinde aramanın ffz fonksiyonunda
    ve __ffs fonksiyonunda yapıldığı görülecektir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    POSIX sistemlerinde dolayısıyla da Linux çekirdeğinde thread'lerin ayrı dosya betimleyici tabloları yoktur. Dosya
    betimleyicileri ve dosya betimleyici tablosu prosese özgüdür. Yani siz bir prosesin hangi thread'inde dosya açmış 
    olursanız olun bu dosya bilgisi prosese özgüdür, siz prossin herhangi bir thread'inde bu dosyaya erişebilirsiniz. 
    Bir thread yaratıldığında thread'e ilişkin task_struct nesnesinin fs gibi files gibi elemanları onu yaratan thread'in
    task_struct nesnesinden sığ kopyalamayla (yani gösterici elemanları söz konusu olduğunda yalnızca göstericilerin 
    içerisindeki adreslerin kopyalamasıyla) kopyalanmaktadır. Dolayısıyla aslında prosesin bütün thread'leri açık 
    dosyalara ilişkin aynı veri yapısı nesnelerini kullanmaktadır. task_struct yapısının ilgili kısmına dikkat ediniz:

    struct task_struct {
        /* ... */
        
        /* Filesystem information: */
        struct fs_struct		*fs;

        /* Open file information: */
        struct files_struct		*files;
        
        /* ... */
    };

    Burada yeni task_struct nesnesi yaratılıp diğer task_struct nesnesinden yeni yaratılan task_struct nesnesine 
    sığ kopyalama yapıldığında files göstericisinin de aslında aynı files_struct nesnesini göstereceğine dikkat ediniz. 
    Yani toplamda aslında proses için tek bir files_struct nesnesi bulunmaktadır. Dolayısıyla bir prosesin tüm thread'leri 
    aslında aynı bilgilere erişip onları kullanmaktadır.  

    Anımsanacağı gibi fork fonksiyonuyla alt proses yaratılırken alt proses üst prosesle aynı açık dosyaları görebiliyordu. 
    Pekiyi bu güncel çekirdeklerde nasıl sağlanmaktadır? Örneğin üst proses open fonksiyonuyla bir dosya açmış olsun. 
    Açılan dosyanın da dosya betimleyicisi 3 olsun. Şimdi bu proses fork yaptığında bu prosesin tamamen özdeş bir kopyası 
    oluşturulacaktır. Ancak alt proses 3 numaralı betimleyiciyi de fork işleminden sonra kullanabilecektir. fork işleminden 
    sonra Üst prosesin 3 numaralı betimleyicisi ile alt prosesin 3 numaralı betimleyicisi aynı dosya nesnesini gösterecektir. 
    Özetle fork işlemi sırasında üst prosesin açmış olduğu dosyalar da adeta alt prosese aktarılmış gibi olmaktadır. 
    Pekiyi bu çekirdek veri yapısında nasıl sağlanmaktadır? Anımsanacağı gibi fork işlemi sonrasında artık prosesler 
    birbirinden bağımsızdır. Yani fork işleminden sonra artık birinin açtığı dosya diğeri tarafından görülemez. İşte 
    fork işlemi sırasında tamamen alt proses için yeni bir files_struct nesnesi ve yeni bir dosya betimleyici tablosu 
    (fd dizisi) yaratılmaktadır. Ancak üst prosesin dosya betimleyici tablosundaki adresler yeni yaratılan alt posesteki 
    dosya betimleeyici tablosuna kopyalanamktadır. Böylece üst prosesin dosya betimleyici tablosunun aynı mumaralı 
    betimleyicileriyle alt prosesin dosya betimleyici tablosunun aynı numaralı betimleyicileri aynı dosya nesnelerini 
    gösteriyor durumda olur. Tabii artık üst ve alt proseslerin yeni açacağı dosyalar onlara özgü olacaktır. Paylaşılan 
    dosya nesneleri yalnızca fork öncesinde açılmış olanlardır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            26. Ders 12/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz şimdiye kadar açık dosyalara ilişkin çekirdeğin oluşturduğu veri yapıları hakkında şu bilgileri edindik:

    - Açık dosyalara ilişkin task_struct içerisindeki veri yapıları.
    - Dosya betimleyicilerinin anlamı ve dosya betimleyicisi yoluyla dosya nesnelerine nasıl erişildiği.
    - En düşük boş betimleyicinin elde edilme biçimi.

    Şimdi dosya sisteminin diğer önemli veri yapıları üzerinde duracağız ve temel dosya sistem fonksiyonlarının 
    gerçekleştirimleri hakkında temel bilgileri edineceğiz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin açılmış olan dosyalara ilişkin bilgileri dosya nesnesi dediğimiz bir yapı içerisinde tuttuğunu belirtmiştik. 
    Güncel çekirdeklerde bu yapı "include/linux/fs.h" dosyasında aşağıdaki gibi bildirilmiştir:

    struct file {
        spinlock_t			            f_lock;
        fmode_t				            f_mode;
        const struct file_operations	*f_op;
        struct address_space		    *f_mapping;
        void				            *private_data;
        struct inode			        *f_inode;
        unsigned int			        f_flags;
        unsigned int			        f_iocb_flags;
        const struct cred		        *f_cred;
        struct fown_struct		        *f_owner;
        /* --- cacheline 1 boundary (64 bytes) --- */
        struct path			            f_path;
        union {
            /* regular files (with FMODE_ATOMIC_POS) and directories */
            struct mutex		        f_pos_lock;
            /* pipes */
            u64			                f_pipe;
        };
        loff_t				            f_pos;
    #ifdef CONFIG_SECURITY
        void				            *f_security;
    #endif
        /* --- cacheline 2 boundary (128 bytes) --- */
        errseq_t			            f_wb_err;
        errseq_t			            f_sb_err;
    #ifdef CONFIG_EPOLL
    struct hlist_head		            *f_ep;
    #endif
        union {
            struct callback_head	    f_task_work;
            struct llist_node	        f_llist;
            struct file_ra_state	    f_ra;
            freeptr_t		            f_freeptr;
        };
        file_ref_t			            f_ref;
        /* --- cacheline 3 boundary (192 bytes) --- */
    } __randomize_layout
    __attribute__((aligned(4)));	/* lest something weird decides that 2 is OK */

    Yapı bildirimindeki bazı elemanların bazı konfigürasyon seçenekleri seçildiğinde yapıya dahil edildiğine dikkat ediniz. 
    Eskiden file yapısı daha az elemana sahipti. Zaman içerisinde bu yapıda değişikler ve eklemeler yapılmış yapı bugünkü 
    durumuna gelmiştir. Örneğin çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu yapı şöyleydi:

    struct file {
        unsigned short f_mode;
        unsigned short f_flags;
        unsigned short f_count;
        struct m_inode * f_inode;
        off_t f_pos;
    };

    Çekirdğein 2.2'li versiyonlarında yapı şöyle bildirilmişti:

    struct file {
        struct file		        *f_next, **f_pprev;
        struct dentry		    *f_dentry;
        struct file_operations	*f_op;
        mode_t			        f_mode;
        loff_t			        f_pos;
        unsigned int 		    f_count, f_flags;
        unsigned long 		    f_reada, f_ramax, f_raend, f_ralen, f_rawin;
        struct fown_struct	    f_owner;
        unsigned int		    f_uid, f_gid;
        int			            f_error;

        unsigned long		    f_version;

        /* needed for tty driver, and maybe others */
        void			        *private_data;
    };

    Çekirdeğin 2.4 versiyonunda file yapısı şöyleydi:

    struct file {
        struct list_head	    f_list;
        struct dentry		    *f_dentry;
        struct vfsmount         *f_vfsmnt;
        struct file_operations	*f_op;
        atomic_t		        f_count;
        unsigned int 		    f_flags;
        mode_t			        f_mode;
        loff_t			        f_pos;
        unsigned long 		    f_reada, f_ramax, f_raend, f_ralen, f_rawin;
        struct fown_struct	    f_owner;
        unsigned int		    f_uid, f_gid;
        int			            f_error;

        size_t			        f_maxcount;
        unsigned long		    f_version;

        /* needed for tty driver, and maybe others */
        void			        *private_data;

        /* preallocated helper kiobuf to speedup O_DIRECT */
        struct kiobuf		    *f_iobuf;
        long			        f_iobuf_lock;
    };

    Çekirdeğin 2.6 versiyonundaki file yapısı güncel versiyonlara daha fazla benzemektedir:

    struct file {
        /*
        * fu_list becomes invalid after file_free is called and queued via
        * fu_rcuhead for RCU freeing
        */
        union {
            struct list_head	        fu_list;
            struct rcu_head 	        fu_rcuhead;
        } f_u;
        struct path		                f_path;
    #define f_dentry	                f_path.dentry
    #define f_vfsmnt	                f_path.mnt
        const struct file_operations	*f_op;
        spinlock_t		                f_lock;  /* f_ep_links, f_flags, no IRQ */
    #ifdef CONFIG_SMP
        int			                    f_sb_list_cpu;
    #endif
        atomic_long_t		            f_count;
        unsigned int 		            f_flags;
        fmode_t			                f_mode;
        loff_t			                f_pos;
        struct fown_struct	            f_owner;
        const struct cred	            *f_cred;
        struct file_ra_state	        f_ra;

        u64			f_version;
    #ifdef CONFIG_SECURITY
        void			                *f_security;
    #endif
        /* needed for tty driver, and maybe others */
        void			                *private_data;

    #ifdef CONFIG_EPOLL
        /* Used by fs/eventpoll.c to link all the hooks to this file */
        struct list_head	            f_ep_links;
    #endif /* #ifdef CONFIG_EPOLL */
        struct address_space	        *f_mapping;
    #ifdef CONFIG_DEBUG_WRITECOUNT
        unsigned long f_mnt_write_state;
    #endif
    };

    file yapısının tüm elemanlarının f_ öneki ile başlatılarak isimlendirildiğine de dikkat ediniz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıda çekirdeğin çeşitli versiyonlarına ilişkin file yapılarını verdik. Pekiyi bu yapının elemanları nelerdir 
    ve ne amaçla bu yapıda yer almaktadır? Aslında file yapısında çekirdeğin bir dosya üzerinde işlem yapabilmesi için 
    gerekli bilgiler bulunmaktadır. file yapısındaki elemanlar farklı konulara ilişkin olduğu için bu noktada bu elemanların 
    hepsini tek tek açıklamayacağız. Ancak file yapısının dosya işlemleri için kritik önemdeki bazı elemanları hakkında 
    açıklamalar yapmak istiyoruz. 

    Açık bir dosyanın open POSIX fonksiyonunda (yani sys_open sistem fonksiyonunda) kullanılan açış bayrakları (O_ ile 
    başlyan bayrakları kastediyoruz) file yapısının f_flags elemanında saklanamktadır. Örneğin open fonksiyonu ile dosya 
    şöyle açılmış olsun:

    fd = open("test.txt", O_RDWR|O_APPEND);

    Buradaki O_RDWR ve O_APPEND bayrakları file yapısının f_flags elemanında saklanmaktadır. Bu f_flags elemanının daha 
    çabuk işleme sokulabilecek yeniden düzenlenmiş hali yapının f_mode elemanında saklanmaktadır. (Bu f_mode elemanının 
    inode yapısındaki i_mode elemanıyla doğrudan bir ilgisi yoktur.) Okuma yazma işlemlerinin "dosya göstericisi (file 
    pointer)" denilen bir offset'en itibaren yapıldığını anımsayınız. İşte dosya göstericisinin konumu da file yapısının 
    f_pos elemanında tutulmaktadır. Dosya nesnelerini birden fazla betimleyici gösterebilmektedir. Örneğin dup ve dup2 
    POSIX fonksiyonları aynı dosya nesnesini gösteren farklı bir betimleyicinin oluşturulmasına yol açmaktadır. Benzer 
    biçimde fork işlemi sonrasında üst prosesin dosya betimleyici tablosununun betimleyicileri ile alt prosesin dosya 
    betimleyici tablosunun aynı numaralı betimleyicileri aynı dosya nesnesini gösteriyor durumda olur. Bu durumda close 
    fonksiyonu ile dosya kapatıldığında dosya hemen silinmez. Çünkü onu kullanan başka betimleyiciler de bulunuyor 
    olabilir. İşte file yapısının içerisindeki f_count elemanı o dosya nesnesinin kaç betimelyici tarafından gösterildiği 
    bilgisini tutmaktadır. Her betimleyici close fonksiyonu ile kapatıldığında bu sayaç 1 eksiltilir. Sayaç 0'a düştüğünde 
    dosya nesnesi silinir. İşte bu referans sayacı file yapısının içerisinde uzunca bir süredir f_count ismiyle bulunyordu. 
    Ancak çekirdeğin 6'lı güncel çekirdeklerde artık bu elemanın ismi f_ref biçimindedir. Aşağıda file yapısının gördüğümüz 
    önemli elemanlarının listesini veriyoruz:

    struct file {
        /* ... */
        	
        fmode_t				f_mode;
        unsigned int		f_flags;
        loff_t				f_pos;
        atomic_long_t		f_count;            /* file_ref_t f_ref */

        /*... */
    };
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi çekirdek açık bir dosya üzerinde read/write gibi işlemleri yaparken bir biçimde dosyanın diskteki bloklarına
    erişeceğine göre ve dosyanın son okunma tarihi, son güncelleme tarihi, dosya uzunluğu gibi bilgileri de güncelleyeceğine 
    göre bu bilgilere nasıl erişmektedir? İşte aslında dosyaların "uzunluk gibi, erişim hakları gibi, tarih-zaman bilgisi 
    gibi" meta data bilgileri diskte tutulmaktadır. (Ext dosya sistemlerinde bu bilgilerin diskte tutulduğu yere i-node 
    blok denilmektedir.) Çekirdek dosya açılırken dosyanın bu meta data bilgilerini diskten okuyarak bellekte inode 
    isimli bir yapı nesnesinin içerisine yerleştirmektedir. Biz dosyaların meta data bilgilerinin tutulduğu bu inode yapısı 
    türünden nesnelere "inode nesnesi" de diyeceğiz. inode yapısı dosyanın diskteki bilgilerini bellekte temsil eden bir 
    veri yapısıdır. Yani çekirdek dosyanın meta data bilgilerine erişmek için sürekli diske başvurmamaktadır. O bilgileri 
    dosya açılırken diskten çekip inode nesnesi biçiminde bellekta saklamaktadır. Bu noktada hikayeye "inode nesnesi" 
    denilen yeni bir aktörün daha katıldığına dikkat ediniz.
    
    Pekiyi bir dosya açıldığında dosyanın dosya sistemindeki yeri (örneğin yol ifadesi) çekirdek tarafından nasıl 
    saklanmaktadır? İşte çekirdek her dosya açıldığında o dosyanın dizin girişi bilgilerini (yani dosya sisteminde nerede 
    olduğu bilgisini ve bazı diğer bilgileri) ismine "dentry" denilen bir yapı nesnesine yerleştirmektedir. Açılmış olan 
    dosyanın dosya sistemindeki yerine ilişkin bu nesnelere biz "dentry nesneleri" diyeceğiz. Bu noktada hikayaye "dentry" 
    isimli başka bir aktörün daha katıldığını görüyorsunuz. 
    
    Şimdi dosya sistemine ilişkin nesneler hakkında bir özet yapalım:

    Dosya Nesnesi (struct file): Açılmış dosyalar üzerinde işlem yapmak için gereken tüm bilgilerin tutulfuğu nesne.
    Inode Nesnesi (struct inode): Dosyanın diskteki meta data bilgilerini tutan nesne. 
    Dentry Nesnesi (struct dentry) : Dosyanın dosya sisteni üzerinde yerini ve buna ilişkin bazı bilgileri tutan nesne.

    Pekiyi dosyanın ilişkin olduğu inode nesnesine ve dentry nesnesine dosya nesnesi yoluyla nasıl erişilmektedir? 
    Uzunca bir süre (2.6'ya kadar ve 2.6'lı versiyonlar da dahil olmak üzere) dosya nesnesinin (file yapısının) içerisinde 
    dentry nesnesinin adresi, dentry nesnesinin içerisinde de o dizin girişinin inode nesnesinin adresi tutuluyordu:

    ┌──────────┐        ┌──────────┐         ┌──────────┐
    │  file    │ -----> │ dentry   │ ----->  │  inode   │
    └──────────┘        └──────────┘         └──────────┘

    Ancak daha sonraları dosya nesnesinden hareketle inode nesnesine daha kolay bir erişimin sağlanabilmesi için dosya
    nesnesinin içerisinde de doğrudan inode nesnesinin adresi tutulmaya başlanmıştır.

    ┌──────────┐        ┌──────────┐         ┌──────────┐
    │  file    │ -----> │ dentry   │ ----->  │  inode   │
    │          │        │          │         │          │
    │          │ --------------------------> │          │
    └──────────┘        └──────────┘         └──────────┘

    UNIX/Linux sistemlerinde bir dosya sistemi kök dizinde bir yere maount edilebilmektedir. Yani aslında bir yol ifadesine 
    ilişkin dosya ile diğer bir yol ifadesine ilişkin dosya farklı fiziksel aygıtlarda bulunuyor olabilir. Çekirdeğin bazı 
    durumlarda bir dosyanın hangi dosya sisteminin içerisinde bulunduğunu anlaması da gerekebilmektedir. Bu bilgilere dosyanın 
    "mount" bilgileri denilmektedir. Eskiden çekirdeğin 2.2 versiyonunda dosyanın mount bilgileri dentry nesnesi içerisinde 
    tutuluyordu. 2.4 ile birlikte dosyanın mount bilgileri daha düzenli bir biçimde file yapısının (dosya nesnesinin) vfsmount 
    isimli yapı türünden f_vfsmnt elemanında tutulmaya başlandı. 2.6 çekirdeklerinden itibaren de dosyanın dentry nesnesinin
    adresiyle vfsmount bilgileri path isimli bir yapıya yerleştirilmiş ve file yapısının içerisindeki f_path elemanında tutulmaya 
    başlanmıştır. 2.6 ve sonrasına ilişkin çekirdeklerdeki durum şöyledir:

    struct file {
        /* ... */
        	
        fmode_t				f_mode;
        unsigned int		f_flags;
        loff_t				f_pos;
        atomic_long_t		f_count;            /* file_ref_t f_ref */
        struct path			f_path;
        struct inode		*f_inode;

        /*... */
    };

    path yapısı da şöyle bildirilmiştir:

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;

    Buradaki __randomize_layout belirleyicisi sonraları eklenmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz aynı dosyayı birden fazla kez open fonksiyonuyla açmış olalım. Örneğin:

    fd1 = open("test.txt", O_RDONLY);
    fd2 = open("test.txt", O_RDONLY);

    Bu durumda ne olacaktır? İşte çekirdek farklı dosya da olsa aynı dosya da olsa her açılan dosya için ayrı bir dosya 
    nesnesi (yani struct file nesnesi) oluşturmaktadır. (Çünkü örneğin aynı dosyayı birden fazla kez açtığımızda bu 
    dosyaların hepsinin dosya göstericileri faklı olmaktadır. Dosya göstericilerinin de dosya nesnesi içerisinde saklandığını 
    anımsayınız.) Ancak örneğimizdeki iki dosya neticide aslında diskte aynı dosyayı belirtmektedir. O halde bu iki dosya 
    için ayrı dentry ve inode nesneslerinin oluşturulmasına gerek yoktur. Kaldı ki farklı prosesler de aynı dosyayı açmış 
    olabilirler. Bunlar için de ayrı dentry ve inode nesnelerinin oluşturulumasına gerek yoktur. O halde her açılan yeni 
    dosya için çekirdek yeni bir dosya nesnesi yarattığı halde aynı dosyalar açıldığında bu dosyalar için tek bir dentry 
    ve inode nesnesi oluşturmaktadır. Bunun için tabii open fonksiynuyla bir dosya açıldığında çekirdeğin "bu dosyaya 
    ilişkin dentry nesnesi ve inode nesnesi daha önce yaratılmış mı" diye bir araştırma yapması gerekmektedir. Bu araştırmayı
    yapabilmesi için de çekirdeğin bir biçimde bütün yaratılmış olan dentry ve inode nesnelerini bir yerde tutması gerekir. 

    İşte çekirdek dentry ve inode neneleri için ayrı önbellek (cache) sistemleri oluşturmaktadır. Bunlara Linux sistemlerinde 
    "dentry önbelleği (dentry cache)" ve "inode önbelleği (inode cache)" denilmektedir. Bir dosya açıldığında çekirdek 
    o dosyaya ilişkin dentry ve inode nesneleri zaten bu önbellek sistemlerinde varsa hiç diske gitmeden doğrudan bu 
    önbelleklerden onları alıp kullanmaktadır. Bir dosyayı onu açmış olan bütün prosesler kapatmış olsa bile o dosyaya 
    ilişkin dentry ve inode nesneleri bu önbellek sistemlerinde kalmaya devam eder. Çünkü dosyalar (özellikle bazı merkezi 
    dosyalar) bir kere değil farklı prosesler tarafından defalarca açılıp kullanılabilmektedir. (Örneğin "/etc/passwd" 
    dosyası pek çok proses tarafından dolaylı bir biçimde açılıp kullanılabilmektedir.) Bu tür durumlarda onların bu 
    önbellekler içerisinde biriktirilmesi sistem performansını oldukça iyileştirmektedir. Tabii bu önbellek sistemlerinin 
    de belli bir büyüklüğü vardır. Bu önbellek sistemleri dolduğunda dentry ve inode nesnelerinin bazıları bu önbelleklerden 
    atılmaktadır. Linux'taki bu tür önbellek sistemlerinde önbellekten çıkarma için genel olarak "LRU (Least Recently 
    Used)" denilen "önbellek yer değiştirme (cache replacement) algoritması" işletilmektedir. Yani önbellekten toplamda 
    en az kullanılanlar değil "son zamanalarda en az kullanılanlar" çıkartılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bu noktada "dentry önbelleği (dentry cache)" ve "inode önbelleği (inode cache)" ile "sayfa önbelleği (page cache)"  
    arasındaki ilişkiye de değinelim. Sayfa önbelleği disk blokları temeleninde organize edilen ve her türlü disk bloğunun 
    transfer edilmesinde kullanılan aşağı seviyeli bir önbellek sistemidir. Halbuki dentry ve inode önbellekleri yalnızca
    dentry ve inode elemanalrından oluşan önbelleklerdir. Bir dosya açıldığında eğer o dosyaya ilişkin dentry ve inode 
    nesneleri dentry ve inode önbelleklerinde yoksa diskten elde edilmektedir. Tabii bu amaçla disk okuması yapılmadna 
    önce bu disk bloklarının sayfa önbelleğinde olup olmadığına da bakılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Geldiğimiz noktaya kadarki konuları dikkate aldığımızda open fonksiyonuyla bir dosyanın açılması durumunda sırasıyla 
    şunların yapıldığını söyleyebiliriz:

    1) Prosesin dosya betimleyici tablosunda boş bir betimleyici hızlı bir biçimde bulunur.
    2) Bir dosya nesnesi (struct file nesnesi) yaratılır ve elemanalarına gerekli ilkdeğerler verilir.
    3) Dosyaya ilişkin dentry nesnesi ve inode nesnesi dentry önbelleğinde ve inode önbelleğinde yoksa diskte arama 
    yapılarak yaratılır ve bunların adresleri dosya nesnesine yerleştirilir.
    4) Dosya nesnesinin adresi dosya beyimeleyici tablosuna (fd dizisine) yerleştirilir ve yerleştirilen dizi indeksi 
    dosya betimleyicisi olarak geri döndürülür.

    Ancak dosya açılırken henüz ele almadığımız başka süreçler de işin içerisine girmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir prosesin "kök dizininin (root directory)" ve "çalışma dizinin (current working directory)" proses kontrol 
    bloğunda yani task_struct nesnesinde tutulduğunu belirtmiştik. Güncel çekirdeklerde bu bilgi şöyle tutulmaktadır:

    struct task_struct {
        /* ... */

        struct fs_struct *fs;

        /* ... */
    };

    struct fs_struct {
        /* ... */
        struct path root, pwd;

        /* ... */  
    } __randomize_layout;

    path yapısını daha önce vermiştik:

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;

    Görüldüğü gibi çekirdek prosesin kök dizinin ve çalışma dizininini yol ifadesi biçiminde değil dentry nesnesi 
    biçiminde tutmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        27. Ders 19/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi dosya sisteminin temel veri yapıları üzerinde görmüş olduğumuz konulara ilişkin bazı testler yapalım. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------  
    İlk örneğimizde bir dosya betimleyicisine ilişkin dosya nesnesini elde ederek onun f_pos elemanını yazdırmaya çalışalım. 
    Dosya nesnesindeki f_pos elemanının dosya göstericisinin değerini tuttuğunu anımsayınız. Bu işlemi bir aygıt sürücü 
    yazarak ioctl yoluyla gerçekleştirebiliriz. (Kursumuzda kullandığımız Linux makinedeki çekirdek sürümü 6.9.2'dir.) 
    Bunun şçin oluşturduğumuz ioctl fonksiyonuna dikkat ediniz:

    static long ioctl_test1(struct file *filp, unsigned long arg)
    {
        struct file *f;
        int fd = (int) arg;

        if (fd < 0 || fd >= current->files->fdt->max_fds) {
            printk(KERN_INFO "file descriptor is not valid!..\n");
            return -EBADF;
        }

        f = current->files->fdt->fd[fd];
        if (f == NULL) {
            printk(KERN_INFO "file descriptor is not valid!..\n");
            return -EBADF;
        }

        printk(KERN_INFO "File pointer offset: %lld\n", (long long)f->f_pos);

        return 0;
    }

    Burada daha önceden de belirttiğimiz gibi dosya betimleyicisinden hareketle dosya nesnesine current->files->fdt->fd[fd]
    ifadesiyle erişilmiştir. Ancak erişmeden önce fd ile belirtilen betimleyicinin o anda çekirdek tarafından tahsis 
    edilmiş olan dosya betimeleyici tablosunun uzunluğundan büyük olup olmadığına da bakılmıştır:

    if (fd < 0 || fd >= current->files->fdt->max_fds) {
        printk(KERN_INFO "file descriptor is not valid!..\n");
        return -EBADF;
    }

    Burada önemli bir nokta üzerinde durmak istiyoruz. Biz test amacıyla dosya betimleyicisinden hareketle dosya 
    nesnesine current->files->fdt->fd[fd] ifadesiyle eriştik. Ancak bu erişim aslında güvenli değildir. Çünkü tam bu 
    erişim yapılırken eğer dosya betimleyici tablosu büyütülürse ya da bu betimleyici üzerinde işlem yapılırsa bizim 
    kodumuz kararsız bir durumla karşı karşıya kalır. Tabii çekirdek durup dururken dosya betimleyici tablomuz üzerinde 
    işlem yapmaz. Çekirdek ancak biz bir dosya işlemi yaptığımızda bizim dosya betimleyici tablomuza erişmektedir. Çünkü 
    dosya betimleyici tablosu prosese özgüdür. O halde biz ioctl çağrısı yaparken aynı zamanda başka bir thread'te 
    (ya da alt proseste) dosya işlemi yapmıyorsak bir sorun da oluşmayacaktır. Ancak genel olarak çekirdeğin uyguladığı 
    senkronizyan mekanizmasına uygun hareket etmek gerekir. Linux çekirdeklerinin bir süredir "kilitsiz (lock-free)" 
    veri yapılarından olan RCU mekanizmasının kullandığını belirtmiştik. Biz bu veri yapısı üzerindeki ayrıntıları 
    "çekirdeğin senkronizasyon mekanizmalarını" anlattığımız bölümde açıklayacağız. 

    Burada diğer bir nokta da nesnelerin referans sayaçlarıyla ilgilidir. Daha önceden de belirttiğimiz gibi çekirdek 
    nesneleri farklı amaçlarla birden fazla kaynak tarafından kullanılabilmektedir. Bu tür çekirdek nesnelerinde bu 
    nedenle hep bir sayaç tutulmaktadır. Bir çekirdek nesnesi üzerinde işlem yapacak kişi eğer bu sayacı artırmazsa 
    çekirdek onu boşaltabilir. İşlem yapan kişi de tanımsız davranışla karşılaşabilir. Tabii yukarıdaki gibi prosese
    özgü test işlemlerinde çekirdek bizim programımız talep etmedikten sonra kapatma işlemleri yapmayacaktır. Ancak 
    genel olarak bu tür durumlarda "çekirdek kaynağı boşaltmasın diye" nesnenin referans sayacı artırılmalı, kullanım 
    bittikten sonra da azaltılmalıdır. Daha önceden de  belirttiğimiz gibi Linux çekirdeklerinde sayacı artırarak nesneyi 
    elde eden fonksiyonlar "get" soneki ile sayacı azaltarak nesneyi bırakan fonksiyonlar ise "put" soneki ile isimlendirilmiştir. 
    Örneğin fget isimli yüksek seviyeli çekirdek fonksiyonu dosya betimelicisinden hareketle bize dosya nesnesini 
    RCU mekanizmasını kullanarak dosya nesnesinin sayacını da artırarak vermektedir. fput fonksiyonu da sayacı azaltarak
    dosya nesnesini geri bırakmaktadır. fget ve fput fonksiyonlarının prototipleri şöyledir:

    struct file *fget(unsigned int fd);
    void fput(struct file *);

    Bu fonksiyonların prototipleri "include/linux.file.h" dosyası içerisindedir. Güncel çekirdeklerde fget fonksiyonun 
    tanımlaması "fs/file.c" dosyası içerisinde fput fonksiyonun tanımlaması ise "fs/file_table.c" içerisinde yapılmıştır. 
    fget ve fput fonksiyonları export edildiği için aygıt sürücüler içerisinde de kullanılabilmektedir. Bu durumda biz 
    yukarıdaki testi daha basit bir biçimde fget ve fput fonksiyonlarını kullanarak aşağıdaki gibi de yapabiliriz:

    static long ioctl_test2(struct file *filp, unsigned long arg)
    {
        struct file *f;
        int fd = (int) arg;

        if ((f = fget(fd)) == NULL) {
            printk(KERN_INFO "file descriptor is not valid!..\n");
            return -BADF;
        }
        
        printk(KERN_INFO "File pointer offset: %ld\n", (long)f->f_pos);

        fput(f);

        return 0;
    }
        
    Güncel çekirdeklere fget ve fput işlemlerini daha etkin gerçekleştirmek için fdget ve fdput isimli fonksiyonlar da
    eklenmiştir. Bu fonksiyonların ptototipleri şöyledir:

    static struct fd fdget(unsigned int fd);
    static void fdput(struct fd fd);

    Kursun yapıldığı makinede bulunan 6.9.2 çekirdeğinde struct fd yapısı şöyle bildirilmiştir:

    struct fd {
        struct file *file;
        unsigned int flags;
    };
    
    Ancak en yeni çekirdeklerde fd yapısı şöyledir:

    struct fd {
        unsigned long word;
    };

    En yeni çekirdeklerde dosya nesnesine erişim için yalnızca fdget değil fd_file fonksiyonun da kullanılması gerekmektedir. 
    Örneğin:

    struct fd f = fdget(fd);
	struct file *file = fd_file(f);

    Güncel çekirdeklerde artık fdget ve fdput fonksiyonları export edilmiştir. Ancak kurusumuzun yapıldığı 6.9.2 
    çekirdeklerinde bu fonksiyonlar export edilmemişti. 

    Her ne kadar yeni çekirdeklerde fdget ve fdput daha hızlı çalışıyorsa da fget ve fput basit arayüzü ve kullanım 
    kolaylığı ve eskiden beri aynı biçimde bulunması nedeniyle tercih edilebilir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        28. Ders 25/10/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dup POSIX fonksiyonun işlevini yerine getiren (yani sys_dup sistem fonksiyonu gibi çalışan) bir fonksiyonu 
    kendimiz yazalım. Çekirdeği yeniden derlememek için bu işlemi de bir aygıt sürücüye yaptırabiliriz. Bunun için bir 
    ioctl kodu oluşturabiliriz. Bu ioctl kodu kullanıcı modundan çağrılırken ioctl fonksiyonunun üçüncü parametresine
    aşağıdaki gibi bir yapı nesnesinin adresini geçebiliriz:

    struct FDDUP {
        int fd;
        int fd_dup;
    };

    Burada yapının fd elemanı çiftlenecek dosya betimleyicisini belirtmektedir. fd_dup elemanı ise çiftleme sonucunda 
    elde edilecek yeni betimleyiciyi belirtmektedir. (Yani biz fonksiyonu çağırmadan önce yapının fd elemanına çiftlenecek 
    betimleyiciyi yerleştireceğiz, fonksiyon da bunu çiftleyip yeni betimleyiciyi yapının fd_dup elemanına yerleştirecek.)
    Bu elemanların dup fonksiyonu ile ilişkisini şöyle temsil edebiliriz:

    fd_dup = dup(fd);
    
    Dosya betimleyicisini çiftleyen ioctl kodu düz bir biçimde test amaçlı olarak (yani bazı kusurlarla) şöyle 
    yazılabilir:

    static long ioctl_test3(struct file *filp, unsigned long arg)
    {
        struct file *f;
        struct fdtable *fdt;
        struct file **fd_table;
        struct FDDUP fddup;
        int i;

        f = NULL;

        if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
            return -EFAULT;

        fdt = current->files->fdt;
        fd_table = fdt->fd;

        if (fddup.fd < 0 || fddup.fd >= fdt->max_fds)
            return -EBADF;
        if (fd_table[fddup.fd] == NULL)
            return -EBADF;

        for (i = 0; i < fdt->max_fds; ++i)
            if (fd_table[i] == NULL) {
                f = fd_table[fddup.fd];	
                fd_table[i] = f;
                ++f->f_count.counter;			/* atomic_long_inc(&f->f_count); */
                fddup.fd_dup = i;
                break;
            }
        if (f == NULL)
            return -EMFILE;

        if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0)
            return -EFAULT;
        
        return 0;
    }

    Fonksiyonumuzda önce kullanıcı modundaki fddup nesnesini çekirdek moduna copy_from_user fonksiyonu ile kopyaldık:

    if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
        return -EFAULT;

    Sonra fonksiyonumuzda çiftlenecek dosya betimelycisinin geçerli bir betimleyici olup olamdığına baktık:

    fdt = current->files->fdt;
    fd_table = fdt->fd;

    if (fddup.fd < 0 || fddup.fd >= fdt->max_fds)
        return -EBADF;
    if (fd_table[fddup.fd] == NULL)
        return -EBADF;

    Bu işlemden sonra fonksiyonumuzda bir döngü içerisinde ilk boş betimleyiciyi bulup, çiftlenecek olan betimeleyicinin
    gösterdiği dosya nesnesinin adresini bu boş betimleyiciye yerleştirdik. Bu tür işlemlerde dosya nesnesinin sayacının
    artırılması gerektiğini anımsayınız:

	for (i = 0; i < fdt->max_fds; ++i)
		if (fd_table[i] == NULL) {
			f = fd_table[fddup.fd];	
			fd_table[i] = f;
			++f->f_count.counter;			/* atomic_long_inc(&f->f_count); */
			fddup.fd_dup = i;
			break;
		}
	if (f == NULL)
		return -EMFILE;

    Kursumuzun yapıldığı 6.9.2 çekirdeğinde dosya nesnesinin sayacının file yapısının içerisinde şöyle tutulduğunu yeniden
    anımsatmak istiyoruz:

    struct file {
        /* ... */

        atomic_long_t		f_count;

        /* ... */
    };

    Burada aslında atomic_long_t türü bir yapı biçiminde bildirilmiştir:

    typedef struct {
        long counter;
    } atomic_long_t;

    Neden bu sayacın doğrudan long bir yapı elemanında tutulmayıp başka bir yapının elemanı yapıldığını merak eedebilirsiniz. 
    Bunun amacı aslında bu elemanın gizlenmek istenmesidir. Bu atomic_long_t türüyle belirtilen değerler üzerinde işlemler
    atomik düzeyde özel fonksiyonlarla yapılmaktadır. Aslında atomic_long_t türünün atomic_t isimi int versiyonu da vardır. 
    Eskiden çekirdeklerde atomic_long_t yerine yalnızca atomic_t türü bulunuyordu. Sonra atomic_long_t türü de eklendi. 
    atomic_long_t türü üzerinde atomik işlem yapan fonksiyonların bizim için bu konu bağlamında önemli olan birkaçını 
    aşağıda veriyoruz:

    void atomic_long_set(atomic_long_t *v, long i);
    long atomic_long_read(const atomic_long_t *v);
    void atomic_long_inc(atomic_long_t *v);
    void atomic_long_dec(atomic_long_t *v);

    atomic_t ve atomic_long_t türleri hakkında ayrıntıları başka bir başlıkta ele alacağız. 

    Fonksiyonumuzda en sonunda yeniden çekirdek modundaki fddup nesnesi kullanıcı modundaki prosesin fddup nesnesine
    copy_to_user fonksiyonu ile kopyalanmıştır:

    if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0)
            return -EFAULT;

    Fonksiyonumuzdaki döngüde aslında mantıksal bir kusur da vardır. Biz döngüde yalnızca o andaki dosya betimleyici 
    tablosunda arama yaptık. Halbuki çekirdek aslında başlangıçta küçük bir dosya betimleyci tablosunu tutarken daha 
    sonra bunu gerektiğinde proses limitinin izin verdiği kadar yükseltebilmektedir. Halbuki bizim kodumuz bunu göz 
    ardı etmiştir. Yani aslında işin başında anımsayacağınız gibi max_fds 64 bit sistemlerde 64 elemanlı bir dosya 
    betimeleyici tablousunun uzunluğunu tutmaktadır. Halbuki default durumda prosesin dosya betimleyici tablosu 1024 
    geçerli uzunluğa sahiptir. O halde aslında bizim max_fds ile belirtilen dosya betimleyici tablosu elemanlarının 
    hepsi doluysa dosya betimleyici tablosunu prosesin kaynak limitlerinde belirtilen değere (default durumda 1024) 
    büyütüp arama işlemini oradan devam ettirmemiz gerekirdi. Ancak bunu biz yapmadık.

    Fonksiyonumuzda ilk boş betimleyicinin aranmasının düz mantıkla bir for döngüsü içerisinde -tıpkı çekirdeğin 
    ilkel versiyonlarındaki gibi- yapıldığına dikkat ediniz. Aslında testi yaptığımız makinedeki Linux çekirdeğinde 
    anımsayacağınız gibi ilk boş betimleyiciyi hızlı bir biçimde bulabilmek için iki düzey bitmap kullanılıyordu. 
    Bu versiyonlarda ilk boş betimleyiciyi bulan get_unused_fd_flags isimli daha yüksek seviyeli bir çeekirdek 
    fonksiyonu bulunmaktadır. Bu fonksiyon export edildiği için aygıt sürücülerden de kullanılabilmektedir. Fonksiyon 
    şöyle yazılmıştır:

    int get_unused_fd_flags(unsigned flags)
    {
        return __get_unused_fd_flags(flags, rlimit(RLIMIT_NOFILE));
    }
    EXPORT_SYMBOL(get_unused_fd_flags);

    Fonksiyonun başka bir fonksiyonu çağırdığını görüyorsunuz. fonksiyonun flags parametresi boş dosya betimleycisini 
    bulurken aynı zamanda dosyaya ilişkin O_CLOEXEC bayrağının da set edilmesini sağlayabilmektedir. Tabii böyle bir 
    şeyi istemiyorsanız bu parametreye 0 geçebilirsiniz. (Yani bu fonksiyonun parametresi ya O_CLOEXEC biçiminde ya da
    0 biçiminde geçilebilmektedir.) get_unused_fd_flags fonksiyonu aynı zamanda "gerektiğinde dosya betimleyici tablosunu 
    büyütme işlemini de" kendisi yapmaktadır. Dolayısıyla bu yüksek seviyeli fonksiyon hem aygıt sürücüler tarafından 
    hem de çekirdek kodları üzerinde değişilik yapacak kişiler tarafından bu tür durumlarda tercih edilmektedir. 
    get_unused_fd_flags fonksiyonu başarı durumunda yeni boş betimleyiciye başarısızlık durumunda ise -EMFILE ya da -ENOMEM 
    değerlerinden birine geri dönmektedir. Eskiden (örneğm çekirdeğin 2.2, 2.4 ve 2.6 versiyonlarında) get_unused_fd_flags 
    fonksiyonu yerine flags'siz get_unused_fd fonksiyonu bulunuyordu:

    int get_unused_fd(void);

    get_unused_fd_flags fonksiyonu çekirdeğin 2.6.23 versiyonunda eklenmiştir. 

    Aslında çekirdek içerisinde RCU mekanizması eşliğinde files_struct yapısının adresini alarak current->files->fdt 
    gösterisini elde eden files_fdtable isimli bir çekirdek fonksiyonu da bulunmaktadır. Bu durumda dosya betimleyici 
    tablosuna şöyle de erişebilirdik:

    fdt = files_fdtable(current->files);
    fd_table = fdt->fd;

    RCU mekanizması eşliğinde dosya nesnesini (strcut file nesnesini) dosya betimelyeici tablosuna yerleştiren 
    fd_install isimli export edilmiş yüksek seviyeli bir çekirdek fonksiyonu da bulunmaktadır:

    void fd_install(unsigned int fd, struct file *file);

    Eğer işlemler RCU mekanizmasına uyumlu olacaksa dosya betimeleyicisini dosya betimleyici tablosuna yerleştirilmesi 
    için fd_install fonksiyonu tercih edilmelidir. 

    Böylece yukarıdaki fonksiyonu şu hale getirebiliriz:
 
    static long ioctl_test4(struct file *filp, unsigned long arg)
    {
        struct file *f;
        struct FDDUP fddup;
        
        if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
            return -EFAULT;

        if ((f = fget(fddup.fd)) == NULL)
            return -EBADF;
            
        if ((fddup.fd_dup = get_unused_fd_flags(0)) < 0) {
            fput(f);
            return fddup.fd_dup;
        }

        if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0) {
            fput(f);
            return -EFAULT;
        }

        fd_install(fddup.fd_dup, f);

        /*
        fd_table = current->files->fdt->fd;
        fd_table[fddup.fd_dup] = f;	
        */

        return 0;
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıdaki test işlemlerine ilişkin aygıt sürücü kodunu bir bütün olarak aşağıda veriyoruz. 
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST1		        _IO(TEST_DRIVER_MAGIC, 0)
#define IOC_TEST2		        _IO(TEST_DRIVER_MAGIC, 1)
#define IOC_TEST3		        _IO(TEST_DRIVER_MAGIC, 3)
#define IOC_TEST4		        _IO(TEST_DRIVER_MAGIC, 4)

struct FDDUP {
    int fd;
    int fd_dup;
};

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include <linux/fdtable.h>
#include <linux/file.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test1(struct file *filp, unsigned long arg);
static long ioctl_test2(struct file *filp, unsigned long arg);
static long ioctl_test3(struct file *filp, unsigned long arg);
static long ioctl_test4(struct file *filp, unsigned long arg);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
    .unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
    long result;
	
    printk(KERN_INFO "test_driver_ioctl...\n");

    switch (cmd) {
        case IOC_TEST1:
            result = ioctl_test1(filp, arg);
            break;  
		case IOC_TEST2:
            result = ioctl_test2(filp, arg);
            break;  
		case IOC_TEST3:
            result = ioctl_test3(filp, arg);
            break;  
		case IOC_TEST4:
            result = ioctl_test4(filp, arg);
            break;  
        default:
            result = -ENOTTY;
    }

    return result;
}

static long ioctl_test1(struct file *filp, unsigned long arg)
{
	struct file *f;
	int fd = (int) arg;

	if (fd < 0 || fd >= current->files->fdt->max_fds) {
		printk(KERN_INFO "file descriptor is not valid!..\n");
		return -EBADF;
	}

	f = current->files->fdt->fd[fd];
	if (f == NULL) {
		printk(KERN_INFO "file descriptor is not valid!..\n");
		return -EBADF;
	}

	printk(KERN_INFO "File pointer offset: %ld\n", (long)f->f_pos);

    return 0;
}

static long ioctl_test2(struct file *filp, unsigned long arg)
{
	struct file *f;
	int fd = (int) arg;

	if ((f = fget(fd)) == NULL) {
		printk(KERN_INFO "file descriptor is not valid!..\n");
		return -EBADF;
	}
	
	printk(KERN_INFO "File pointer offset: %ld\n", (long)f->f_pos);

	fput(f);

    return 0;
}

static long ioctl_test3(struct file *filp, unsigned long arg)
{
	struct file *f;
	struct fdtable *fdt;
	struct file **fd_table;
	struct FDDUP fddup;
	int i;

	f = NULL;

	if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
		return -EFAULT;

	fdt = current->files->fdt;
	fd_table = fdt->fd;

	if (fddup.fd < 0 || fddup.fd >= fdt->max_fds)
		return -EBADF;
	if (fd_table[fddup.fd] == NULL)
		return -EBADF;

	for (i = 0; i < fdt->max_fds; ++i)
		if (fd_table[i] == NULL) {
			f = fd_table[fddup.fd];	
			fd_table[i] = f;
			++f->f_count.counter;			/* atomic_long_inc(&f->f_count); */
			fddup.fd_dup = i;
			break;
		}
	if (f == NULL)
		return -EMFILE;

	if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0)
		return -EFAULT;
	
    return 0;
}

static long ioctl_test4(struct file *filp, unsigned long arg)
{
	struct file *f;
	struct FDDUP fddup;
	
	if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
		return -EFAULT;

	if ((f = fget(fddup.fd)) == NULL)
		return -EBADF;
		
	if ((fddup.fd_dup = get_unused_fd_flags(0)) < 0) {
		fput(f);
		return fddup.fd_dup;
	}

	if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0) {
		fput(f);
		return -EFAULT;
	}

	fd_install(fddup.fd_dup, f);

	/*
	fd_table = current->files->fdt->fd;
	fd_table[fddup.fd_dup] = f;	
	*/

	return 0;
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include "test-driver.h"

void exit_sys(const char *msg);


int main(int argc, char *argv[])
{
    int fd_dev;
    int fd;
    struct FDDUP fddup;
    off_t pos;
    
    if (argc != 2) {
        fprintf(stderr, "wrong number of arguments!..\n");
        exit(EXIT_FAILURE);
    }
       
    if ((fd_dev = open("test-driver", O_RDONLY)) == -1)
        exit_sys("open");
   
    if ((fd = open(argv[1], O_RDONLY)) == -1)
        exit_sys("open");
    lseek(fd, 100, 0);
   
    fddup.fd = fd;
    if (ioctl(fd_dev, IOC_TEST4, &fddup) == -1)
        exit_sys("ioctl");

    pos = lseek(fddup.fd_dup, 0, 1);
    printf("%jd\n", (intmax_t)pos);

    close(fd);
    close(fddup.fd);
    close(fd_dev);

    return 0;
}

void exit_sys(const char *msg)
{
    perror(msg);

    exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi dosyanın açılması ve kapatılması sürecini temel düzeyde inceleyelim. Bir dosya açılırken çekirdek kabaca neler 
    yapmaktadır? Bilindiği gibi kullanıcı modundan open POSIX fonksiyonuyle bir dosya açılmak istendiğinde open fonksiyonu 
    sys_open sistem fonksiyonunu çağırmaktadır. Dosya açma işlemi sys_open fonksiyonu tarafından yapılmaktadır. 2.6 
    çekirdeğinden itibaren sys_open sistem fonksiyonun kodlarında önemli bir değişiklik olmamıştır. Güncel çekirdeklerde 
    sys_open fonksiyonu "fs/open.c"" dosyası içerisinde şöyle tanımlanmıştır:

    SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)
    {
        if (force_o_largefile())
            flags |= O_LARGEFILE;
        return do_sys_open(AT_FDCWD, filename, flags, mode);
    }

    2.6 çekirdeğinden bu yana asıl açım işlemleri do_sys_open fonksiyonu tarafından yapılmaktadır. Ancak fonksiyonların 
    kodları üzerinde zamanla çeşitli değişiklikler yapılmıştır. do_sys_open fonksiyonu export edilmemiştir. Fonksiyonun 
    prototiği şöyledir:

    int do_sys_open(int dfd, const char __user *filename, int flags, mode_t mode);
    
    do_sys_open fonksiyonu aynı zamanda openat fonksiyonu (yani sys_openat sistem fonksyonu) tarafından da çağrılan ortak 
    bir fonksiyondur. Bu nedenle fonksiyonun birinci parametresi openat fonksiyonundaki "göreli yol ifadeleri için orijin 
    olarak kullanılacak dosya betimleyicisini" belirtmektedir. do_sys_open fonksiyonun birinci parametresine dikkat eediniz. 
    Bu parametreye AT_FDCWD özel değeri geçilmiştir. openat fonksiyonunun birinci parametresindeki betimleyiciye AT_FDCWD 
    özel değeri geçilirse fonksiyonun tamamen open gibi gibi davrandığını anımsayınız. sys_openat sistem fonksiyonu da 
    güncel çekirdeklerde "fs/open.c" dosaysında şöyle tanımlanmıştır:

    SYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags,
		umode_t, mode)
    {
        if (force_o_largefile())
            flags |= O_LARGEFILE;
        return do_sys_open(dfd, filename, flags, mode);
    }

    Görüldüğü gibi openat sistem fonksiyonu da aslında do_sys_open fonksiyonunu çağırmaktadır.

    Şimdi çağrıu zincirin dvam edelim. Güncel çekirdeklerde do_sys_open fonksiyonu "fs/open.c" dosyasında şöyle 
    tanımanmıştır:

    long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)
    {
        struct open_how how = build_open_how(flags, mode);
        return do_sys_openat2(dfd, filename, &how);
    }

    Fonksiyonda önce build_open_how fonksiyonu çağrılmıştır. Bu fonksiyon açış bayraklarını ve erişim haklarını bazı 
    kontroller uygulayarak ek bir yapı içinde toplamaktadır. Fonksiyonun bu işlemden sonra do_sys_openat2 fonksiyonunu 
    çağırdığını görüyorsunuz. Güncel çekirdeklerde bu fonksiyon da "fs/open.c" dosyasında şöyle tanımlanmıştır:

    static long do_sys_openat2(int dfd, const char __user *filename,
			   struct open_how *how)
    {
        struct open_flags op;
        int fd = build_open_flags(how, &op);
        struct filename *tmp;

        if (fd)
            return fd;

        tmp = getname(filename);
        if (IS_ERR(tmp))
            return PTR_ERR(tmp);

        fd = get_unused_fd_flags(how->flags);
        if (fd >= 0) {
            struct file *f = do_filp_open(dfd, tmp, &op);
            if (IS_ERR(f)) {
                put_unused_fd(fd);
                fd = PTR_ERR(f);
            } else {
                fd_install(fd, f);
            }
        }
        putname(tmp);
        return fd;
    }

    Fonksiyondaki bazı ayrıntıları göz ardı edersek birkaç noktaya dikkatinizi çekmek istiyoruz. build_open_flags fonksiyonu 
    open fonksiyonunda verilen bayrakları çekirdeğin işleyebilmesi için daha uygun bir hale getirmektedir. Bu fonksiyon 
    tarafından daha uygun hale getirilen açış bayrakları open_flags isimli bir yapı nesnesinde saklanmaktadır. getname 
    fonksiyonu ise kullanıcı modundaki yol ifadesini çekirdek moduna kopyalayarak çekirdek bir yapıya yerleştirmektedir. 
    Bundan sonra yukarıda görmüş olduğumuz get_unused_fd_flags fonksiyonu çağrılarak ilk boş betimleyici elde edilmiştir. 
    (POSIX standartlarına göre open fonksiyonunun dosya betimleyici tablosundaki ilk boş betimleyiciyi vermek zorunda 
    oldupuna dikkat ediniz.) Bundan sonra bütün geri kalan önemli işlemler do_filp_open fonksiyonu tarafından yapılmaktadır. 
    do_sys_open ve do_sys_openat2 fonksiyonları aygıt sürücüler için export edilmemiştir. Ancak do_filp open fonksiyonu 
    export edilmiş bir fonksiyondur. Güncel çekirdeklerdeki bu çağrı silsilesini şöyle gösterebiliriz:

    ┌────────────┐
    │  sys_open  │
    └───────┬────┘
            │
            ├──► ┌──────────────┐
            │    │ do_sys_open  │
            │    └──────┬───────┘
            │           │
            │           ├──► ┌─────────────────┐
            │           │    │ do_sys_openat2  │
            │           │    └────────┬────────┘
            │           │             │
            │           │             ├──► ┌───────────────┐
            │           │             │    │ do_filp_open  │
            │           │             │    └───────────────┘
            │           │             │
            └───────────┴─────────────┘

    Akış do_filp_open fonksiyonuna geldiğinde artık dosya betimleyici tablosunda boş betimleyici bulunmuştur. "filp"
    sözcüğü Linux çekirdeklerinde dosya nesnesini (yani struct file nesnesi) gösteren göstericiler için kullanılmaktadır. 
    (Yani "filp" sizcüğü struct file * anlamında kullanılmaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            29. Ders 26/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    do_filp_open fonksiyonu güncel çekirdeklerde "fs/namei.c" dosyasında şöyle tanımlanmıştır:

    struct file *do_filp_open(int dfd, struct filename *pathname,
		const struct open_flags *op)
    {
        struct nameidata nd;
        int flags = op->lookup_flags;
        struct file *filp;

        set_nameidata(&nd, dfd, pathname, NULL);
        filp = path_openat(&nd, op, flags | LOOKUP_RCU);
        if (unlikely(filp == ERR_PTR(-ECHILD)))
            filp = path_openat(&nd, op, flags);
        if (unlikely(filp == ERR_PTR(-ESTALE)))
            filp = path_openat(&nd, op, flags | LOOKUP_REVAL);
        restore_nameidata();
        return filp;
    }
    
    Fonksiyon kabaca şu bilgileri parametre yoluyla almaktadır:

    - Göreli yol ifadelerinin nereden itibaren çözüleceğine ilişkin dosya betimleyicisini
    - Açılacak dosyanın yol ifadesini 
    - Dosya açış bayraklarını

    do_flip_open fonksiyonu çekirdek modülleri ve aygıt sürücüler için export edilmemiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeklerinde çok eskiden beri çekirdek modülleri aygıt sürücüler tarafından kullanım için tasarlanmış 
    filp_open isimli bir fonksiyon da bulunmaktadır. do_filp_open fonksiyonu çekirdek tarafından kullanılan bir fonksiyondur. 
    Halbuki filp_open fonksiyonu aygıt sürücüler tarafından kullanılabilsin diye tasarlanmıştır.do_filp_open foksiyonu 
    yol ifadesini kullanıcı alanından çekerken filp_open fonksiyonunda yol ifadesi zaten çekirdek alanı içerisinde bulunmak 
    zorundadır. Tabii filp_open fonksiyonu da nihayetinde do_filp_open fonksiyonunu çağırmaktadır. Güncel çekirdeklerde 
    filp_open fonksiyonu "fs/open.c" dosyasında şöyle tanımlanmıştır:

    struct file *filp_open(const char *filename, int flags, umode_t mode)
    {
        struct filename *name = getname_kernel(filename);
        struct file *file = ERR_CAST(name);

        if (!IS_ERR(name)) {
            file = file_open_name(name, flags, mode);
            putname(name);
        }
        return file;
    }
    EXPORT_SYMBOL(filp_open);

    filp_open fonksiyonunun parametrelerinin open fonksiyonuna (dolayısıyla sys_open sistem fonksiyonuna) benzediğinde 
    dikkat ediniz. filp_open fonksiyonunun birinci parametresi olan yol ifadesi kullanıcı modunda bir adres değil çekirdek 
    modunda bir adres belirtmektedir. Buradaki file_open_name fonksiyonu da "fs/open.c" dosyasında şöyle tanımlanmıştır:

    struct file *file_open_name(struct filename *name, int flags, umode_t mode)
    {
        struct open_flags op;
        struct open_how how = build_open_how(flags, mode);
        int err = build_open_flags(&how, &op);
        if (err)
            return ERR_PTR(err);
        return do_filp_open(AT_FDCWD, name, &op);
    }

    Görüldüğü gibi bu fonksiyon da aslında çekirdek alanındaki yol ifadesini uygun biçime dönüştürerek do_filp_open 
    fonksiyonunu çağırmaktadır. do_filp_open fonksiyonun hangi fonksiyonlar tarafından çağrıldığını aşağdaki şeekille 
    görselleştirmek istiyoruz:

    ┌─────────────────────────────────────────────────────────────┐
    │           Kullanıcı Alanı (User Space)                      │
    │                                                             │
    │     open()                    openat()                      │
    │       │                          │                          │
    └───────┼──────────────────────────┼──────────────────────────┘
            │                          │
            │ (syscall)                │ (syscall)
            │                          │
    ┌─── ───▼──────────────────────────▼───────────────────────────┐
    │           Çekirdek Alanı (Kernel Space)                      │
    │                                                              │
    │    sys_open()              sys_openat()                      │
    │       │                         │                            │
    │       └──────────┬──────────────┘                            │
    │                  │                                           │
    │                  ▼                                           │
    │          do_sys_open()                                       │
    │                  │                                           │
    │                  ▼                                           │
    │          do_sys_openat2()                                    │
    │                  │                                           │
    │                  │                                           │
    │       ┌──────────┴──────────┐                                │
    │       │                     │                                │
    │       ▼                     ▼                                │
    │  filp_open()         ┌─────────────────┐                     │
    │       └─────────────►│ do_filp_open()  │  ◄─── Ortak Nokta   │
    │                      └─────────────────┘                     │
    │                              │                               │
    │                              ▼                               │
    │                    [Dosya Açma İşlemleri]                    │
    └──────────────────────────────────────────────────────────────┘  
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek modülleri ve aygıt sürücüler için bulundurulmuş olan filp_open fonksiyonu çağrıldığında bir dosya nesnesi 
    oluşturulup o dosya nesnesinin adresi (yani struct file nesnesinin adresi) geri dönüş değeri olarak verilmektedir. 
    filp_open fonksiyonunun parametrik yapısına dikkat ediniz:

    struct file *filp_open(const char *filename, int flags, umode_t mode);

    filp_open fonksiyonunun herhngi bir prosesle ilişkili bir dosya açmamaktadır. Yani bu fonksiyon oluşturduğu bu dosya 
    nesnesinin adresini herhangi bir prosesin dosya betimleyici tablosuna yazmamaktadır. Açılan dosya herhangi bir prroses 
    tarafından kullanılabilecek bir dosya değildir. Bu dosya yalnızca çekirdek modülleri ve aygıt sürücüler tarafından 
    kullanılabilmektedir. filp_open fonksiyonuyla açılmış olan dosyalar filp_close fonksiyonuyla kapatılabilirler. 
    Güncel çekirdeklerde filp_close fonksiyonu da "fs/open.c"" içerisinde şöyle tanımlanmıştır:

    int filp_close(struct file *filp, fl_owner_t id)
    {
        int retval;

        retval = filp_flush(filp, id);
        fput_close(filp);

        return retval;
    }
    EXPORT_SYMBOL(filp_close);

    filp_open fonksiyonu ile çekirdek modülü ya da aygıt sürücü bir dosyayı açtığında artık o dosyadan okuma ve o dosyaya 
    yazma işlemleri güncel çekirdeklerde "fs/read_write.c" dosyası içerisineki kernel_read ve kernel_write fonksiyoları 
    yoluyla yapılmaktadır. Bu fonksiyonların da prototipleri şçyledir:

    ssize_t kernel_read(struct file *file, void *buf, size_t count, loff_t *pos);
    ssize_t kernel_write(struct file *file, const void *buf, size_t count, loff_t *pos);

    Fonksiyonların parametrik yapılarının read ve write POSIX fonksiyonlarını andırdığına dikkat ediniz. Ancak bu fonksiyonlar 
    dosya betimeleyicisini değil dosya nesnesinin adresini parametre olarak almaktadır. Ayrıca fonksiyonalar dosya göstericisinin 
    gösterdiği yerden itibaren değil son parametreleri ile belirtilen offset'ten işlemlerini yapmaktadır. Bu fonksiyonlardaki 
    aktarım adresleri kullanıcı modundaki adresler değil çekirdek modundaki adreslerdir. Her ne kadar bu fonksiyonlar zaten 
    dosya göstericisini de parametre olarak alıyorsa da dosya göstericisini konumlandırmak için ayrıca vfs_llseek isimli 
    bir fonksiyon da bulunmaktadır:

    loff_t vfs_llseek(struct file *file, loff_t offset, int whence);

    kernel_read, kernel_write ve vfs_llseek fonksiyonlarrının hepsi export edilmiş durumdadır. 

    Aşağıda çekirdek modülü içerisinde dosya işlemlerinin yapılmasına yönelik basit bir örnek veriyoruz. İşlemler çekirdek 
    modülünün init fonksiyonunda yaoılmaktadır:

    static int test_module_init(void)
    {
        struct file *f;
        char *buf;
        ssize_t nread;
        loff_t pos;

        if ((f = filp_open("/etc/hostname", O_RDONLY, 0)) == NULL) {
            printk(KERN_INFO "cannot open file!...\n");
            return PTR_ERR(f);
        }

        if ((buf = kzalloc(100 + 1, GFP_KERNEL)) == NULL) {
            printk(KERN_INFO "cannot allocate memory!...\n");
            filp_close(f, NULL);
            return -ENOMEM;
        }

        pos = 0;
        if ((nread = kernel_read(f, buf, 100, &pos)) < 0) {
            kfree(buf);
            filp_close(f, NULL);
            return (int)nread;
        }

        buf[nread] = '\0';
        printk(KERN_INFO "%s\n", buf);

        kfree(buf);
        filp_close(f, NULL);

        return 0;
    }

    Burada dosyanın filp_open fonksiyonuyla açıldığını, kernel_read fonksiyonuyla dosyadan okuma yapıldığını ve dosyanın
    filp_close fonksiyonuyla kaapatıldığını görüyorsunuz. kzalloc fonksiyonu çekirdek alanında içi 0'larla dolu dinamik
    tahsisat yapmaktadır. Modülü yükledikten sonra okunanları görmek için "dmesg" komutunu çalıştırmalınınız.
----------------------------------------------------------------------------------------------------------------------*/

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);

static int test_module_init(void)
{
    struct file *f;
    char *buf;
    ssize_t nread;
    loff_t pos;

    if ((f = filp_open("/etc/hostname", O_RDONLY, 0)) == NULL) {
        printk(KERN_INFO "cannot open file!...\n");
        return PTR_ERR(f);
    }

    if ((buf = kzalloc(100 + 1, GFP_KERNEL)) == NULL) {
        printk(KERN_INFO "cannot allocate memory!...\n");
        filp_close(f, NULL);
        return -ENOMEM;
    }

    pos = 0;
    if ((nread = kernel_read(f, buf, 100, &pos)) < 0) {
        kfree(buf);
        filp_close(f, NULL);
        return (int)nread;
    }

    buf[nread] = '\0';
    printk(KERN_INFO "%s\n", buf);

    kfree(buf);
    filp_close(f, NULL);

    return 0;
}

static void test_module_exit(void)
{
    printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

obj-m += ${file}.o

all:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

/*----------------------------------------------------------------------------------------------------------------------
    Pekiyi prosesin sys_open sistem fonksiyonuyla açtığı bir dosya nasıl kapatılmaktadır? Dosya kapatılırken çekirdeğin  
    dosya betimleyici tablosunda ilgili betimleyicinin boşaltması, dosyanın blokları page cache içerisinde kirlenmiş 
    (dirty) olarak bulunuyorsa onların da flush etmesi gerekir. Ancak dosya nesnesi başka prosesler tarafından da 
    (alt prosesleri kastediyoruz) kullanılabilidiği için ve dup gibi fonksiyonlarla çiftlenmiş olabileceği için doğrudan 
    serbest bırakılmamakta yalnızca sayacı eksiltilmektedir. Tabii dosya betimleyicisi serebest bırakıldığında aynı 
    zamanda artık bu dosya betimleyicisinin boş olduğunu gösteren ilgili bitmap'lerde de güncellemeler yapılmaktadır. 
    Kapatılan dosyaların dentry ve inode nesneleri dentry ve inode öncelleklerinden (cache) atılmak zorunda değildir. 
    Ancak dosyanın dentry ve inode nesnesi üzerinde de bazı güncellemeler yapılmaktadır. Güncel çekirdeklerde sys_close 
    fonksiyonu "fs/open.c" dosyasında şöyle tanımlanmıtır:

    SYSCALL_DEFINE1(close, unsigned int, fd)
    {
        int retval;
        struct file *file;

        file = file_close_fd(fd);
        if (!file)
            return -EBADF;

        retval = filp_flush(file, current->files);

        /*
        * We're returning to user space. Don't bother
        * with any delayed fput() cases.
        */
        fput_close_sync(file);

        if (likely(retval == 0))
            return 0;

        /* can't restart close syscall because file table entry was cleared */
        if (retval == -ERESTARTSYS ||
            retval == -ERESTARTNOINTR ||
            retval == -ERESTARTNOHAND ||
            retval == -ERESTART_RESTARTBLOCK)
            retval = -EINTR;

        return retval;
    }

    Bu fonksiyonda kabaca şunlar yapılmaktadır:

    - file_close_fd fonksiyonu ile prosesin dosya betimleyici tablosundaki ilgili betimleyiciye ilişkin adrese NULL 
    yerleştirilir ve betimeleyici için boş betimleyicilerin hızlı bulunmasını sağlayan için iki düzey bitmap'te 
    güncellenemeler yapılır. Bu fonksiyon aynı zamanda betimleyiciye ilişkin dosya nesnesini adresini de geri 
    döndürmektedir. 

    - filp_flush fonksiyonu dosyanın page cache içerisinde bulunan kirlenmiş bloklarını flush etmektedir. Dosyanın 
    meta data bilgileri de bu sırada flush edilmektedir. 

    - fput_close_sync fonksiyonu dosya nesnesinin sayacını azaltıp dosyaya ilişkin dentry ve inode nesneleri üzerinde 
    ayrıntıları daha sonra anlatılacak olan bazı güncellemeleri yapmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi yeniden çekirdeğim öğrenci ödevi gibi olan ilkel 0.01 versiyonundaki sys_open fonksiyonuna göz gezdirelim:

    int sys_open(const char * filename,int flag,int mode)
    {
        struct m_inode * inode;
        struct file * f;
        int i,fd;

        mode &= 0777 & ~current->umask;
        for(fd=0 ; fd<NR_OPEN ; fd++)
            if (!current->filp[fd])
                break;
        if (fd>=NR_OPEN)
            return -EINVAL;
        current->close_on_exec &= ~(1<<fd);
        f=0+file_table;
        for (i=0 ; i<NR_FILE ; i++,f++)
            if (!f->f_count) break;
        if (i>=NR_FILE)
            return -EINVAL;
        (current->filp[fd]=f)->f_count++;
        if ((i=open_namei(filename,flag,mode,&inode))<0) {
            current->filp[fd]=NULL;
            f->f_count=0;
            return i;
        }
    /* ttys are somewhat special (ttyxx major==4, tty major==5) */
        if (S_ISCHR(inode->i_mode))
            if (MAJOR(inode->i_zone[0])==4) {
                if (current->leader && current->tty<0) {
                    current->tty = MINOR(inode->i_zone[0]);
                    tty_table[current->tty].pgrp = current->pgrp;
                }
            } else if (MAJOR(inode->i_zone[0])==5)
                if (current->tty<0) {
                    iput(inode);
                    current->filp[fd]=NULL;
                    f->f_count=0;
                    return -EPERM;
                }
        f->f_mode = inode->i_mode;
        f->f_flags = flag;
        f->f_count = 1;
        f->f_inode = inode;
        f->f_pos = 0;
        return (fd);
    }

    Bu ilkel versiyonda sistemde neredeyse modern işletim sistemlerinin sahip olduğu hiçbir cache sistemi ve hızlandırıcı 
    unsur yoktur. Fonksiyon işlemine şöyle başlatılmıştır:

    mode &= 0777 & ~current->umask;
    
    Burada önce porsesin umask değeri ile dosya açış modu maskelenmiştir. Sonra dosya betimeleyici tablosundaki ilk 
    boş betimleyici bir döngü ile elde edilmiştir:

    for(fd=0 ; fd<NR_OPEN ; fd++)
        if (!current->filp[fd])
            break;
    if (fd>=NR_OPEN)
		return -EINVAL;

    Bu ilkel versiyonda zaten prosesin açabileceği dosya sayısı (NR_OPEN değeri) 20 idi. Bundan sonra prosesin close-on-exec 
    bayrağı reset edilmiştir:

    current->close_on_exec &= ~(1<<fd);

    Tüm dosyaların close-on-exec bayrakları task_struct içerisindeki unsigned long türden  close_on_exec elemanında 
    tutulmaktadır. Zaten bu versiyonda prosesin açabileceği dosya sayısı 20 ile kısıtlıdır. 

    Bu ilkel versiyonda henüz çekirdekte bir heap sistemi yoktu. Dolayısıyla bütün dosya nesneleri baştan statik düzeyde 
    bir dizide saklanmıştır:

    struct file file_table[NR_FILE];

    Bütün dosya nesnelerinin toplam sayısı da (NR_FILE değeri) 64'tü. Fonksiyonda daha sonra bu 64 dosya betimleyicisi 
    dolaşılarak sıralı arama ile boş olan bir tanesi elde edilmiştir:

    f=0+file_table;
    for (i=0 ; i<NR_FILE ; i++,f++)
        if (!f->f_count) break;
    if (i>=NR_FILE)
        return -EINVAL;

    Dosya nesnesinin o zamanlarda da f_count isimli bir sayaç elemanı bulunmaktaydı. Burada sayacı 0 olan dosya nesnesinin 
    elde edildiğine dikkat ediniz. Bundan sonra bu nesnein adresi dosya betimleeyici tablosndaki ilgili betimleyicinin
    belirttiği dizi elemanına yerleştirilmiştir:
    
    (current->filp[fd]=f)->f_count++;

    Bu noktada henüz dosya nesnesinin elemanlarına atamalar yapılmamıştır. Bu versiyonda dosya açım işleminin büyük kısmı 
    open_nami isimli fonksiyon tarafından yapılmaktadır:

    if ((i=open_namei(filename,flag,mode,&inode))<0) {
		current->filp[fd]=NULL;
		f->f_count=0;
		return i;
	}

    open_namei fonksyionu isimsel olarak uzun süre kullanılmıştır. Nihayet bu ilkel versiyonda dosya nesnesinin elemanlarrına 
    ilkdeğerler verilerek fonksiyon dosya betimleyicisiyle geri döndürülmüştür:

    f->f_mode = inode->i_mode;
    f->f_flags = flag;
    f->f_count = 1;
    f->f_inode = inode;
    f->f_pos = 0;
    return (fd);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dosya açım işleminde kullanılan çekirdek mekanizmaları üzerinde duracağız. Yukarıda da belirttiğimiz gibi 
    bu ana işlevler güncel çeekirdeklerde do_filp_open fonksiyonu tarafından ilk versiyonlarda da open_namei fonksiyonu 
    tarafından yapılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinde bir yol ifadesi verildiğinde çekirdek yol ifadesine ilişkin dosya ya da dizinin ve o dosya ya 
    da dizinin içinde bulunduğu dizininin dentry ve inode nesnelerini bulmaya çalışır. (Bir dizindeki dosya ya da dizin 
    üzerinde işlem yapıldığında o dosya ya da dizinin içersinde bulunduğu dizinler üzerinde de işlemlerin yapılması 
    gerekebilmektedir.) Bu sürece işletim sistemlerinde "yol ifadesinin çözümlenmesi (pathname resolution)" ya da "yol 
    ifadesinin aranması (pathname lookup)" denilmektedir. Örneğin open fonksiyonuyla aşağıdaki gibi bir dosya açmak 
    isteyelim:

    fd = open("/home/kaan/Study/LinuxKernel/sample.c", O_RDONLY);

    İşletim sistemi "/home/kaan/Study/LinuxKernel/sample.c" biçiminde bir yol ifadesini aldığında öncelikle bu yol 
    ifadesinin hedefi olan dosya ya da dizinin (burada "sample.c") ve o dosya ya da dizinin içinde bulunduğu dizinin 
    ("burada "LinuxKernel") inode ve dentry nesnelerini bulmaya çalışır. Çekirdek bu işlemi bir döngü içerisinde yol 
    ifadesini "yol bileşenlerine (path component)" ayırarak adım adım gerçekleştirmektedir. Yukarıda yol ifadesinin yol 
    bileşenleri şunlardır:

    "home"
    "kaan"
    "Study"
    "LinuxKernel"
    "sample.c"

    Tabii çekirdeğin bu yol bileşenlerini tek tek doğrulaması da gerekmektedir. Örneğin burada "kaan"" yol bileşeninin 
    bir dizin belirtmesi gerekir. Eğer "kaan" dizin girişi "home" dizinin altında varsa ama bir dosya belirtiyorsa (yani 
    bir dizin belirtmiyorsa) çözümleme işlemine devam etmenin bir anlamı yoktur. Ayrıca çekirdeğin yol ifadesini çözümlerken 
    ilgili dizinler için erişim haklarını da kontrol etmesi gerekmektedir. Bir yol ifadesindeki tüm dizinlerin "x" hakkına 
    sahip olması gerektiğini anımsayınız. 

    Yol ifadelerinin çözümlenmesi (pathname resolution) sırasında yol bileşenleri öncelikle "dentry önbelliğinde (dentry
    cache)" aranmaktadır. Örneğin "kökün altında home dizini var mı?" araması yapılırken. "home" dizininine ilişkin 
    dentry nesnesi dentry önbelleğinden hızlı bir biçimde elde edilebilmektedir. İzleyen paragraflarda da açıklayacağımız 
    gibi dentry önbelleği içerisindeki bir dentry nesnesi "negatif değilse" zaten ona ilişkin inode nesnesi de inode 
    önbelleği içerisinde bulunmaktadır. Böylece pek çok yol ifadesi aslında hiç diske başvurulmadan dentry ve inode 
    önbellek mekanizması sayesinde hızlı bir biçimde çözülebilmektedir. Tabii bir yol bileşeni bu önbelleklerde yoksa 
    bu durumda artık disk okumaları yapılıp ilgili yol bileşenine ilişkin dentry ve inode nesneleri önbelleklere alınacaktır. 
    Daha önceden de belirttiğimiz gibi aslında diskte son okunan bloklar da "sayfa önbelliğinin (page cache)" içerisindedir. 
    Disk okuması sırasında hemen diske yönelinmemekte önce bu sayfa önbelleğine bakılmaktadır. Şimdi yol ifadesinin bu
    önbelleklek sistemleri kullanılarak nasıl çözüldüğüne ilişkin örnek verelim. Yol ifadesi aşağıdaki gibi olsun:

    "/home/kaan/Study/LinuxKernel/sample.c"

    Çekirdek önce dentry önbelleğinde üst dizini kök olan "home" isminde dizin belirten bir giriş var mı diye arama 
    yapar. Bu girişin bulunduğunu düşünelim. Bundan sonra çekirdek bu sefer üst dizini "home" olan "kaan" isminde dizin 
    belirten bir giriş var mı diye bakar. Aramalar böyle devam eder. Örneğin çekirdeğin üst dizini "LinuxKernel" olan 
    "sample.c" isimli girişi dentry önbelleğinde bulamadığını varsayalım. Bu durumda artık gerçekten bu bilgiler 
    diskten alınacak ve "sample.c" dosyasına ilişkin dentry nesnesi ve inode nesnesi ilgili önbelleklere yerleştirilecektir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
<BURADA KALDIK>
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

