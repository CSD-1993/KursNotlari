
/*----------------------------------------------------------------------------------------------------------------------
                                        C ve Sistem Programcıları Derneği

     "Linux Kernel - İşletim Sistemlerinin Tasarımı ve Gerçekleştirilmesi" Kursunda Yapılan Örnekler ve Özet Notlar
                                                   1. Bölüm

                                             Eğitmen: Kaan ASLAN

        Bu notlar Kaan ASLAN tarafından oluşturulmuştur. Kaynak belirtmek koşulu ile her türlü alıntı yapılabilir.
        Kaynak belirtmek için aşağıdaki referansı kullanabilirsiniz:

    Aslan, K. (2025), "Linux Kernel - İşletim Sistemlerinin Tasarımı ve Gerçekleştirilmesi Kursu, Sınıfta Yapılan 
        Örnekler ve Özet Notlar", C ve Sistem Programcıları Derneği, İstanbul.

                    (Notları sabit genişlikli font kullanan programlama editörleri ile açınız.)
                        (Editörünüzün "Line Wrapping" özelliğini pasif hale getiriniz.)

                                    Son Güncelleme: 31/12/2025 - Çarşamba

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										1. Ders 19/07/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kurs katılımcılarıyla tanışıldı.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursun amacı, kapsamı ve genel işleyişi hakkında açıklamalar yapıldı. Yardımcı kaynaklar (kitaplar, dokümanlar ve 
    web sayfaları) tanıtıldı.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuz için bir sanal makineye herhangi bir Linux dağıtımının kurulması gerekmektedir. Dağıtım minimalist biçimde 
    kurulabilir. Ancak Linux içerisinde C derleyicisinin (gcc ya da clang) ve binary utility araçlarının bulunuyor olması 
    gerekir. Kursumuzda Linux kaynak kodları üzerinde değişiklikler yapıp çekirdeği yeniden derleyerek çeşitli denemeler 
    yapacağız. Bu nedenle kurduğunuz sistemin bir kopyasını da saklamanızı salık veriyoruz.

    Kurusumuzda Debian türevi bir Linux dağıtımının kurulmuş olduğu varsayılacaktır. Biz bazı konularda açıklamalar 
    yaparken Debian dağıtımının "apt" paket sistemini kullanacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzda Linux işletim sisteminin kaynak kodlarında gezinmek için https://elixir.bootlin.com/linux/ sitesini 
    kullanacağız. Bu sitede Linux'un 0.01 versiyonundan günümüzdeki en son versiyonuna kadar bütün resmi versiyonlarının 
    kaynak kodları bulunmaktadır ve bu kaynak kodlar üzerinde gezintiler (navigation) yapılabilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										2. Ders 20/07/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri bilgisayar donanımının kaynaklarını yöneten, bilgisayar donanımı ile kullanıcı arasında arayüz 
    oluşturan sistem programlarıdır. Bilgisayar bilimlerinin akademik öncülerinin çoğu işletim sistemlerini bir kaynak 
    yöneticisi (resource manager) olarak tanımlamıştır.

            +----------------------+
            | Uygulama Programları |
            +----------------------+
        +-----------------------------+
        |       İşletim Sistemi       |
        +-----------------------------+
    +-----------------------------------+
    |        Bilgisayar Donanımı        |
    +-----------------------------------+

    İşletim sistemlerinin yönettiği kaynakların en önemlileri şunlardır:

    - CPU: İşletim sistemi hangi programın ne zaman, ne kadar süre için CPU'ya atanacağına karar verip bu işlemleri 
    gerçekleştirmektedir.

    - Ana Bellek (Main Memory (RAM)): İşletim sistemi programların ana belleğin neresine yükleneceğine karar verir 
    ve ana bellek kullanımını düzenler.

    - İkincil Bellekler: İşletim sistemi bir dosya sistemi (file system) oluşturarak dosyaların parçalarını ikincil 
    belleklerde etkin bir biçimde tutar ve kullanıcılara bir dosya kavramıyla sunar.

    - Çevre Birimleri (klavye, fare, yazıcı vs.): İşletim sistemi fare, klavye, yazıcı gibi çevre birimlerini yöneterek 
    onları kullanıma hazır hale getirir. Yardımcı işlemcileri (denetleyicileri) programlayarak onların işlev görmesini 
    sağlamaktadır.

    - Ağ İşlemleri: İşletim sistemi ağa ilişkin donanım birimlerini yöneterek dışarıdan gelen bilgileri onları talep 
    eden programlara iletir.

    İşletim sistemleri kaynak yönetimine göre alt sistemlere ayrılarak da incelenebilmektedir. Örneğin işletim 
    sisteminin "çizelgeleyici (scheduler)" alt sistemi demekle CPU yönetimini sağlayan alt sistemi kastedilmektedir. 
    Ana bellek yönetimi (memory management) yine soyutlanarak incelenen önemli alt sistemlerden biridir. İşletim 
    sistemlerinin ikincil bellek yönetimine "dosya sistemi (file system)" da denilmektedir. Tabii bütün bu sistemler 
    birbirinden kopuk olarak değil birbirleriyle ilişkili bir biçimde işlev görmektedir. Bu durumu insanın "solunum 
    sistemi", "dolaşım sistemi", "sinir sistemi", "boşaltım sistemi" gibi alt sistemlerine benzetebiliriz. Bu alt 
    sistemlerin birinde bile çalışma bozukluğu oluşsa insan yaşamını yitirebilmektedir.

    İşletim sistemleri yapı olarak iki kısımdan oluşmaktadır: Çekirdek (kernel) ve kabuk (shell). Çekirdek işletim 
    sisteminin donanımı kontrol eden ve kaynakları yöneten motor kısmıdır. Aslında işletim sistemi denildiğinde akla 
    çekirdek gelmektedir. Kabuk ise işletim sisteminin kullanıcı ile arayüz oluşturan önyüzüdür. Örneğin UNIX/Linux 
    sistemlerinde bash gibi komut satırı, GNOME, KDE gibi pencere yöneticileri, Windows'taki masaüstü (Explorer), 
    macOS'teki masaüstü (Aqua) bu işletim sistemlerinin kabuk kısımlarını oluşturmaktadır.

    +-----------------------------------+
    |           Kabuk (Shell)           |
    |   +---------------------------+   |
    |   |                           |   |
    |   |    Çekirdek (Kernel)      |   |
    |   |                           |   |
    |   +---------------------------+   |
    +-----------------------------------+

    Peki işletim sistemi bu kadar temel donanım yönetimini sağlıyorsa işletim sistemi olmadan programlama yapılabilir 
    mi? İşletim sistemi olmadan programlama faaliyetine halk arasında "bare metal programlama" denilmektedir. Bare metal 
    programlama gömülü sistemlerde, mikrodenetleyicilerin kullanıldığı uygulamalarda kullanılmaktadır. Bare metal 
    programlama genellikle özel bir amaca hizmet edecek biçimde yapılmaktadır. Amaçlar fazlalaştığı zaman ve sistem 
    karmaşıklaştığı zaman artık işletim sistemlerine gereksinim duyulmaktadır.

    Bazı kontrol yazılımları işletim sistemlerinin bazı etkinliklerini de sağlamaktadır. Bir kontrol yazılımının 
    işletim sistemi olarak isimlendirilmesi için yukarıda açıkladığımız kaynak yönetimlerinin önemli bir bölümününü 
    sağlıyor olması gerekir. Bu kaynak yönetimlerinin çoğunu sağlamayan kontrol yazılımlarına genel olarak "firmware" 
    de denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri çeşitli biçimlerde sınıflandırılabilmektedir:

    - Proses Yönetimine Göre: Aynı anda tek bir programı çalıştıran işletim sistemlerine "tek prosesli (single processing)", 
    aynı anda birden fazla programı çalıştırabilen işletim sistemlerine ise "çok prosesli (multiprocessing) işletim sistemleri 
    denilmektedir. Örneğin DOS işletim sistemi tek prosesli bir sistemdi. Biz bu işletim sisteminde bir programı çalıştırırdık 
    ancak çalıştırdığımız program sonlanınca başka bir programı çalıştırabilirdik. Halbuki Windows, UNIX/Linux, MacOS gibi 
    işletim sistemleri çok prosesli işletim sistemleridir.

    - Kullanıcı Sayısına Göre: Birden fazla farklı kullanıcının çalışabildiği sistemlere "çok kullanıcılı (multiuser)", 
    tek bir kullanıcının çalışabildiği sistemlere "tek kullanıcılı (single user)" sistemler denilmektedir. Genellikle 
    çok prosesli işletim sistemleri aynı zamanda çok kullanıcılı sistemlerdir. Birden fazla kullanıcının söz konusu 
    olduğu sistemlerde kullanıcıların yetkilerinin ayarlanması, kullanıcıların birbirlerinin alanlarına erişmesinin 
    engellenmesi, sistem kaynaklarını belli oranlarda bölüşmesi gerekebilmektedir. Örneğin DOS tek kullanıcılı bir 
    sistemdi. Halbuki Windows, UNIX/Linux ve macOS sistemleri çok kullanıcılı sistemlerdir.

    - Çekirdek Yapısına Göre: İşletim sistemleri çekirdek yapısına göre "tek parçalı çekirdekli (monolithic kernel)" 
    ve "mikro çekirdekli (microkernel)" olmak üzere ikiye ayrılmaktadır. Tek parçalı çekirdekli işletim sisteminin büyük 
    kısmı çekirdek modunda çalışır. Mikro çekirdekli sistemlerde ise çekirdek modunda çalışan kısım minimize edilmeye 
    çalışılmıştır. Aslında tek parçalı ve mikro çekirdekli tesarımları bir spektrum olarak düşünebiliriz. (Örneğin bu 
    spektrumda bazı çekirdekler tek parçalı tarafa yakın bazıları ise mikro tarafa yakın olabilmektedir.)

    - Dışsal Olaylarla Yanıt Verebilme Özelliğine Göre: İşletim sistemleri dışsal olaylara yanıt verme bakımından gerçek 
    zamanlı olan (real-time) ve gerçek zamanlı olmayan (non-real-time) sistemler olmak üzere ikiye ayrılabilir. Dışsal 
    olaylara hızlı bir biçimde yanıt verebilecek çekirdek yapısına sahip olan işletim sistemlerine "gerçek zamanlı (real-time) 
    işletim sistemleri denilmektedir. Gerçek zamanlı işletim sistemleri de kendi aralarında "katı (hard real-time)" ve 
    "gevşek (soft real-time)" işletim sistemleri olmak üzere ikiye ayrılabilmektedir. Katı gerçek zamanlı sistemler dışsal 
    olaylara yanıt verme bakımından çok güvenilir olma iddiasındadır. Gevşek gerçek zamanlı sistemler ise bu konuda daha 
    toleranslıdır.

    - Dağıtıklık Durumuna Göre: İşletim sistemleri dağıtıklık durumuna göre "dağıtık olan (distributed)" ve "dağıtık olmayan 
    (non-distributed)" sistemler biçiminde ikiye ayrılabilmektedir. Dağıtık işletim sistemlerinde sistem birden fazla 
    bilgisayardan oluşan tek bir sistem gibi davranmaktadır. Örneğin 10 tane makineyi tek bir sistem olarak düşünebilirsiniz. 
    Bu durumda bu bilgisayarların kaynakları (örneğin diskleri ve CPU'ları) bu 10 makine tarafından paylaşılmaktadır. 
    Windows, UNIX/Linux ve macOS dağıtık işletim sistemleri değildir. Ancak bu sistemlerde dağıtık uygulamalar yapılabilmektedir.

    - Donanım Özelliğine Göre: Neredeyse her yaygın masaüstü işletim sisteminin bir mobil versiyonu da oluşturulmuştur. 
    IOS (Iphone Operating System) ve ipadOS Apple firmasının (yani macOS sistemlerinin) mobil işletim sistemleridir. Android 
    bir çeşit mobil Linux sistemi olarak değerlendirilebilir. Android projesinde Linux çekirdeği alınmış, biraz özelleştirilmiş, 
    bazı parçaları atılmış, buna bir mobil arayüz giydirilmiş ve sistem akıllı telefonlara ve tabletlere uygun hale getirilmiştir. 
    Nokia eskiden Symbian sistemlerinde büyük bir pazar payına sahipti. Ancak bu firma akıllı telefon geçişini iyi yönetemedi. 
    MeeGo ve Maemo gibi işletim sistemlerini denedi. Sonra ekonomik sıkıntılar sonucunca büyük ölçüde Microsoft tarafından 
    satın alındı. Windows'un mobil versiyonuna genel olarak Windows CE denilmektedir. Windows CE'nin akıllı telefonlar ve 
    tabletler için özelleştirilmiş biçimine ise Windows Mobile ve Windows Phone denilmektedir. Ancak Microsoft 2010 yılında 
    Windows Mobile işletim sistemini 2017'de de Windows Phone işletim sistemini sonlandırmıştır ve bu alandaki rekabetten 
    tamamen çekilmiştir. Windows CE ise Windows IoT Core ismiyle farklı bir tasarımla evrimleşerek devam ettirilmektedir.

    - Kaynak Kod Lisansına Göre: Kaynak kod lisansına göre işletim sistemlerini kabaca "açık kaynak kodlu (open source)" 
    ve "mülkiyete bağlı (proprieatary)" olmak üzere ikiye ayırabiliriz. Açık kaynak kodlu işletim sistemleri değişik açık 
    kaynak kod lisanslarına sahip olabilmektedir. Bunların kaynak kodları indirilip üzerinde değişiklikler yapılabilmektedir. 
    Örneğin Windows işletim sistemi mülkiyete sahiptir. Oysa Linux, BSD sistemleri, Solaris, Android gibi sistemler açık 
    kaynak kodludur. macOS sistemlerinin ise çekirdeği açık diğer kısımları (örneğin kabuk kısmı ve diğer katmanları) 
    kapalıdır.

    - Kaynak Kodun Özgünlüğüne Göre: Bazı işletim sistemleri bazı işletim sistemlerinin kodları alınıp değiştirilerek 
    oluşturulmuştur (örneğin Android ve macOS'ta olduğu gibi). Bazı işletim sistemlerinin kodları ise sıfırdan yazılmıştır. 
    Kodları sıfırdan yazılan yani orijinal kod temeline dayanan işletim sistemlerinden bazıları şunlardır:

    AT&T UNIX
    DOS
    Windows
    Linux
    BSD'ler (belli bir yıldan sonra)
    Solaris
    XENIX
    VMS

    Burada orijinal mimari ile orijinal kod tabanını birbirine karıştırmamak gerekiyor. Linux UNIX işletim sisteminin 
    mimarisini temel almıştır. Ancak tüm kodları sıfırdan yazılmıştır. Yani orijinal AT&T UNIX sistemindeki kaynak kodların 
    bir bölümü kopyalanarak kullanılmamıştır.

    - GUI Çalışma Desteğine Göre: Bazı işletim sistemleri GUI çalışma modelini doğrudan desteklerken bazıları desteklememektedir. 
    Örneğin Windows sistemleri çekirdekle entegre edilmiş bir GUI çalışma modeli sunmaktadır. UNIX/Linux sistemleri de 
    X Window (ya da X11) ve Wayland katmanlarıyla benzer bir model sunmaktadır. Fakat örneğin DOS işletim sisteminin böyle 
    bir doğal GUI desteği yoktu.

    - Ağ Üzerinde Hizmet Alıp Verme Rollerine Göre: İşletim sistemlerini ağ altında hizmet alıp verme rollerine göre 
    "istemci (client) ve sunucu (server) biçiminde de iki gruba ayırabiliriz. Bazı işletim sistemlerinin istemci versiyonları 
    birbirlerinden ayrılmıştır. Bazılarında ise bu ayrım yapılmamıştır. Örneğin Windows 7, 8, 10, 11 sistemleri bu bakımdan 
    istemci (client) sistemleridir. Halbuki Windows Server 2016, 2019 sunucu sistemleri olarak piyasaya sürülmüştür. 
    Eskiden Mac OS X'in istemci ve sunucu versiyonları farklıydı. Fakat Mac OS X 10.7 (Lion) ile birlikte istemci ve 
    sunucu versiyonları birleştirildi. Linux dağıtımlarının çoğu da hem istemci hem de sunucu olarak kullanılabilmektedir. 
    Ancak bazı dağıtımların ise istemci ve sunucu versiyonları farklıdır. Peki işletim sistemlerinin istemci ve sunucu 
    versiyonları arasındaki farklılıklar nelerdir? Kabaca iki tür farklılığın olduğunu söyleyebiliriz. Birincisi çekirdekle 
    ilgili farklılıklardır. Genellikle sunucu sistemlerinde çizelgeleyici alt sistemde istemci sistemlerine göre farklılıklar 
    bulunmaktadır. İkincisi ise barındırdıkları yardımcı yazılımlardır. işletim sistemlerinin sunucu versiyonları hazır 
    bazı sunucu programlarını da içermektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de işletim sistemlerinin tarihsel gelişimi üzerinde duracağız. 1940’lı yıllarda ilk elektronik bilgisayarlar 
    yapıldığında henüz bir işletim sistemi kavramı yoktu. Bu bilgisayarlara program yazacak olanlar işletim sistemi 
    faaliyetlerini de kendileri yapmak zorunda kalıyordu. (Yani şimdi mikrodenetleyicilere bare metal kod yazanlarda 
    olduğu gibi.) Transistör bulunduktan sonra 1950’li yıllarda artık elektronik bilgisayarlar yavaş yavaş transistörlerle 
    yapılmaya başlandı. Transistörlerin ortaya çıkması hem bilgisayarların kapasitelerini ve güvenilirliklerini artırmış, 
    hem de güç harcamalarını düşürmüştür.

    1950'li yıllarda IBM gibi pek çok bilgisayar üreticisi firma yalnızca donanım satıyordu. İşletim sistemi gibi programları 
    yazmak kullanıcıların yapması gereken bir işti. Böylece donanımı satın alan her kurum işletim sistemine benzeyen 
    programları da kendisi yazıyordu. Bu anlamda standart bir işletim sistemi yoktu. Bugünkü anlamda ilk işletim sisteminin 
    General Motors'un 1956 yılında IBM'in 701 sistemi için yazdığı NAA IO (North American Aviation Input Output System) 
    olduğu söylenebilir.

    1960'lara gelindiğinde IBM System/360 isminde yeni bir bilgisayar donanımı geliştirme işine girişti ve artık donanımla 
    işletim sistemini birlikte satma fikrini benimsedi. Bu donanım 1964 yılında duyuruldu ve 1965 yılında gerçekleştirildi. 
    İlk System/360 Model 30 bilgisayarları o zamanın "Solid Logic Technology (SLD)" teknolojisiyle üretilmişti. Hem öncekilerden 
    daha güçlüydü hem de daha az yer kaplıyordu. Saniyede 34500 işlem yapabiliyordu ve 8K ile 64K ana belleğe sahipti. 
    1967 yılında System/360'ın Model 60'ı piyasaya sürüldü. Bu model saniyede 16.6 milyon komut çalıştırabiliyordu ve ana 
    belleği de tipik olarak 512K, 768K ve 1 MB idi. IBM Sistem 360 donanımları için 1964 yılında ilk kez OS/360 işletim 
    sistemini geliştirdi. IBM daha sonra 1967 yılında OS/360 Model 67 için OS/360'ın TSS 360 isminde zaman paylaşımlı (time 
    sharing system) bir versiyonunu daha geliştirmiştir. IBM'in System/360 makineleri ve işletim sistemleri önemli ticari 
    başarı kazandı. System/360'ı System/370 izledi. System/360 ve System/370 için başka kurumlar da işletim sistemleri 
    geliştirmiştir. Michigan Terminal System (MTS) ve MUSIC/SP bunlar arasında önemli olanlardandır.

    1960'lı yıllarda başka firmalar da işletim sistemleri geliştirmiştir. Örneğin Control Data Corporation firmasının 
    SCOPE işletim sistemi batch işlemler yapabiliyordu. Aynı firma MACE isminde bu işletim sisteminin zaman paylaşımlı 
    bir versiyonunu da yazmıştır. Firma bu çalışmalarını 1970'li yıllarda Kronos işletim sistemiyle devam ettirmiştir. 
    Burroughs firması 1961 yılında MCP işletim sistemi ile B5000 bilgisayarlarını, GE firması da 1962 yılında GECOS işletim 
    sistemiyle GE-600 serisi bilgisayarlarını piyasaya sürdü. UNIVAC dünyanın ilk ticari bilgisayarlarını üreten firmadır. 
    Bu firma da 1962 yılında UNIVAC 1107 için EXEC I işletim sistemini yazdı. Bu işletim sistemini sırasıyla Exec 2 ve 
    Exec 8 izledi.

    DEC (Digital Equipment Corporation) eskilerin en önemli bilgisayar üretici firmalarından biriydi. (DEC 1998 yılında 
    Compaq firması tarafından Compaq firması da 2002 yılında HP firması tarafından satın alındı.) Firmanın en önemli 
    ürünleri PDP (Programmed Data Processor) isimli bilgisayarlarıdır. Firma PDP-1'den (1959) başlayarak PDP-16'ya (1971-
    1972) kadar PDP makinelerinin 16 versiyonunu piyasaya sürmüştür. DEC'in PDP-8'inin mini bilgisayar devrimini başlattığı 
    söylenebilir. Bu model 50000'in üzerinde satışa ulaşmıştır. UNIX işletim sistemi 1969 yılında ilk kez DEC'in PDP-7 modeli 
    üzerinde yazılmıştır. 1965 yılında piyasaya sürülen DEC PDP-7 18 bitlik bir makineydi. Makine DECsys denilen işletim 
    sistemi benzeri bir yönetici programla beraber satılıyordu. DEC'in 1966 yılında çıkardığı PDP-10 26 bitlik bir makineydi 
    DEC bu modelle birlikte işletim sistemi olarak TOPS-10 isimli bir sisteme geçti.

    1960'lı yılların sonuna kadar işletim sistemleri ağırlıklı olarak sembolik makine diliyle yazılıyordu. 1960’lı yılların 
    sonlarında AT&T Bell Lab. tarafından UNIX işletim sistemi geliştirildiğinde önemli bir devrim yaşandı. UNIX işletim 
    sistemi 1973 yılında C ile yeniden yazılmıştır. Böylece artık işletim sistemlerinin yüksek seviyeli dillerle de yazılabildiği 
    görülmüştür. PDP-11'i 16 bitlik PDP-12 izledi. PDP-12 Intel'in x86 ve Motorola'nın 6800 işlemcileri için ilham kaynağı 
    olmuştur.

    1970’li yılların ikinci yarısında entegre devrelerin de geliştirilmesiyle "ev bilgisayarları (home computer)" ortaya 
    çıkmaya başladı. Bunlarda genellikle BASIC yorumlayıcıları ile iç içe geçmiş CP/M tya da GEOS işletim sistemleri 
    kullanılıyordu. 1970'li yıllarda pek çok firma farklı ev bilgisayarları üretmiştir. BBC Micro, Commodore 64, 
    Apple II, Atari, Amstrad, ZX Spectrum dönemin en ünlü ev bilgisayarlarındandı. Bu makinelerde kullanılan işlemciler 
    Intel'in 8080'i, Zilog'un Z80'i, Motorola'nın 6800'ü gibi 8 bitlik işlemcilerdi.

    DEC firması 1977 yılında VAX isimli bilgisayarı ve 32 bitlik işlemci birimini piyasaya sürdü. VAX ailesi makineler 
    o yıllarda önemli bi ticari başarı kazanmıştır. DEC VAX makineleri için VAX/VMS isimli bir işletim sistemi yazmıştı. 
    DEC bu işletim sisteminin ismini 1992 yılında OpenVMS olarak değiştirdi. DEC 1992 yılında 64 bitlik RISC tasarımı olan 
    Alpha işlemcilerini piyasaya sürdü ve OpenVMS Alpha işlemcilerine port edildi. OpenVMS hala kullanılmaya devam etmektedir. 
    Itanium ve X86-64 portları da vardır.

    Apple firması 1976 yılında kuruldu. Apple'ın ilk bilgisayarı Apple I idi. Bunu 1977'de Apple II, 1980'de de Apple III 
    izledi. Bu ilk Apple bilgisayarlarında AppleDOS isimli işletim sistemleri kullanılıyordu. Daha sonra Apple 1983'te 
    Lisa modelini piyasaya sürdü. 1983'ün sonlarında da ilk Macintosh bilgisayarını çıkarttı. Lisa ile birlikte Apple 
    grafik tabanlı işletim sistemlerine geçiş yaptı. Lisa ve sonraki Apple bilgisayarlarının hepsi grafik bir arayüze 
    sahiptir. Macintosh markası daha sonra Mac olarak telaffuz edilmeye başlandı. Lisa bilgisayarlarında kullanılan 
    işletim sistemi LisaOS ismindeydi. Apple daha sonra Macintosh bilgisayarlarının değişik versiyonlarını piyasaya sürdü. 
    Bunlardaki işletim sistemini "System Software 1 (1984), System Software 2 (1985), System Software 3 (1986), System 
    Software 4 (1987), System Software 5 (1987), System Software 6 (1988), ve System Software 7 (1991)" olarak isimlendirdi. 
    Apple System Software 7.5'ten sonra işletim sisteminin ismini "System Software" yerine Mac OS olarak değiştirdi ve 
    System Software 7.6 versiyonu Mac OS 7.6 ismiyle çıktı. Daha sonra Apple 1997 yılında Mac OS 8'i, 1999 yılında da 
    Mac OS 9'u çıkarmıştır.

    1980'li yıllarda Mac bilgisayarlarının fiyatı çok yüksekti ve satışları da iyi gitmiyordu. Çünkü Steve Jobs bilgisayarların 
    program yazmak için değil kullanmak için alınması gerektiğini düşünüyordu. Nihayet Apple'daki çalkantılar sonucunda 
    Steve Jobs 1985 yılında Apple'dan ayrılmak zorunda kaldı (kovuldu da denebilir) ve NeXT firmasını kurdu. NeXT firması 
    NeXT isimli bilgisayarları geliştirdi. Bu bilgisayarlarda NeXTSTEP isimli işletim sistemi kullanılıyordu. Daha sonra 
    bu sistem açık hale getirildi ve OPENSTEP ismini aldı. Dünyanın ilk Web tarayıcısı Tim Berners Lee tarafından Cern’de 
    NeXT bilgisayarları üzerinde gerçekleştirilmiştir.

    Steve Jobs 1997 yılında Apple’a geri döndü. Apple da NeXT firmasını 200 milyon dolara satın aldı. Sonra piyasaya 
    iMac ve Power Mac serileri çıktı. Daha sonra Steve Jobs Mac’lerin çekirdeklerini tamamen değiştirme kararı aldı. 
    Mac’ler Mac OS'un 10 versiyonu ile birlikte yeni bir çekirdeğe geçtiler. Mac OS işletim sistemlerinin 10'lu versiyonları 
    Roma rakamıyla Mac OS X biçiminde isimlendirilmiştir. Apple Mac OS X ismini 2012 yılında Mountain Lion (10.8) sürümü 
    ile OS X olarak, 2016 yılında da Sierra (10.12) sürümüyle birlikte de macOS olarak değiştirmiştir.

    DOS işletim sistemi text ekranda çalışıyordu. Microsoft da geleceğin grafik tabanlı işletim sistemlerinde olduğunu 
    gördü ve yavaş yavaş DOS'u bırakarak grafik tabanlı bir sisteme geçmeyi planladı. Bunun için Windows isimli grafik 
    arayüzün birinci versiyonunu 1985'te çıkardı. Bunu 1987'de Windows 2, 1990'da Windows 3.0 ve 1992'de de Windows 3.1 
    izledi. Bu 16 bit Windows sistemleri işletim sistemi değildi. DOS üzerinden çalıştırılan birer grafik arayüz gibiydi. 
    Microsoft daha sonra Windows'u Windows NT 3.1 ile bağımsız bir işletim sistemi haline getirdi. Microsoft bundan sonra 
    sırasıyla 1994 yılında Windows NT 3.5'i, 1995 yılında Windows NT 3.51'i ve Windows 95'i, 1998 yılında Windows 98'i, 
    2000 yılında Windows 2000 ve Windows ME'yi, 2001 yılında Windows XP'yi, 2006 yılında Windows Vista'yı, 2012 yılında 
    Windows 8'i, 2015 yılında Windows 10'u ve nihayet 2021 yılında da Windows 11'i çıkarmıştır.

    Linux işletim sistemi 1992 yılında bir dağıtım biçiminde piyasaya çıkmıştır. Linux işletim sisteminin hikayesi daha 
    geniş olarak izleyen paragraflarda ele alınmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										3. Ders 26/07/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de UNIX türevi işletim sistemlerinin tarihsel gelişimi üzerinde durmak istiyoruz. UNIX İşletim sistemi AT&T Bell 
    Laboratuvarlarında 1969-1971 yılları arasında geliştirildi. Proje ekibinin lideri Ken Thompson'du. Çalışma ekibinde 
    Dennis Ritchie, Brian Kernighan gibi önemli isimler de vardı. Ekip daha önce General Electric'in GE-645 mainframe 
    bilgisayarı için Multics işletim sistemi üzerinde çalışıyordu. (Multics işletim sisteminin geliştirilmesine 1964 yılında 
    başlandı. Projede General Electric, MIT ve Bell Lab birlikte çalışıyordu. Sonra proje Honeywell şirketi tarafından 
    devralınmıştır.)

    AT&T 1969 yılında bu projeden çekilerek kendi işletim sistemini geliştirmek istemiştir. Geliştirme çalışmasına DEC'in 
    PDP-7 makinelerinde başlanmıştır. UNIX ismi 1970 yılında Brian Kernighan tarafından Multics'ten kelime oyunu yapılarak 
    uydurulmuştur. Proje ekibi AT&T'yi DEC PDP-11 almaya ikna etti ve böylece geliştirme çalışmaları PDP-11 ile devam etti. 
    UNIX'in resmi olarak ilk sürümü Ekim 1971'de ikinci sürümü Aralık 1972'de, Üçüncü ve dördüncü sürümleri de 1973 yılında 
    yayınlanmıştır. UNIX işletim sistemi büyük ölçüde PDP'nin sembolik makine dili ve Ken Thompson'ın B isimli programlama 
    diliyle geliştirilmiştir. B programlama dili fonksiyonları alıp DEC'in makine diline dönüştürüyordu. Bu bakımdan B bir 
    yorumlayıcı değil derleyiciydi. İşte 1972 yılında Dennis Ritchie, Ken Thompson'ın B programlama dilinden hareketle C 
    Programlama dilini geliştirmiştir. UNIX işletim sisteminin dördüncü sürümü 1973 yılında yeniden C Programlama Dili ile 
    yazılmıştır. 1974 yılında UNIX'in beşinci sürümü oluşturuldu. Bu sürümlerin hepsi araştırma amaçlıydı ve "educational 
    license" ismiyle lisanslanmıştı. UNIX işletim sistemi bir araştırma projesi olarak organize edilmişti. Bu nedenle AT&T 
    kaynak kodlarını araştırma kuruluşlarına ücretsiz dağıtılmıştır. 1975 yılında UNIX'in altıncı sürümü şirketlere yönelik 
    hazırlandı. UNIX'in altıncı versiyonunun kaynak kodları 20,000 dolara (şimdikinin 120,000 doları) şirketlere sunuldu. 
    1977 yılında Bell Lab, UNIX'i Interdata 7/32 isimli 32 bit mimariye port etti. Bunu 1978'de VAX portu izledi.

    1974 yılında California Üniversitesi (Berkeley) işletim sisteminin kopyasını Bell Lab'tan aldı. 1978 yılında "Berkeley 
    Software Distribution (1BSD)" ismiyle AT&T dışındaki ilk UNIX dağıtımını gerçekleştirdi. Bu dağıtım hayatını hala FreeBSD, 
    OpenBSD ve NetBSD olarak devam ettirmektedir. 1979'da BSD'nin ikinci versiyonu (2BSD) ve 1979'un sonlarına doğru da üçüncü 
    versiyonu (3BSD) piyasaya sürüldü. Bunu 1980 yılında versiyon 4 (4BSD) izlemiştir. 1991 yılında BSD UNIX'ten AT&T kodları 
    tamamen arındırılmış ve kod bakımından özgün hale getirilmiştir. BSD'nin son versiyonu 1995'te 4.4BSD Lite Release 2 
    olarak çıkmıştır.

    1980'li yıllarda pek çok kurum ve ticari firma UNIX kodlarını lisans ücreti ödeyerek AT&T'den satın alıp kendilerine 
    yönelik UNIX sistemleri oluşturmuştur. Bunların önemli olanları şunlardır:

    AIX: IBM tarafından geliştirilmiş olan UNIX türevi sistemlerdir. İlk kez 1986 yılında piyasaya sürülmüştür. IBM AIX'i 
    System/370, RS/6000 PS2 bilgisayarlarında kullanıyordu. Bu sistemler AT&T UNIX System 5 kodları temel alınarak geliştirilmiştir. 
    AIX hala kullanılmaktadır. Son sürümü 2021 yılında 7.3 olarak piyasaya sürülmüştür. AIX PowerPC, x86 işlemcileri için 
    de port edilmiştir.

    IRIX: SGI firması tarafından AT&T ve BSD kodları değiştirilerek 1988'de oluşturulmuştur. 2006'da bırakılmıştır.

    HP-UX: HP 9000 bilgisayarları için 1982'de oluşturulmuştur. Motorola 68000 ve Itanium işlemcileri için yazılmıştır. 
    Hala devam ettirilmektedir.

    ULTRIX: DEC firmasının PDP-7, PDP-11 ve VAX donanımları için geliştirdiği UNIX sistemiydi. İlk versiyonu 1984 yılında 
    çıktı. 1995 yılında piyasadan çekildi.

    XENIX: Microsoft tarafından 1980 yılında geliştirilmeye başlanmıştır. İlk versiyonu 1980'in sonlarına doğru çıkmıştır. 
    Daha sonra SCO firması Microsoft'la bu konuda iş birliği yapmış 1987 yılında da Microsoft sistemi tamamen SCO'ya 
    devretmiştir. Bu sistemi daha sonra SCO firması, SCO-UNIX olarak devam ettirmiştir.

    SCO-UNIX: SCO firması XENIX'i Microsoft'tan alınca bunu SCO-UNIX olarak devam ettirdi. SCO-UNIX'in ilk versiyonu 1989 
    yılında çıktı. SCO sonra bunu OpenServer ismiyle devam ettirmiştir.

    FreeBSD, NETBSD ve OpenBSD: 4.3BSD sistemleri temel alınarak geliştirilmiştir. FreeBSD ve NetBSD 1993 yılında, OpenBSD 
    ise 1996 yılında piyasaya çıkmıştır. Sürdürülmeye devam etmektedir. Önemli bir UNIX varyantı durumundadır. Bu üç 
    sistem de birbirlerine çok benzemektedir. FreeBSD genel amaçlı client ve server işletim sistemi olma niyetindedir. 
    NetBSD daha taşınabilirdir ve geniş bir port'a sahiptir. Daha çok bilimsel çalışmalarda tercih edilmektedir. OpenBSD 
    güvenliğin önemli olduğu alanlarda tercih edilmektedir.

    SunOS (Solaris): Sun firmasının BSD kodlarıyla oluşturduğu UNIX türevi işletim sistemiydi. İlk versiyonu 1982 yılında 
    çıktı. SunOS işletim sistemi 5.2 versiyonundan sonra (1992) Solaris ismiyle pazarlanmaya başlamıştır. Solaris daha 
    sonra OpenSolaris biçiminde açık kaynak kodlu olarak bir süre varlığını devam ettirdi. Oracle firmasının Sun firmasını 
    2010'da satın almasından sonra bu proje de durduruldu. Bu proje Illumos ismiyle başka ekip tarafından devam ettirilmektedir.

    Linux: Linux Torvalds'ın öncülüğünde geliştirilmiş en yaygın UNIX türevi işletim sistemidir. İlk versiyonu 1991 
    yılında çıkmıştır. Hala devam ettirilmektedir. Linux'un tarihsel gelişimi izleyen bölümde ayrıntılı bir biçimde 
    açıklanmaktadır.

    Mac OS X, OS X, macOS: Carnegie Mellon üniversitesinin Mach isimli çekirdeği ile BSD UNIX sisteminin bir araya 
    getirilmesiyle oluşturulmuştur. İlk versiyonu 2001 yılında piyasaya sürülmüştür. İzleyen bölümlerde Mac OS işletim 
    sistemlerinin tarihsel gelişimi ayrıntılı olarak ele alınmaktadır.

    İzleyen paragrafta özel olarak Linux sistemlerinin tarihsel gelişimi üzerinde duracağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinin tarihsel gelişimini ele aldığımız önceki bölümde de belirttiğimiz gibi Apple firmasının Mac 
    bilgisayarları Mac OS'un 10 versiyonu ile birlikte yeni bir çekirdeğe geçtiler. Mac OS işletim sistemlerinin 10'lu 
    versiyonları Roma rakamıyla Mac OS X biçiminde isimlendirildi. Apple Mac OS X ismini 2012 yılında Mountain Lion (10.8) 
    sürümü ile OS X olarak, 2016 yılında da Sierra (10.12) sürümüyle birlikte de macOS olarak değiştirdi. Biz Mac OS X, 
    OS X ve macOS sistemlerine bu bölümde "Mac OS X türevi sistemler" de diyeceğiz.

    Mac OS X türevi işletim sistemleri UNIX türevi sistemlerdir. Bu işletim sistemlerinin çekirdeğine Darwin denilmektedir. 
    Darwin açık kaynak kodlu bir işletim sistemdir. Ancak Mac OS X türevi sistemler tam anlamıyla açık sistemler değildir. 
    Bu sistemlerin çekirdeği açık olsa da geri kalan kısımları mülkiyete sahip (proprietary) biçimdedir.

    Darwin'in hikayesi 1989 yılında NeXT'in NeXTSTEP işletim sistemiyle başladı. NeXTSTEP daha sonra OpenStep ismiyle API 
    düzeyinde standart hale getirildi. 1996'nın sonunda 1997'nin başında Steve Jobs Apple'a dönerken Apple da NeXT firmasını 
    satın aldı ve sonraki işletim sistemini OpenStep üzerine kuracağını açıkladı. Bundan sonra Apple 1997’de OpenStep üzerine 
    kurulu olan Rapsody'yi çıkardı. 1998'de de yeni işletim sisteminin Mac OS X olacağını açıkladı. Daha sonra 2000 yılında 
    Apple Rapsody'den Darwin projesini türetti. Darwin her ne kadar Mac sistemlerinin çekirdeği olarak tasarlanmışsa da ayrı 
    bir işletim sistemi olarak da yüklenebilmektedir. Ancak Darwin grafik arayüzü olmadığı için Mac programlarını çalıştıramamaktadır. 
    Daha sonra Darwin'i bağımsız bir işletimn sistemi haline getirmek amacıyla Darwin'den de çeşitli projeler türetilmiştir. 
    Bunlardan biri Apple tarafından 2002'de başlatılan OpenDarwin'dir. Bu proje 2006'da sonlandırılmıştır. 2007'de PureDarwin 
    projesi başlatılmıştır.

    Darwin'in çekirdeği XNU üzerine oturtulmuştur. XNU NeXT firması tarafından NEXTSTEP işletim sisteminde kullanılmak üzere 
    geliştirilmiş bir çekirdektir. XNU, Carnegie Mellon ("Karnegi" diye okunuyor) üniversitesi'nin Mach 3 mikrokernel çekirdeği 
    ile 4.3BSD karışımı hibrit bir sistemdir.

    Mac OS X türevi sistemlerin versiyonları şunlardır:

    - Mac OS X 10.0 (Cheetah, 2001)
    - Mac OS X 10.1 (Puma, 2001)
    - Mac OS X 10.2 (Jaguar, 2002)
    - Mac OS X 10.3 (Panther, 2003)
    - Mac OS X 10.4 (Tiger, 2005)
    - Mac OS X 10.5 (Leopard, 2007)
    - Mac OS X 10.6 (Snow Leopard, 2009)
    - Mac OS X 10.7 (Lion, 2011)
    - OS X 10.8 (Mountain Lion, 2012)
    - OS X 10.9 (Maverics, 2013)
    - OS X 10.10 (Yosemite, 2014)
    - OS X 10.11 (El Capitan, 2015)
    - macOS 10.12 (Sierra, 2017)
    - macOS 10.13 (High Sierra, 2017)
    - macOS 10.14 (Mojave, 2018)
    - macOS 10.15 (Catalina, 2019)
    - macOS 11 (Big Sur, 2020)
    - macOS 12 (Monterey, 2021)
    - macOS 13 (Ventura, 2022)
    - macOS 14	(Sonoma, 2023)
    - macOS 15	(Sequoia, 2024)

    MacOS büyük ölçüde POSIX uyumlu bir sistemdir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de UNIX/Linux dünyasında önemli bir yeri olan "GNU Projesi", "özgür yazılım" ve "açık kaynak kod akımları" üzerinde 
    durmak istiyoruz.

    1970'lerdeki mikro bilgisayarlar devrimine kadar yazılımda bir telif anlayışı yoktu. Yani yazılımın dağıtılması konusunda 
    sözleşmeler ve hukuki yaptırımlara gerek duyulmamıştı. Yazılım zaten donanımla birlikte satılıyordu ya da kuruma özel 
    yapılıyordu. 1969 yılında IBM yazılımı donanımla birlikte verdiği için rekabet kurallarına uymadığı gerekçesiyle mahkemeye 
    verilmiştir ve cezaya çarptırılmıştır. 1970'li yıllarda yazılım maliyetleri artmış, yazılım sektörü genişlemiş ve lisanslama 
    politikaları da uygulamaya sokulmuştur. Pek çok yazılım bu yıllarda özel lisanslarla piyasaya sürülmeye başlanmıştır. 
    1980'li yıllarda bu lisanslama faaliyetleri hız kazanmıştır.

    1980'li yıllarda tüm UNIX türevi sistemler çeşitli biçimlerde sınırlandırıcı lisanslara sahipti. Yani 1980'li yıllarda 
    sınırlaması olmayan UNIX türevi sistemler kalmamıştı. Bu nedenle bedava ve sınırlamasız UNIX türevi bir işletim sistemine 
    gereksinim duyulmaya başlandı. İşte durumdan vazife çıkaran ünlü Emacs editörünün yazarı Richard Stallman 1983 yılının 
    sonlarına doğru GNU projesini başlattı ve özgür yazılım (free software) fikrini oraya attı. GNU projesinin amacı açık 
    kaynak kodlu UNIX benzeri bir işletim sistemini ve geliştirme araçlarını yazmaktı. Proje fiilen 1984 yılında başlamıştır.

    Stallman 1985 yılında özgür yazılım kavramını yaygınlaştırmak amacıyla Free Software Foundation (www.fsf.org) isimli 
    kurumu kurdu ve atık GNU projesi bu kurum tarafından yürütülmeye başlandı. FSF özgür yazılım modeli için GPL (GNU Public 
    License) denilen lisansı ortaya çıkardı. Özgür yazılım akımında oluşturulan bir yazılım istenildiği gibi çalıştırılabilir, 
    kopyalanabilir, incelenebilir, dağıtılabilir, değiştirilebilir ve iyileştirilebilir. Daha açık bir biçimde özgür yazılım 
    tipik olarak aşağıdaki dört özgürlükle tanımlanmıştır:

    Özgürlük 0: Programı her türlü amaç için çalıştırma özgürlüğü
    Özgürlük 1: Programın kaynak kodunu inceleme ve değiştirebilme özgürlüğü
    Özgürlük 2: Programın kopyalarını çıkartabilme ve yeniden dağıtabilme özgürlüğü
    Özgürlük 3: Programı iyileştirebilme ve iyileştirilmiş programı yayınlama özgürlüğü

    GNU projesi bağlamında pek çok temel araç (gcc derleyici, ld bağlayıcı vs.) geliştirilmiştir. Fakat hedeflenen bir 
    çekirdek bir türlü oluşturulamamıştır.

    Aslında özgür yazılım (free software) ile açık kaynak kod (open source) akımları arasında bazı farklar olmakla birlikte 
    her iki akımın da hedefleri benzerdir. Özgür yazılım bir sosyal harekete benzetilirken açık kaynak kod akımı bir geliştirme 
    metodolojisine benzetilmektedir. Biz kursumuzda tüm bu akımları "açık kaynak kod (open source)" olarak nitelendireceğiz. 
    Özgür yazılım akımının temel lisansı GPL'dir (GNU Public Licence). Bunun yumuşatılmış LGPL (Lesser GPL) biçiminde bir 
    versiyonu da oluşturulmuştur. Ayrıca Apache, MIT, BSD gibi açık kaynak kodlu başka lisanslar da vardır. Şüphesiz bu 
    lisansların aralarında birtakım farklılıklar bulunmakla birlikte pek çok yönleri de ortaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux Torvalds Helsinki Üniversitesinde öğrenciyken bir işletim sistemi yazmaya niyetlenmiştir. O zamanlarda telif 
    uygulanmayan UNIX türevi bir işletim sistemi kalmamıştı ve GNU projesinin işletim sistemi de (GNU Hurd) bitirilememişti. 
    MINIX isimli bir işletim sistemi Andrew Tanenbaum tarafından yazılmıştı ancak bu sisteminin lisansı yalnızca akademik 
    kullanımlara izin verecek biçimde sınırlandırılmıştı. Linus Torvalds projesini USENET haber gruplarında duyurdu ve 
    zamanla kendisine gönüllü yardım edecek sistem programcıları buldu. Yazılım dünyasında bu tür girişimlerle sık karşılaşıldığı 
    halde başarı olasılığı nispeten düşük olmaktadır. Linus Torvalds'ın bu girişimi başarıya ulaşmıştır.

    1991 yılında Linux'un 0.01 sürümü oluşturuldu. 1994 yılında stabil bir biçimde Linux 1.0 versiyonu dağıtılmaya başlandı. 
    Bunu 1996 yılında Linux 2.0, 1999 yılında 2.2, 2000 yılında 2.4 ve 2003 yılında 2.6 izledi. Daha sonra Linux versiyon 
    numaralandırma sistemi değiştirilmiştir. 2011 yılında 3.0, 2015 yılında 4.0, 2019 yılında 5.0, 2022 yılında da 6.0 
    versiyonu çıkmıştır. Kursun yapıldığı zamandaki son çekirdek sürümü Ekim 2022'de çıkmış olan 6.0'dır.

    Linux çekirdeklerinin versiyonları ve bu versiyonlarda eklenen önemli özellikleri şöyle listeleyebiliriz:

    Sürüm	Tarih	        Önemli Yenilikler

    0.01	Ağustos 1991	Linus Torvalds tarafından duyurulan ilk sürüm; sadece temel fonksiyonlara sahipti.
    1.0	    Mart 1994	    İlk "resmi" sürüm; çoklu işlemci desteği yoktu. Ağ üzerinden TCP/IP desteği sağlandı.
    1.2	    Mart 1995	    x86 dışı mimarilere (Alpha, MIPS) ilk destekler geldi.
    2.0	    Haziran 1996	SMP (Simetrik Çoklu İşlemci) desteği eklendi. Daha fazla mimari desteği sunuldu.
    2.2	    Ocak 1999	    Gelişmiş ağ yığını, IPv6 desteği, daha fazla SMP ölçeklenebilirliği.
    2.4	    Ocak 2001	    USB, PCMCIA ve Bluetooth desteği, 64 GB RAM'e kadar bellek desteği (PAE ile).
    2.6	    Aralık 2003	    Yeni scheduler (O(1)), udev ile dinamik aygıt yönetimi, sysfs, Native POSIX thread kütüphanesi 
                            (NPTL), preemptive kernel
    3.0	    Temmuz 2011	    Büyük bir teknik değişiklik yok; sadece sürüm numarası sadeleştirildi (2.6.x'lerin devamı).
    4.0	    Nisan 2015	    Canlı kernel güncelleme (live patching) özelliği eklendi.
    5.0	    Mart 2019	    Yeni donanım desteği, enerji verimliliği iyileştirmeleri, Adiantum şifreleme algoritması.
    5.4     Kasım 2019	    Lockdown modu, fs-verity desteği.
    5.10    Aralık 2020	    EXT4 ve Btrfs iyileştirmeleri, AMD GPU desteği.
    5.15    Kasım 2021	    NTFS3 dosya sistemi, yeni I/O kontrolcüsü.
    6.0	    Ekim 2022	    Rust diline ilk çekirdek içi destek, Scheduler ve TCP performans iyileştirmeleri.
    6.1     Aralık 2022	    Rust desteği genişletildi, Intel AMX desteği.
    6.6     Kasım 2023	    Apple Silicon (M1/M2) desteği, yeni enerji yönetimi özellikleri.

    Linux monolithic bir çekirdek yapısına sahiptir. Büyük ölçüde POSIX uyumu bulunmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Açık kaynak kodlu yazılımlar bir araya getirilip paketlenerek istenildiği gibi dağıtılabilmektedir. Dağıtım (distribution) 
    bu anlamda kullanılan genel bir terimdir ve her türlü açık kaynak kodlu yazılım için dağıtım söz konusu olabilir. 
    Ancak biz burada Linux dağıtımları üzerinde duracağız.

    Linux temel olarak bir çekirdek geliştirme projesidir. Linux kaynak kodlarına baktığınızda tüm kodların çekirdekle 
    ilgili olduğunu görürsünüz. Çekirdeğin dışındaki tüm yazılımlar (örneğin init prosesinden başlayarak, kabuk yazılımları, 
    paket yöneticileri, pencere yöneticileri vs.) hep başka proje grupları tarafından gerçekleştirilmiş açık kaynak kodlu 
    yazılımlardır. İşte tüm bu açık kaynak kodlu yazılımların Linux çekirdeği temelinde bir araya getirilmesi ve doğrudan 
    kullanıcının install edip çalıştırabileceği biçimde paketlenmesine Linux dağıtımları denilmektedir. Linux dağıtımları 
    pencere yöneticileri (KDE, GNOME gibi), paket yöneticileri (APT, RPM, YUM, DPKG, PACMAN, ZYPPER gibi) ve diğer yararlı 
    uygulama programları bakımından farklılıklar gösterebilmektedir.

    Toplamda iki yüzün üzerinde Linux dağıtımının olduğu söylenebilir. Ancak bunlar arasında az sayıda dağıtım çok popüler 
    olmuştur. Bazı dağıtımlar bazı dağıtımlardan fork edilerek oluşturulmuştur. Aşağıda en çok kullanılan dağıtımlara 
    ilişkin dağıtım ağacını veriyoruz:

    Linux
    ├── Debian
    │   ├── Ubuntu
    │   │   ├── Linux Mint
    │   │   ├── Pop!_OS
    │   │   ├── elementary OS
    │   │   └── Zorin OS
    │   ├── Devuan       # Systemd olmayan Debian
    │   └── Kali Linux   # Güvenlik test amaçlı
    ├── Red Hat Linux (eski)
    │   ├── Fedora       # Topluluk temelli, RHEL'in test yatağı
    │   │   └── RHEL (Red Hat Enterprise Linux)
    │   │       ├── CentOS (→ 2021 sonrası CentOS Stream)
    │   │       ├── AlmaLinux
    │   │       └── Rocky Linux
    ├── Slackware
    │   └── Slax         # Hafif sürüm
    ├── Arch Linux
    │   ├── Manjaro
    │   └── EndeavourOS
    ├── Gentoo
    │   └── Calculate Linux
    ├── SUSE Linux
    │   ├── openSUSE Leap
    │   └── openSUSE Tumbleweed
    ├── Android          # Mobil, Linux çekirdeğine dayalı
    ├── Alpine Linux     # Minimal, güvenli, konteyner dostu
    └── Chrome OS
        └── Chromium OS  # Açık kaynak tabanı

    Burada en çok kullanılan Linux dağıtımlarından bahsedeceğiz.

    Debian Dağıtımı: En önemli ve en eski Linux dağıtımlarından biridir. Knoppix, Mint, Ubuntu dağıtımları Debian türevi 
    dağıtımlardır.

    Fedora: Red Hat firması tarafından çıkarılmış olan dağıtımdır. İlk kez 2003 yılında oluşturulmuştur. RPM paket 
    yöneticisini kullanır. CentOS ve Scientific Linux en önemli Fedora türevi dağıtımlardır. 2000 yılında ilk sürümü 
    yapılan Red Hat Enterprise Linux (RHEL) en önemli Fedora türevidir. Ondan da CentOS, Scientific Linux gibi dağıtımlar 
    türetilmiştir. CentOS server makinelerde en yaygın kullanılan Linux versiyonudur.

    OpenSUSE: Alman SUSE firmasının desteklediği dağıtımdır. SUSE Linux Enterprise isminde ticari bir versiyonu da vardır. 
    ZYpp, YaST ve RPM paket yöneticilerini kullanmaktadır.

    Slackware: En eski Linux dağıtımıdır. 1993 yılında oluşturulmuştur. Sürdürümü yavaş olmakla birlikte hala devam 
    etmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    1980'li yıllarda AT&T ya da BSD kodlarından türetilmiş olan ve çoğunluğu şirketlere ait olan pek çok UNIX türevi 
    işletim sistemi oluşturuldu. Bu işletim sistemleri birbirlerine çok benzemekle birlikte aralarında bazı farklılıklara 
    da sahipti. İşte IEEE durumdan vazife çıkartarak bu UNIX türevi sistemleri standardize etmek için kolları sıvadı ve 
    bunun sonucu olarak da POSIX standartlarını oluşturdu.

    POSIX sözcüğü Richard Stallman tarafından önerilmiştir. "Portable Operating System Interface for UNIX" sözcüklerinden 
    kısaltılarak uydurulmuştur ve "poziks" biçiminde okunmaktadır. POSIX standartları üzerinde çalışmalar 1985 yılında 
    başlamıştır ve ilk standartlar 1988 yılında "IEEE Std 1003.1-1988" kod numarasıyla oluşturulmuştur. POSIX her ne kadar 
    UNIX türevi sistemler için düşünülmüşse de UNIX türevi mimariye sahip olamayan sistemler için de kullanılabilecek bir 
    standarttır. (Örneğin Windows sistemleri Interix denilen alt sistemle POSIX uyumlu olarak da kullanılabilmekteydi. 
    Interix alt sistemi daha sonra Windows 8 ile birlikte Windows'tan kaldırılmıştır.)

    POSIX standartları 4 bölümden oluşmaktadır:

    1) Base Definitions: Bu bölümde temel tanımlamalar bulunmaktadır.
    2) Shell & Utilities: Bu bölümde kabuk komutları ve standart utility programlar ele alınmaktadır.
    3) System Interfaces: Bu bölümde C programcıları için hazır bulunan POSIX fonksiyonları açıklanmaktadır.
    4) Rationale: Çeşitli kuralların ve özelliklerin gerekçeleri bu bölümde açıklanmaktadır.

    POSIX standartlarının zaman içerisinde çeşitli versiyonları çıkartılmıştır. Bu versiyonlarda hem yeni POSIX fonksiyonları 
    kütüphaneye eklenmiş hem de standartlardaki bazı bozukluklar ve uyumsuzluklar düzeltilmiştir. Standardın önemli 
    versiyonları şu senelerde yayınlanmıştır: 1992, 1993, 1995, 1997, 2001, 2004, 2008, 2017. Standartlardaki en önemli 
    değişim 1993 yılında "POSIX 1.b" diye de isimlendirilen "Realtime-extensions" ile 1995 yılında "POSIX 1.c" diye 
    isimlendirilen "Thread-extensions" isimli eklemelerdir. Bu eklemelerle POSIX'e gerçek zamanlı işlemler için çeşitli 
    özelliklerle thread kullanımı eklenmiştir.

    "Single UNIX Specification" UNIX türevi sistemler için oluşturulmuş diğer önemli standarttır. Bir sistemin UNIX olarak 
    değerlendirilebilmesi için bu standartlara uygun olması gerekmektedir. Standartlar "Austin Group" isimli toplulukla 
    "Open Group" isimli dernek tarafından geliştirilmiştir. Sürdürümü Open Group tarafından yapılmaktadır. Open Group 
    hali hazırda UNIX sistemlerinin isim haklarını elinde bulundurmaktadır. Single UNIX Specification isimli standardın 
    zamanla pek çok versiyonu oluşturulmuştur.

    POSIX standartları ile "Single UNIX Specification" standartları arasında eskiden daha fazla farklılıklar vardı. 
    Ancak bugün itibari ile bu iki standart birbirlerine yaklaştırılmış ve son versiyonlarla aynı hale getirilmiştir. 
    Single UNIX Specification dokümanlarına Internet'ten Open Group'un web sitesinden erişilebilir:

    https://pubs.opengroup.org/onlinepubs/9799919799/
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzda önce Linux çekirdeğinin kaynak kodlarının genel dizi yapısı üzerinde duracağız.

    Linux çekirdeğinin kaynak kod ağacı üzerinde temel dizinler aynı kalmak üzere zaman içerisinde çeşitli değişikler 
    yapılmıştır. Biz burada çekirdeğin son versiyonlarını dikkate alarak açıklamalar yapacağız. Linux kaynak kod ağacındaki 
    temel dizinler şunlardır:

    linux-6.x/
    ├── arch/          → Mimarilere özel kodlar (x86, ARM, RISC-V, vs.)
    ├── block/         → Blok aygıt altyapısı
    ├── certs/         → Kernel modül imzalama için sertifikalar
    ├── crypto/        → Kriptografik algoritmalar ve API’ler
    ├── Documentation/ → Belgeler (API, özellikler, davranışlar)
    ├── drivers/       → Aygıt sürücüleri (net, usb, gpu, vs.)
    ├── fs/            → Dosya sistemi sürücüleri (ext4, btrfs, vs.)
    ├── include/       → Global başlık dosyaları (headers)
    ├── io_uring/      → 5.1 çekirdeği ile eklenen asenkron io_uring sistem fonksiyonları için çekirdek kodları
    ├── init/          → Kernelin ilk başlatma kodları
    ├── ipc/           → IPC mekanizmaları (semaphore, message queue, vs.)
    ├── kernel/        → Temel kernel işlevleri (zamanlayıcı, process, vs.)
    ├── lib/           → Genel amaçlı yardımcı fonksiyonlar
    ├── mm/            → Bellek yönetimi
    ├── net/           → Ağ yığını (TCP/IP, socket, protokoller)
    ├── samples/       → Örnek kodlar (örnek eBPF, modül kodları, vs.)
    ├── scripts/       → Derleme sürecine yardımcı betikler
    ├── security/      → Güvenlik altyapısı (LSM, SELinux, AppArmor, vs.)
    ├── sound/         → Ses sürücüleri (ALSA, etc.)
    ├── tools/         → Kullanıcı uzayı araçları (perf, bpftool, vs.)
    ├── usr/           → Yerleşik initramfs oluşturmak için
    ├── virt/          → Sanallaştırma (KVM, Xen, vs.)
    ├── MAINTAINERS    → Dosyaların kim tarafından korunduğu bilgisi
    ├── Makefile       → Ana derleme talimatı
    └── Kconfig        → Kernel yapılandırma seçenekleri

    arch => arch sözcüğü "architecture" sözcüğünden kısaltılmıştır. Bu dizinde işlemci mimarisine göre değişebilen 
    çekirdek kodları her bir işlemci ailesi ayrı bir dizinde olacak biçimde bulundurulmaktadır.

    block => Eskiden bu dizin "drivers" dizini içerisindeydi. Burada işletim sisteminin blok düzeyinde işlemler yapan 
    kodları bulundurulmuştur.

    certs => Bu dizinde çekirdek modüllerine ilişkin sertifikasyonlarla ilgili kodlar bulunmaktadır.

    crypto => Çekirdeğin içerisinde kullanılan şifreleme işlemlerine yönelik kaynak kodlar bulunmaktadır.

    Documentation => Burada çeşitli alt sistemlere ilişkin dokümanlar bulunmaktadır.

    drivers => Burada aygıt sürücülere ilişkin çekirdek kodları bulunmaktadır. Tabii bu aygıt sürücüler isteğe göre 
    seçilerek yalnızca bir grubu derlenmektedir.

    fs => Bu dizinde dosya sistemlerine yönelik çekirdek kodları bulundurulmaktadır.

    include => Çekirdek kodlarında include edilen tüm başlık dosyaları bu dizinin içerisinde bulundurulmaktadır. Tabii 
    bu dizinde de alt dizinler vardır.

    init => Çekirdeğin çalışabilir hale gelmesi için yapılan ilk işlemlere ilişkin kodlar burada bulundurulmaktadır.

    iu_uring => 5.1 çekirdekleri ile eklenen yüksek performanslı asenkron IO işlemleri için kullanılan sistem fonksiyonlarının 
    gerçekleştirimine ilişkin kodlar bu dizinde bulunmaktadır.

    ipc => Borular, mesaj kuyrukları gibi IPC nesnelerine ilişkin kaynak kodlar bu dizin içerisindedir.

    kernel => Bu dizin çekirdeğin en temel işlevlerine ilişkin kaynak kodları barındırmaktadır. (Çekirdek kodlarının 
    içerisinde ayrıca bir "kernel" dizinin bulunması biraz tuhaf olsa da bu aslında adeta "çekirdeğin çekirdeği" gibi 
    bir anlama gelmektedir.)

    lib => Bu dizinde çekirdeğin değişik yerlerinde kullanılan genel amaçlı utility fonksiyonların kodları bulundurulmaktadır. 
    Burada "lib" sözcüğünün statik ve dinamik kütüphane ile bir ilgisi yoktur. Zaten Linux derlenirken bu biçimde kütüphane 
    dosyaları oluşturulmamaktadır.

    mm => Çekirdeğin tüm ana bellek yönetim kodları bu dizinde bulunmaktadır.

    net => Çekirdeğin tüm "ağ (network)" alt sistemine ilişkin kodları bu dizinde bulunmaktadır.

    rust => Linux çekirdeğinde 6'lı versiyonlarla Rust Programlama Dili de kullanılmaya başlanmıştır. Bu dizinde çekirdeğe 
    ilişkin Rust kodları bulunmaktadır. Kursun yapıldığı tarihteki Linux çekirdeklerinde Rust kodları çok az yer kaplamaktadır.

    samples => Burada alt sistemlere ilişkin örnek kodlar bulunmaktadır.

    scripts => Bu dizinde çekirdek derlemesi sırasında faydalanılan script dosyaları bulundurulmuştur.

    security => Çekirdeğin sistem güvenliği ile ilgili kodları bu dizinde bulunmaktadır.

    sound => Ses işlemlerine yönelik çekirdek kodları bu dizinde bulunmaktadır.

    tools => Bu dizinde çekirdek geliştirmesinde ve analizinde kullanılan yardımcı programlar bulundurulmaktadır. Aslında
    bu dizindeki programların çekirdekle bir ilgisi yoktur. Bunlar yardımcı araçlardır. Çekirdek kodlarının içerisinde 
    yer almazlar.

    usr => Bu dizinde "geçici kök dosya sistemine (initial ramdisk)" ilişkin kodlar bulunmaktadır. Bunların bazı sistemlerde 
    konfigürasyon yoluyla çekirdek kodlarına aktarılmaktadır.

    virt => Burada çekirdeğin sanallaştırmaya hizmet eden kodları bulundurulmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										4. Ders 27/07/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzun bu bölümünde Linux sistemlerinin boot edilmesi süreci ele alınacaktır. İşletim sisteminin otomatik olarak 
    yüklenerek çalışır hale getirilmesi sürecine "boot" işlemi denilmektedir. (Boot terimi İngilizce "askeri bottan 
    (çizmeden)" gelmektedir.) Biz bilgisayar sistemine güç verdiğimizde bir süre sonra işletim sisteminin otomatik 
    yüklendiğini görmekteyiz. Aslında işletim sisteminin yüklenmesi biraz karmaşık bir süreçle gerçekleşmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Mikroişlemciler güç uygulandığında (reset edildiğinde) belli bir adresten itibaren çalışacak biçimde tasarlanmıştır. 
    Buna işlemcilerin "reset vektörü" denilmektedir. İşlemcilerin reset vektörleri genellikle fiziksel belleğin başında 
    ya da sonunda bulunmaktadır. Örneğin Intel işlemcilerinde reset vektörü belleğin sonunda, ARM işlemcilerinde ise 
    belleğin başında bulunmaktadır. İşlemci reset edildiğinde belli bir adresten çalışmaya başladığına göre orada çalışmaya 
    hazır bir kodun bulunuyor olması gerekir. Tabii bu kod RAM (DRAM) bellekte bulunamaz. Çünkü sisteme güç uygulandığında 
    RAM belleğin de içeriği sıfırlanmaktadır, ayrıca bugünkü DRAM belleklerin kullanılabilmesi için de onların donanımsal 
    olarak programlanması gerekmektedir. İşte bilgisayar sistemlerinde işlemcinin reset vektöründe tipik olarak ROM bellekler 
    bulundurulur. Eskiden bu amaçla EPROM bellekler kullanılıyordu. Artık uzunca bir süredir EEPROM (flash EPROM) bellekler 
    kullanılmaktadır. Peki reset vektöründeki kod ne yapmaktadır?

    Bugün kullandığımız DRAM bellekler güç kaynağı verilir verilmez kullanıma hazır hale getirilememektedir. Onların da 
    kullanıma hazır hale getirilmesi (initialize edilmesi) gerekir. Benzer biçimde yine bazı donanım aygıtlarının kullanılmadan 
    önce kullanıma hazır hale getirilmesi (initialize edilmesi) gerekmektedir. İşte reset vektörüne yerleştirilmiş olan 
    kodlar bilgisayar sistemini kullanıma hazır hale getirecek kodları barındırmaktadır. Tabii burada donanımdan donanıma 
    değişen bazı ayrıntılar da söz konusu olabilmektedir. Reset vektöründeki kodlar genellikle donanımı üreten firmalar 
    tarafından yazılmaktadır. Bunlar çok temel kodlar olduğu için bunlara "BIOS" ya da "firmware" gibi isimler verilmiştir. 
    Bu tür temel BIOS ya da firmware kodlarının donanımı kullanıma hazır hale getirmek için yaptığı işlemlerden bazıları 
    şunlardır:

    - DRAM bellekleri (bildiğimiz RAM) kullanıma hazır getirme
    - Çok çekirdekli sistemlerde çekirdeklerin kullanıma hazır hale getirilmesi
    - Donanım aygıtlarının ve çevre birimlerinin tespit edilmesi ve bunların tablolara (örneğin ACPI tablosu) yerleştirilmesi
    - Çevre birimlerinin (yani yardımcı işlemcilerin) kullanıma hazır hale getirilmesi
    - SSD ve HDD gibi depolama birimlerinin kullanıma hazır hale getirilmesi
    - Klavye, fare gibi giriş çıkış aygıtlarının kullanıma hazır hale getirilmesi
    - Ekran kartı gibi görüntü aygıtlarının kullanıma hazır hale getirilmesi

    Reset vektöründeki kodlar çalıştırılınca artık donanım genel olarak çalışmaya hazır bir duruma getirilmiştir. Bundan 
    sonra akışın bir biçimde sistem programcısına devredilmesi gerekir. Akışın devredilmesi tipik olarak ikincil bellekteki 
    belli bir disk bloğunun RAM'e yüklenmesi ve oradaki kodun çalıştırılması yoluyla yapılmaktadır. Tersten gidersek sistem 
    programcısı programını ikincil belleğin (diskin) bir bloğuna yerleştirir ve bu süreçler sonucunda bu program çalışır 
    hale gelir. Peki işletim sistemi nasıl yüklenmektedir? ROM'daki BIOS ya da firmware kodunun işletim sistemini yüklemesi 
    genel olarak mümkün değildir. Bunun temel nedenleri şunlardır:

    - İşletim sistemleri çok büyük olabilir. ROM'daki kod bunu yapabilecek yeterlilikte olmayabilir.
    - İşletim sistemlerinin yüklenmesi sistemden sisteme değişebilen ve nispeten karmaşık bir süreçtir. ROM'daki küçük kodun 
    bunu yapması genellikle mümkün olamamaktadır.
    - İkincil bellekte birden fazla işletim sistemi bulunuyor olabilir. Bu durumda bunların hangisinin nasıl yükleneceğine 
    karar verilmesi ve karar verilen işletim sisteminin yüklenmesi ROM'daki küçük program tarafından genellikle yapılamaz.

    O halde tek çıkar yol ROM'daki programın diskteki bir önyükleyiciyi yüklemesi ve işletim sisteminin yüklenmesinin bu 
    program tarafından yapılmasıdır. İşletim sistemlerini yükleyen bu tür programlara "önyükleyici (bootloader)" denilmektedir. 
    (Biz "bootloader" terimi yerine bunun Türkçesi olan "önyükleyici" terimini de kullanacağız.) Yani yukarıda da belirttiğimiz 
    gibi sistem programcısı diskteki önceden tespit edilmiş alana kendi kodunu yerleştirir. (Genellikle BIOS ya da firmware 
    kodları küçük bir disk bloğunu yükleyip çalıştırmaktadır.) İşte sistem programcısının özel disk bloğuna yerleştirdiği 
    bu program da önyükleyici (bootloader) programını yüklemektedir. Bugün değişik platformlarda kullanılan değişik "önyükleyici" 
    programlar bulunmaktadır. Örneğin Microsoft kendi Windows sistemleri için kendi "önyükleyici" programını kullanmaktadır. 
    Buna "Windows Boot Manager (bootmgr)" denilmektedir. UNIX/Linux sistemlerinde değişik proje grupları tarafından yazılmış 
    olan değişik önyükleyici programlar kullanılabilmektedir. Örneğin Linux sistemlerinde eskiden "LILO" isimli önyükleyici 
    yoğun biçimde kullanılıyordu. Daha sonra "GRUB" isimli önyükleyici yaygın biçimde kullanılmaya başlandı. Son yıllarda 
    SYSLINUX isimli bootloader paketi de belli bir ölçüde kullanım bulmuştur. Bu önyükleyici minimalist bir tasarıma sahiptir 
    ve daha çok küçük sistemlerde tercih edilmektedir. Gömülü Linux sistemlerinde bugün en çok tercih edilen ise "Das U-Boot" 
    ya da kısaca "U-Boot (Universal Bootloader)" denilen önyükleyici programdır.

    O halde işletim sisteminin yüklenmesi pek çok donanım ve platformda aşağıdaki aşamalardan geçilerek yapılmaktadır:

    Mikroişlemci RESET ediliyor ---> ROM'daki RESET vektöründe bulunan kodlar çalışıyor ---> ROM'daki kodlar diskteki 
    önceden belirlenmiş olan bloğu RAM'e yükleyerek çalıştırıyor ---> RAM'e yüklenen bu küçük program önyükleyici 
    programı RAM'e yüklüyor ---> Önyükleyici program işletim sistemini yükleyerek işletim sisteminin başlangıç kodlarını 
    çalıştırıyor.

    Burada birkaç soru aklınıza gelebilir. Bunlardan biri ROM'daki RESET vektöründe bulunan kodların oraya kimin tarafından 
    yerleştirilmiş olduğudur. ROM'daki RESET vektöründe bulunan kodlar çok aşağı seviyeli kodlar olduğu için bunlar genellikle 
    donanımı tasarlayan kurumlar tarafından yazılıp oraya yerleştirilmektedir. Örneğin bugün kullandığımız PC sistemlerinde 
    ROM'daki bu kodlara BIOS (Basic Input Output System) denilmektedir. BIOS kodları PC donanımını tasarlayan IBM tarafından 
    yazılmıştı. Ancak bugün BIOS üretici firmalar tarafından yazılmaktadır. Bu ROM bellekler bugün EEPROM teknolojisi ile 
    üretildikleri için BIOS güncellemesi de yapılabilmektedir. Örneğin Beaglebone Black kartlarındaki ROM programı Texas 
    Instruments (TI) firması tarafından yazılmıştır. Raspberry PI kartlarındaki ROM programı ise Broadcom firması tarafından 
    yazılmış durumdadır. Özetle ROM'da bulunan bu aşağı seviyeli kodlar ilgili donanımı tasarlayan kurumlardaki sistem 
    programcıları tarafından yazılmıştır.

    Peki ROM'daki program önyükleyici (bootloader) programını ikincil bellekte nasıl arayıp bulmaktadır? Bunun için 
    birkaç teknik kullanılabilmektedir. Birincisi ROM'daki programın doğrudan ikincil belleğin önceden belirlenmiş bloklarını 
    yüklemesidir. Örneğin klasik PC sistemlerinde ROM'daki (BIOS'taki) program diskin 0'ıncı bloğunu (yani ilk 512 byte'ı 
    içeren bloğu) yüklemektedir. (Ancak daha sonra PC sistemlerinde "UEFI BIOS" ismi altında eski klasik BIOS kodları 
    oldukça geliştirilmiştir. Artık bu UEFI BIOS kodları bazı dosya sistemlerini de tanımaktadır.) Örneğin Raspberry 
    Pi'da ROM'daki kodları FAT dosya sistemini tanıyabilmektedir. FAT dosya sistemi Microsoft'un DOS sistemlerinde 
    kullandığı karmaşık olmayan sade bir dosya sistemidir. İşte ROM'daki kodlar FAT gibi bir dosya sistemini tanıyorsa 
    oranın kök dizininde belli isimdeki dosyayı bulup RAM'e de yükleyebilmektedir.

    Sistem reset edildiğinde tüm boot işlemi bir bütünün parçaları gibi düşünülürse burada devreye giren programlara birer 
    aşama numarası da verilebilmektedir. Örneğin boot işleminin RESET vektöründe bulunan kısmına "birinci aşama bootloader  
    (stage-1 / first stage bootloader)", bu programın yüklediği programa "ikinci aşama bootloader (stage-2 / second stage 
    bootloader)" ve bunun da yüklediği programa (yani işletim sistemini yükleyen GRUB gibi programlara da) "üçüncü 
    aşama bootloader (stage-3 / third stage bootloader)" denilebilmektedir. Ancak bazı kaynaklar bu ROM'daki kodu boot 
    sürecinin bir parçası olarak ele almamaktadır. Dolayısıyla bu kaynaklar ROM kodunun kendisine değil, onun yüklediği 
    programa "birinci aşama bootloader (stage-1 / first stage bootloader), işletim sistemini yükleyen programa (yani 
    GRUB gibi) ise "ikinci aşama bootloader (stage-2 / second stage bootloader)" demektedir. Örneğin Wikipedia boot 
    işleminde donanım bileşenlerini hazır hale getiren reset vektöründeki programı "birinci aşama bootloader", işletim 
    sistemini yükleyen programı ise (GRUB gibi) "ikinci aşama bootloader" olarak isimlendirmiştir. Biz kursumuzda izleyen 
    paragraflarda bu terminolojiyi kullanacağız.

    Bugün pek çok masaüstü Linux dağıtımında GRUB isimli önyükleyici program kullanılmaktadır. Bu önyükleyici program 
    Linux çekirdek imajını bellek yükleyerek çalıştırmaktadır. Linux'un çekirdek parametreleri de bu önyükleyici tarafından 
    kullanıcıdan alınıp Linux'a verilmektedir. Biz kursumuzda çekirdek derlemesi yaparken ve sistemi çekirdekle boot 
    ederken önyükleyicinin GRUB olduğunu varsayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kursumuzdaki örnekler x86 tabanlı masaüstü bilgisayar kullanılarak verilemektedir. (Burada "masaüstü (desktop)" terimi 
    yalnızca kasalı bilgisayarlar için değil notebook'lar için de kullanılan genel bir terimdir.) x86 tabanlı masaüstü 
    bilgisayarlara tarihsel bakımdan "PC (Personal Computer)" da denilmektedir. (2000'lerin başında Apple Intel tabanlı 
    PC mimarisine geçtiyse de kullandığı PC donanımında bazı farklılıklar da oluşturmuştur. Sonra Apple'ın Intel mimarisini 
    de bırakıp ARM mimarisine geçtiğini biliyorsunuz. Ancak PC denildiğinde Intel ve ARM tabanlı macOS sistemleri kastedilmemektedir.)

    Bugün kullandığımız PC sistemlerinin temel donanımları 70'li yılların sonlarında ve 80'li yılların başlarında IBM 
    tarafından tasarlanmıştır. Bu bilgisayarlar 1980'in Aralık ayında IBM'in tasarladığı donanım ve Microsoft'un tasarladığı 
    DOS işletim sistemiyle piyasaya sürüldü. Zaman ilerledikçe bu PC donanımlarında bazı iyileştirmeler yapıldıysa da 
    temel mimari büyük ölçüde aynı kalmıştır.

    Eskiden PC'lerde klasik BIOS (legacy BIOS) kullanılıyordu. Ancak 2010'lardan sonra modern UEFI BIOS'lar yaygınlaştı. 
    Artık bugün ağırlıklı olarak UEFI BIOS'lar kullanılıyor. (Ancak bu modern UEFI BIOS'lar eski klasik BIOS (legacy BIOS) 
    gibi de çalışabilecek özelliklere sahip olabilmektedir). Biz burada klasik eski BIOS'a sahip PC'lerin boot sürecinden 
    bahsedeceğiz.

    Eski klasik BIOS'lara (legacy BIOS) sahip PC'ler reset edildiğinde BIOS kodları DRAM belleği ve pek çok donanım birimini 
    programladıktan sonra CMOS setup'ta belirtilen "boot sırasına (boot sequence)" göre ilgili medyanın 0'ıncı sektörünü 
    belleğe yükleyip akışı oraya devretmektedir. (PC mimarisinde diskten okunabilecek ya da diske yazılabilecek en küçük 
    birime "sektör (sector)" denilmektedir.) İkincil belleklerdeki ilk sektöre "MBR (Master Boot Record)" denilmektedir. 
    MBR'de toplam 512 byte'lık bir program bulunur. MBR'nin sonunda 64 byte'lık "Disk Bölümleme Tablosu (Disk Partition 
    Table)" vardır. MBR'deki program duruma göre ya bir önyükleyici programını yüklemekte ya da default durumda aktif 
    disk bölümünün 0'ıncı sektörünü RAM'e yükleyip akışı oraya devretmektedir. (PC sistemlerinde her disk bölümünün ilk 
    sektörüne (0'ıncı sektörüne) "boot sektör" denilmektedir. Boot sektör ilgili işletim sisteminin yüklenmesinden sorumludur.) 
    Tabii MBR'deki program daha büyük bir önyükleyici programını da yükleyebilmektedir. Bu durumda hangi işletim sisteminin 
    yükleneceği önyükleyici program tarafından bir menü yoluyla kullanıcıya da sorulabilmektedir. UEFI BIOS öncesindeki 
    eski BIOS sistemini kullanan (legacy BIOS) PC sistemlerindeki boot sürecini aşağıdaki gibi özetleyebiliriz:

    PC'ye güç veriliyor ve mikroişlemci RESET ediliyor ---> EEPROM'daki BIOS kodları temel hazırlık işlemlerini yapıyor 
    ---> EEPROM'daki BIOS kodları diskin ilk sektörünü (MBR) RAM'e yüklüyor ve akışı ona devrediyor ---> MBR'deki program 
    önyükleyiciyi RAM'e yüklüyor ---> Önyükleyici seçilen disk bölümünün boot sektörünü RAM'e yüklüyor ---> Seçilen disk 
    bölümünün boot sektörü işletim sistemini yüklüyor ---> Akış işletim sistemi kodlarına devrediliyor.

    Kursumuzda UEFI BIOS sistemlerinin işlevi temel çalışma mekanizması başka bir bölümde ele alınacaktır.

    Yukarıda da belirttiğimiz gibi bugünkü Linux yüklü olan Intel x86 tabanlı PC sistemlerinde genellikle önyükleyici 
    olarak GRUB tercih edilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinin kodlarına dokunmadan onunla ilgili bazı davranış değişikliklerinin yapılması temelde beş biçimde 
    sağlanabilmektedir:

    1) Çekirdek modunda çalışan "çekirdek modülleri (kernel modules)" ve "aygıt sürücüler (device drivers)" yoluyla: 
    İşletim sistemlerinin çoğu çekirdeğin bir parçası gibi işlev görecek kodların yüklenmesine ve çalıştırılmasına olanak 
    sağlamaktadır. Bu yöntemde çekirdeğin yeniden derlenmesine gerek yoktur. Zaten çekirdek modülleri ve aygıt sürücüleri 
    çalışmakta olan çekirdek içerisine yüklenmektedir. Çekirdek modüllerini ve aygıt sürücüleri "kasalı bilgisayarlardaki 
    genişleme yuvalarına takılan kartlara benzetebiliriz. Nasıl takılan bu kartlar donanımın bir parçası haline geliyorsa 
    çekirdek modülleri ve aygıt sürücüler de çekirdeğin bir parçası haline gelmektedir. Linux'taki çekirdek modüllerinin 
    ve aygıt sürücülerinin yazımı kursumuzun konularına dahil değildir.

    2) Çekirdek yüklenip başlatılırken (initialize ederken) ismine "çekirdek komut satırı parametreleri (kernel command 
    line parameters)" denilen parametreler yoluyla çekirdeğin davranışı değiştirilebilmektedir. Çekirdek komut satırı 
    parametreleri "önyükleyici tarafından" çekirdeğe iletilmektedir. Linux çekirdeğinin pek çok komut satırı parametresi 
    vardır. Bu parametreler yoluyla çekirdekte bazı davranış değişiklikleri oluşturulabilmektedir. Bunun için de çekirdeğin 
    yeniden derlenmesi gerekmez.

    3) Çekirdek derlenirken çekirdek kodlarına hiç dokunmadan bazı konfigürasyon parametreleri ile oynanarak çekirdekte 
    davranış değişiklikleri oluşturulabilmektedir. Çekirdek konfigüre edilirken bazı alt sistemler çekirdekten çıkartılabilmekte, 
    bazı alt sistemler üzerinde ince ayarlar yapılabilmektedir. Tabii çekirdek konfigüre edilirken her ne kadar biz çekirdek 
    kodlarını değiştirmiyor olsak da aslında sembolik sabitler yoluyla arka planda derleme işlemine sokulan kodlar üzerinde 
    de değişikler yapılmış olmaktadır. O halde çekirdeğin konfigüre edilmesinin iki amacı vardır:

    - Çekirdek içerisindeki alt sistemlere ilişkin bileşenlerin çekirdeğe eklenmesini ve çekirdekten çıkartılmasını sağlamak.
    - Çekirdeğin üzerinde bazı davranış değişikliklerini oluşturmak.

    Çekirdek konfigüre edildikten sonra yeniden derlenmelidir. Yani bu yöntem çekirdeğin yeniden derlenmesini gerektirmektedir.

    4) Çekirdek kodlarında doğrudan değişiklikler yapılıp çekirdek yeniden derlenebilir. Bu çekirdekte davranış değişikliği 
    oluşturmak için kullanılan en aşağı seviyeli yöntemdir.

    5) Nihayet çekirdek çalışırken de çekirdeğin davranışı bazı komutlarla (bu komutlar bazı mekanizmaları ve sistem 
    fonksiyonlarını kullanmaktadır), konfigürasyon dosyalarıyla ve bazı mekanizmalarla da (örneğin "proc" ve "sys" 
    dosya sistemi yoluyla) çekirdek davranışları değiştirilebilmektedir. Tabii bunun için de çekirdeğin yeniden derlenmesi 
    gerekmez. Örneğin sistem çalışırken bir prosesin açabileceği dosya sayısını "proc" dosya sistemi yoluyla şöyle 
    değiştirebiliriz:

    $ echo 2048 | sudo tee /proc/sys/fs/file-max
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										5. Ders 02/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'ta çekirdek komut satırı parametreleri birbirinden boşluklarla ayrılmış yazılardan oluşmaktadır. Bazı parametrelerin 
    argümanları yoktur, bazılarının vardır. Eğer parametrenin bir argümanı varsa "parametre=değer" biçiminde ('=' karakteri 
    boşluksuz olarak kullanılmalıdır) yoksa yalnızca "parametre" biçiminde belirtilmektedir. Çekirdek komut satırı parametreleri 
    tek bir yazı biçiminde çekirdeğe aktarılmaktadır. Çekirdek bu komut satırı parametrelerini kendini kullanıma hazır 
    hale getirmenin (kendini initialize etmenin) ön aşamalarında parse eder ve bu değerleri yapılandırma amacıyla kullanır. 
    Tabii çekirdeğin komut satırı parametreleri tipik olarak önyükleyici (bootloader) tarafından çekirdeğe iletilmektedir. 
    Örneğin çekirdek komut satırı parametreleri aşağıdaki gibi bir görünümde olabilir:

    console=serial0,115200 console=tty1 root=PARTUUID=382d6f16-02 rootfstype=ext4 fsck.repair=yes rootwait quiet splash 
    plymouth.ignore-serial-consoles cfg80211.ieee80211_regdom=TR

    Linux çekirdeğinin onlarca farklı komut satırı parametresi vardır. Bunların çoğu spesifik konulara ilişkindir ve ancak 
    çekirdeğin yapısını iyi bilen kişiler tarafından anlamlandırılabilir. Tabii bazı parametrelerin anlamları herkes 
    tarafından anlaşılacak kadar açıktır. Çekirdek parametrelerinin dokümantasyonuna aşağıdaki bağlantıdan erişebilirsiniz:

    https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html

    Linux çekirdeği komut satırı parametrelerini parse ederken gerçekte olmayan (yani çekirdek tarafından kullanılmayan) 
    bir parametre gördüğünde onu yok saymaktadır. Ancak biz kendi parametrelerimizin de çekirdek tarafından saklanmsını 
    sağlayabiliriz.

    Biz kursumuzda çeşitli bölümlerde çekirdeğin komut satırı parametreleri hakkında açıklamalarda bulunacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bazı durumlarda çekirdeğin sıfırdan derlenmesi gerekebilmektedir. Çekirdeğin yeniden derlenmesinin gerektiği tipik 
    durumlar şunlardır:

    - Bazı çekirdek modüllerinin ve aygıt sürücülerin çekirdek imajından çıkartılması ve dolayısıyla çekirdeğin küçültülmesi 
    için.
    - Yeni birtakım modüllerin ve aygıt sürücülerin çekirdek imajına eklenmesi için.
    - Çekirdeğe tamamen başka birtakım özelliklerin ve alt sistemlerin eklenmesi için.
    - Çekirdek üzerinde çekirdek parametreleriyle sağlanamayacak bazı konfigürasyon değişikliklerinin yapılabilmesi için.
    - Çekirdek kodlarında yapılan değişikliklerin etkin hale getirilmesi için.
    - Çekirdeğe yama (patch) yapılması için.
    - Yeni çıkan çekirdek kodlarının kullanılabilir hale getirilmesi için ya da eski çekirdeklerin kullanımını sağlamak 
    için.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin derlenmesi için öncelikle çekirdek kaynak kodlarının derleme yapılacak bilgisayara indirilmesi gerekir. 
    Bazı dağıtımlar default durumda çekirdeğin kaynak kodlarını da kurulum sırasında makineye çekmektedir. Çekirdek 
    kodları "kernel.org" sitesinde bulundurulmaktadır. Tarayıcdan "kernel.org" sitesine girip "pub/linux/kernel" 
    dizinine geçtiğinizde tüm yayınlanmış çekirdek kodlarını göreceksiniz. İndirmeyi tarayıcıdan doğrudan yapabilirsiniz. 
    Eğer indirmeyi komut satırından "wget" programıyla yapmak istiyorsanız aşağıdaki URL'yi kullanabilirsiniz:

    https://cdn.kernel.org/pub/linux/kernel/[MAJOR_VERSION].x/linux-[VERSION].tar.xz

    Buradaki MAJOR_VERSION "3", "4", "5", "6" gibi tek bir sayıyı belirtmektedir. VERSION ise çekirdeğin büyük ve küçük 
    numaralarını belirtmektedir. Örneğin biz çekirdeğin 6.9.2 versiyonunu şöyle indirebiliriz:

    $ wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.9.2.tar.xz
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek kodları indirildikten sonra onun açılması gerekir. Açma işlemi tar komutuyla aşağıdaki gibi yapılabilir:

    $ tar -xvJf linux-5.15.12.tar.xz

    Burada Linux'ta kullanılan sıkıştırma formatlarında kısaca bahsetmek istiyoruz.

    UNIX/Linux sistemlerinde kullanılan "tar" utlity programı sıkıştırma yapmamaktadır. Yalnızca dosyaları uç uca ekleyip 
    onları tek bir dosya haline getirmektedir. Bu sayede kullanılmayan dosyalar dosya sisteminde daha az yer kaplar hale 
    getirilebilir. Aynı zamanda onların iletilmesi ve kopyalanması da kolaylaştırılmış olur. Sıkışıtma programları ise 
    aslında tek bir dosyayı sıkıştırmaktadır. O halde UNIX/Linux sistemlerinde kullanıcılar bir grup dosyayı sıkıştırmak 
    için önce onları tar'layıp tek dosya haline getirirler, sonra bu dosyayı sıkıştırırlar. Bu işlemler sonucunda dosya 
    uzantısı da ".tar.gz", gibi "tar.xz" gibi dosyanın hem tar'lanmış hem de sıkıştırılmış olduğunu belirten biçimde olur. 
    Açım sırasında da önce sıkıştırılan dosyalar açılır. Buradan ".tar" dosyası elde edilir. Sonra da bu ".tar" dosyasının 
    içerisindeki dosyalar dışarı çıkartılır.

    Linux'ta çeşitli sıkıştırma yöntemleri (formatları da diyebiliriz) kullanılmaktadır. Bunların bazıları şunlardır:

    - gzip formatı (dosyaların uzantıları ".gz" biçimindedir)
    - bz2 formatı (dosyaların uzantıları ".bz2" biçimindedir)
    - xz formatı (dosyaların uzantıları ".xz" biçimindedir)
    - Klasik zip formatı (dosyaların uzantıları ".zip" ya da ".z" biçimindedir)

    Bu formatlar sıkıştırma bakımından farklı performans göstermektedir. Ancak formatın sıkıştırma performansı ne kadar 
    yüksekse işlem yapma süresi de o kadar uzun olmaktadır. Tipik olarak bu formatların sıkıştırma performansları için 
    aşağıdaki ilişki söz konusudur:

    xz > bz2 > gzip == zip

    Yani en iyi sıkıştırma "xz" formatında, daha sonra "bz2" formatında daha sonra da "gzip" formatındadır. gzip formatı 
    ile zip formatı aynı algoritmaları kullanmaktadır. Dolayısıyla bunların performansları birbirine benzerdir. Ancak 
    yukarıda da belirttiğimiz gibi sıkıştırma performansı yükselirken (yani daha iyi hale gelirken) sıkıştırma ve açma 
    için gereken zaman da uzamaktadır.

    Yukarıdaki formatlara göre sıkıştıran ve açan programlar hazır biçimde bulunmaktadır. Bu programların isimleri şunlardır:

    gzip, gunzip
    bzip2, bunzip2
    xz, unxz
    zip, unzip

    zip programının dışındaki programların hepsi tek bir dosyayı sıkıştırıp açmaktadır. Dolayısıyla yukarıda da belirttiğimiz 
    gibi eğer birden fazla dosya sıkıştırılacaksa önce onların birleştirilmesi gerekir. O halde önce "tar" programının 
    kullanımı hakkında bazı bilgiler verelim.

    tar programının pek çok komut satırı seçeneği olsa da en çok kullanıcılan seçenekler "-c" "-x" "-f" "-v" seçenekleridir. 
    "-c" tar'lamak için "-x" ise açmak için kullanılır. "-v" seçeneği programın daha fazla bilgi vermesini sağlamaktadır. 
    "-f" seçeneği bir argümanla kullanılır. Argüman ".tar" dosyasını belirtir. tar programı birden fazla dosyayı komut 
    satırı argümanıyla alabilir. Tabii birden fazla dosyayı belirtmek için kabuğun joker karakterlerinden faydalanabilirsiniz. 
    Argüman olarak dosya yerine dizinler de verilebilir. Bu durumda bu dizinin içerisindeki dosyaların hepsi tar'lanıp 
    açılmaktadır. Örneğin:

    $ tar -c -f test.tar x.txt y.txt z.txt

    Genellikle kullanıcılar seçenekleri aşağıdaki gibi birleştirmektedir:

    $ tar -cf test.tar x.txt y.txt z.txt

    Tabii burada "f" seçeneğinin seçenek listesinde en sonra olması gerekmektedir.

    gzip programı ile sıkıştırmak oldukça kolaydır. Örneğin:

    $ gzip test.tar

    Açma işlemi de şöyle yapılabilir:

    $ gunzip test.tar.gz

    Buradan biz "test.tar" dosyasını elde edeceğiz. Onu da açmamız gerekir. gzip ve gunzip programlarının eski dosyayı da 
    sildiğine dikkat ediniz. tar komutu ile hem tar'lamak hem de aynı zamanda gzip işlemini yapmak için tar komutunda "-z" 
    seçeneği kullanılmaktadır. Yani "-z" seçeneği "önce tar'la sonra gzip yap" anlamına gelmektedir. Tabii tar programı da 
    aslında kendi içerisinde gzip programını da çalıştırmaktadır. Örneğin:

    $ tar -cvzf test.tar.gz x.txt y.txt

    Burada biz dosyaları hem tar'ladık hem de sıkıştırdık. Açma işlemi de aynı biçimde yapılmaktadır:

    $ tar -xvzf test.tar.gz

    bzip2 programının kullanımı gzip programına oldukça benzemektedir. Örneğin:

    $ bzip2 test.tar

    Açım da benzer biçimde yapılmaktadır:

    $ bunzip2 test.tar.bz2

    Önce tar'layıp sonra bzip2 ile sıkıştırma işlemi tek hamlede tar programı tarafından "-j" seçeneği ile yapılabilmektedir. 
    ("-z" seçeneğinin gzip için "-j" seçeneğinin bz2 için kullanıldığına dikkat ediniz.) Örneğin:

    $ tar -cvjf test.tar.bz2 x.txt y.txt

    Açım da benzer biçimde yapılabilir:

    $ tar -xvjf test.tar.bz2

    xz programı ile sıkıştırma yapma da benzer biçimdedir. Örneğin:

    $ xz test.tar

    Açım da benzerdir:

    $ unxz test.tar.xz

    Hem tar'lamak hem de xz haline getirmek için tar programında "-J" seçeneği kullanılmaktadır. Örneğin:

    $ tar -cvJf test.tar.xz x.txt y.txt

    Açım işlemi de şöyle yapılabilir:

    $ tar -xvJf test.tar.xz
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Dağıtımlar (Ubuntu, Mint, Fedora gibi) çekirdek kodlarında küçük değişiklikler ve kendilerine özgü özelleştirmeler ve 
    yamamalar da yapabilmektedir. Debian tabanlı sistemlerde o anda makinede yüklü olan mevcut çekirdeğin ilgili dağıtıma 
    ilişkin kaynak kodlarını indirmek için aşağıdaki komutu da kullanabilirisiniz:

    $ sudo apt-get install linux-source

    Burada yükleme "/usr/src" dizinine yapılacaktır. Ancak bu komut doğrudan sıkıştırılmış dosyayı indirmektedir. Yani açım 
    yapmamaktadır. Aslında istenirse bulunulan makinedeki versiyon numarasına ilişkin dağıtıma özgü kaynak kodlar yerine 
    istenilen bir versiyona ilişkin kaynak kodlar da indirilebilir. Bunun için komutta linux-source argümanına istenilen 
    versiyonun majör, minör ve patch numarası "-majör.minör.patch" biçiminde eklenir. Örneğin:

    $ sudo apt-get install linux-source-6.8.0

    Bu biçimde indirdiğimiz çekirdek kaynak kodları Debian ya da Ubuntu depolarından indirilmektedir. Bunlar bu dağıtımlar 
    tarafından yamanmış kodlardır. Örneğin Mint'te çalışıyorsanız indirdiğiniz kodlar Ubuntu için yamanmış kodlar olacaktır.

    BBB için derleme yapmak istiyorsanız yine "kernel.org"deki kaynak kodları indirebilirsiniz. Ancak BBB için bazı 
    özelleştirmelerin de yapılmış olduğu kaynak kodların indirilip derlenmesi birtakım kolaylıklar sağlamaktadır. Bu aşağıdaki 
    komutla yapabilirsiniz:

    $ git clone https://github.com/beagleboard/linux.git

    Benzer biçimde Raspbbey Pi için de "kernel.org"deki kaynak kodlar kullanılabilir. Ancak Raspberry Pi'a özgü daha 
    güncel aygıt sürücüler ve aygıt ağacı dosyalarını içeren Linux kaynak kodlarının projenin kendi sitesinden indirilmesi 
    daha uygun olur. İndirmeyi aşağıdaki bağlantıdan yapabilirsiniz:

    $ git clone --depth=1 https://github.com/raspberrypi/linux.git
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux kaynak kodlarının versiyonlanması eskiden daha farklıydı. Çekirdeğin 2.6 versiyonlarından sonra versiyon 
    numaralandırma sistemi değiştirildi. Eskiden (2.6 ve öncesinde) versiyon numaraları çok yavaş ilerletiliyordu. 2.6 
    sonrasındaki yeni versiyonlamada versiyon numaraları daha hızlı ilerletilmeye başlanmıştır. Bugün kullanılan Linux 
    versiyonları nokta ile ayrılmış üç sayıdan oluşmaktadır:

    Majör.Minör.Patch

    Buradaki "majör numara" büyük ilerlemeleri, "minör numara" ise küçük ilerlemeleri belirtmektedir. Eskiden (2.6 ve 
    öncesinde) tek sayı olan minör numaralar "geliştirme versiyonlarını (ya da beta versiyonlarını)", çift olanlar ise 
    stabil hale getirilmiş dağıtılan versiyonları belirtiyordu. Ancak 2.6 sonrasında artık tek ve çift minör numaralar 
    arasında böyle bir anlam farklılığı kalmamıştır. Patch numarası birtakım böceklerin giderildiği ya da çok küçük 
    yeniliklerin çekirdeğe dahil edildiği versiyonları belirtmektedir. Patch numarası minör numaralardan daha küçük 
    bir ilerlemenin söz konusu olduğunu anlatmaktadır.

    Linux kaynak kodları konfigüre edilip derlendiğinde çekirdek imajının ismine bir alan daha eklenebilmektedir. Bu alana 
    biz "Extra" alanı diyeceğiz. Bu durumda çekirdek imaj ismi (kaynak kod versiyon ismi değil) şu hale gelecektir:

    Majör.Minör.Patch-Extra (Extra için -rcX, -stable, -custom, -generic gibi sözcükler kullanılabilir)

    Bu extra alanı tamamen derleme işlemini yapan kişi ya da kurumun versiyon bilgisine eklediği, onların isteklerine göre 
    belirlenmiş bir alandır. Burada Extra ile temsil edilen alanda "rcX (X burada bir sayı belirtir) "stable", "custom", 
    "generic", "realtime" gibi sözcükler bulunabilmektedir. "rc" harfleri "release candidate" sözcüklerinin kısaltmasıdır. 
    "stable" sözcüğü dağıtılan sürümün "kararlı sürüm" olduğunu belirtir. Eğer sistem programcısı çekirdekte kendisi birtakım 
    değişiklikler yapıyorsa genellikle "Extra" alanında "custom" sözcüğünü kullanır. Ayrıca "Extra" alanındaki sözcüğün başına 
    ya da sonuna versiyon ya da build numaraları getirilebilmektedir. Örneğin "custom" sözcüğünü ayrıca "-<custom_version_number>" 
    biçiminde bir versiyon numarası da izleyebilir. Buradaki numaralar sistem programcısının kendi özelleştirmesine ilişkin 
    numaralardır. "generic" sözcüğünü dağıtımlar sıkça kullanmaktadır. Bu "generic" sözcüğü çekirdeğin "genel kullanım için 
    konfigüre edilmiş olduğunu belirtmektedir. "realtime" sözcüğü genellikle gerçek zamanlı bir yapılandırmada kullanılmaktadır. 
    "generic" ve "realtime" sözcüklerinin öncesinde "-N-" biçiminde bir sayı da bulunabilmektedir. Bu sayı "dağıtıma özgü 
    yama ya da derleme numarasını belirtmektedir. Örneğin:

    6.8.0-51-generic

    Burada -51 yapılandırmaya özgü bir numara belirtmektedir. "-generic" ise yapılandırmanın genel kullanım için olduğunu 
    belirtmektedir.

    Çalışmakta olan Linux sistemi hakkında bilgiler "uname -a" komutu ile elde edilebilir. Örneğin:

    $ uname -a
    Linux kaan-virtual-machine 5.15.0-91-generic Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

    Bu bilgi içerisinden yalnızca çekirdek versiyonu görüntülenmek isteniyorsa "uname -r" seçeneği kullanılmalıdır:

    $ uname -r
    6.8.0-51-generic

    Buradan biz çekirdeğin "6.8.0" sürümünün kullanıldığını anlıyoruz. Burada genel yapılandırılmış bir çekirdek söz 
    konusudur. "91" sayısı dağıtıma özgü yama ya da derleme numarasını belirtir.

    Daha önceden de belirttiğimiz gibi "uname" komutu bu bilgileri "/proc" dosya sisteminin içerisinde almaktadır. 
    Örneğin:

    $ cat /proc/version
    Linux version 5.15.0-91-generic (buildd@lcy02-amd64-045) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld
    (GNU Binutils for Ubuntu) 2.38) #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi büyük projelerin derlenmesi için "build otomasyon araçları" denilen araçlar kullanılmaktadır. Bunların 
    en yaygın kullanılanı "make" denilen araçtır. Linux çekirdeklerinin derlenmesi de "make" aracı ile yapılmaktadır. 
    Ancak Linux çekirdeklerinin derlenmesinde projeye özgü bazı yapılar ve yöntemler de kullanılmıştır. Buna "KConfig 
    sistemi" ya da "KBuild sistemi" denilmektedir. Biz önce çekirdek derleme işleminin hangi adımlardan geçilerek yapılacağını 
    göreceğiz. Sonra çekirdeğin önemli konfigürasyon parametreleri üzerinde biraz duracağız. Sonra da çekirdekte bazı 
    değişiklikler yapıp değiştirilmiş çekirdekle sistemin açılmasını sağlayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'ta çekirdek derlemesi tipik olarak aşağıdaki aşamalardan geçilerek gerçekleştirilmektedir:

    1) Derleme öncesinde derlemenin yapılacağı makinede bazı programların yüklenmiş olması gerekmektedir. Çünkü KBuild 
    sistemi yalnızca binary araçları değil bazı başka kütüphaneleri de kullanmaktadır. Çekirdeğin derlenmesi için gerekebilecek 
    programları şöyle yükleyebilirsiniz:

    $ sudo apt update
    $ sudo apt install build-essential libncurses-dev bison flex libssl-dev wget gcc-arm-linux-gnueabihf \
    binutils-arm-linux-gnueabihf libelf-dev dwarves

    2) Çekirdek kodları indirilerek açılır. Biz bu konuyu yukarıda ele almıştık. İndirmeyi şöyle yapabiliriz:

    $ wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.9.2.tar.xz

    Bu işlemden sonra "linux-6.9.2.tar.xz" isimli dosya indirilmiş durumdadır. Onu aşağıdaki gibi açabiliriz:

    $ tar -xvJf linux-6.9.2.tar.xz

    Bu işlemden sonra "linux-6.9.2" isminde bir dizin oluşturulacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										6. Ders 03/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    3) Çekirdek derlenmeden önce konfigüre edilmelidir. Çekirdeğin konfigüre edilmesi birtakım çekirdek özelliklerin 
    belirlenmesi anlamına gelmektedir. Konfigürasyon bilgileri çekirdek kaynak kod ağacının kök dizininde (örneğimizde 
    "linux-6.9.2" dizini) ".config" ismiyle bulunmalıdır. Bu ".config" dosyası default durumda kaynak dosyaların kök 
    dizininde bulunmamaktadır. Bunun çekirdeği derleyen kişi tarafından oluşturulması gerekmektedir. Çekirdek konfigürasyon 
    parametreleri oldukça fazladır ve bunların bazılarının anlamlandırılması özel bilgi gerektirmektedir. Biz izleyen 
    paragraflarda önemli çekirdek konfigürasyon parametrelerini açıklayacağız. Çekirdek konfigürasyon parametreleri çok 
    fazla olduğu için bunlar bazı genel amaçları karşılayacak biçimde default değerlerle önceden oluşturulmuş durumdadır. 
    Önceden oluşturulmuş default konfigürasyon dosyaları kaynak kod ağacında "arch/<mimari>/configs" dizininin içerisinde 
    bulunmaktadır. Örneğin Intel x86 mimarisi için bu default konfigürasyon dosyaları şöyledir:

    $ ls arch/x86/configs
    hardening.config  i386_defconfig  tiny.config  x86_64_defconfig  xen.config

    Burada biz 64 bit Linux sistemleri için "x86_64_defconfig" dosyasını kullanabiliriz. O halde bu dosyayı kaynak dosyaların 
    bulunduğu dizininin kök dizinine ".config" ismiyle kopyalayabiliriz:

    $ cp arch/x86/configs/x86_64_defconfig .config

    Biz bütün işlemlerde çekirdek kaynak kodlarının kök dizininde bulunduğumuzu (current working directory) varsayacağız. 
    Ancak burada bir noktaya dikkatinizi çekmek istiyoruz. Linux kaynak kodlarındaki default konfigürasyon dosyaları 
    minimalist biçimde konfigüre edilmiştir. Bu nedenle pek çok modül bu default konfigürasyon dosyalarında işaretlenmiş 
    değildir. Bu tür denemeleri zaten çalışan çekirdeğin derlenmesinde kullanılan konfigürasyon dosyalarından hareketle 
    yaparsanız daha fazla modül dosyası oluşturulabilir ancak daha az zahmet çekebilirsiniz. Linux sistemlerinde genel 
    olarak "/boot" dizini içerisinde "config-<çekirdek_sürümü>" ismiyle mevcut çekirdeğe ilişkin konfigürasyon dosyası 
    bulundurulmaktadır.

    Burada bir noktaya dikkatinizi çekmek istiyoruz. Çekirdek kaynak kodlarındaki "arch/<platform>/configs" dizinindeki 
    "x86_64_defconfig" konfigürasyon dosyası ".config" ismiyle kopyalandıktan sonra ayrıca "make menuconfig" ya da 
    "make oldconfig" gibi bir işlemle onun satırlarına delenecek çekirdekte bulunan yeni birtakım özelliklere ilişkin 
    bazı default değerlerin de eklenmesi gerekir. Bu işlem sonraki aşamada açıklayacağımız "make menuconfig" komutyuyla 
    gerçekleştirilmektedir.

    Aslında ".config" dosyasını oluşturmanın başka alternatif yolları da vardır:

    make defconfig: Bu komut çalıştığımız sisteme uygun olan konfigürasyon dosyasını temel alarak mevcut donanım bileşenlerini 
    de gözden geçirerek sistemin açılması için gerekli minimal bir konfigürasyon dosyasını ".config" ismiyle oluşturmaktadır. 
    Örneğin biz 64 bit Intel sistemine ilişkin bir bilgisayarda çalışıyorsak "make defconfig" dediğimizde "arch/x86/configs/x86_64_defconfig" 
    dosyası temel alınarak o anda çalışılmakta olan çekirdek donanımları da göz önünde bulundurularak nispeten minimal 
    olan bir konfigürasyon dosyası oluşturmaktadır.

    make oldconfig: Bu seçeneği kullanmak için kaynak kök dizinde bir ".config" dosyasının bulunuyor olması gerekir. 
    Ancak bu seçenek "KConfig" dosyalarındaki ve kaynak dosya ağacındaki diğer değişiklikleri de göz önüne alarak bu eski 
    ".config" dosyasını eğer söz konusu mimaride birtakım değişiklikler yapılmışsa o değişikliklere uyumlandırmaktadır. Yani örneğin 
    biz eski bir ".config" dosyasını kullanıyor olabiliriz. Ancak çekirdeğin yeni versiyonlarında ek birtakım başka konfigürasyon 
    parametreleri de eklenmiş olabilir. Bu durumda "make oldconfig" bize bu eklenenler hakkında da bazı sorular sorup bunların 
    dikkate alınmasını sağlayacaktır. Başka bir deyişle "make oldconfig" eski bir konfigürasyon dosyasını yeni çekirdekler 
    için uyumlandırmaktadır.

    make <platform>_defconfig: Bu seçenek belli bir platformun default konfig dosyasını ".config" dosyası olarak save 
    etmektedir. Örneğin biz Intel makinelerinde çalışıyor olabiliriz ancak "BeagleBone Black (BBB)" için default konfigürasyon 
    dosyası oluşturmak isteyebiliriz. Eğer biz "make defconfig" yaparsak Intel tabanlı bulunduğumuz platform dikkate 
    alınarak ".config" dosyası oluşturulur. Ancak biz burada örneğin "make bb.org_defconfig" komutunu uygularsak bu durumda 
    Intel mimarisinde çalışıyor olsak da "bb.org_defconfig" konfigürasyon dosyası ".config" olarak save edilir. Tabii biz 
    aslında bu komutu kullanmak yerine ilgili platformun konfigürasyon dosyasını manuel olarak da ".config" biçiminde 
    kopyalayabiliriz.

    make modules: Bu seçenek ile yalnızca modüller derlenir. Yani bu seçenek ".config" dosyasında belirtilen aygıt sürücü 
    dosyalarını derler ancak çekirdek derlemesi yapmaz. Yalnızca "make" işlemi zaten aynı zamanda bu işlemi de yapmaktadır.

    make uninstall: "make install" işlemi ile yapılanları geri alır.

    Aşağıdaki "make xxxconfig" komutları ise seyrek kullanılmaktadır:

    make allnoconfig: Tüm seçenekleri "hayır (no)" olarak ayarlar (minimal özellikler).

    make allyesconfig: Tüm seçenekleri "evet (yes)" olarak ayarlar (maksimum özellikler).

    make allmodconfig: Tüm aygıt sürücülerin çekirdeğin dışında modül (module) biçiminde derleneceğini belirtir.

    make localmodconfig: Sistemde o anda yüklü modüllere dayalı bir yapılandırma dosyası (".config" dosyası) oluşturur.

    make silentoldconfig: Yeni seçenekler için onları görmezden gelir ve o yeni özellikler ".config" dosyasına yansıtılmaz.

    make dtbs: Kaynak kod ağacında "/arch/platform/boot/dts" dizinindeki aygıt ağacı kaynak dosyalarını derler ve "dtb" 
    dosyalarını elde eder. Gömülü sistemlerde bu işlemin yapılması ve her çekirdek versiyonuyla o versiyonun "dtb" dosyasının 
    kullanılması tavsiye edilir. Ancak gömülü sistemlerde zaten "make" işlemi aygıt ağacı dosyalarını da derlemektedir.

    Yukarıda da belirttiğimiz gibi aslında pek çok dağıtım o anda yüklü olan çekirdeğe ilişkin konfigürasyon dosyasını "/boot" 
    dizini içerisinde "config-$(uname -r)" ismiyle bulundurmaktadır. Örneğin kursun yapılmakta olduğu Mint dağıtımında "/boot" 
    dizinin içeriği şöyledir:

    $ ls /boot
    config-6.8.0-51-generic      initrd.img.old
    System.map-6.8.0-51-generic  efi
    grub                         vmlinuz
    initrd.img                   vmlinuz-6.8.0-51-generic
    initrd.img-6.8.0-51-generic

    Buradaki "config-6.8.0-51-generic" dosyası çalışmakta olduğumuz çekirdekte kullanılan konfigürasyon dosyasıdır. Buradaki 
    "config-6.8.0-51-generic" dosyası sistem açılırken herhangi bir biçimde kullanılmamaktadır. (Yani bu dosyayı silseniz 
    hiçbir sorun oluşmaz.) Bu dosya o çekirdeği yeniden derleyecek kişiler için kolaylık sağlamak amacıyla bulundurulmaktadır.

    Daha önceden de belirttiğimiz gibi eğer çalışılan sistemdeki konfigürasyon dosyasını temel alacaksanız bu dosyayı Linux kaynak 
    kodlarının bulunduğu kök dizine ".config" ismiyle kopyalayabilirsiniz:

    $ cp /boot/config-$(uname -r) .config

    Fakat eski bir konfigürasyon dosyasını yeni bir çekirdekle kullanmak için ayrıca "make oldconfig" işleminin de yapılması 
    gerekmektedir. Sonraki maddede göreceğimiz "make menuconfig" işlemi aynı zamanda "make oldconfig" işlemini de kendi 
    içerisinde barındırmaktadır.

    4) Şimdi elimizde pek çok değerin set edilmiş olduğu ".config" isimli bir konfigürasyon dosyası vardır. Artık bu konfigürasyon 
    dosyasından hareketle yalnızca istediğimiz özellikleri değiştirebiliriz. Bunun için "make menuconfig" komutunu kullanabiliriz:

    $ make menuconfig

    Bu komut ile birlikte text ekranda konfigürasyon seçenekleri listelenecektir. Tabii buradaki seçenekler ".config" dosyasındaki 
    içerikten hareketle oluşturulmuş durumdadır. Bunların üzerinde değişiklikler yaparak ".config" dosyasını yeniden save edebiliriz. 
    Aslında "make menuconfig" işlemi hiç ".config" dosyası oluşturulmadan doğrudan da yapılabilmektedir. Bu durumda hangi sistemde 
    çalışılıyorsa o sisteme özgü default config dosyası temel alınmaktadır. Biz en azından "General stup/Local version - append 
    to kernel release" seçeneğine "-custom" gibi bir sonek girmenizi böylece yeni çekirdeğe "-custom" soneki iliştirmenizi tavsiye 
    ederiz. Yukarıda da belirttiğimiz gibi "make menuconfig" işlemi zaten "make oldconfig" işlemini de kendi içerisinde barındırmaktadır.

    Peki biz hazır bir ".config" dosyasını kaynak kod ağacının kök dizinine kopyaladıktan sonra hiç "make menuconfig" ya da 
    "make oldconfig" yazmazsak ne olur? Bu durumda sorun çıkmayabilir. Eğer kaynak kod çekirdeği yeniyse "make" işlemi sırasında 
    "make oldconfig" gibi bir işlem de yapılmaktadır. Ancak biz ".config" dosyasını kaynak kod ağacının kök dizinine kopyaladıktan 
    sonra "make menuconfig" ya da "make oldconfig" işlemini yapmanızı salık veriyoruz. "make menuconfig" işlemini yapıyorssanız 
    ayrıca "make oldconfig" işlemini yapmanıza gerek yoktur.

    ".config" dosyası elde edildiğinde çekirdek imzalamasını ortadan kaldırmak için dosyayı açıp aşağıdaki özellikleri belirtildiği 
    gibi değiştirebilirsiniz (bunların bazıları zaten default durumda aşağıdaki gibi de olabilir):

    CONFIG_SYSTEM_TRUSTED_KEYS=""
    CONFIG_SYSTEM_REVOCATION_KEYS=""
    CONFIG_SYSTEM_TRUSTED_KEYRING=n
    CONFIG_SECONDARY_TRUSTED_KEYRING=n

    CONFIG_MODULE_SIG=n
    CONFIG_MODULE_SIG_ALL=n
    CONFIG_MODULE_SIG_KEY=""

    Çekirdek imzalaması konusu daha ileride ele alınacaktır.

    Yukarıda da belirttiğimiz gibi derlenecek çekirdeklere yerel bir versiyon belirteci ve numarası da atanabilmektedir. 
    Bu işlem Bu "make menuconfig" menüsünde "General Setup/Local version - append custom release" seçeneği kullanılarak 
    ya da ".config" dosyasında "CONFIG_LOCALVERSION" satırı edit edilerek yapılabilir. Örneğin:

    CONFIG_LOCALVERSION="-custom"

    Artık çekirdek sürümüne "-custom" sonekini eklemiş olduk.

    5) Derleme işlemi için "make" komutu kullanılmaktadır. Örneğin:

    $ make

    Eğer derleme işleminin birden fazla CPU ya da çekirdek ile yapılmasını istiyorsanız "-j<cpu_sayısı>" seçeneğini 
    komuta dahil edebilirsiniz. Çalışılan sistemdeki CPU sayısının "nproc" komutuyla elde edildiğini anımsayınız. O halde 
    biz derleme için make komutunu şöyle kullanabiliriz:

    $ make -j$(nproc)

    Derleme işlemi bittiğinde ürün olarak biz "çekirdek imajını (yani çekirdek kodlarının bulunduğu dosyayı)", "çekirdek 
    tarafından yüklenecek olan modül dosyalarını (aygıt sürücü dosyalarını)" ve "diğer bazı dosyaları" elde etmiş oluruz. 
    Derleme işleminden sonra oluşturulan bu dosyalar ve onların yerleri şöyledir (buradaki <çekirdek_sürümü> "uname -r" 
    ile elde edilecek yazıyı belirtiyor):

    - Sıkıştırılmış Çekirdek Imajı: "arch/<platform>/boot" dizininde "bzImage" ismiyle oluşturulmaktadır. Denemeyi yaptığımız 
    Intel makinede dosyanın yol ifadesi "arch/x86_64/boot/bzImage" biçimindedir. (Ancak buradaki dosya x86_64 platformu için 
    "arch/x86/boot/bzImage" dosyasına sembolik link de yapılmış olabilir.)

    - Çekirdeğin Sıkıştırılmamış ELF İmajı: Kaynak kök dizininde "vmlinux" ismiyle oluşturulmaktadır.

    - Çekirdek Modülleri (Aygıt Sürücü Dosyaları): Çekirdek modülleri "drivers" dizininin altındaki dizinlerde, "fs" dizininin 
    altındaki dizinlerde ve "net" dizininin altındaki dizinlerde bulunur. Ancak "make modules_install" ile bunların hepsi 
    belirli bir dizine çekilebilir.

    - Çekirdek Sembol Tablosu: Kaynak kök dizininde "System.map" ismiyle bulunur. Çekirdek sembol tablosundan yalnızca çekirdek 
    debug edilirken faydalanılmaktadır. Bu dosya silinse bile sistemin çalışmasında bir sorun oluşmaz.

    Çekirdeğin derlenmesi ne kadar zaman almaktadır? Şüphesiz bu derlemenin yapıldığı makineye göre değişebilir. Ancak derleme 
    süresinin uzamasına yol açan en önemli etken çekirdek konfigüre edilirken seçilen modül (aygıt sürücü) sayısıdır. Pek 
    çok dağıtım "belki ileride lazım olur" gerekçesiyle konfigürasyon dosyalarında pek çok modülü dahil etmektedir. Bu 
    nedenle bir dağıtımın konfigürasyon dosyasını kullandığınız zaman çekirdek derlemesi uzayacaktır. Ayrıca çekirdek 
    konfigüre edilirken çok fazla modülün dahil edilmesi modüllerin çok fazla yer kaplamasına da yol açabilmektedir. Çekirdek 
    kodlarındaki platforma özgü default konfigürasyon dosyaları daha minimalist bir biçimde oluşturulmuş durumdadır. Derleminin 
    yapıldığı makine ve o makinedeki CPU sayısı da önemlidir. Örneğin sanal makineler genellikle düşük donanım konfigürasyonuyla 
    çalıştırıldığı için sanal makinelerde derleme süresi uzayacaktır. Tabii çekirdek bütünsel olarak bir kez derlendikten sonra 
    çekirdek kodlarında değişiklik yapıp çekirdeği yeniden derlemek istediğimizde artık derleme süresi bütünsel derleme kadar 
    uzun olmayacaktır. Çekirdek kodlarını değiştirdiğimizde ya da çekirdek kodlarına yeni bir dosya ya da dizin eklediğimizde 
    hangi dosyaların yeniden derleneceği yapılan değişikliğin ya da eklemelerin yerine göre değişebilmektedir. Temel dosyalardaki 
    değişiklikler çok fazla dosyanın yeniden derlenmesine yol açmaktadır.

    Bu tür durumlarda biz kursumuzda zamanı kısaltmak için ana makinenin Linux olduğu makine de kullanacağız. Bu makineye 
    dersin yapıldığı Windows host sisteminden uzak bağlantıyla bağlanacağız.

    6) Derleme sonrasında farklı dizinlerde oluşturulmuş olan aygıt sürücü dosyalarını (modülleri) belli bir dizine kopyalamak 
    için "make modules_install" komutu kullanılmaktadır. Bu komut seçeneksiz kullanılırsa default olarak "/lib/modules/<çekirdek_sürümü>" 
    dizinine kopyalama yapar. Her ne kadar bu komut pek çok ".ko" uzantılı aygıt sürücü dosyasını hedef dizine kopyalıyorsa 
    da bunlar çekirdek tarafından otomatik olarak yüklenmemektedir. Bu modüller kullanıcı tarafından yapılan birtakım işlemler 
    sonucunda başka bir deyişle ancak talep edildiğinde yüklenmektedir. Örneğin:

    $ sudo make modules_install

    Aslında "make modules_install" komutunun modül dosyalarını (aygıt sürücü dosyalarını) istediğimiz bir dizine kopyalamasını 
    da sağlayabiliriz. Bunun için INSTALL_MOD_PATH çevre değişkeni kullanılmaktadır. Örneğin:

    Eskiden make işlemi çekirdek modüllerinin derlenmesini sağlamıyordu. Çekirdek modüllerinin derlenmesi için 
    "make modules" işlemi gerekiyordu. Ancak çekirdeğin 2.6 versiyonundan itibaren make işlemi zaten çekirdek modüllerinin
    de derlenmesini sağlamaktadır.

    $ sudo INSTALL_MOD_PATH=modules make modules_install

    Burada aygıt sürücü dosyaları "/lib/modules/<çekirdek_sürümü>" dizinine değil bulunulan yerdeki "modules" dizinine 
    kopyalanacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki "make modules_install" komutu yalnızca modül dosyalarını mı hedef dizine kopyalamaktadır? Hayır aslında bu 
    komut modül dosyalarının kopyalanması dışında bazı dosyaları da oluşturup onları da hedef dizine kopyalamaktadır. 
    make modules_install komutu sırasıyla şunları yapmaktadır:

    - Modül dosyalarını "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.dep" isimli dosyayı oluşturur ve bunu "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.alias" isimli dosyayı oluşturur ve bunu "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.order" isimli dosyayı oluşturur ve "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.
    - "modules.builtin" isimli dosyayı "/lib/modules/<çekirdek_sürümü>" dizinine kopyalar.

    Aslında burada oluşturulan dosyaların bazıları mutlak anlamda bulundurulmak zorunda değildir. Ancak sistemin öngörüldüğü 
    gibi işlev göstermesi için bu dosyaların ilgili dizinde bulundurulması uygundur.

    Bir aygıt sürücü başka aygıt sürücüleri de kullanıyor olabilir. Bu durumda bu aygıt sürücü yüklenirken onun kullandığı 
    tüm sürücülerin özyinelemeli olarak yüklenmesi gerekir. İşte "modules.dep" dosyası bir aygıt sürücünün yüklenmesi için 
    başka hangi aygıt sürücülerin yüklenmesi gerektiği bilgisini tutmaktadır. Aslında "modules.dep" bir text dosyadır. Bu 
    text dosya" satırlardan oluşmaktadır. Satırların içeriği şöyledir:

    <modül_yolu>: <bağımlılık1> <bağımlılık2> ...

    Dosyanın içeriğine şöyle örnek verebiliriz:
    ...
    kernel/arch/x86/crypto/nhpoly1305-sse2.ko.zst: kernel/crypto/nhpoly1305.ko.zst kernel/lib/crypto/libpoly1305.ko.zst
    kernel/arch/x86/crypto/nhpoly1305-avx2.ko.zst: kernel/crypto/nhpoly1305.ko.zst kernel/lib/crypto/libpoly1305.ko.zst
    kernel/arch/x86/crypto/curve25519-x86_64.ko.zst: kernel/lib/crypto/libcurve25519-generic.ko.zst
    ...

    Eğer bu "modules.dep" dosyası olmazsa bu durumda "modeprob" komutu çalışmaz ve çekirdek modülleri yüklenirken eksik 
    yükleme yapılabilir. Dolayısıyla sistem düzgün bir biçimde açılmayabilir. Eğer bu dosya elimizde yoksa ya da bir 
    biçimde silinmişse bu dosyayı yeniden oluşturabiliriz. Bunun için "depmod -a" komutu kullanılmaktadır. Komut doğrudan 
    kullanıldığında o anda çekirdek sürümü için "modules.dep" dosyasını oluşturmaktadır. Örneğin:

    $ sudo depmod -a

    Ancak siz yüklü olan başka bir çekirdek sürümü için "modules.dep" dosyasını oluşturmak istiyorsanız bu durumda çekirdek 
    sürümünü de komut satırı argümanı olarak aşağıdaki gibi komuta vermelisiniz:

    $ sudo depmod -a <çekirdek sürümü>

    Tabii depmod komutunun çalışabilmesi için "/lib/modules/<çekirdek_sürümü>" dizininde modül dosyalarının bulunuyor olması 
    gerekir. Çünkü bu komut bu dizindeki modül dosyalarını tek tek bulup ELF formatının ilgili bölümlerine bakarak modülün 
    hangi modülleri kullandığını tespit ederek "modules.dep" dosyasını oluşturmaktadır.

    "modules.alias" dosyası belli bir isim ya da id ile aygıt sürücü dosyasını eşleştiren bir text dosyadır. Bu dosyanın 
    bulunmaması bazı durumlarda sorunlara yol açmayabilir. Ancak örneğin USB port'a bir aygıt takıldığında bu aygıta ilişkin 
    aygıt sürücünün hangisi olduğu bilgisi bu dosyada tutulmaktadır. Bu durumda bu dosyanın olmayışı aygıt sürücünün yüklenememesine 
    neden olabilir. Dosyanın içeriği aşağıdaki formata uygun satırlardan oluşmaktadır:

    alias <tanımlayıcı> <modül_adı>

    Örnek bir içerik şöyle olabilir:

    ...
    alias usb:v05ACp*d*dc*dsc*dp*ic*isc*ip*in* apple_mfi_fastcharge
    alias usb:v8086p0B63d*dc*dsc*dp*ic*isc*ip*in* usb_ljca
    alias usb:v0681p0010d*dc*dsc*dp*ic*isc*ip*in* idmouse
    alias usb:v0681p0005d*dc*dsc*dp*ic*isc*ip*in* idmouse
    alias usb:v07C0p1506d*dc*dsc*dp*ic*isc*ip*in* iowarrior
    alias usb:v07C0p1505d*dc*dsc*dp*ic*isc*ip*in* iowarrior
    ...

    Bu dosya bir biçimde silinirse yine "depmod -a" komutu ile oluşturulabilir. (Yani "depmod" komutu yalnızca "modules.dep" 
    dosyasını değil bu dosyayı da oluşturmaktadır.)

    "modules.order" dosyası aygıt sürücü dosyalarının yüklenme sırasını barındıran bir text dosyadır. Bu dosyanın her 
    satırında bir çekirdek aygıt sürücüsünün dosya yol ifadesi bulunur. Daha önce yazılmış aygıt sürücüler daha sonra 
    yazılanlardan daha önce yüklenir. Bu dosyanın olmaması genellikle bir soruna yol açmaz. Ancak modüllerin belli sırada 
    yüklenmemesi bazı durumlarda bozukluklara da neden olabilmektedir. Bu dosyanın silinmesi durumunda yine bu dosya da 
    "depmod -a" komutuyla oluşturulabilmektedir.

    7) Eğer gömülü sistemler için derleme yapıyorsanız kaynak kod ağacında "arch/<platform>/boot/dts" dizini içerisindeki aygıt 
    ağacı kaynak dosyalarını da derlemelisiniz. Tabii elinizde zaten o versiyona "özgü aygıt (device tree blob)" dosyası 
    bulunuyor olabilir. Bu durumda bu işlemi hiç yapmayabilirsiniz. Aygıt ağacı kaynak dosyalarını derlemek için "make dtbs" 
    komutunu kullanabilirsiniz:
<BURADA KALDIK>
    $ make dtbs

    Derlenmiş aygıt ağacı dosyaları "arch/<platform>/boot/dts" dizininde ya da bu dizinin altındaki ilgili "vendor" dizininde 
    oluşturulacaktır. Yukarıda da belirttiğimiz gibi aygıt ağaçları gömülü sistemlerde kullanılmaktadır. Intel tabanlı PC'lerde 
    donanım birimlerinin tespit edilmesi otomatik olarak ACPI protokolü yoluyla yapıldığı için aygıt ağacı dosyaları bu platformda 
    kullanılmamaktadır. Ancak örneğin ARM platformunu kullanan gömülü sistemlerde ya da BBB ve Raspberry Pi gibi SBC'lerde aygıt 
    ağaçları kullanılmaktadır.

    8) Bizim çekirdek imajını, geçici kök dosya sistemine ilişkin dosyayı (bunu bizim oluşturmamız gerekir) ve aygıt ağacı 
    dosyasını "/boot" dizinine kopyalamamız gerekir. Ancak aslında bu işlem de "make install" komutuyla otomatik olarak 
    yapılabilmektedir. "make install" komutu bu dosyaları "/boot" dizinine kopyalamanın yanı sıra aynı zamanda GRUB önyükleyici 
    programın konfigürasyon dosyalarında da güncellemeler yapıp yeni çekirdeğin GRUB menüsü içerisinde görünmesini de 
    sağlamaktadır. Komut şöyle kullanılabilir:

    $ sudo make install

    Burada biz "geçici kök dosya sistemi ("initial ramdisk" ya da "initrd") diye bir terim kullandık. Geçici kök dosya sistemi 
    diskteki asıl kök dosya sistemi mount edilene kadar geçici bir süre sanki diskteki dosya sistemiymiş gibi işlev gören bir 
    RAM disk imajıdır. Tipik Linux sistemlerinde önce geçici kök dosya sistemi mount edilerek temel dosyalara oradan erişilir. 
    Sonra bu geçici dosya sistemi RAM'den atılıp diskteki gerçek kök dosya sistemi mount edilmektedir. Şimdi "geçici kök dosya 
    sistemine ne gerek var, doğrudan diskteki asıl kök dosya sistemi neden kullanılamıyor?" sorusu aklınıza gelebilir. Geçici 
    kök dosya sistemine gereksinimi basit bir örnekle anlayabiliriz. Diyelim ki diske erişmekte kullanılan aygıt sürücüsü 
    çekirdeğin içerisine yerleştirilmemiş olsun yani dışarıda "lib/modules/$(uame -r)" dizininde bir dosya biçiminde bulunuyor 
    olsun. Şimdi çekirdeğin diske erişebilmesi için bu aygıt sürücüye ihtiyacı olacaktır. Ancak aygıt sürücü de disktedir. 
    İşte böyle bir durumda bu aygıt sürücüleri de barındıran bir RAM disk dosya sistemi oluşturulmakta ve önyükleyici 
    tarafından (örneğin GRUB önyükleyicisi) bu dosya da RAM'e yüklenmektedir. Böylece çekirdek artık diske erişebilir 
    hale gelir. Tabii bu gereklilik yalnızca diske erişim için söz konusu değildir. Diske erişmeden önce de başka aygıt 
    sürücülere gereksinim olabilmektedir. Ayrıca bazı kabuk komutlarının da çalıştırılabilmesi gerekebilmektedir. Peki 
    bir Linux sistemi hiç geçici kök dosya sistemi olmadan da boot edilebilir mi? Evet teorik olarak bu mümkündür. Eğer 
    çekirdeğin gereksinim duyacağı bütün aygıt sürücü dosyaları konfigürasyon aşamasında çekirdeğin içerisine gömülmüşse 
    geçici kök dosya sistemi oluşturmadan da sistem boot edilebilir. Ancak bu sırada çözülmesi gereken problemlerle de 
    karşılaşılabilmektedir. Özellikle masaüstü sistemlerinde geçici kök dosya sistemi olmadan sistemi boot etmek oldukça 
    zahmetlidir. Geçici kök dosya sistemi aynı zamanda işletim sistemini "güvenli kipte (safe mode)" açmak için de 
    kullanılmaktadır. Geçici kök dosya sistemi sistem güncellemelerinde de kullanılmaktadır. Pek çok durumda çalışmakta 
    olan dosyalar diskte değiştirilemediği için mecburen sistem güncellemeleri geçici kök dosya sistemi yoluyla yapılmaktadır.

    "make install" komutu sırasıyla yapılanlar şunlardır:

    - Çekirdek imajı "arch/<platform>/boot/bzImage" dizininden alınarak hedef "/boot" dizinine "vmlinuz-<çekirdek_sürümü>" 
    ismiyle kopyalanır.
    - "System.map" dosyası hedef "/boot" dizinine "System.map-<çekirdek_sürümü>" ismiyle kopyalanır.
    - ".config" dosyası "/boot" dizinine "config-<çekirdek_sürümü>" ismiyle kopyalanır.
    - "Geçici kök dosya sistemine ilişkin dosyayı oluşturur ve hedef "/boot" dizinine "initrd.img-<çekirdek_sürümü>" 
    ismiyle kopyalanır. (Aslında "make install" geçici kök dosya sistemini "update-initramfs" isimli programla oluşturmaktadır.)
    - Eğer GRUB önyükleyicisi kullanılıyorsa GRUB konfigürasyonu güncellenir ve GRUB menüsüne yeni girişler eklenir.

    Böylece sistemin otomatik olarak yeni çekirdekle açılması sağlanır.

    "make install" komutu uygulandığında eğer çekirdeğin versiyon bilgisi aynı ise "/boot" dizinindeki bir önceki kurulumun 
    dosyaları ".old" uzantısıyla saklanmaktadır. Böylece son "make install" komutundan önceki kuruluma manuel olarak geri 
    dönebilirsiniz. Örneğin yeniden aynı çekirdek versiyonunu "make install" yaptığımızda "/boot" dizininin içeriği şöyle 
    olacaktır:

    kaan@kaan-Huawei:~/Study/LinuxKernel/linux-6.9.2$ ls -l /boot
    total 655480
    -rw-r--r-- 1 root root    287375 Haz  7  2024 config-6.8.0-38-generic
    -rw-r--r-- 1 root root    287766 Ağu 15 11:57 config-6.9.2-custom
    -rw-r--r-- 1 root root    287766 Ağu 15 11:55 config-6.9.2-custom.old
    drwx------ 3 root root      4096 Oca  1  1970 efi
    drwxr-xr-x 6 root root      4096 Ağu 15 11:57 grub
    lrwxrwxrwx 1 root root        23 Ağu 10 11:20 initrd.img -> initrd.img-6.9.2-custom
    -rw-r--r-- 1 root root  73052139 Kas 19  2024 initrd.img-6.8.0-38-generic
    -rw-r--r-- 1 root root 526888758 Ağu 15 11:57 initrd.img-6.9.2-custom
    -rw------- 1 root root   9055262 Haz  7  2024 System.map-6.8.0-38-generic
    -rw-r--r-- 1 root root   8365115 Ağu 15 11:57 System.map-6.9.2-custom
    -rw-r--r-- 1 root root   8365115 Ağu 15 11:55 System.map-6.9.2-custom.old
    lrwxrwxrwx 1 root root        20 Ağu 15 11:57 vmlinuz -> vmlinuz-6.9.2-custom
    -rw-r--r-- 1 root root  14944648 Haz  7  2024 vmlinuz-6.8.0-38-generic
    -rw-r--r-- 1 root root  14819840 Ağu 15 11:57 vmlinuz-6.9.2-custom
    -rw-r--r-- 1 root root  14819840 Ağu 15 11:55 vmlinuz-6.9.2-custom.old
    lrwxrwxrwx 1 root root        24 Ağu 15 11:57 vmlinuz.old -> vmlinuz-6.9.2-custom.old
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										7. Ders 09/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz "make install" komutu ile yapılan işlemleri manuel olarak da yapabiliriz. Yukarıda da belirttiğimiz gibi derleme 
    işlemi sonucunda elde edilmiş olan dosyaların hedef sistemde bazı dizinlerde bulunuyor olması gerekir. Aslında çekirdek 
    imajı ve geçici kök dosya sistemi dosyaları default yerlerin dışında başka yerlerde de bulundurulabilir. Önyükleyiciye 
    bu konuda bilgi verilebilir. Ancak yukarıdaki dosyaların hedef sistemde bulundurulduğu default yerler şöyledir:

    - Çekirdek İmajı ---> "/boot" dizini
    - Çekirdek Sembol Tablosu ---> "/boot" dizini
    - Modül Dosyaları ---> "/lib/modules/<çekirdek_sürümü>/kernel" dizini
    - Geçici Kök Dosya Sistemi Dosyası ---> "/boot" dizinine

    Ancak yukarıdaki dosyalar dışında isteğe bağlı olarak aşağıdaki dosyalar da hedef sisteme konuşlandırılabilir:

    - Konfigürasyon Dosyası ---> "/boot" dizinine
    - Modüllere İlişkin Bazı Dosyalar ---> "/lib/modules/<çekirdek_sürümü>" dizinine

    Peki yukarıda belirttiğimiz dosyalar hedef sistemdeki ilgili dizinlere hangi isimlerle kopyalanmalıdır? İşte tipik 
    isimlendirme şöyle olmalıdır (buradaki <çekirdek_sürümü> "uname -r" komutuyla elde edilecek olan yazıdır):

    - Çekirdek İmajı: "/boot/vmlinuz-<çekirdek_sürümü>". Örneğin "vmlinuz-6.9.2-custom" gibi.
    - Çekirdek Sembol Tablosu: "/boot/System.map-<çekirdek_sürümü>". Örneğin "System.map-6.9.2-custom" gibi.
    - Modüllere İlişkin Dosyalar: Bunlar yukarıda da belirttiğimiz gibi "/lib/modules/<çekirdek_sürümü>" dizininin içerisine 
    kopyalanmalıdır.
    - Konfigürasyon Dosyası: "/boot/config-<çekirdek_sürümü>". Örneğin "config-6.9.2-custom" gibi.
    - Geçici Kök Dosya Sistemine İlişkin Dosya: "/boot/initrd.img-<çekirdek_sürümü>". Örneğin "initrd.img-6.9.2-custom" gibi. 
    Bu dosyayı "update-initramfs" programıyla oluşturabilirsiniz.

    Ayrıca bazı dağıtımlarda "/boot" dizini içerisindeki "vmlinuz" dosyası default olan "vmlinuz-<çekirdek_sürümü>" dosyasına, 
    "initrd.img" dosyası da "/boot/initrd.img-<çekirdek_sürümü>" dosyasına sembolik link yapılmış durumda olabilir. Ancak bu 
    sembolik bağlantıları "GRUB" kullanmamaktadır. Aşağıda Intel sistemindeki 6.8.0 çekirdeğinin yüklü olduğu "/boot" dizininin 
    default içeriğini görüyorsunuz:

    $ ls -l /boot
    -rw-r--r-- 1 root root    287375 Haz  7  2024 config-6.8.0-38-generic
    drwx------ 3 root root      4096 Oca  1  1970 efi
    drwxr-xr-x 6 root root      4096 Ağu 15 11:57 grub
    lrwxrwxrwx 1 root root        23 Ağu 10 11:20 initrd.img -> initrd.img-6.8.0-38-generic
    -rw-r--r-- 1 root root  73052139 Kas 19  2024 initrd.img-6.8.0-38-generic
    -rw------- 1 root root   9055262 Haz  7  2024 System.map-6.8.0-38-generic
    lrwxrwxrwx 1 root root        20 Ağu 15 11:57 vmlinuz -> vmlinuz-6.8.0-38-generic
    -rw-r--r-- 1 root root  14944648 Haz  7  2024 vmlinuz-6.8.0-38-generic

    Geçici kök dosya sisteminin içerisindeki dosyalar "cpio" denilen bir arşiv formatıyla arşivlenmektedir. cpio arşivi 
    aslında tıpkı "tar" arşivinde olduğu gibi yalnızca dosyaların uç uca eklenmesiyle oluşturulmaktadır. İsterseniz geçiçi 
    kök dosya sistemine ilişkin "initrd-xxx" dosyalarını açabilirsiniz. Ancak bu dosyaların içeriği dağıtımların versiyonlarına 
    göre değişebilmektedir. Yeni dağıtımlarda bu "initrd-xxx" arşiv dosyası birkaç bölümden oluşmaktadır. Eğer biz bu dosyayı 
    "cpio" programıyla açmaya çalışırsanız bu program yalnızca ilk bölümü açacaktır. Tüm bölümleri açmak için "unmkinitramfs" 
    programından faydalanabilirsiniz. unmkinitramfs programı ile açım şöyle yapılabilir:

    unmkinitramfs /boot/initrd.img-6.9.2-custom initrd

    Bu komutla geçici kök dosya sistemine ilişkin arşiv dosyası "initrd" isimli dizinin altına açılacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki derleme sonucunda elde ettiğimiz dosyaları manuel isimlendirirken çekirdek sürüm yazısını nasıl bileceğiz? 
    Bunun için "uname -r" komutunu kullanamayız. Çünkü bu komut bize o anda çalışmakta olan çekirdeğin sürüm yazısını 
    verir. Biz yukarıdaki denemede Linux'un "6.9.2" sürümünü derledik. Bunun sonuna da "-custom" getirdik. Bu durumda 
    sürüm yazısının da "6.9.2-custom" olmasını bekleriz. Burada önemli bir uyarıda bulunmak istiyoruz. Bu sürüm yazısı 
    manuel olarak dosyaların isimlerinin değiştirilmesiyle değiştirilememektedir. Çünkü sürüm yazısı çekirdek imajının 
    içerisine de yazılmaktadır ve bizim bazı dosyalara verdiğimiz isimlerin çekirdek içerisindeki bu yazıyla uyumlu olması 
    gerekir. Default olarak "kernel.org" sitesinden indirilen kaynak kodlar derlendiğinde çekirdek sürümü "6.9.2" gibi üç 
    haneli bir sayılardan oluşmaktadır. Yani yazının sonunda "-generic" ya da "-custom" gibi sonekler yoktur. Tabii çekirdeği 
    derlemeden önce yukarıda da belirttiğimiz gibi ".config" dosyasında "CONFIG_LOCALVERSION" özelliğine bu sürüm numarasından 
    sonra eklenecek bilgiyi girebilirsiniz. Örneğin:

    CONFIG_LOCALVERSION="-custom"

    Anımsayacağınız gibi bu işlem "make menuconfig" menüsünde "General Setup/Local version - append custom release" seçeneği 
    kullanılarak da yapılabilmektedir. Biz buradaki örneğimizde bu işlemi yaparak çekirdeği derledik. Dolayısıyla bizim derlediğimiz 
    çekirdekte çekirdek imajı içerisinde yazan sürüm ismi "6.9.2-custom" biçimindedir. Peki biz bu ismi unutsaydık nasıl 
    öğrenebilirdik? Bunun basit bir yolu sıkıştırılmamış çekirdek dosyası içerisindeki (kaynak kök dizindeki "vmlinux" dosyası) 
    string tablosunda "Linux version" yazısını aramaktır. Örneğin:

    $ strings vmlinux | grep "Linux version"
    Linux version 6.9.2-custom (kaan@kaan-virtual-machine) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for 
    Ubuntu) 2.38) # SMP PREEMPT_DYNAMIC
    Linux version 6.9.2-custom (kaan@kaan-virtual-machine) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for 
    Ubuntu) 2.38) #2 SMP PREEMPT_DYNAMIC Thu Dec 5 17:55:14 +03 2024

    Buradan sürüm yazısının "6.9.2-custom" olduğu görülmektedir. O halde derleme sonucunda elde ettiğimiz dosyaları manuel 
    biçimde kopyalarken sürüm bilgisi olarak "6.9.2-custom" yazısını kullanmamız gerekir. Çekirdek imajının "/boot" 
    dizinine manuel kopyalanması işlemi şöyle yapılabilir (kaynak kök dizinde bulunduğumuzu varsayıyoruz):

    $ sudo cp arch/x86_64/boot/bzImage /boot/vmlinuz-6.9.2-custom

    Konfigürasyon dosyasını da şöyle kopyalayabiliriz:

    $ sudo cp .config /boot/config-6.9.2-custom

    Tabii bizim çekirdek modüllerini de "/lib/modules/6.9.2-custom/kernel" dizinine, geçici kök dosya sistemine ilişkin 
    dosyayı da "/boot" dizinine kopyalamamız gerekir. Çekirdek modüllerinin kopyalanması biraz zahmetli bir işlemdir. 
    Çünkü bunlar derlediğimiz çekirdekte farklı dizinlerde bulunmaktadır. Bu kopyalamanın en etkin yolu "make modules_install" 
    komutunu kullanmaktır. Benzer biçimde çekirdek dosyalarının ve gerekli diğer dosyaların uygun yerlere kopyalanması 
    için de en etkin yöntem "make install" komutudur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Normal olarak biz "make install" yaptığımızda eğer sistemimizde GRUB önyükleyicisi varsa komut GRUB konfigürasyon 
    dosyalarında da güncellemeler yaparak sistemin yeni çekirdekle açılmasını sağlamaktadır. Böylece kullanıcı bir menü 
    yoluyla sistemin kendi istediği çekirdekle açılmasını da sağlayabilmektedir. GRUB menüsü otomatik olarak görüntülenmemektedir. 
    Boot işlemi sırasında ESC tuşuna basılırsa menü görüntülenir. Eğer GRUB menüsünün her zaman görüntülenmesi isteniyorsa 
    "/etc/default/grub" dosyasındaki iki satır aşağıdaki gibi değiştirilmelidir:

    GRUB_TIMEOUT_STYLE=menu
    GRUB_TIMEOUT=5

    Buradaki GRUB_TIMEOUT satırı eğer müdahale yapılmamışsa menünün en fazla 5 saniye görüntüleneceğini belirtmektedir.

    Bu işlemden sonra "update-grub" programı da çalıştırılmalıdır:

    $ sudo update-grub

    Bu tür denemeler yapılırken GRUB menüleri bozulabilmektedir. Düzeltme işlemleri bazı konfigürasyon dosyalarının edit 
    edilmesiyle manuel biçimde yapılabilir. Konfigürasyon dosyaları güncellendikten sonra "update-grub" programı mutlaka 
    çalıştırılmalıdır. Ancak eğer GRUB konfigürasyon dosyaları konusunda yeterli bilgiye sahip değilseniz GRUB işlemlerini 
    görsel bir biçimde "grub-customizer" isimli programla da yapabilirsiniz. Bu program "debian depolarında" olmadığı için 
    önce aşağıdaki gibi programın bulunduğu yerin "apt" kayıtlarına eklenmesi gerekmektedir:

    $ sudo add-apt-repository ppa:danielrichter2007/grub-customizer
    $ sudo apt-get update
    $ sudo apt-get install grub-customizer

    Bu işlemden sonra kurulum yapılabilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıda çekirdek derleme ve yeni çekirdeği kurma sürecini maddeler halinde açıkladık. Şimdi yukarıdaki adımları 
    özet haline getirelim:

    1) Çekirdek derlemesi için gerekli olan araçlar kurulur.

    2) Çekirdek kodları indirilir ve açılır.

    3) Zaten hazır olan konfigürasyon dosyası "/boot" dizininden alınarak ".config" ismiyle kaynak kök dizine kopyalanır.

    4) Konfigürasyon dosyası üzerinde "make menuconfig" komutu ile değişiklikler yapılır.

    5) Eğer çekirdeğin imzalanması istenmiyorsa yukarıda belirtildiği gibi ".config" dosyasındaki bazı satırlar üzerinde 
    değişiklikler yapılır.

    6) Çekirdek derlemesi "make -j$(nproc)" komutu ile gerçekleştirilir.

    7) Modüller ve ilgili dosyalar hedefe "sudo make modules_install" komutu ile konuşlandırılır.

    8) Çekirdek imajı ve ilgili dosyalar "sudo make install" komutu ile hedefe konuşlandırılır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki yeni çekirdeği derleyip sisteme dahil ettikten sonra nasıl onu sistemden tamamen çıkartabiliriz? Tabii yapılan 
    işlemlerin tersini yapmak gerekir. Kaldırma işlemi manuel biçimde şöyle yapılabilir:

    - "/lib/modules/<çekirdek_sürümü>" dizini tamamen silinir.
    - "/boot" dizinindeki çekirdek sürümüne ilişkin dosyalar silinir.
    - "/boot" dizininden çekirdek sürümüne ilişkin dosyalar silindikten sonra "update-grub" programı sudo ile çalıştırılmalıdır.
    Bu program "/boot" dizinini inceleyip otomatik olarak ilgili girişleri GRUB menüsünden siler. Yani aslında GRUB 
    konfigürasyon dosyaları üzerinde manuel değişiklik yapmaya gerek yoktur. GRUB işlemleri için diğer bir alternatif 
    ise "grub-customizer" programı ile görsel silme yapmaktır. Ancak bu program "/boot" dizini içerisindeki dosyaları 
    ve modül dosyalarını silmez. Yalnızca ilgili girişleri GRUB menüsünden çıkartmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										8. Ders 10/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeği yeniden derlemenin gerekçelerinden bahsetmiştik. Bunlardan biri de çekirdek kodları üzerinde değişikliklerin 
    yapılmış olmasıydı. Peki çekirdek kodları üzerinde değişiklikler nasıl yapılabilir? Çekirdek kodları üzerinde değişiklikler 
    tipik olarak dört yolla yapılmaktadır:

    1) Çekirdek kodlarındaki bir dosya içerisinde bulunan fonksiyon kodlarında değişiklik yapılması.
    2) Çekirdek kodlarındaki bir dosya içerisine yeni bir fonksiyon eklenmesi.
    3) Çekirdek kodlarındaki bir dizin içerisine yeni bir C kaynak dosyası eklenmesi.
    4) Çekirdek kodlarındaki bir dizin içerisine yeni bir dizin ve bu dizinin içerisine de çok sayıda C kaynak dosyalarının 
    eklenmesi.

    Eğer biz birinci maddedeki ve ikinci maddedeki gibi çekirdek kodlarına yeni bir dosya eklemiyorsak çekirdeğin derlenmesini 
    sağlayan make dosyalarında bir değişiklik yapmamıza gerek yoktur. Ancak çekirdeğe yeni bir kaynak dosya ya da dizin 
    ekleyeceksek bu eklemeyi yaptığımız dizindeki make dosyasında izleyen paragraflarda açıklayacağımız biçimde bazı 
    güncellemelerin yapılması gerekir. Böylece çekirdek yeniden derlendiğinde bu dosyalar da çekirdek imajının içerisine 
    eklenmiş olacaktır. Eğer kaynak kod ağacında bir dizinin altına yeni bir dizin eklemek istiyorsak bu durumda o dizini 
    yine üst dizine ilişkin make dosyasında belirtmemiz ve o dizinde ayrı bir Makefile oluşturmamız gerekir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    GNU Make aracı oldukça ayrıntılı özelliklere sahip bir build aracıdır. Bu aracın ayrıntılarını öğrenmek ayrı bir 
    çabayı gerektirmektedir. Make dili aslında oldukça aşağı seviyeli bir build dilidir. Bu nedenle özellikle son yirmi 
    yıldır programcılar doğrudan GNU Make aracını kullanmak yerine daha üst düzey make araçlarını kullanmayı tercih 
    etmektedir. Bunlardan en yaygın olanlardan biri CMake denilen araçtır. Microsoft MSBuild isimli kendi tasarladığı 
    build aracını kullanmaktadır.

    Make dilinde değişkenler oluşturulabilmektedir. Örneğin:

    obj-y = a.o

    Burada obj-y isimli değişken a.o bilgisini tutmaktadır. Değişkenleri bir çeşit makro gibi düşünebilirsiniz. Bir 
    değişkene ekleme yapmak için Make dilinde += operatörü kullanılmaktadır. Örneğin:

    obj-y = a.o
    obj-y += b.o
    obj-y += c.o

    Burada artık obj-y değişkeni a.o b.o c.o biçiminde olacaktır.

    Linux çekirdeğinde özyinelemeli bir make yöntemi kullanılmaktadır. Her dizinde bir Makefile dosyası vardır. Bunun 
    içerisindeki obj-y gibi, obj-m gibi bazı değişkenler += operatörüyle eklenerek biriktirilmektedir. Bunlar da derleme 
    ve bağlama işlemine sokulmaktadır. Yukarıda da belirttiğimiz gibi Linux'taki bu build sistemine KBuild ya da KConfig 
    sistemi denilmektedir.

    Bizim Linux'ta Makefile dosyaları üzerinde gerekli güncellemeleri yapmak için çok fazla bilgiye sahip olmamız gerekmez. 
    Bazı yönergeleri uygun bir biçimde yerine getirirsek hedefimize ulaşabiliriz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux kaynak kod ağacında dizinlerin altında "Makefile" isimli make dosyaları bulunur. Eğer bir dizinin altına yeni 
    bir dosya eklenecekse o dizinin içerisinde bulunan Makefile dosyasının içerisine aşağıdaki gibi bir satırın eklenmesi 
    gerekir:

    obj-y += dosya_ismi.o

    Buradaki += operatörü obj-y isimli hedefe ekleme yapma anlamına gelmektedir. "obj" sözcüğünün yanındaki "-y" harfi 
    ilgili dosyanın çekirdeğin bir parçası biçiminde çekirdek imajının içerisine gömüleceğini belirtmektedir. Make dosyalarının 
    bazı satırlarında "obj-y" yerine "obj-m" de görebilirsiniz. Bu da ilgili dosyanın ayrı bir modül biçiminde derleneceği 
    anlamına gelmektedir. Eklemeler genellikle çekirdek imajının içine yapıldığı için biz de genellikle "obj-y" kullanırız. 
    Eğer bir dosyanın (aygıt sürücüler için bu durum söz konusudur) çekirdek imajının içine gömülmesi yerine ayrı bir çekirdek 
    modülü olarak derlenmesi isteniyorsa bu durumda dosyanın yerleştirildiği dizinin "Makefile" dosyasına aşağıdaki gibi bir 
    eklemenin yapılması gerekir:

    obj-m += dosya_ismi.o

    Eğer çekirdek kaynak kodlarına tümden bir dizin eklemek isteniyorsa bu durumda önce o dizininin oluşturulduğu dizindeki 
    "Makefile" dosyasına aşağıdaki gibi bir ekleme yapılmalıdır:

    obj-y += dizin_ismi/

    Burada dizin isminden sonra '/' karakterini unutmayınız. Tabii bu ekleme bir modül biçiminde de olabilirdi:

    obj-m += dizin_ismi/

    Fakat bu ekleme tek başına yetmemektedir. Bu ekleme yapıldıktan sonra ayrıca yaratılan dizinde "Makefile" isimli 
    bir dosyanın oluşturulması ve o dosyanın içerisinde o dizindeki kaynak dosyaların da belirtilmesi gerekmektedir. 
    Örneğin biz "drivers" dizininin altında "mydriver" isimli bir dizin oluşturup onun da içerisine "a.c" "b.c" ve "c.c" 
    dosyalarını eklemiş olalım. Bu durumda önce "drivers" dizini içerisindeki "Makefile" dosyasına aşağıdaki gibi bir 
    satır ekleriz:

    obj-y += mydriver/

    Sonra da "mydriver" dizini içerisinde "Makefile" isimli bir dosya oluşturup bu dosyanın içerisinde de dizin içerisindeki 
    dosyaları belirtiriz. Örneğin:

    obj-y += a.o
    obj-y += b.o
    obj-y += c.o
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kaynak kod ağacında Makefile dosyasının dışında build sistemiyle ilgili "Kconfig" isimli dosyalar da bulunmaktadır. 
    Bu dosyaların içerisinde ilgili dosyaların ya da dizinlerin "konfigürasyon dosyasına" yansıtılması için gerekli 
    bilgiler bulundurulmaktadır. Örneğin biz eklediğimiz "mydriver" dizinindeki dosyaların çekirdek kodlarına dahil edilip 
    edilmeyeceğini çekirdeği derleyenin konfigürasyon aşamasında belirlemesini sağlayabiliriz. Bunun için bu "Kconfig" 
    dosyasına bir giriş eklememiz gerekir. Böylece bu giriş de "menuconfig" yapıldığında bir seçenek olarak karşımıza 
    gelecektir. Tabii ekleyeceğimiz dosya ve dizinleri "Kconfig" dosyasında belirtmek zorunda değiliz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    "menuconfig" menüsünde seçenekler için birkaç seçme biçimi bulunmaktadır. Eğer seçenekte [] varsa bu seçenek "seçilebilir 
    ya da seçilmeyebilir" anlamına gelmektedir. Eğer bu seçenek seçilirse köşeli parantez içerisinde bir * karakteri 
    gösterilmektedir. Eğer ilgili seçenekte <> varsa bu açısal parantezlerin içersinde M karakteri ya da * getirilebilmekte 
    ya da bunun içi boş bırakılabilmektedir. Bu açısal parantezli seçeneklere "üç konumlu seçenekler" de denilmektedir. 
    Eğer ilgili seçenekte -*- varsa bu seçenek için seçilememezlik yapılamaz. Yani mutlaka çekirdek kodlarında bu seçeneğin 
    bulunması gerekir. Tabii menuconfig menüsünde yapılan her şey aslında ".config" dosyasına yansıtılmaktadır.

    []      ---> seçilebilir ya da seçilmeyebilir
    <?>     ---> üç konumlu, seçilebilir, seçilmeyebilir ya da 'M' olarak belirtilebilir
    -*-     ---> seçilememezlik yapılamaz
    değer   ---> ilgili özellik

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki çekirdeğin konfigüre edilmesi aşamasında "menuconfig" işleminde belirlediğimiz seçenekler kaynak kodlara nasıl 
    yansıtılmaktadır? Örneğin biz "menuconfig" işleminde bir modülün çekirdek kodlarına dahil edilmesini ilgili girişi "*" 
    ile seçerek sağlayabilmekteyiz. Benzer biçimde biz konfigürasyon aşamasında bazı çekirdek parametrelerini de değiştirebilmekteyiz. 
    Örneğin "timer tick" frekansı "menuconfig" menüsünde bir sayı biçiminde belirlenebilmektedir. Peki buradaki belirlemeler 
    çekirdek kodlarına ve build sistemine nasıl yansıtılmaktadır?

    Anımsanacağı gibi "menuconfig" ve diğer config menülerinde yapılan seçimler daha önce de belirttiğimiz gibi ".config" 
    isimli bir text dosyaya save edilmektedir. Bu ".config" dosyası "özellik=değer" biçiminde satırlardan oluşmaktadır. 
    Aşağıda dosyanın birkaç satırını görüyorsunuz:

    ...
    CONFIG_CC_HAS_ASM_INLINE=y
    CONFIG_CC_HAS_NO_PROFILE_FN_ATTR=y
    CONFIG_PAHOLE_VERSION=125
    CONFIG_IRQ_WORK=y
    CONFIG_BUILDTIME_TABLE_SORT=y
    CONFIG_THREAD_INFO_IN_TASK=y
    ...

    Çekirdek derlenirken ilk aşamada bu ".config" dosyasının içeriği "include/generated/autoconf.h" dosyasının içerisine 
    #define önişlemci komutları biçiminde aktarılmaktadır. İşte eğer ilgili konfigürasyon dosyasındaki değer "y" ya da "m" 
    ise bu "autoconf.h" dosyası içerisinde buna ilişkin sembolik sabit 1 olarak görünür. (Başka bir deyişle "menuconfig" 
    menüsünde [*] ya da <*> ya da <M> seçenekleri için sembolik sabit 1 olur.) Eğer konfigürasyon dosyasında ilgili seçenek 
    gerçekten bir değer belirtiyorsa "autoconf.h" dosyası içerisinde bu sembolik sabit o değerde olur. Eğer konfigürasyon 
    dosyasında ilgili seçenek "n" biçiminde seçilmişse (yani "menuconfig" menüsünde ilgili seçenek [] ya da <> biçiminde 
    seçilmişse) bu durumda ilgili sembolik sabit hiç define edilmemiş hale gelir. Özetle aslında ".config" dosyası içerisindeki 
    satırlardan C dilinde anlamlı #define önişlemci komutları oluşturulmaktadır. Aşağıda üretilmiş olan "autoconf.h" dosyasının 
    birkaç satırını görüyorsunuz:

    ...
    #define CONFIG_IGB_HWMON 1
    #define CONFIG_ACPI_HOTPLUG_CPU 1
    #define CONFIG_DEV_DAX_KMEM_MODULE 1
    #define CONFIG_RIONET_RX_SIZE 128
    #define CONFIG_USB_SERIAL_KEYSPAN_PDA_MODULE 1
    #define CONFIG_BOOTTIME_TRACING 1
    ...

    Çekirdeğe ilişkin bir C kodu içerisinde "ilgili seçenek seçilmişse" bir kod parçasını derlemeye dahil etmek için 
    #ifdef önişlemci komutundan faydalanabilirsiniz. Örneğin:

    #ifdef CONFIG_XXX
    ...
    #endif

    Tabii üretilen bu "autoconf.h" dosyası çekirdek kaynak kodlarındaki çeşitli include dosyalarında doğrudan ya da 
    dolaylı bir biçimde include edilmiş durumdadır. Linux kaynak kodları da bu sembolik sabitleri kullanacak biçimde 
    yazılmıştır. Tabii Linux'un kaynak kodlarında "autoconf.h" dosyasını bulamazsınız. Çünkü bu dosya make işlemi 
    sırasında oluşturulmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki paragrafta ".config" dosyasındaki konfigürasyon parametrelerinin nasıl C'ye sembolik sabitler biçiminde 
    yansıtıldığını gördük. Peki bu konfigürasyon seçenekleri "Makefile" dosyalarına nasıl yansıtılmaktadır? Aşağıda 
    çekirdeğin bir "Makefile" içeriğini görüyorsunuz:

    ...
    obj-$(CONFIG_I8254)             += i8254.o
    obj-$(CONFIG_104_QUAD_8)        += 104-quad-8.o
    obj-$(CONFIG_INTERRUPT_CNT)     += interrupt-cnt.o
    obj-$(CONFIG_RZ_MTU3_CNT)       += rz-mtu3-cnt.o
    obj-$(CONFIG_STM32_TIMER_CNT)   += stm32-timer-cnt.o
    ...

    Bu satırlar aslında "ilgili konfigürasyon seçenekleri seçilmişse ilgili dosyaların derlemeye dahil edileceğini" belirtmektedir.

    İşte "KBuild" sistemi aynı zamanda bu ".config" dosyasından hareketle make programı için anlamlı olan değişkenler de 
    oluşturmaktadır. Bu değişkenleri yukarıda açıkladığımız sembolik sabitlerle karıştırmayınız. Bu değişkenler make dili 
    için anlamlı olan make dilinin değişkenleridir. Örneğin eğer ".config" dosyasında bir seçenek "y" olarak belirtilmişse 
    (başka bir deyişle "menuconfig" menüsünde seçenek [*] ya <*> biçiminde seçilmişse bu konfigürasyon seçeneği için make 
    dilinde "y" değeri, eğer "m" olarak belirtilmişse de (başka bir deyişle "menuconfig" menüsünde seçenek <M> biçiminde 
    seçilmişse) "m" değeri oluşturulmaktadır. Böylece aslında yukarıdaki make satırları ilgili seçenek "y" olarak seçilmişse 
    "obj-y" biçimine, "m" olarak olarak seçilmişse "obj-m" biçimine dönüştürülmektedir. Örneğin:

    obj-$(CONFIG_I8254)         += i8254.o

    Burada CONFIG_I8254 seçeneği "y" ise ilgili dosya obj-y değişkenine dahil olacaktır. Yani çekirdeğin içerisinde bulunacaktır. 
    Ancak bu CONFIG_I8254 seçeneği "m" ise ilgili dosya obj-m değişkenine dahil olacaktır. Eğer bu seçenek "n" ise (yani hiç seçilmemişse) 
    çekirdek build sistemi ya bu CONFIG_I8254 değişkenini hiç tanımlamamakta ya da bunu "n" olarak tanımlamaktadır. Her iki 
    durumda da artık bu dosya herhangi bir biçimde derleyemeye dahil edilmeyecektir.

    Çekirdeğe birtakım kodlar ekleyenler eğer eklemeleri "Kconfig" dosyası yoluyla konfigürasyona yansıtmışlarsa bu durumda 
    kendi "Makefile" dosyasına bu eklemeleri yukarıdaki gibi girebilirler. Örneğin biz "mymodule" ile temsil ettiğimiz bir 
    modül dosyası oluşturup bu modül dosyasının çekirdek kodlarına eklenip eklenmeyeceğini konfigürasyonda "Kconfig" dosyası 
    yoluyla belirtebiliriz. Bu durumda "Makefile" içerisindeki girişi aşağıdaki gibi de oluşturabiliriz:

    obj-$(CONFIG_MYMODULE) += mymodule.o

    Görüldüğü gibi burada aslında konfigüre eden nasıl seçmişse biz onun seçimini yansıtmış olmaktayız. ".config" dosyasında 
    bir özellik "no" ise bu durumda ilgili "Makefile" satırı hale gelecektir:

    obj-n += mymodule.o

    obj-n biçiminde bir birikim yapılmadığı için zaten bu satır derleme aşamasında dikkate alınmayacaktır. Ancak bazen 
    sistem programcıları "y" durumu için aşağıdaki gibi bir kontrol ile modülü koşullu bir biçimde de derleme sürecine 
    ekleyebilmektedir:

    ifeq ($(CONFIG_MYSYSCALL), y)
        obj-y += mysyscall.o
    endif

    Burada eğer konfigürasyon yapılırken ilgili seçenek "y" biçiminde (yani [*] ya da <*> biçiminde) geçilmişse bu durumda 
    biz de ilgili dosyayı derlemeye dahil etmiş olduk.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										9. Ders 16/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir C dosyasını ya da dizini çekirdek kodlarına ekledikten sonra onun konfigürasyon sırasında (örneğin "make menuconfig" 
    işlemi sırasında) sırasında görünebilirliğini sağlamak için "Kconfig" dosyalarının kullanıldığını belirtmiştik. Yani 
    "Kconfig" dosyaları yaptığımız değişikliklerin konfigüre edilebilirliğini sağlamak için kullanılmaktadır. "Kconfig" 
    dosyalarının genel formatı için aşağıdaki bağlantıya başvurabilirsiniz:

    https://docs.kernel.org/kbuild/kconfig-language.html

    "Kconfig" dosyaları tıpkı "Makefile" dosyalarında olduğu gibi özyinelemeli biçimde işletilmektedir. Yani biz çekirdek 
    kaynak kod ağacında bir dizin yaratmayıp zaten var olan bir dizinin içerisine bir ".c" dosyası yerleştiriyorsak "Makefile" 
    ve "Kconfig" dosyaları oluşturmamıza gerek yoktur. Gerekli işlemleri zaten dizin içerisinde var olan bir "Makefile" ve 
    "Kconfig" dosyaları üzerinde yapabiliriz. Ancak eğer biz bir dizin oluşturup onun içerisine dosyalar yerleştireceksek 
    o dizin için bir tane "Makefile" ve bir tane de "Kconfig" dosyası oluşturmamız gerekir.

    Önceki paragrafta Linux kaynak kod ağacında bir dizin yaratıp onun içerisine dosyalar yerleştirirken o dizin için "Makefile" 
    ve "Kconfig" dosyalarının yazılması gerektiğini belirtmiştik. (Tabii aslında "Kconfig" dosyasının bulundurulması zorunlu 
    değildir. Ancak eklenen özelliğin konfigüre edilebilirliğinin sağlanması için gerekmektedir.) Bu dosyalar oluşturulduktan 
    sonra dış dizindeki "Makefile" ve "Kconfig" dosyalarında aşağıda belirtilen işlemler de yapılmalıdır:

    1) Daha önceden de belirttiğimiz gibi dış dizindeki "Makefile" dosyasında alt dizinin dikkate alınacağı aşağıdaki gibi 
    bir satırla belirtilmelidir:

    obj-y += <dizin_ismi>/

    2) Dış dizinin "Kconfig" dosyasında iç dizindeki "Kconfig" dosyasının dikkate alınması aşağıdaki gibi bir satırın 
    eklenmesiyle sağlanmaktadır:

    source "kaynak_kod_ağacının_köküne_göreli_yol_ifadesi"

    Örneğin:

    source "drivers/mydriver/Kconfig"

    Buradaki yol ifadesi çekirdek kodlarının kök dizinine göreli olmalıdır.

    Biz "Kconfig" dosyasına yukarıdaki gibi bir giriş yerleştirdiğimizde artık "make menuconfig" gibi konfigürasyon menülerinde 
    eklediğimiz "Kconfig" elemanı bir menü seçeneği biçiminde karşımıza çıkacaktır.

    Örneğin biz bir aygıt sürücümüzün dosyalarını çekirdeğin kaynak kod ağacında "drivers" dizininin altına "mydriver" dizinini 
    açarak eklemek isteyelim. Bu durumda şunları yapmamız gerekir:

    1) "drivers" dizini içerisinde "mydriver" dizinini yaratıp içerisine "mydriver.c" dosyasını (belki de "mydriver.h" gibi 
    bir başlık dosyasını da) yerleştirmeliyiz.

    2) "drivers/mydriver" dizininde aşağıdaki gibi bir "Kconfig" dosyasını oluşturmalıyız:

    config MYDRIVER
    tristate "My Character Device Driver"
    default y
    help
    Enable this option to include support for My Device Driver.
    It can either be built as a module or statically linked into the kernel.

    Buradaki "config MYDRIVER" satırı aslında make dilinde CONFIG_MYDRIVER değişkeninin oluşturulmasına yol açmaktadır.

    3) Üst dizindeki ("drivers" dizinindeki) "Kconfig" dosyasına aşağıdaki satırı yerleştirmeliyiz:

    source "drivers/mydriver/Kconfig"

    4) "drivers/mydriver" dizinindeki "Makefile" dosyası içerisine aşağıdaki gibi bir satır eklemeliyiz:

    obj-$(CONFIG_MYDRIVER) += mydriver.o

    5) Üst dizindeki (yani "drivers" dizinindeki) "Makefile" içerisine aşağıdaki gibi bir satır eklemeliyiz:

    obj-$(CONFIG_MyDRIVER) += mydriver/
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi çekirdeğe bazı kodlar ekleyip onu yeniden derleyerek bir deneme yapalım. Örneğin çekirdeğe yeni bir çekirdek 
    modülü ekleyelim ve çekirdeğin o modül gömülü olarak başlatılmasını sağlayalım. Ancak burada biz aynı zamanda bu 
    çekirdek modülünün "make menuconfig" ile seçilebilmesini de sağlayalım. Çekirdek modüllerinin nasıl yazılacağını 
    bilmediğinizi varsayıyoruz. Ancak biz yine de örneğimizde "hiçbir şey yapmayan iskelet bir çekirdek modülü" oluşturacağız. 
    Bu işlem şu adımlardan geçilerek yapılabilir (kaynak kod ağacının kök dizininde bulunduğumuzu varsayıyoruz):

    1) "drivers/mydriver" dizini yaratılır.

    2) İskelet bir çekirdek modülü "mydriver.c" biçiminde "drivers/mydriver" dizininde aşağıdaki gibi oluşturulur:

    /* mydriver.c */

    #include <linux/module.h>
    #include <linux/kernel.h>

    MODULE_LICENSE("GPL");
    MODULE_AUTHOR("Kaan Aslan");
    MODULE_DESCRIPTION("General Device Driver");

    static int __init mydriver_init(void)
    {
        printk(KERN_INFO "Hello World...\n");

        return 0;
    }

    static void __exit mydriver_exit(void)
    {
        printk(KERN_INFO "Goodbye World...\n");
    }

    module_init(mydriver_init);
    module_exit(mydriver_exit);

    3) "drivers/mydriver" dizininde "Kconfig" dosyası aşağıdaki gibi oluşturulmalıdır:

    config MYDRIVER
    tristate "My Character Device Driver"
    default y
    help
      Enable this option to include support for My Device Driver.
      It can either be built as a module or statically linked into the kernel.

    Burada konfigürasyon makrosunun ismi CONFIG_MYDRIVER biçiminde olacaktır.

    4) Üst dizinin (yani "drivers" dizininin) "Kconfig" dosyasına aşağıdaki ekleme yapılmalıdır:

    source "drivers/mydriver/Kconfig"

    5) "drivers/mydriver" dizininde aşağıdaki içeriğe sahip bir "Makefile" dosyası oluşturulmalıdır:

    obj-$(CONFIG_MYDRIVER) += my_driver.o

    6) Üst dizindeki ("drivers" dizinindeki) "Makefile" dosyasına aşağıdaki satır eklenmelidir:

    obj-$(CONFIG_MYDRIVER) += mydriver/

    Artık çekirdeği derleyebiliriz. "menuconfig" menüsünde kendi aygıt sürücümüze ilişkin seçenek de çıkacaktır.

    Çekirdek imzalamasını devre dışı bırakmak için konfigürasyon dosyasındaki satırlarda aşağıdaki değişiklikleri yapmayı 
    unutmayınız:

    CONFIG_SYSTEM_TRUSTED_KEYS=""
    CONFIG_SYSTEM_REVOCATION_KEYS=""
    CONFIG_SYSTEM_TRUSTED_KEYRING=n
    CONFIG_SECONDARY_TRUSTED_KEYRING=n

    CONFIG_MODULE_SIG=n
    CONFIG_MODULE_SIG_ALL=n
    CONFIG_MODULE_SIG_KEY=""

    Yeni çekirdeğimize "-custom" ismini de ekleyebiliriz. Daha önceden de belirttiğimiz gibi eğer çekirdeğin eski versiyonundan 
    konfigürasyon dosyası alınacaksa "make oldconfig" uygulanıp o versiyondan sonra eklenmiş olan özelliklerin gözden geçirilmesi 
    sağlanmalıdır. Ancak "make menuconfig" işlemi zaten "make oldconfig" işlemini de içermektedir.

    7) Artık çekirdek derlemesi aşağıdaki gibi yapılabilir:

    $ make -j$(nproc)

    8) Derleme işlemi bittikten sonra önce çekirdek modüllerini "sudo make modules_install" ile sonra da çekirdeğin kendisini 
    "sudo make install" ile install edebilirsiniz:

    $ sudo make modules_install
    ...
    $ sudo make install
    ...

    Anımsanacağı gibi "make install" komutu artık sistemin yeni çekirdekle açılmasını sağlayacaktır. "make install" aynı 
    zamanda geçici kök dosya sistemini "update-initramfs" komutu ile oluşturup "/boot" dizinine yerleştirmektedir. Tabii 
    "update-initramfs" programını siz de gerektiğinde kullanabilirsiniz. Programın tipik kullanımı şöyledir:

    $ sudo update-initramfs -c -k <çekirdek_sürümü>

    Buradaki "çekirdek_sürümü" yalnızca çekirdeğin numarasını değil ona verdiğiniz ekleri de içermelidir. (Örneğin 
    "6.9.2-custom" gibi.) Bu komut geçici kök dosya sistemini o anda çalışmakta olan sistemin konfigürasyonunu da dikkate 
    alarak oluşturur ve "/boot" dizinine kopyalar. Yukarıda da belirttiğimiz gibi "make install" zaten bu programı 
    çalıştırarak geçici kök dosya sistemini "/boot" dizininde oluşturmaktadır.

    Peki çekirdeğin kaynak kodlarına yaptığımız eklemenin gerçekten yapılmış olduğunu nasıl anlayabiliriz? Bizim 
    yazdığımız iskelet aygıt sürücü kodlarında çekirdek aygıt sürücümüz yüklediğinde "mydriver_init" fonksiyonu çağrılacaktır. 
    Bu fonksiyonun içinde de printk isimli çekirdek fonksiyonu ile biz bir log mesajı yazdırdık. Bu log mesajları "kernel 
    ring buffer" denilen bir kuyruk sistemine yazılmaktadır. "dmesg" komutuyla bu kuyruk sistemi görüntülenebilir. Eğer 
    "dmesg" yaptığımızda biz bu mesajları görürsek aygıt sürücümüzün yüklenmiş olduğu sonucunu çıkartabiliriz. Örneğin:

    $ dmesg | grep "Hello"
    Hello World...

    Çekirdeğe gömülü olan modüller "/proc/modules" dosyasında görünmezler, dolayısıyla da "lsmod" komutu ile de bunları 
    göremeyiz. Bunlar için "/sys/module" dizininde de bir giriş oluşturulmamaktadır. "modinfo" komutu ise çekirdeğe 
    ilişkin bazı dosyalara da baktığı için bize bu konuda bilgi verebilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki çekirdek kodlarında küçük değişiklikler yaptıktan sonra yeniden "make modules_install" ve "make install" 
    işlemlerine gerek var mı? Aslında küçük değişiklikler için bu işlemler yapılmazsa genellikle bir sorun ortaya çıkmaz. 
    Yeni oluşturulan çekirdek imajı doğrudan eskisinin üzerine kopyalanabilir. Ancak değişikliğin yerine ve kapsamına göre 
    çekirdeğin sembol tabloları değişebileceği için genel olarak her derlemeden sonra "make install" yapabilirsiniz. 
    "drivers" dizininde "obj-m" biçiminde değişiklikler yapılmışsa "make modules_install" yapılmalıdır. Yukarıdaki örnekte 
    biz "drivers" dizininin içerisine "obj-y" ile eklemeler yaptık. Bu durumda aslında "make modules_install" yapmaya 
    gerek yoktur. Ancak aygıt sürücüler "obj-m" biçiminde ekleniyorsa make modules_install" komutu uygulanmalıdır. Çekirdeğin 
    modüllerle ilgili olmayan kısımlarında yapılan değişikler için "make modules_install" yapılmasına gerek olmadığını 
    bir kez daha belirtmek istiyoruz. "make modules_install" işleminden önce eski "/lib/modules/<çekirdek_sürümü>" dizinini 
    de "rm -r" komutu ile silmek daha güvenli bir yaklaşımdır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki çekirdek derlemesi sürecinde imzalama (signing) işlemlerini devre dışı bırakmıştık. Çekirdek kodları 
    ve özellikle de aygıt sürücüler belli imzalara sahip olacak biçimde derlenebilmektedir. Böylece onlar üzerinde birtakım 
    istenmeyen değişikliklerin yapılarak değiştirilmiş çekirdek ya da aygıt sürücülerin yüklenmesi engellenmiş olur. 
    Yukarıda da gördüğünüz gibi çekirdek kodları ve aygıt sürücülerde bu imzalama işlemi devre dışı da bırakılabilmektedir. 
    Ancak imzalama süreci sistem güvenliğini artırmaktadır. Bu tür imzalama işlemleri yalnızca Linux sistemlerinde değil 
    diğer UNIX türevi sistemlerde, Windows ve macOS sistemlerinde de bulunmaktadır.

    Çekirdeğin imza kontrolü temel olarak UEFI BIOS (eğer "secure boot" seçeneği aktif ise) ve önyükleyiciler (örneğin GRUB) 
    tarafından yapılmaktadır. Ancak Linux çekirdeği de aygıt sürücüler ve modüller yüklenirken imza kontrolü uygulayabilmektedir. 
    Biz burada çekirdek ve modül imzalamalarının nasıl yapılacağı üzerinde duracağız.

    İmzalama işlemi tipik olarak şu adımlardan geçilerek yapılmaktadır:

    1) İmzalama işlemi için öncelikle "openssl" kütüphanesinin yüklenmiş olması gerekir. Yükleme işlemi aşağıdaki gibi 
    yapılabilir:

    $ sudo apt-get install openssl

    Daha sonra "openssl" programı ile aşağıdaki gibi bir ".pem" dosyası üretilir:

    $ openssl req -new -x509 -newkey rsa:2048 -sha256 \
    -keyout signing_key.key -out certs/signing_key.crt \
    -nodes -days 36500 -subj "/CN=Local Kernel Module Key/"

    Üretilen ".pem" dosyası genellikle kaynak kod ağacında "certs" isimli bir dizine yerleştirilir:

    $ mkdir -p certs
    $ mv signing_key.pem /certs

    Daha sonra konfigürasyon dosyasında imzalama için aşağıdaki değişiklikler yapılmalıdır:

    CONFIG_MODULE_SIG=y
    CONFIG_MODULE_SIG_ALL=y
    CONFIG_MODULE_SIG_SHA256=y
    CONFIG_SYSTEM_TRUSTED_KEYRING=y
    CONFIG_MODULE_SIG_KEY="certs/signing_key.pem"
    CONFIG_SYSTEM_TRUSTED_KEYS="certs/signing_key.pem"

    Biz yukarıdaki işlemleri yaptığımızda yalnızca aygıt sürücüleri imzalamış oluruz. Çekirdeğin kendisinin imzalanması 
    ayrıca yapılmalıdır. Yukarıda da belirttiğimiz gibi UEFI BIOS'lar ve GRUB gibi önyükleyiciler çekirdek imajını 
    yüklemeden önce ayarları uygun biçime getirildiyse çekirdek imzasına bakmaktadır. Eğer çekirdek imzası yanlışsa 
    (çekirdek dışarıdan kasti ya da yanlışlıkla bozulmuş olabilir) çekirdeği hiç yüklememektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										10. Ders 17/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin imzalanması şöyle yapılabilir:

    - Oluşturmuş olduğumuz "signing_key.pem" dosyasından private ve public key ayrıştırılır:

    $ openssl x509 -in signing_key.pem -outform DER -out db.crt
    $ openssl rsa -in signing_key.pem -outform PEM -out db.key

    - Sonra da çekirdek sbsign programı ile aşağıdaki gibi imzalanır:

    $ sbsign --key db.key --cert db.crt --output vmlinuz.signed vmlinuz-6.9.2-custom

    Genellikle bu biçimde bir çekirdek imzalaması seyrek olarak yapılmaktadır. Önceki paragrafta biz aygıt sürücü 
    dosyalarının imzalandığını belirtmiştik. Aynı makinede aygıt sürücüyü derlerken (build ederken) oluşturulan imza 
    bilgisi de kullanılmaktadır. Yani biz aynı makinede bir aygıt sürücü derlediğimizde aygıt sürücümüz de zaten imzalanmış 
    olacaktır.

    Peki aygıt sürücüler yukarıdaki gibi imzalandıktan sonra ya biz başka bir makinede oluşturulmuş olan ya da başka 
    bir uygulamanın oluşturmuş olduğu aygıt sürücüleri yüklemek istersek ne olacaktır? Normal olarak o aygıt sürücüler 
    bizim ürettiğimiz imza ile imzalanmadığı için onların yüklenmemesini bekleriz. Ancak bu durum kullanıcıları ek bir 
    çabaya sürükleyebilmektedir. İşte default durumda imzası uyuşmayan aygıt sürücüler sistem tarafından bir uyarı verilerek 
    yine de yüklenmektedir. Ancak bu tür durumlarda daha katı bir kontrolün uygulanması isteniyorsa ".config" dosyasında
    aşağıdaki gibi ekstra bir üst düzey güvenlik belirtilebilir:

    CONFIG_MODULE_SIG_FORCE=y

    Bu işlem "menuconfig" menüsünde "Enable loadable module support/Module signature verification/Require modules to 
    be validly signed" seçeneğinden de yapılabilir. Bu durumda çekirdek kendi oluşturduğumuz imzayla imzalanmamış olan 
    modülleri yüklemeyecektir.

    Eğer biz başkalarının yazdığı bir aygıt sürücüyü yüklerken çekirdeğin uyarı vermesini ya da yüklemeyi reddetmesini 
    istemiyorsak o aygıt sürücüyü de kendi ürettiğimiz anahtarla (ya da dağıtımın public anahtarıyla) imzalamalıyız. Bu 
    işlem çekirdek kodlarındaki "scripts" dizini içerisinde bulunan "sign-file" betiği (script) yapılmaktadır. Bu betiğin 
    tipik kullanımı şöyledir:

    scripts/sign-file <hash_alg> <private_key.pem> <public_cert.pem> <module.ko>

    Örneğin:

    $ scripts/sign-file sha256 signing_key.pem.pem signing_key.pem mydriver.ko

    Peki Ubuntu, Mint gibi dağıtımlar çekirdek imzalaması uygulamakta mıdır? Evet genel olarak dağıtımlar kendi 
    private anahtarlarıyla (private keys) çekirdeği ve aygıt sürücüleri imzalamaktadır. Ancak aygıt sürücülerin yüklenmesinde 
    imza kontrolünü zorunlu hale getirmemektedir. İmzalama için kullanılacak public anahtarlar "/proc/keys" dosyasında 
    belirtilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir Linux sisteminin düzgün bir biçimde açılması için belli dizinlerin kök dosya sisteminde bulunuyor olması gerekir. 
    Biz bir dağıtımı kurduğumuzda zaten bu kök dosya sistemi de oluşturulmaktadır. Peki sıfırdan dağıtımı tamamen 
    kurmadan kök dosya sistemini nasıl oluşturulabiliriz? Bu işlem tamamen manuel biçimde yapılabilir. Yani uygulamacı 
    kök dizin içerisindeki gerekli dizinleri elle yaratır. Sonra gerekli programları kaynak kodlarından hareketle hedef 
    makine için derler ve onları konuşlandırır. Sonra yine gerekli birtakım konfigürasyon dosyalarını elle oluşturur. 
    Ancak bu manuel yöntem oldukça zahmetlidir. Bunun yerine bu işlemi pratik bir biçimde yapan araçlar geliştirilmiştir. 
    Örneğin gömülü sistemlerde "BusyBox" denilen araç bu amaçla sıkça kullanılmaktadır. Kullanımı da oldukça kolaydır. 
    Gömülü sistemler için "Buildroot" ve "Yocto" gibi projeler daha genel amaçlar için gerçekleştirilmiştir ancak bunlarla 
    kök dosya sistemi de oluşturulabilmektedir. Bazı dağıtımların bu işi yapan özel utility programları da vardır. Örneğin 
    "debootstrap" programı Debian tabanlı kök dosya sistemini Internet'ten indirerek yerel makinede oluşturabilmektedir. 
    Ancak bu araçların bazıları esnek değildir. Özellikle gömülü sistemlerde düşük bir sistem kaynağının olduğu dikkate 
    alındığında bu araçların bazıları minimalist bir kurulum sağlayamamaktadır.

    debootstrap programı default olarak sisteminizde yüklü değildir. Bunu aşağıdaki gibi kurabilirsiniz:

    $ sudo apt-get install debootstrap

    debootstrap programının pek çok komut satırı argümanı vardır. Biz burada en önemli birkaç argüman üzerinde duracağız. 
    --arch komut satırı seçeneği hedef CPU mimarisini belirtmektedir. Bu argüman girilmezse o andaki platform temel 
    alınmaktadır. 64 bit Intel platformu için burada "amd64", BBB gibi 32 bit ARM platformu için "armhf", 64 bit ARM 
    platformu için bu seçeneğe "arm64" girilmelidir. Programın ilk seçeneksiz argümanı Debian sisteminin varyantını 
    belirtmektedir. Bu argüman için "buster" girebilirsiniz. İkinci komut satırı argümanı hedef kök dosya sisteminin 
    oluşturulacağı dizini, üçüncü komut satırı argümanı ise paketlerin indirileceği depoyu (repository) belirtmektedir. 
    Örneğin:

    $ sudo debootstrap --arch=amd64 --include=systemd bullseye myrootfs http://deb.debian.org/debian/

    Yukarıda da belirttiğimiz gibi "--arch" seçeneği girilmemişse programın çalıştırıldığı makine için kök dosya sistemi 
    indirilip kurulmaktadır.

    Default durumda "debootstrap" pek çok paketi kök dosya sistemine dahil ettiği için paketlerin indirilmesi ve kök dosya 
    sisteminin oluşturulması biraz zaman alacaktır.

    Uygulamacı isterse "--include" ve "--exclude" komut satırı seçenekleriyle birtakım paketleri dahil edebilir ya da 
    dışlayabilir. Ancak bu işlem biraz yorucudur. Örneğin biz "systemd" dışında "sudo" ve "gcc" paketlerini de aşağıdaki 
    gibi kuruluma dahil edebiliriz:

    $ sudo debootstrap --arch=amd64 --include=systemd,sudo,gcc bullseye myrootfs http://deb.debian.org/debian/

    debootstrap programı ile eğer host makineyle aynı platform için indirme işlemi yapılıyorsa deboostrap önce Internetten 
    gerekli paketleri indirip yerel makinede bir dizin içerisinde kök dosya sistemini oluşturur sonra da indirmenin yapıldığı 
    dizinde ikinci aşama işlemleri gerçekleştirir. Eğer host makineden farklı bir sistem için indirme yapılıyorsa (örneğin 
    host sistem Intel tabanlı bir makineyse ve ARM tabanlı bir Debian kök dosya sistemi oluşturulmak isteniyorsa) debootstrap 
    yüklemesini yapmadan önce aşağıdaki gibi "qemu" emülatör paketinin statik versiyonu ve "binfmt" destek paketi kurulmalıdır:

    $ sudo apt install qemu-user-static binfmt-support

    Bu işlemden sonra debootstrap programı yukarıda belirttiğimiz biçimde çalıştırılabilir:

    $ sudo debootstrap --include=systemd --arch armhf buster myrootfs http://deb.debian.org/debian/

    Bu komut hem birinci aşama hem de ikinci aşama işlemleri yapıp bitirecektir. Artık istenildiği zaman chroot işlemi 
    de yapılabilir:

    $ sudo chroot myrootfs
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    "debootstrap" programı ile biz Debian kök dosya sistemi için geçici kök dosya sistemi de oluşturabiliriz. Bunun en 
    pratik yolu kök dosya sistemini kurduktan sonra "chroot" yapıp "update-initramfs" programı ile geçici kök dosya 
    sistemini oluşturmaktır. Ancak bunun için "/boot" ve "/lib/modules" dizinlerinin uygun biçimde oluşturulmuş olması 
    gerekir. "update-initramfs" programı bu dizinlerdeki içerikten faydalanmaktadır. "update-initramfs" programı "initramfs-tools" 
    isimli pakettedir. chroot yaptıktan sonra öncelikle bu paketi aşağıdaki gibi kurmalısınız:

    $ sudo apt-get install initramfs-tools

    Bundan sonra geçici kök dosya sistemini aşağıdaki gibi oluşturabilirsiniz (Debian kök dosya sisteminin kökünde olduğumuzu 
    varsayıyoruz):

    $ update-initramfs -c -k 6.9.2-custom -b .

    Burada "6.9.2-custom" çekirdeğin sürüm ismidir. Geçici kök dosya sistemi "initrd.img-6.9.2-custom" ismiyle bulunulan 
    dizinde oluşturulacaktır. "-b ." seçeneği oluşturulacak dosyanın dizinini belirtmektedir.

    Biz bu tür konuların ayrıntılarına girmeyeceğiz. Bu konular daha çok "Gömülü Linux Sistemleri - Geliştirme ve Uygulama" 
    kursunun konularını oluşturmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aygıt sürücü geliştirirken çekirdeğin kaynak kodlarına gereksinim duyulmaz. Ancak çekirdeğin başlık dosyalarının 
    geliştirmenin yapıldığı bilgisayarda yüklü olması gerekir. Çekirdek kodlarının kendisini değil de yalnızca başlık 
    dosyalarını indirmek için aşağıdaki komut kullanılabilir:

    $ sudo apt install linux-headers-$(uname -r)

    Buradaki $(uname -r) çalışılmakta olan makinedeki çekirdek sürümünü belirtmektedir. Tabii biz istediğimiz 
    çekirdek sürümünün başlık dosyalarını da indirebiliriz. İndirilen dosyalar "/usr/src" dizininin altına $(uname -r) 
    isimli dizin içerisine yerleştirilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinin gerçekleştiriminde çeşitli veri yapılarından (data structures) faydalanılmıştır. Çekirdekte 
    en çok kullanılan veri yapıları şunlardır:

    - Çift Bağlı Listeler
    - Bitmap'ler
    - Hash Tabloları
    - Kuyruk Sistemleri
    - Dengelenmiş Ağaçlar ve Radix Ağaçları

    Çekirdek içerisinde bu veri yapıları türden bağımsız bir biçimde yani genelleştirilerek gerçekleştirilmiştir.

    Biz bu bölümde "bağlı listelerin (linked lists)" Linux çekirdeğindeki gerçekleştirimleri üzerinde duracağız. Diğer 
    veri yapılarını çeşitli konular içerisinde yeri geldikçe açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin en önemli veri yapılarından biri bağlı listelerdir. Genel olarak çekirdekteki neredeyse tüm bağlı listeler 
    "çift bağlı (doubly linked)" biçimde kullanılmaktadır. Önce bağlı listelerin Linux çekirdeğindeki gerçekleştirimleri 
    üzerinde duracağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aralarında öncelik-sonralık ilişkisi olan veri yapılarına "liste (list)" tarzı veri yapıları denilmektedir. Örneğin 
    bu tanıma göre diziler de "liste tarzı" veri yapılarıdır. Liste tarzı veri yapılarının en yaygın kullanılanlarından 
    biri "bağlı liste (linked list)" denilen veri yapısıdır. Önceki elemanın sonraki elemanın yerini gösterdiği dolayısıyla 
    elemanların ardışıl olma zorunluluğunun ortadan kaldırıldığı listelere "bağlı liste" denilmektedir. Dizi elemanlarının 
    bellekte fiziksel olarak ardışıl biçimde bulunduğunu anımsayınız. Bağlı listeler adeta "elemanları bellekte ardışıl 
    olmak zorunda olmayan diziler" gibidir.

    Bağlı listelerin her elemanına "düğüm (node)" denilmektedir. Bağlı listelerde her düğüm sonraki düğümün yerini tuttuğuna 
    göre ilk elemanın yeri biliniyorsa liste elemanlarının hepsine erişilebilmektedir. Örneğin:

    head ---> node ---> node ---> node ---> node (NULL)

    Her düğümün yalnızca sonraki düğümün yerini değil aynı zamanda önceki düğümün yerini de tuttuğu bağlı listelere 
    "çift bağlı listeler (double linked lists)" denilmektedir. Çift bağlı listelerde belli bir düğümün adresini biliyorsak 
    yalnızca ileriye doğru değil, geriye doğru da gidebiliriz.

    head <---> node <---> node <---> node <---> node (NULL)

    Çift bağlı listelere ilişkin bir düğümün bellekte daha fazla yer kaplayacağına dikkat ediniz. Çift bağlı listelerin 
    tek bağlı listelere göre en önemli özelliği "adresi bilinen bir düğümün" silinebilmesidir. Tek bağlı listelerde bu 
    durum mümkün değildir. Uygulamalarda ve özellikle çekirdek kodlarında buna çok sık gereksinim duyulmaktadır.

    Eğer bir bağlı listede son eleman da ilk elemanı gösteriyorsa bu tür bağlı listelere "döngüsel bağlı listeler 
    (circular linked lists)" denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki bağlı listelere neden gereksinim duyulmaktadır? Diziler varken bağlı listelere gerek var mıdır? Dizilerle 
    bağlı listeler arasındaki farklılıkları, benzerlikleri ve bağlı listelere neden gereksinim duyulduğunu birkaç maddede 
    açıklayabiliriz:

    1) Diziler ardışıl alana gereksinim duymaktadır. Ancak belleğin bölündüğü (fragmente olduğu) durumlarda bellekte yeteri 
    kadar küçük boş alanlar olduğu halde bunlar ardışıl olmadığı için dizi tahsisatı mümkün olamamaktadır. Bu tür durumlarda 
    ardışıllık gereksinimi olmayan bağlı listeler kullanılabilir. Özellikle heap gibi bir alanda çok sayıda dinamik dizi 
    bellek kullanımı bakımından verimsizliğe yol açabilmektedir. Bu dinamik diziler zamanla büyüdükçe birbirini engeller 
    hale gelebilmektedir. İşte uzunluğu baştan belli olmayan çok sayıda dizinin oluşturulacağı durumlarda dinamik dizi 
    yerine bağlı listeler toplamda daha iyi performans gösterebilmektedir. Dinamik dizilerde dinamik dizinin büyütülmesi 
    yavaş bir işlemdir. Çünkü büyütme sırasında bloklar yer değiştirebilmektedir.

    2) Dizilerde araya eleman ekleme (insert etme) ve aradaki bir elemanı silme dizinin kaydırılmasına ("expand" ve "shrink" 
    edilmesine) yol açacağından yavaş bir işlemdir. Teknik olarak dizilerde eleman insert etme ve eleman silme O(N) karmaşıklıkta 
    bir işlemdir. Halbuki bağlı listelerde eğer düğümün yeri biliniyorsa bu işlem O(1) karmaşıklıkta (yani döngü olmadan 
    tekil işlemlerle) yapılabilmektedir. O halde araya eleman eklemenin ve aradan eleman silmenin çok yapıldığı sistemlerde 
    diziler yerine bağlı listeler tercih edilebilmektedir. Çekirdek veri yapılarında araya eleman ekleme ve aradan eleman 
    silme gibi işlemler çok yoğun yapılmaktadır.

    3) Bağlı listelerde belli bir indeksteki elemana erişmek O(N) karmaşıklıkta bir işlemdir. Halbuki dizilerde elemana 
    erişim O(1) karmaşıklıkta yani çok hızlıdır. O halde belli bir indeks değeri ile elemana erişimin yoğun yapıldığı 
    durumlarda bağlı listeler yerine diziler tercih edilmelidir.

    4) Bağlı listeler toplamda bellekte daha fazla yer kaplama eğilimindedir. Çünkü bağlı listenin her düğümü sonraki 
    (ve duruma göre önceki) elemanın yerini de tutmaktadır.

    O halde bağlı listeler tipik olarak şu durumlarda dizilere tercih edilmelidir:

    - Eleman insert etmenin ve eleman silmenin sık yapıldığı durumlarda.
    - Uzunluğu baştan belli olmayan çok sayıda dizinin kullanıldığı durumlarda.
    - İndeks yoluyla erişimin az yapıldığı durumlarda.
    - Toplam bellek miktarının yeteri kadar fazla olduğu sistemlerde.

    İşte tüm yukarıdaki nedenlerden dolayı bağlı listeler çekirdek için en önemli veri yapılarından biridir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinde bağlı liste gerçekleştirimi "include/linux/list.h" dosyasında yapılmıştır. Bu dosya içerisindeki 
    fonksiyonlar static inline biçiminde tanımlanmıştır. Çekirdek derlemesi sırasında derleyicinin optimizasyon seçeneği 
    ayarlandığı için derleyici buradaki inline fonksiyonları sanki bir makro gibi koda açmaktadır.

    Linux çekirdeğindeki bağlı listelerde bağlı listelerin düğümleri list_head isimli bir yapıyla temsil edilmektedir:

    struct list_head {
        struct list_head *next;
        struct list_head *prev;
    };

    Aslında belli yapılar değil, bu list_head yapıları bağlı liste içerisinde birbirine bağlanmaktadır:

    list_head (kök) <---> list_head <---> list_head <---> list_head <---> list_head <---> list_head <---> list_head

    Tabii eğer bu list_head yapıları başka bir yapının (buna asıl yapı diyelim) içerisindeyse bu durumda aslında bir 
    list_head yapısının adresi asıl yapının bir elemanının adresi haline gelmektedir. Biz C'de bir yapının bir elemanının 
    adresini biliyorsak kolaylıkla o yapının başlangıç adresini elde edebiliriz. Çünkü yapı elemanları ardışıldır ve 
    standart "offsetof" makrosuyla belli bir yapı elemanının yapının başlangıcından itibaren hangi offset'te bulunduğu 
    bilgisi elde edilebilmektedir. "offsetof" makrosu <stddef.h> içerisinde aşağıdakine benzer biçimde tanımlanmıştır:

    #define offsetof(type, member)	((size_t)&(((type *)0)->member))

    "offsetof" makrosunun birinci parametresi yapının tür ismini, ikinci parametresi ilgili yapı elemanının ismini almaktadır. 
    Tabii "offsetof" makrosu yalnızca size_t türünden bir byte offset'i vermektedir. Elemanın adresinin makroyla elde 
    edilen bu değerden çıkartılarak tür dönüştürmesinin yapılması gerekir. İşte bunun için Linux çekirdeğinde "container_of" 
    isimli bir makro bulundurulmuştur. Bu makronun basit yazımı şöyle yapılabilir:

    #define container_of(ptr, type, member)	((type *)((char *)ptr - myoffsetof(type, member)))

    "container_of" makrosunun birinci parametresi yapı elemanın adresini, ikinci parametresi yapının tür ismini ve üçüncü 
    parametresi de ilgili yapı elemanının ismini almaktadır. Bu makro ikinci parametresine girilmiş olan yapı türünden 
    bir adres vermektedir.

    "container_of" makrosunu "list_entry" ismiyle de kullanabilirsiniz:

    #define list_entry(ptr, type, member) container_of(ptr, type, member)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										11. Ders 23/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğindeki bağlı listeler kullanılırken önce bir başlangıç düğümünün oluşturulması gerekir. Başlangıç 
    düğümünde next ve prev göstericilerinin başlangıç düğümünün kendisini göstermesi gerekir. Örneğin:

    struct list_head head = {&head, &head};

    Bu ilkdeğer verme C'de geçerlidir. Çünkü C'de bir değişken faaliyet alanına "=" atomu ile ilkdeğer verme kısmından 
    önce sokulmaktadır. "list.h" dosyasında bu işlemi yapan aşağıdaki gibi bir makro da bulundurulmuştur:

    #define LIST_HEAD_INIT(name) {&(name), &(name)}

    Bu durumda başlangıç düğümü bu makroyla şöyle de oluşturulabilir:

    struct list_head head = LIST_HEAD_INIT(head);

    Aslında bu tanımlamanın tamamını yapan aşağıdaki gibi bir makro da bulundurulmuştur:

    #define LIST_HEAD(name) \ 
        struct list_head name = LIST_HEAD_INIT(name)

    O halde biz başlangıç düğümünü basit bir biçimde şöyle oluşturabiliriz:

    LIST_HEAD(head);

    Linux'un bağlı liste gerçekleştiriminde next ve prev göstericilerinde NULL adres kullanılmamıştır. Son düğümün next 
    göstericisi NULL yerine başlangıç düğümünü, ilk düğümün prev göstericisi de NULL yerine son düğümü göstermektedir. 
    Yani aslında bağlı liste gerçekleştirimi döngüsel (circular) gibidir. Herhangi bir düğümden başlanarak başlangıç düğümü 
    de atlanırsa tam dolaşım sağlanabilir. Bağlı listenin başlangıç düğümünün prev göstericisi de son düğümü göstermektedir.

    Bağlı listenin başına list_head düğümünü eklemek için list_add fonksiyonu kullanılmaktadır:

    static inline void list_add(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head, head->next);
    }

    Fonksiyonun birinci parametresi eklenecek düğümün adresini, ikinci parametresi başlangıç düğümünün adresini almaktadır.

    Çekirdek kodlarında iki alt tire ile başlayan fonksiyonlar aşağı seviyeli fonksiyonlardır. Yani doğrudan çağrılmayan 
    ancak başka fonksiyonların içerisinden dolaylı biçimde çağrılan fonksiyonlardır. Buradaki __list_add fonksiyonu şöyle 
    yazılmıştır:

    static inline void __list_add(struct list_head *new,
                  struct list_head *prev,
                  struct list_head *next)
    {
        if (!__list_add_valid(new, prev, next))
            return;

        next->prev = new;
        new->next = next;
        new->prev = prev;
        WRITE_ONCE(prev->next, new);
    }

    Buradaki WRITE_ONCE makrosu çok işlemcili ya da çok çekirdekli sistemlerde bellek bariyeri oluşturarak atama yapmaktadır. 
    Siz WRITE_ONCE(a, b) çağrısını a = b gibi düşünebilirsiniz.

    Bağlı listenin sonuna düğüm eklemek için list_add_tail fonksiyonu kullanılmaktadır:

    static inline void list_add_tail(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head->prev, head);
    }

    Peki biz bağlı listeyi nasıl dolaşabiliriz? Başlangıç düğümünü geçerek sürekli ileriye gidersek son düğüm de 
    başlangıç düğümünü gösterdiğine göre dolaşımı aşağıdaki gibi bir for döngüsüyle yapabiliriz:

    struct list_head *lh;
    /* ... */

    for (lh = head.next; lh != &head; lh = lh->next) {
        /* ... */
    }

    Tabii böyle bir döngüde dolaşım yapılırken aslında biz her yinelemede ilgili yapının başlangıç adresini değil 
    list_head yapısının başlangıç adresini elde ederiz. container_of makrosu ile bu adresin yapı adresine dönüştürülmesi 
    gerekir. Örneğin biz struct SAMPLE türünden yapı nesnelerini birbirine bağlamış olalım:

    LIST_HEAD(head);

    struct SAMPLE {
        int a;
        struct list_head link;
    };

    Eklemeleri şöyle yapmış olalım:

    struct SAMPLE *ps;

    for (int i = 0; i < 10; ++i) {
        if ((ps = (struct SAMPLE *)malloc(sizeof(struct SAMPLE))) == NULL) {
            fprintf(stderr, "cannot allocate memory!...\n");
            exit(EXIT_FAILURE);
        }
        ps->a = i;
        list_add_tail(&ps->link, &head);
    }

    Çekirdek kodlarında malloc gibi kullanıcı modu fonksiyonları kullanılamaz. Ancak biz burada yalnızca anlaşılır 
    bir test örneği vermek istiyoruz. struct SAMPLE yapımızdaki link elemanı list_head yapılarını birbirine bağlamak için 
    bulundurulmuştur. Örneğimizdeki list_add_tail fonksiyonunda ekleme yapılacak düğümün SAMPLE yapısının içerisindeki 
    link elemanının adresi olduğuna dikkat ediniz. Şimdi dolaşımı şöyle yapabiliriz:

    struct SAMPLE *ps;
    struct list_head *lh;
    /* ... */

    for (lh = head.next; lh != &head; lh = lh->next) {
        ps = container_of(lh, struct SAMPLE, link);
        /* ... */
    }

    Burada artık ps göstericisi list_head nesnesini değil struct SAMPLE nesnesini göstermektedir. Aslında "list.h" dosyası 
    içerisinde buradaki döngüyü oluşturan list_for_each isimli bir makro da bulundurulmuştur:

    static inline int list_is_head(const struct list_head *list, const struct list_head *head)
    {
        return list == head;
    }

    #define list_for_each(pos, head) \
	    for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)

    Makronun birinci parametresi list_head türünden bir göstericiyi, ikinci parametresi başlangıç düğümünün adresini 
    almaktadır. Döngünün her yinelenmesinde bu göstericiye sonraki düğümün list_head adresi atanmaktadır. Örneğin:

    struct list_head *lh;
    /* ... */

    list_for_each(lh, &head) {
        ps = container_of(lh, struct SAMPLE, link);
        printf("%d\n", ps->a);
    }

    Aslında çekirdekteki "list.h" dosyası içerisinde yukarıdaki dolaşımı tek hamlede yapan list_for_each_entry isimli bir 
    makro da bulunmaktadır:

    #define list_for_each_entry(pos, head, member)				\
	for (pos = list_first_entry(head, typeof(*pos), member);	\
	     !list_entry_is_head(pos, head, member);			    \
	     pos = list_next_entry(pos, member))

    Makronun birinci parametresi asıl yapı türünden bir göstericidir. Makro her yinelemede bu göstericiye sonraki düğümün 
    adresini yerleştirmektedir. Makronun ikinci parametresi başlangıç düğümünün adresini, üçüncü parametresi ise yapı 
    içerisindeki list_head elemanın ismini belirtmektedir. Bu makronun eşdeğeri bazı ayrıntılar ihmal edilerek şöyle de 
    yazılabilir:

    #define my_list_for_each_entry(pos, head, member)                                           \
        for (pos = container_of((head)->next, typeof(*pos), member);  &(pos)->member != head;   \
            pos = container_of((pos)->member.next, typeof(*pos), member))

    C standartlarında bir ifadenin türünü veren bir tür belirleyicisi yoktu. C++'a C++11 ile birlikte decltype ismi ile 
    böyle bir belirleyici eklendi. gcc derleyicileri uzun süredir bu işi yapan typeof isimli belirleyiciyi desteklemektedir. 
    Nihayet C'ye de C23 ile birlikte resmi olarak typeof belirleyicisi eklenmiştir. Makroda türün tür isminin typeof(*pos) 
    ifadesiyle elde edildiğine dikkat ediniz. Bu makro ile dolaşım şöyle sağlanabilir:

    struct SAMPLE *ps;
    /* ... */

    list_for_each_entry(ps, &head, link) {
        /* ... */
    }

    Bağlı liste makrolarının ve fonksiyonlarının isimlerinin sonunda entry sözcüğü varsa bu makro ya da fonksiyon asıl 
    yapıya ilişkin adres, yoksa list_head türünden adres verip almaktadır.

    Ayrıca "list.h" içerisinde list_for_each_entry_safe isimli bir döngü makrosu da bulundurulmuştur. Bu makro eğer liste 
    boşsa hiç dolaşım yapmamaktadır. Makro aşağıdaki gibi yazılmıştır:

    #define list_for_each_entry_safe(pos, n, head, member)			\
	for (pos = list_first_entry(head, typeof(*pos), member),	    \
		n = list_next_entry(pos, member);			                \
	     !list_entry_is_head(pos, head, member); 			        \
	     pos = n, n = list_next_entry(n, member))

    Makrounun birinci elemanı dolaşımda kullanılacak asıl yapı türünden göstericiyi almaktadır. Makronun ikinci parametresi 
    de asıl yapı göstericidir. Üçüncü ve dördüncü parametreler sırasıyla kök düğümün adresi ve bağ düğümünün ismini 
    almaktadır.

    Bağlı listeden bir düğümü silmek için list_del fonksiyonu kullanılmaktadır. Fonksiyon şöyle tanımlanmıştır:

    static inline void __list_del(struct list_head * prev, struct list_head * next)
    {
        next->prev = prev;
        WRITE_ONCE(prev->next, next);
    }

    static inline void __list_del_entry(struct list_head *entry)
    {
        if (!__list_del_entry_valid(entry))
            return;

        __list_del(entry->prev, entry->next);
    }

    static inline void list_del(struct list_head *entry)
    {
        __list_del_entry(entry);
        entry->next = LIST_POISON1;
        entry->prev = LIST_POISON2;
    }

    Fonksiyonun parametresi silinecek düğüme ilişkin list_head nesnesinin adresini almaktadır. Burada silinen düğümdeki 
    next ve prev göstericilerine özel bazı değerlerin atandığını görüyorsunuz. Bu değerler silinmiş düğümün kullanılması 
    durumunda "page fault" oluşmasına yol açmaktadır. Bu değerlerin debug mekanizması bir çeşit debug mekanizması oluşturmak 
    amacıyla atandığını söyleyebiliriz.

    Bağlı listenin arasına düğüm ekleyen insert isimli fonksiyonlar yoktur. Zaten list_add fonksiyonu araya ekleme 
    işlemini de yapmaktadır. Örneğin biz araya şöyle eklama yapabiliriz:

    list_add(&ps->link, &ps_insert->link);

    Burada ps eklenecek düğümün asıl yapı adresini, ps_insert ise önüne eklemenin yapılacağı asıl yapı adresini 
    belirtmektedir.

    Bir bağlı listeyi tümden serbest bırakmak için düğümlerin tek tek serbest bırakılması gerekmektedir. Buradaki 
    düğümler aslında başka yapıların içerisinde olduğuna göre liste içerisindeki o yapı nesnelerinin serbest bırakılması 
    gerekir.

    Aşağıda kullanıcı alanında çekirdekteki bağlı liste kullanımına bir örnek verilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

#include <stdio.h>
#include <stdlib.h>
#include <stddef.h>

struct list_head {
	struct list_head *next, *prev;
};

#define LIST_HEAD_INIT(name) { &(name), &(name) }

#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

#define container_of(ptr, type, member) ({				\
		void *__mptr = (void *)(ptr);					\
		((type *)(__mptr - offsetof(type, member))); })

#define list_entry(ptr, type, member) \
	container_of(ptr, type, member)

#define list_entry_is_head(pos, head, member)				\
	(&pos->member == (head))

#define list_first_entry(ptr, type, member) \
	list_entry((ptr)->next, type, member)

#define list_next_entry(pos, member) \
	list_entry((pos)->member.next, typeof(*(pos)), member)

#define list_for_each(pos, head) \
	for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)

#define list_for_each_entry(pos, head, member)				\
	for (pos = list_first_entry(head, typeof(*pos), member);	\
		!list_entry_is_head(pos, head, member);			\
		pos = list_next_entry(pos, member))

static inline int list_is_head(const struct list_head *list, const struct list_head *head)
{
	return list == head;
}

static inline void __list_add(struct list_head *new,
				struct list_head *prev,
				struct list_head *next)
{
	next->prev = new;
	new->next = next;
	new->prev = prev;
	prev->next = new;
}

static inline void list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
}

static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
}

static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
	prev->next = next;
}

static inline void __list_del_entry(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
}

static inline void list_del(struct list_head *entry)
{
	__list_del_entry(entry);
}

LIST_HEAD(head);

struct SAMPLE {
	int a;
	struct list_head link;
};

#define my_list_for_each_entry(pos, head, member)											\
	for (pos = container_of((head)->next, typeof(*pos), member); &(pos)->member != head;	\
			pos = container_of((pos)->member.next, typeof(*pos), member))

#define list_for_each_entry_safe(pos, n, head, member)			    \
	for (pos = list_first_entry(head, typeof(*pos), member),	    \
		n = list_next_entry(pos, member);			                \
	     !list_entry_is_head(pos, head, member); 			        \
	     pos = n, n = list_next_entry(n, member))

int main(void)
{
	struct SAMPLE *ps, *ps_node, *ps_temp, *ps_del, *ps_insert;
	struct list_head *lh;

	for (int i = 0; i < 10; ++i) {
		if ((ps = (struct SAMPLE *)malloc(sizeof(struct SAMPLE))) == NULL) {
			fprintf(stderr, "cannot allocate memory!...\n");
			exit(EXIT_FAILURE);
		}
		ps->a = i;
		list_add_tail(&ps->link, &head);

		if (i == 5)
			ps_del = ps;

		if (i == 7)
			ps_insert = ps;
	}

	list_for_each(lh, &head) {
		ps = container_of(lh, struct SAMPLE, link);
		printf("%d ", ps->a);
	}
	printf("\n");

	list_del(&ps_del->link);

	list_for_each(lh, &head) {
		ps = container_of(lh, struct SAMPLE, link);
		printf("%d ", ps->a);
	}
	printf("\n");

	if ((ps = (struct SAMPLE *)malloc(sizeof(struct SAMPLE))) == NULL) {
		fprintf(stderr, "cannot allocate memory!...\n");
			exit(EXIT_FAILURE);
	}
	ps->a = 100;

	list_add(&ps->link, &ps_insert->link);

	list_for_each(lh, &head) {
		ps = container_of(lh, struct SAMPLE, link);
		printf("%d ", ps->a);
	}
	printf("\n");

	ps_temp = NULL;
	list_for_each_entry_safe(ps, ps_node, &head, link) {
		free(ps_temp);
		ps_temp = ps;
	}

	return 0;
}

/*----------------------------------------------------------------------------------------------------------------------
    Belli bir süreden sonra "list.h" dosyasına RCU (Read-Copy_Update) mekanizmasını destekleyecek biçimde dolaşım yapan 
    list_for_each_rcu isimli makro da eklenmiştir. Bu makro bir yazıcı varsa okuyucuları bekletmeden işlem yapabilmeyi 
    sağlamaktadır. "Read-copy-update" mekanizması ileride ele alınacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin eski sürümlerinde "list.h" dosyası oldukça sade idi. Sonra bu dosyanın içeriğinde de bazı faydalı değişiklikler 
    yapıldı. Ancak bu değişiklikler veri yapısının anlaşılmasını da biraz zorlaştırmıştır. Örneğin 2.2.26 çekirdeğindeki 
    "list.h" dosyası oldukça sade bir biçimde aşağıdaki gibi oluşturulmuştur:

    /* 2.2.26 <list.h> dosyasının içeriği */

    #ifndef _LINUX_LIST_H
    #define _LINUX_LIST_H

    #ifdef __KERNEL__

    /*
    * Simple doubly linked list implementation.
    *
    * Some of the internal functions ("__xxx") are useful when
    * manipulating whole lists rather than single entries, as
    * sometimes we already know the next/prev entries and we can
    * generate better code by using them directly rather than
    * using the generic single-entry routines.
    */

    struct list_head {
        struct list_head *next, *prev;
    };

    #define LIST_HEAD_INIT(name) { &(name), &(name) }

    #define LIST_HEAD(name) \
        struct list_head name = { &name, &name }

    #define INIT_LIST_HEAD(ptr) do { \
        (ptr)->next = (ptr); (ptr)->prev = (ptr); \
    } while (0)

    /*
    * Insert a new entry between two known consecutive entries.
    *
    * This is only for internal list manipulation where we know
    * the prev/next entries already!
    */
    static __inline__ void __list_add(struct list_head * new,
        struct list_head * prev,
        struct list_head * next)
    {
        next->prev = new;
        new->next = next;
        new->prev = prev;
        prev->next = new;
    }

    /*
    * Insert a new entry after the specified head..
    */
    static __inline__ void list_add(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head, head->next);
    }

    /*
    * Insert a new entry at the tail
    */
    static __inline__ void list_add_tail(struct list_head *new, struct list_head *head)
    {
        __list_add(new, head->prev, head);
    }

    /*
    * Delete a list entry by making the prev/next entries
    * point to each other.
    *
    * This is only for internal list manipulation where we know
    * the prev/next entries already!
    */
    static __inline__ void __list_del(struct list_head * prev,
                    struct list_head * next)
    {
        next->prev = prev;
        prev->next = next;
    }

    static __inline__ void list_del(struct list_head *entry)
    {
        __list_del(entry->prev, entry->next);
    }

    static __inline__ int list_empty(struct list_head *head)
    {
        return head->next == head;
    }

    /*
    * Splice in "list" into "head"
    */
    static __inline__ void list_splice(struct list_head *list, struct list_head *head)
    {
        struct list_head *first = list->next;

        if (first != list) {
            struct list_head *last = list->prev;
            struct list_head *at = head->next;

            first->prev = head;
            head->next = first;

            last->next = at;
            at->prev = last;
        }
    }

    #define list_entry(ptr, type, member) \
        ((type *)((char *)(ptr)-(unsigned long)(&((type *)0)->member)))

    #define list_for_each(pos, head) \
            for (pos = (head)->next; pos != (head); pos = pos->next)

    #endif /* __KERNEL__ */

    #endif
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										12. Ders 24/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinde "program" terimi çoğu kez "çalıştırılabilen bir dosyayı" ya da "bir kaynak dosyayı" belirtmektedir. 
    Çalışmakta olan programlara ise "proses" denilmektedir. Bir program çalıştırıldığında artık o bir proses haline gelmektedir. 
    Aynı programı birden fazla kez de çalıştırabiliriz. Bu durumda birbirinden bağımsız birden fazla proses oluşacaktır.

    İşletim sistemlerinin proses yönetimlerindeki en önemli veri yapısı "proses kontrol bloğu (process control block)" 
    denilen veri yapısıdır. İşletim sistemleri her proses için kavramsal olarak ismine "proses kontrol bloğu" denilen bir 
    yapı türünden nesne oluşturmaktadır. Proseslerin yönetimi proses kontrol bloğu denilen bu veri yapısına başvurularak 
    yapılmaktadır. Proses kontrol bloğu terimi yerine bazı kaynaklar "proses betimleyicisi (process descriptor)" terimini 
    de kullanmaktadır.

    Proses kontrol blokları içerisinde prosese ilişkin gerekli bütün bilgiler bulundurulmaktadır. Bu bilgilerden bazıları 
    şunlardır:

    - Prosesin kullanıcı id'si, grup id'si gibi hesap (credential) bilgileri
    - Proses id'si
    - Prosesin üst prosesinin, alt proseslerinin, kardeş proseslerinin hangi prosesler olduğu bilgisi
    - Prosesin durumsal bilgisi
    - Prosesin bağlamsal geçişini (context switch) sağlama için gereksinim duyulan alanlar
    - Prosesin bellekte nereye yüklü olduğu gibi bellekle ilgili bilgileri
    - Prosesin çizelgeleyici ile ilgili olan bilgileri (örneğin çizelgeleme politikası, önceliği gibi)
    - Prosesin CPU kullanım istatistiği
    - Prosesin sinyal bilgileri
    - Prosesin proseslerarası haberleşme için gerekli olan bilgileri
    - Prosesin çalışma dizini (current working directory)
    - Prosesin açmış olduğu dosyalara ilişkin bilgiler
    - ...
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinde proses kontrol bloğu <include/linux/sched.h> dosyası içerisinde bulunan task_struct isimli yapıyla
    temsil edilmiştir. Bu task_struct nesnesi eskiden daha az eleman içeriyordu. Sonra çekirdek gittikçe geliştirilince
    dev bir yapı haline geldi. Örneğin bu task_struct nesnesi Linus projeye ilk başlandığında (Linux 0.01) şu biçimdeydi:

    struct task_struct {
    /* these are hardcoded - don't touch */
        long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
        long counter;
        long priority;
        long signal;
        fn_ptr sig_restorer;
        fn_ptr sig_fn[32];
    /* various fields */
        int exit_code;
        unsigned long end_code,end_data,brk,start_stack;
        long pid,father,pgrp,session,leader;
        unsigned short uid,euid,suid;
        unsigned short gid,egid,sgid;
        long alarm;
        long utime,stime,cutime,cstime,start_time;
        unsigned short used_math;
    /* file system info */
        int tty;		/* -1 if no tty, so it must be signed */
        unsigned short umask;
        struct m_inode * pwd;
        struct m_inode * root;
        unsigned long close_on_exec;
        struct file * filp[NR_OPEN];
    /* ldt for this task 0 - zero 1 - cs 2 - ds&ss */
        struct desc_struct ldt[3];
    /* tss for this task */
        struct tss_struct tss;
    };

    Örneğin çekirdeğin 2.4'lü versiyonunda bu yapı şu hale gelmiştir:

    struct task_struct {
        /*
        * offsets of these are hardcoded elsewhere - touch with care
        */
        volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
        unsigned long flags;	/* per process flags, defined below */
        int sigpending;
        mm_segment_t addr_limit;	/* thread address space:
                            0-0xBFFFFFFF for user-thread
                            0-0xFFFFFFFF for kernel-thread
                        */
        struct exec_domain *exec_domain;
        volatile long need_resched;
        unsigned long ptrace;

        int lock_depth;		/* Lock depth */

    /*
    * offset 32 begins here on 32-bit platforms. We keep
    * all fields in a single cacheline that are needed for
    * the goodness() loop in schedule().
    */
        long counter;
        long nice;
        unsigned long policy;
        struct mm_struct *mm;
        int processor;
        /*
        * cpus_runnable is ~0 if the process is not running on any
        * CPU. It's (1 << cpu) if it's running on a CPU. This mask
        * is updated under the runqueue lock.
        *
        * To determine whether a process might run on a CPU, this
        * mask is AND-ed with cpus_allowed.
        */
        unsigned long cpus_runnable, cpus_allowed;
        /*
        * (only the 'next' pointer fits into the cacheline, but
        * that's just fine.)
        */
        struct list_head run_list;
        unsigned long sleep_time;

        struct task_struct *next_task, *prev_task;
        struct mm_struct *active_mm;
        struct list_head local_pages;
        unsigned int allocation_order, nr_local_pages;

    /* task state */
        struct linux_binfmt *binfmt;
        int exit_code, exit_signal;
        int pdeath_signal;  /*  The signal sent when the parent dies  */
        /* ??? */
        unsigned long personality;
        int did_exec:1;
        unsigned task_dumpable:1;
        pid_t pid;
        pid_t pgrp;
        pid_t tty_old_pgrp;
        pid_t session;
        pid_t tgid;
        /* boolean value for session group leader */
        int leader;
        /*
        * pointers to (original) parent process, youngest child, younger sibling,
        * older sibling, respectively.  (p->father can be replaced with
        * p->p_pptr->pid)
        */
        struct task_struct *p_opptr, *p_pptr, *p_cptr, *p_ysptr, *p_osptr;
        struct list_head thread_group;

        /* PID hash table linkage. */
        struct task_struct *pidhash_next;
        struct task_struct **pidhash_pprev;

        wait_queue_head_t wait_chldexit;	/* for wait4() */
        struct completion *vfork_done;		/* for vfork() */
        unsigned long rt_priority;
        unsigned long it_real_value, it_prof_value, it_virt_value;
        unsigned long it_real_incr, it_prof_incr, it_virt_incr;
        struct timer_list real_timer;
        struct tms times;
        unsigned long start_time;
        long per_cpu_utime[NR_CPUS], per_cpu_stime[NR_CPUS];
    /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
        unsigned long min_flt, maj_flt, nswap, cmin_flt, cmaj_flt, cnswap;
        int swappable:1;
    /* process credentials */
        uid_t uid,euid,suid,fsuid;
        gid_t gid,egid,sgid,fsgid;
        int ngroups;
        gid_t	groups[NGROUPS];
        kernel_cap_t   cap_effective, cap_inheritable, cap_permitted;
        int keep_capabilities:1;
        struct user_struct *user;
    /* limits */
        struct rlimit rlim[RLIM_NLIMITS];
        unsigned short used_math;
        char comm[16];
    /* file system info */
        int link_count, total_link_count;
        struct tty_struct *tty; /* NULL if no tty */
        unsigned int locks; /* How many file locks are being held */
    /* ipc stuff */
        struct sem_undo *semundo;
        struct sem_queue *semsleeping;
    /* CPU-specific state of this task */
        struct thread_struct thread;
    /* filesystem information */
        struct fs_struct *fs;
    /* open file information */
        struct files_struct *files;
    /* namespace */
        struct namespace *namespace;
    /* signal handlers */
        spinlock_t sigmask_lock;	/* Protects signal and blocked */
        struct signal_struct *sig;

        sigset_t blocked;
        struct sigpending pending;

        unsigned long sas_ss_sp;
        size_t sas_ss_size;
        int (*notifier)(void *priv);
        void *notifier_data;
        sigset_t *notifier_mask;

    /* Thread group tracking */
        u32 parent_exec_id;
        u32 self_exec_id;
    /* Protection of (de-)allocation: mm, files, fs, tty */
        spinlock_t alloc_lock;

    /* journalling filesystem info */
        void *journal_info;
    };

    Bugünkü 6'lı çekirdeklerde bu yapı birkaç sayfa uzunluğundadır. Eskiden yukarıda da gördüğünüz gibi yapının her 
    elemanı çekirdek derlemesine dahil ediliyordu. Ancak 2.6'lı versiyonlardan başlanarak artık yapı elemanlarının bazıları 
    konfigürasyona (yani "menuconfig" menüsündeki seçeneklere ya da doğrudan ".config" dosyasındaki seçeneklere) bağlı 
    olarak yapıya dahil edilmektedir. Bu nedenle gelişmiş çekirdeklerde yapıda aşağıdaki gibi #ifdef blokları görürseniz 
    şaşırmayınız:

    struct task_struct {
    #ifdef CONFIG_THREAD_INFO_IN_TASK
        /*
        * For reasons of header soup (see current_thread_info()), this
        * must be the first element of task_struct.
        */
        struct thread_info		thread_info;
    #endif
        unsigned int			__state;

        /* saved state for "spinlock sleepers" */
        unsigned int			saved_state;

        /*
        * This begins the randomizable portion of task_struct. Only
        * scheduling-critical items should be added above here.
        */
        randomized_struct_fields_start

        void				*stack;
        refcount_t			usage;
        /* Per task flags (PF_*), defined further below: */
        unsigned int			flags;
        ...
    };

    Bilindiği gibi Linux sistemlerinde yeni bir proses fork isimli POSIX fonksiyonuyla yaratılmaktadır. fork POSIX 
    fonksiyonu çekirdek içerisindeki sys_fork isimli sistem fonksiyonunu çağırmaktadır. Yeni çekirdeklerde bu fonksiyon 
    da kernel_clone isimli çekirdek fonksiyonu çağırmaktadır. İşte task_struct nesnesi bu fonksiyonlar tarafından çekirdeğin 
    heap alanında yaratılmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi UNIX/Linux sistemlerinde her prosesin (yani çalışmakta olan her programın) o anda sistem genelinde 
    tek (unique) olan bir "proses id (process id)" değeri vardır. Proses id değeri hem çekirdek tarafından hem de 
    kullanıcı modundan çağrılabilen sistem fonksiyonları tarafından kullanılan bir kavramdır. Proses id değeri bir 
    prosesi temsil etmekte kullanılan tamsayısal bir değerdir. ps komutuyla proseslere ilişkin proses id değerlerinin 
    görüntülendiğini anımsayınız. Kullanıcı modunda getpid POSIX fonksiyonu çalışmakta olan programın proses id değerini, 
    getppid POSIX fonksiyonu ise üst prosesin proses id değerini vermektedir. Tabii prosesin proses id değeri ve üst 
    prosesin proses id değeri proses kontrol bloğunda yani task_struct nesnesi içerisinde saklanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerine thread kavramı 90'lı yıllarda sokulmuştur. Ancak ilk thread denemeleri çeşitli çalışmalar 
    eşliğinde daha önceleri yapılmıştır. Linux işletim sistemine thread'ler ilk kez çekirdeğin 2.0 versiyonuyla eklenmiş 
    olsa da daha sonra çeşitli düzeltmelerle thread yapısı biraz değiştirilmiştir.

    Proses kavramı çalışmakta olan programın tüm bilgilerini temsil etmektedir. Halbuki thread yalnızca bir akış belirtmektedir. 
    Bir proses tek thread'le çalışmaya başlar. Prosesin çalışmaya başladığı thread'e "ana thread (main thread)" de 
    denilmektedir. Thread'ler de kullanıcı modundan sistem fonksiyonları (sys_clone sistem fonksiyonu) ile yaratılmaktadır.

    UNIX türevi sistemlere thread'ler ilk kez sokulduğunda farklı varyantlar farklı thread kütüphaneleri kullanıyordu. 
    Sonra thread kavramı POSIX standartlarına da POSIX.1c ile 1995 yılında sokuldu. Böylece POSIX taşınabilir thread 
    fonksiyonları oluşturdu. POSIX'in taşınabilir thread fonksiyonlarının gerçekleştirimi için Linux ve diğer UNIX 
    varyantları çekirdek içerisinde de değişiklikler yapmak zorunda kalmıştır.

    Thread'lerin yalnızca bir akış belirttiğini söylemiştik. Proses kavramı ise tüm thread'lerle birlikte çalışmakta 
    olan programın tüm bilgilerini temsil etmektedir. Pek çok bilgi thread'e özgü değildir, prosese özgüdür. Örneğin 
    bir thread'in "çalışma dizini (current working directory)" diye bir kavram yoktur. Prosesin çalışma dizini diye bir 
    kavram vardır. Örneğin bir thread'in kullanıcı id'si, grup id'si diye bir kavram yoktur. Prosesin kullanıcı id'si 
    ve grup id'si diye bir kavram vardır. Yani çalışmakta olan programa ilişkin pek çok bilgi programın tüm thread'leri 
    için de geçerlidir.

    Aslında thread'ler aynı bellek alanını ve ortak bilgileri kullanan prosesler gibi de düşünülebilir. Zaten Linux 
    işletim sisteminde thread yaratmakla proses yaratmak aslında aynı çekirdek fonksiyonuyla yapılmaktadır. Yani bir 
    prosesin thread'lerini biz aynı bellek alanını kullanan prosesler gibi de düşünebiliriz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux işletim sisteminde thread'ler de adeta aynı bellek alanı üzerinde çalışan birer proses gibi düşünülmüştür. 
    Bu nedenle yalnızca prosesler için değil thread'ler için de task_struct nesneleri oluşturulmaktadır. Tabii işletim 
    sistemi bir prosesin thread'lerini yani ana thread'in ve diğer thread'lerin task_struct nesnelerini izleyen paragraflarda 
    göreceğimiz biçimde bağlı listelerde tutmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    UNIX/Linux sistemlerinde prosesler arasında kuvvetli bir "altlık-üstlük (parent-child)" ilişkisi vardır. Bir proses 
    yaratıldığında prosesin task_struct bilgilerinin çoğu üst prosesten (parent process) alınmaktadır. Örneğin bir program 
    başka bir programı fork/exec ile çalıştırdığında çalıştırılan program çalıştıran programın pek çok özelliğini almaktadır. 
    Örneğin fork işlemi sırasında fork yapan programın kullanıcı id'si ve çalışma dizini neyse yeni yaratılan alt prosesin 
    (child process) kullanıcı id'si ve çalışma dizini de aynı olur. Daha teknik olarak ifade edersek fork işlemi sırasında 
    alt proses için yaratılan task_struct nesnesinin pek çok elemanı üst prosesin task_struct nesnesinden kopyalanmaktadır.

    POSIX thread sisteminde thread'ler arasında önemli bir altlık-üstlük ilişkisi yoktur. Yani birkaç özel durum dışında 
    bir thread'i hangi thread'in yarattığının önemi yoktur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    task_struct yapısının pek çok gösterici elemanı vardır. Bu gösterici elemanları başka yapıları göstermektedir. Hatta 
    o gösterici elemanların gösterdiği yapıların elemanları da başka yapıları gösterebilmektedir. O halde task_struct 
    aslında dallı budaklı bir yapıdır. Biz bir bilginin proses kontrol bloğunda olduğunu söylediğimiz zaman o bilginin 
    hemen task_struct içerisindeki bir elemanda olduğunu kastetmeyeceğiz. O bilgi task_struct içerisindeki bir elemanın 
    gösterdiği başka bir yapının içerisinde de olabilir. Önemli olan o bilgiye task_struct yapısından hareketle erişilebilmesidir.

    Bir task_struct yapı nesnesinin ana elemanları diğer bir task_struct nesnesine kopyalanırsa (C'de zaten aynı türden 
    iki yapı nesnesi birbirine atandığında yapının karşılıklı elemanları kopyalanmaktadır) aslında asıl yapı nesnesi ile 
    kopyalanmış olan yapı nesnesinin gösterici elemanları aynı yerleri gösterir duruma gelir. Programlamada bu tür kopyalamaya 
    "sığ kopyalama (shallow copy)" denildiğini anımsayınız. Aslında task_struct yapısının bu biçimdeki dallı budaklı tasarımı 
    üst proses-alt proses ilişkisinde alt prosesin üst prosesten birtakım özellikleri alması (inherit etmesi) sürecinde 
    faydalar sağlamaktadır. Böylece alt prosesin task_struct nesnesine daha az eleman kopyalanmaktadır. Örneğin güncel 
    çekirdeklerdeki task_struct yapısının aşağıdaki kısmına dikkat ediniz:

    struct task_struct {
        /* ... */

        /* Filesystem information: */
        struct fs_struct		*fs;

        /* Open file information: */
        struct files_struct		*files;

        /* ... */
    };

    Burada aslında fs göstericisi aşağıdaki gibi bir yapıyı göstermektedir:

    struct fs_struct {
        int users;
        spinlock_t lock;
        seqcount_spinlock_t seq;
        int umask;
        int in_exec;
        struct path root, pwd;
    } __randomize_layout;

    Prosesin kök dizininin ve çalışma dizinin burada tutulduğuna dikkat ediniz. files göstericisi ise aşağıdaki gibi 
    bir yapıyı göstermektedir:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    fork işlemi sırasında sığ kopyalama yapıldığından dolayı fork işleminden sonra üst ve alt prosesin task_struct 
    nesnelerinin fs ve files göstericileri aynı fs_struct ve files_struct nesnelerini gösteriyor olacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Thread'lerin de tıpkı prosesler gibi task_struct nesnelerine sahip olduğunu belirtmiştik. Peki işletim sisteminin 
    çizelgeleyici (scheduler) alt sistemi neleri çizelgelemektedir? İşte çizelgeleyici alt sistem aslında yalnızca 
    thread'leri yani thread'lere ilişkin task_struct nesnelerini çizelgelemektedir. Yani çizelgeleyici alt sistemin 
    "çalışma kuyruğunda (run queue)" yalnızca task_struct nesnelerinin olduğunu varsayabilirsiniz. Bir proses yaratıldığında 
    zaten o proses için yaratılan task_struct nesnesi aynı zamanda prosesin ana thread'inin task_struct nesnesi gibidir. 
    Başka bir deyişle aslında bütün task_struct nesneleri birer thread belirtmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Eskiden Linux sistemleri yalnızca tek işlemciyi destekliyordu. Sonra 2.0 versiyonuyla birlikte çekirdek zamanla birden 
    fazla işlemciyi ya da çekirdeği (core) destekler hale geldi. İşte bir thread çalışırken task_struct * türünden 
    current isimli global değişken o anda çalışmakta olan thread'e ilişkin task_struct nesnesini gösterecek biçimde 
    ayarlanmaktadır. Yani bir çekirdek kodunda current göstericisini görürseniz bu current göstericisi o anda çalışmakta 
    olan thread'e ilişkin task_struct nesnesini gösteriyor durumda olacaktır. Tabii bu current göstericisinin o anda 
    çalışan thread'e ilişkin task_struct nesnesini göstermesini "bağlamsal geçişi (context switch)" gerçekleştiren çizelgeleyici 
    alt sistem sağlamaktadır. Eskiden Linux yalnızca tek işlemciyi desteklerken current global değişkeni de toplamda bir 
    taneydi. Daha sonra Linux birden fazla işlemciyi ya da çekirdeği desteklemeye başlayınca current global değişkeni de 
    biçim değiştirdi.

    Bir süredir birden fazla işlemciyi ya da çekirdeği destekleyen Linux çekirdeklerinde artık current değişkeni bir 
    gösterici değil fonksiyon belirtmektedir. (Eskiden doğrudan task_struct türünden bir gösterici belirtiyordu.) Mevcut 
    çekirdeklerde current aşağıdaki gibi bir makro biçiminde tanımlanmıştır:

    #define current get_current()

    Görüldüğü gibi artık biz current değişkenini kullandığımızda aslında get_current() fonksiyonunu çağırıp bu fonksiyonun 
    geri dönüş değerini kullanmış olmaktayız. Peki bu fonksiyon o anda işlemcide ya da çekirdekte çalışmakta olan 
    thread'in task_struct nesnesinin adresini nasıl bulup geri döndürmektedir? İşte bunun için Linux çekirdeklerinde 
    zaman içerisinde platforma da (işlemciye de) bağlı olacak biçimde çeşitli teknikler kullanılmıştır. Bu konu thread'in 
    "çekirdek stack alanı (kernel stack)" ve "işlemciye özgü global alanlar" konusuyla ilgilidir. Biz bu konuya başka 
    bir bölümde değineceğiz. Ancak burada pratik bir açıklama yapmak gerekirse şunları söyleyebiliriz: O anda thread'i 
    çalıştırmakta olan işlemcinin ya da çekirdeğin bir yazmacı (register) özel bir alanı göstermektedir (tabii bu 
    gösterme bağlamsal geçiş sırasında ayarlanmaktadır), çalışmakta olan thread'in task_struct nesnesinin adresi de 
    buradan hareketle elde edilmektedir.

    Biz kursumuzda current için "current göstericisi" ya da "current makrosu" terimlerini kullanacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi UNIX/Linux sistemlerinde kullanıcı modunda yeni bir proses fork POSIX fonksiyonu ile, yeni bir thread 
    de pthread_create fonksiyonu ile yaratılmaktadır. Yukarıda da belirttiğimiz gibi aslında bu iki çağrı da belirli bir 
    noktadan sonra çekirdek içerisindeki aynı fonksiyonları çağırmaktadır. Kullanıcı modundaki fork fonksiyonunun çağrı 
    mekanizması şöyledir:

    fork (kullanıcı modu) ---> sys_fork (çekirdek modu) ---> kernel_clone ---> ...

    pthread_create fonksiyonunun çağrı mekanizması da tipik olarak şöyledir (ancak kütüphaneden kütüphaneye ön aşamalar 
    değişebilir):

    pthread_create (kullanıcı modu) ---> sys_clone (çekirdek modu) ---> kernel_clone ---> ...

    O halde aslında çekirdek gözüyle bakıldığında bir thread başka bir thread tarafından yaratılmaktadır. Yani bu 
    yaratımda iki thread söz konusudur: Yaratan thread ve yaratılan thread. Yaratan thread'e ilişkin zaten task_struct 
    nesnesi mevcuttur. O halde yaratılan thread için de bir task_struct nesnesi oluşturulup bir biçimde bu yapılar 
    ilişkilendirilecektir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										13. Ders 30/08/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de task_struct nesneleri arasındaki ilişkilerin bağlı listelerle nasıl oluşturulduğu üzerinde duracağız. 
    Çekirdeğin bir prosesin thread'lerine sonra da alt proseslerine nasıl eriştiğini açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir prosesin thread'lerine erişim için eskiden task_struct içerisindeki thread_group isimli döngüsel bağlı 
    liste bağı kullanılıyordu:

    struct task_struct {
        /* ... */

        struct list_head thread_group;

        /* ... */
    };

    Bu thread_group listesi herhangi bir thread'ten başlanarak dolaşılırsa liste döngüsel olduğu için prosesin tüm thread'leri 
    elde edilebiliyordu. Yani elimizde bir prosesin herhangi bir thread'inin task_struct nesnesi varsa biz o thread'in 
    ilişkin olduğu prosesin tüm thread'lerinin task_struct nesnelerini bu bağlı listeyi dolaşarak elde edebiliyorduk. Ancak 
    4.2 çekirdeği (2015) ile birlikte bu veri yapısında bir değişiklik yapılmıştır. Yeni sistemde prosesin thread'lerine 
    ilişkin bağlı listenin kök düğümü prosesin sinyal bilgilerinin bulunduğu yerde saklanmaktadır. Dolayısıyla artık thread 
    dolaşımına bu kök düğümden başlanmalıdır. Prosesin sinyal bilgileri task_struct içerisindeki signal göstericisinin 
    gösterdiği yerdeki signal_struct yapısı içerisinde tutulmaktadır. İşte signal_struct yapısının thread_head elemanı da 
    prosesin thread'lerine ilişkin bu kök düğümü belirtmektedir. Prosesin thread'lerinin bağlı listesi için ise task_struct 
    içerisindeki thread_node elemanı kullanılmaktadır. Thread listesine ilişkin ilgili elemanları şöyle betimleyebiliriz:

    struct signal_struct {
        /* ... */

        struct list_head thread_head;

        /* ... */
    };

    struct task_struct {
        /* ... */

        struct signal_struct *signal;
        struct list_head thread_node;

        /* ... */
    };

    signal_struct nesnesi içerisindeki thread_head kök düğümü prosesin ilk thread'ine ilişkin task_struct nesnesindeki 
    thread_node düğümünü göstermektedir. Prosesin thread'lerine ilişkin task_struct nesneleri de bu thread_node düğümüyle 
    birbirine bağlanmıştır.

    thread_node bağlı listesi çekirdeğe ilk eklendiğinde bir süre eski thread_group listesi de muhafaza edilmişti. Yani 
    her iki sistem birlikte kullanılabiliyordu. Sonra tamamen eski thread_group listesi task_struct içerisinden kaldırıldı. 
    Artık yeni çekirdeklerde prosesin thread'lerinin task_struct nesnelerini elde etmek için prosesin signal_struct 
    nesnesindeki thread_head kök düğümünden başlanarak bağlı listenin thread_node düğümlerinin dolaşılması gerekmektedir.

    thread_head <---> thread_node <---> thread_node <---> thread_node <---> thread_node <---> thread_head

    signal_struct içerisindeki list_head kök düğümünün hemen önündeki düğümün ana thread'e ilişkin olacağı varsayımında 
    bulunmayınız. Ancak çekirdek kodları incelendiğinde mevcut çekirdeklerde ilk düğümün ana thread'e ilişkin olduğu 
    görülmektedir. Çekirdek kodlarının bazı yerlerinde bu varsayıma dayanılarak da birtakım işlemler yapılmıştır.

    Aslında Linux çekirdeklerinde prosesin tüm thread'lerine ilişkin task_struct içerisinde bulunan signal göstericisi 
    aynı signal_struct nesnesini göstermektedir. Yani thread_head kök düğümüne biz prosesin ana thread'inden erişmek 
    zorunda değiliz. Proesin herhangi bir thread'ine ilişkin task_struct nesnesindeki signal göstericisi yoluyla bu 
    list_head kök düğümüne erişebiliriz.

    Çekirdek içerisinde bir task_struct yapısının prosesin ana thread'ine ilişkin olup olmadığını belirleyen 
    thread_group_leader isimli bir makro da vardır:

    #include <linux/sched/signal.h>

    static inline bool thread_group_leader(struct task_struct *p)
    {
        return p->exit_signal >= 0;
    }

    Tabii aslında bir task_struct nesnesinin ana thread'e ilişkin olup olmadığı p == p->group_leader ya da 
    p->pid == p->tgid işlemiyle anlaşılabilir. Ancak özel bir bilgi olarak task_struct içerisindeki exit_signal elemanı 
    da bu bilgiyi verebilmektedir. Bu eleman eğer thread ana thread değilse -1 değerinde, ana threda ise >= 0 değerinde 
    olmaktadır.

    Linux çekirdeğinde terminoloji bağlamında "prosesin ana thread'i" yerine "thread grup lideri (thread group leader)" 
    terimi tercih edilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi UNIX/linux sistemlerinde her prosesin sistem genelinde tek (unique) olan bir "proses id (pid)" değeri 
    vardır. Linux çekirdeği proses id değerini task_struct içerisindeki pid elemanında saklamaktadır:

    struct task_struct {
        /* ... */

        pid_t	pid;

        /* ... */
    };

    Linux'ta her thread'in ayrı bir task_struct nesnesi olduğuna göre ve pid değeri de task_struct içerisinde tutulduğuna 
    göre bu durumda her thread'in de ayrı bir pid değeri mi vardır? Bu konuya açıklık getirelim. POSIX standartlarına göre 
    thread'lerin pid değeri olmaz, pid değerleri proseslere özgüdür. getpid POSIX fonksiyonu prosesin hangi thread akışı 
    içerisinde çağrılırsa çağrılsın prosese ilişkin pid değerini vermektedir. İşte Linux çekirdeği bu uyumu korumak için 
    şöyle bir yöntem izlemiştir: Prosesin pid değeri aslında prosesin ana thread'ine ilişkin task_struct nesnesi içerisindeki 
    pid değeridir. Evet Linux'ta her thread'in task_struct nesnesinde ayrı bir pid değeri vardır fakat prosesin pid değeri 
    denildiğinde prosesin ana thread'inin pid değeri anlaşılmaktadır. Ana thread'in pid değeri aynı zamanda task_struct 
    içerisindeki tgid isimli bir eleman da tutulmaktadır. task_struct nesnesi hangi thread'e ilişkin olursa olsun tgid elemanı 
    her zaman prosesin ana thread'inin pid değerini tutmaktadır:

    struct task_struct {
        /* ... */

        pid_t   pid;        /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;       /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */

        /* ... */
    };

    Bu durumda biz prosesin thread'lerine ilişkin task_struct nesnelerini dolaştığımızda bu task_struct nesnelerinin 
    birinde pid == tgid olacaktır. İşte bu thread prosesin ana thread'idir. O halde aslında bazı yarıntıları da göz ardı 
    edersek getpid POSIX fonksiyonun çağırdığı sys_getpid sistem fonksiyonu prosesin proses id değerini doğrudan current 
    göstericisinin gösterdiği task_struct nesnesinin içerisindeki tgid değerinden alarak vermektedir:

    getpid (kullanıcı modu) ---> sys_getpid (öekirdek modu) ---> current tarafından gösterilen task_struct nesnesindeki 
    tgid değeri

    Şimdi aklınıza "neden her thread'te ayrıca ana thread'in pid değeri tgid ismiyle tutuluyor?" sorusu gelebilir. Bunun 
    iki nedeni vardır: Birincisi prosesin ana thread'i sonlanabilir, bu durumda ana thread'e ilişkin task_struct nesnesine 
    erişilemeyebilir. İkincisi ise bu yolla prosesin id değerine bu yolla daha hızlı erişimin sağlanabilmesidir.

    Anımsanacağı gibi POSIX thread kütüphanesinde (pthread kütüphanesini kastediyoruz) thread'lerin id değerleri sistem 
    genelinde tek değil proses genelinde tektir. Yani POSIX'teki pthread_t ile temsil edilen thread id değerinin Linux 
    çekirdeğindeki task_struct içerisinde bulunan pid değeri ile bir ilgisi yoktur. Peki Linux'ta "her thread'in ayrı 
    bir pid değerinin olmasının" Linux için bir anlamı var mıdır? İşte Linux'ta thread'ler sanki birer proses gibi ele 
    alındığı için bu durum thread'lerin bazı ek yeteneklere sahip olmasını sağlamıştır. Örneğin bu sayede biz POSIX 
    standartlarında olmayan bazı işlemleri Linux'ta yapabilmekteyiz. Yani Linux çekirdeğinde aslında pid isteyen 
    fonksiyonlara biz thread'teki pid değerini geçirerek thread'e özgü işlemlerin yapılmasını da sağlayabilmekteyiz. 
    Kullanıcı modundan thread'e ilişkin pid değeri Linux'a özgü gettid fonksiyonu ile elde edilebilmektedir:

    #define _GNU_SOURCE
    #include <unistd.h>

    pid_t gettid(void);

    gettid fonksiyonun bir POSIX fonksiyonu olamdığına dikkat ediniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında Linux çekirdeği thread'lere ilişkin task_struct nesnelerinde yalnızca prosesin ana thread'ine ilişkin pid 
    değerini değil aynı zamanda ana thread'in task_struct nesnesinin adresini de tutmaktadır. Ana thread'in task_struct 
    nesnesinin adresi task_struct yapısının group_leader elemanında tutulmaktadır. Yani herhangi bir thread akışı 
    içerisinde o thread'in task_struct nesnesindeki group_leader göstericisi zaten prosesin ana thread'inin task_struct 
    nesnesinin adresini tutmaktadır:

     struct task_struct {
        /* ... */

        pid_t   pid;                        /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                       /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;   /* ana thread'e ilişkin task_struct nesnesinin adresi tutuluyor */

        /* ... */
    };

    Peki prosesin ana thread'i sonlandığında group_leader göstericisinin ve signal_struct yapısı içerisindeki list_head 
    düğümünün durumu ne olacaktır? İşte Linux çekirdeği prosesin ana thread'i sonlandığında istisna olarak ana thread'e 
    ilişkin task_struct nesnesini yok etmemektedir (başka bir deyişle "hortlak (zombie)" olarak tutmaktadır). Yani bu 
    group_leader göstericisi her zaman geçerli bir task_struct nesnesini gösteriyor durumda olmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi aklınıza "mademki Linux'ta her thread'in task_struct nesnesinde zaten prosese ilişkin bilgiler tutuluyor bu 
    durumda prosesin ana thread'ine erişmenin ne gereği var?" sorusu gelebilir. Evet gerçekten de aslında bir prosesin 
    bilgilerinin çok büyük kısmı zaten thread'lere ilişkin task_struct nesnelerinde da bulunmaktadır. Ancak yine de bazı 
    bilgilere yalnızca ana thread'ten hareketle elde edilmektedir. Fakat çekirdeğin ana thread'in task_struct nesnesine 
    erişim gerekliliği çok az düzeydedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de Linux çekirdeğinde posesler arasndaki altlık-üstlük ilişkisinin nasıl sağlandığı üzerinde duralım. Bilindiği 
    gibi UNIX/Linux sistemlerinde her proses başka bir prosesin thread'i tarafından fork işlemi ile yaratılmaktadır. Bu 
    durumda prosesi yaratan thread'in ilişkin olduğu prosese üst proses (parent process) yeni yaratılan prosese de "alt 
    process (child process)" denilmektedir. UNIX/Linux sistemlerinde prosesler arasındaki altlık-üstlük ilişkisi çok 
    önemlidir. Örneğin wait fonksiyonları alt prosesler sonlanana kadar üst prosesi bekletip alt prosesin exit kodunu 
    almaktadır. Öte yandan Linux çekirdeğinde yalnızca prosseslerin değil her thread'in bir task_struct nesnesi vardır. 
    Peki altlık-üstlük ilişkisi task_struct nesnelerinde nasıl sağlanmaktadır?
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Her thread'in task_struct nesnesi içerisindeki real_parent ve parent isimli göstericiler o thread'i yaratan thread'in 
    task_struct nesnesinin adresini tutmaktadır. fork işlemi sırasında alt prosesin ana thread'i için yeni bir task_struct 
    nesnesi yaratılmakta ve bu nesnenin real_parent ve parent elemanları fork işlemini yapan thread'in task_struct nesnesini 
    gösterir duruma getirilmektedir. Aynı durum tamamen pthread_create fonksiyonu tarafından yaratılan thread'ler için de 
    geçerlidir. Dolayısıyla fork işlemi ile artık yeni yaratılan alt prosesin ana thread'ine ilişkin tast_struct nesnesinin 
    real_parent ve parent elemanları üst prosese ilişkin thread'in task_struct nesnesini gösteriyor durumda olur.

    struct task_struct {
        /* ... */

        pid_t   pid;                                /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                               /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;           /* ana thread'e ilişkin task_struct nesnesini gösteriyor */

        struct task_struct __rcu	*real_parent;   /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */
        struct task_struct __rcu	*parent;        /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */

        /* ... */
    };

    Burada real_parent üst prosese ilişkin thread'in task_struct nesnesinin adresini tutmaktadır. Normalde real_parent 
    elemanı ile parent elemanı aynı task_struct nesnesini gösterir. Ancak seyrek durumlarda (örneğin debug işlemlerinde 
    ve ptracce işlemlerinde) geçici olarak parent başka bir task_struct nesnesini de (reparenting işlemi) gösteriyor durumda 
    olabilmektedir.

    Burada bir noktayı yeniden vurgulamak istiyoruz: fork işlemini yapan thread hangi thread olursa olsun yaratılan alt 
    prosesin ana thread'ine ilişkin task_struct nesnesindeki real_parent ve parent göstericileri üst prosesteki fork yapan 
    thread'in task_struct nesnesini göstermektedir. Anımsanacağı gibi getppid POSIX fonksiyonu üst prosesin pid değerini 
    vermektedir. İşte bu fonksiyonun çağırdığı sys_getppid sistem fonksiyonu real_parent elemanının gösterdiği task_struct 
    nesnesi içerisindeki tgid değerini geri döndürmektedir.

    Bir thread başka bir thread'i yarattığında da benzer biçimde yaratılan thread'in task_struct nesnesinin real_parent 
    ve parent göstericileri onu yaratan thread'in task_struct nesnesini göstermektedir. Thread'ler böyle birbirini 
    yarattığında bunlara ilişkin task_struct nesnelerinin tgid ve group_leader elemanlarının her zaman ana thread'e ilişkin 
    olarak kalacağına dikkat ediniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de bir prosesin alt proseslerinin nasıl bağlı listelerde tutulduğu üzerinde duralım.

    Örneğin bir proses üç kez fork yapmış olsun. Bu durumda bu prosesin üç tane alt prosesi olacaktır. Üst prosesleri aynı 
    olan proseslere "kardeş prosesler (sibling processes) de denilmektedir. İşte bir prosesin alt prosesleri children isimli 
    kök düğümle erişilen sibling bağları yoluyla bağlı listede tutulmaktadır:

    struct task_struct {
        /* ... */

        pid_t   pid;                                /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                               /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;           /* ana thread'e ilişkin task_struct nesnesini gösteriyor */

        struct task_struct __rcu	*real_parent;   /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */
        struct task_struct __rcu	*parent;        /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */

        struct list_head children;                  /* alt proses listesinin kök düğümü */
        struct list_head sibling;                   /* alt proseslerin bağlı liste bağı */

        /* ... */
    };

    Burada children bir kök düğüm biçimindedir. children aslında alt proses listesindeki ilk task_struct nesnesinin 
    sibling elemanını göstermektedir. Sonra her kardeş task_struct nesnesi bir sonraki kardeş task_struct nesnesini 
    gösterir. O halde bir prosesin alt prosesleri bu sibling düğümleri dolaşılarak elde edilmektedir. Proseslerin her 
    thread'inin ayrı bir task_struct ile temsil edildiğini belirtmiştik. Peki bu durumda sibling bağları hangi task_struct 
    nesnelerini göstermektedir? İşte sibling bağları her zaman prosesin ana thread'ine ilişkin (grup liderine ilişkin) 
    task_struct nesnelerini göstermektedir. Örneğin bir prosesin bir thread'i (buna tp1 thread'i diyelim) fork yapsın 
    sonra diğer bir thread'i de (buna da tp2 diyelim) fork yapsın. İlk fork işleminden elde edilen ana thread'e tc1 
    ikinci fork işleminden elde edilen ana thread'e de tc2 diyelim. İşte burada sibling bağlı listesinde tc1 ve tc2 
    thread'lerine ilişkin task_struct nesneleri tutulmaktadır. tc1'in parent ve real_parent göstericileri tp1 thread'inin 
    task_struct nesnesini, tc2'nin parent ve real_parent götericileri ise tp2'nin task_struct nesnesini göstermektedir. 
    (Tabii aslında bu göstericiler doğrudan task_struct nesnesini göstermemektedir, asıl nesnenin adresi container_of 
    makrosuyla elde edilmektedir. Biz pratik bir anlatım için durumu böyle ifade ediyoruz.)

    Bir thread akışında yeni bir thread yaratıldığı zaman yaratılan thread bu children/sibling listesinde bulunmaz. 
    prosesin thread'leri yalnızca list_head/list_node listesinde tutulmaktadır. children/sibling listesinde her zaman 
    yalnızca alt proseslere ilişkin ana thread'lerin task_struct nesneleri tutulmaktadır.

    Peki bir prosesin tüm thread'lerine ilişkin task_struct nesnelerindeki children elemanları aynı listeyi mi 
    göstermektedir? Yani ben alt proseslerin ana thread'lerine ilişkin task_struct nesnelerini dolaşmak istesem bunu 
    kendi ana thread'imin children kök düğümünden itibaren mi yapmak zorundayım? İşte Linux'ta alt proses listesi 
    ana thread'in children kök düğümünden hareketle dolaşılmak zorundadır. Alt prosesler yaratıldığında yalnızca ana 
    thread'in children kök düğümü güncellenmektedir. Yani prosesin diğer thread'lerinin children düğümleri bu işlemlerde 
    kullanılmamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek ayrıca proseslerin ana thread'lerine ilişkin task_struct nesnelerini de task_struct içerisindeki tasks 
    isimli bir düğüm ile birbirine bağlamaktadır. Yani eğer sistemdeki tüm proseslerin ana thread'lerine ilişkin task_struct 
    nesnelerinin dolaşılması için bu tasks düğümü kullanılabilir:

        struct task_struct {
        /* ... */

        pid_t   pid;                                /* o thread'e ilişkin pid değeri, POSIX'te böyle bir kavram yok */
        pid_t   tgid;                               /* ana thread'e ilişkin pid değeri, getpid bu değeri veriyor */
        struct task_struct *group_leader;           /* ana thread'e ilişkin task_struct nesnesini gösteriyor */

        struct task_struct __rcu	*real_parent;   /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */
        struct task_struct __rcu	*parent;        /* fork yapan üst prosesteki thread'in task_struct nesnesini gösteriyor */

        struct list_head children;                  /* alt proses listesinin kök düğümü */
        struct list_head sibling;                   /* alt proseslerin tutulduğu liste bağı */
        struct struct list_head	tasks;              /* ana thread'lere ilişkin task_struct nesnelerinin tutulduğu liste bağı */

        /* ... */
    };

    Peki bu tasks düğümlerine ilişkin bağlı listenin kök düğümü nerededir? İşte Linux'ta boot işlemi sırasında çekirdek 
    yüklenip birtakım ilk işlemler yapıldıktan sonra ilk oluşturulan prosese init_task denilmektedir. Bu init_task 
    prosesine ilişkin task_struct nesnesi statik bir biçimde yeni çekirdeklerde "init/init_task.c" dosyasında bulunmaktadır:

    struct task_struct init_task __aligned(L1_CACHE_BYTES) = {
        /* ... */

        .ptraced	= LIST_HEAD_INIT(init_task.ptraced),
        .ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
        .real_parent	= &init_task,
        .parent		= &init_task,
        .children	= LIST_HEAD_INIT(init_task.children),
        .sibling	= LIST_HEAD_INIT(init_task.sibling),
        .group_leader	= &init_task,
        RCU_POINTER_INITIALIZER(real_cred, &init_cred),
        RCU_POINTER_INITIALIZER(cred, &init_cred),
        .comm		= INIT_TASK_COMM,
        .thread		= INIT_THREAD,
        .fs		= &init_fs,
        .files		= &init_files,
    #ifdef CONFIG_IO_URING
        .io_uring	= NULL,
    #endif
        .signal		= &init_signals,
        .sighand	= &init_sighand,

        /* ... */
    };

    init_task prosesinin pid değeri 0'dır. Anımsanacağı gibi pid için 0 değeri POSIX sistemlerinde geçerli bir değer 
    değildir. Geleneksel olarak UNIX/Linux sistemlerinde sistem boot edildiğinde yaratılan bu tür proseslere "swapper" 
    ya da "pager" da deniliyordu. init_task prosesi init prosesini yarattıktan sonra işlevini sonlandırarak durdurulmaktadır. 
    Ancak bu prosese ilişkin task_struct nesnesi gerçek anlamda hiçbir zaman yok edilmemektedir. İşte init_task prosesinin 
    task_struct nesnesi proseslerin ana thread'lerine ilişkin task_struct nesnelerini dolaşmak için bir kök düğüm olarak 
    kullanılmaktadır. O halde tasks düğümlerinin kök düğümü init_task.tasks elemanıdır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeklerinde 2.6'lı versiyonlara kadar proses grubundaki prosesler ve oturuma ilişkin prosesler bağlı 
    listelerde tutulmuyordu. Sistem bir id'ye ilişkin proses grubuna ve oturuma ilişkin task_struct nesnelerini sistemdeki 
    tüm task_struct nesnelerini dolaşarak buluyordu. Ancak 2.6'lı çekirdeklerle birlikte artık bu arama işlemi için de 
    bağlı liste zincirleri kullanılmaya başlanmıştır. Bu işlemin nasıl yürütüldüğünü izleyen paragraflarda açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi yukarıda açıkladığımız konuyu madde madde özetleyelim:

    - Her thread'in ayrı bir pid değeri vardır, ancak POSIX'teki getpid fonksiyonu ana thread'in (thread grup liderinin) 
    pid değerini vermektedir.

    - Bir prosesin tüm thread'lerini dolaşmak için signal göstericisinin gösterdiği signal_struct yapısı içerisindeki 
    thread_head bağlı listesi thread_node düğümleriyle dolaşılır.

    - task_struct içerisindeki real_parent ve parent göstericileri o prosesi ya da thread'i yaratan thread'in task_struct
    nesnesini göstermektedir.

    - task_struct içerisindeki tgid ana thread'in pid değerini, thread_group göstericisi ise ana thread'in task_struct 
    nesnesinin adresini tutmaktadır. Ana thread sonlansa bile onun task_struct nesnesi proses sonlanana kadar muhafaza 
    edilmektedir.

    - Bir prosesin bütün alt proseslerine ilişkin ana thread'lerinin task_struct nesneleri children/sibling bağlı 
    listesinde tutulmaktadır.

    - Prosesin alt proseslerini elde etmek için prosesin ana thread'inin children kök düğümünden yola çıkmak gerekir.

    - Çekirdek yaratılmış olan tüm proseslerin ana thread'lerine ilişkin task_struct nesnelerini ayrıca task_struct 
    içerisindeki tasks düğümüyle birbirine bağlamaktadır. Bu düğümüm kökü de init_task.tasks elemanıdır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										14. Ders 31/08/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda konuyla ilgili bazı işlemlerin bu bağlı listeler kullanılarak nasıl gerçekleştirileceğine ilişkin tipik
    soruları ve bunların yanıtlarını veriyoruz:

    Soru: Bir prosesin tüm thread'leri nasıl elde edilebilir?
    Yanıt: Bir prosesin tüm threadleri prosesin signal yapısı içerisinde bulunan thread_head elemanı kök yapılarak 
    thread_node düğümlerinin dolaşılmasıyla elde edilebilir.

    Soru: Bir prosesin bütün alt prosesleri nasıl elde edilebilir?
    Yanıt: Prosesin ana thread'indeki children düğümü kök yapılarak sibling düğümlerinin dolaşılmasıyla prosesin tüm alt 
    proesleri elde edilebilir.

    Soru: Bir task_struct nesnesinin prosesin ana thread'ine ilişkin olduğu nasıl anlaşılır?
    Yanıt: Eğer task_struct nesnesinde pid == tgid ise bu task_struct ilgili prosesin ana thread'ine ilişkin task_struct 
    nesnesidir. Zaten prosesin tüm thread'lerine ilişkin task_struct nesnelerinde group_leader göstericisi ana thread'e 
    ilişkin bu task_struct nesnesini göstermektedir. O anda çalışmakta olan thread eğer prosesin ana thread'i ise 
    current == group_leader koşulunun sağlanacağına da dikkat ediniz.

    Soru: Sistemdeki tüm proseslerin ana thread'lerine ilişkin task_struct nesneleri nasıl dolaşılır?
    Yanıt: init_task prosesindeki tasks düğümü kök düğüm yapılıp tasks düğümleri dolaşılırsa tüm proseslerin ana 
    thread'lerine ilişkin task_struct nesneleri elde edilebilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de yukarıda gördüğümüz bağlı listelerin nasıl dolaşıldığına yönelik açıklamalar eşliğinde küçük örnekler 
    yapacağız. Ancak örnekleri yapmadan önce pratik bir ortam oluşturmamız gerekiyor. Bu biçimdeki küçük testler için 
    çekirdeği yeniden derlememize gerek yoktur. Basit bir karakter aygıt sürücüsü yazarak da bu işlemleri yapabiliriz. 
    Aygıt sürücülerinin yazımı kursumuzun konusu içerisinde değildir. Bu konu "UNIX/Linux Sistem Programlama" ve "Gömülü 
    Linux Sistemleri - Geliştirme ve Uygulama" kursunda ele alınmaktadır. Ancak biz burada bir karakter aygıt sürücüsünün 
    nasıl derlenip yükleneceği ve boşaltılacağı konusunda hedefe yönelik açıklamalar yapmakla yetineceğiz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Genellikle bir Linux sistemini yüklediğimizde zaten çekirdek modüllerini ve aygıt sürücüleri oluşturabilmek için 
    gereken başlık dosyaları ve diğer gerekli öğeler "/usr/src" dizini içerisindeki "linux-headers-$(uname -r)" dizininde 
    yüklü biçimde bulunmaktadır. Ancak bunlar yüklü değilse Debian tabanlı sistemlerde bunları şöyle yükleyebilirsiniz:

    $ sudo apt-get install linux-headers-$(uname -r)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Karakter aygıt sürücüleri bir ya da birden fazla kaynak ".c" dosyası oluşturularak yazılabilmektedir. Tabii bunların 
    yanı sıra programcı "*.h" uzantılı bazı başlık dosyaları da oluşturabilmektedir. Aygıt sürücülerin derlenmesi oldukça 
    karmaşık bir süreçle gerçekleşmektedir. Bu nedenle derleme işlemi için hazır "make dosyaları" bulundurulmuştur. Programcı 
    kendisi bir "Makefile" dosyası oluşturup asıl make dosyalarını buradan devreye sokar. Aygıt sürücüler için en basit 
    "Makefile" dosyası aşağıdaki gibi oluşturulabilir:

    obj-m += ${file}.o

    all:
        make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
    clean:
        make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

    Bu "Makefile" dışarıdan "file" isimli bir argüman almaktadır. Aygıt sürücünün derlenmesi şöyle yapılır:

    make file=<aygıt_sürücü_dosyasının_ismi>

    Örneğin:

    make file=mydriver

    Kaynak dosya için uzantı belirtilmediğine dikkat ediniz.

    Aygıt sürücüler derlendikten sonra ".ko" uzantılı bir dosya elde edilecektir. Bu dosya çekirdeğe sisteme yüklenmelidir. 
    Aygıt sürücü dosyalarını (".ko" dosyalarını) çekirdeğe yüklemek için "insmod" ya da "depmod" komutları kullanılmaktadır. 
    "Ancak "depmod" komutu birtakım bağımlılıklara da bakmaktadır. Biz kursumuzda aygıt sürücüleri "insmod" komutuyla 
    yükleyeceğiz. Tabii aygıt sürücüleri yükleyebilmek için "root" önceliğine sahip olmak gerekir. Modern UNIX/Linux 
    sistemlerinde bu işlem "sudo" komutuyla yapılmaktadır. Örneğin:

    $ sudo insmod mydriver.ko

    "insmod" ile yüklenmiş olan aygıt sürücü çekirdekten "rmmod" komutuyla çıkartılır. Komut şöyle kullanılır:

    $ sudo rmmod mydriver.ko

    Burada ".ko" uzantısı belirtilmeyebilir.

    Aygıt sürücünün yüklenip yüklenmediğini anlayabilmek için "/proc/devices" dosyasına başvurabilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aygıt sürücüler çekirdek modunda çekirdeğin bir parçasıymış gibi çalışan modüllerdir. Nasıl masaüstü bilgisayarların 
    genişleme yuvalarına kart takıldığında o kart donanımın bir parçası haline geliyorsa aygıt sürücüler de benzer biçimde 
    adeta çekirdeğin bir parçası haline gelmektedir. Aygıt sürücüler içerisinde kullanıcı modundaki standart C fonksiyonları 
    ya da POSIX fonksiyonları kullanılamaz. Aygıt sürücüler yalnızca çekirdek içerisinde "export" edilmiş fonksiyonları 
    kullanabilirler. (Yani çekirdekteki her fonksiyonu aygıt sürücü kullanamaz, yalnızca export edilmiş olan başka bir 
    deyişle "kullanılmasına izin verilmiş olan" fonksiyonları kullanabilir.) Bir fonksiyon ya da nesne çekirdek kodlarında 
    EXPORT_SYMBOLS makrosu ile export edilmektedir. Örneğin:

    ...
    void clear_nlink(struct inode *inode)
    {
        if (inode->i_nlink) {
            inode->__i_nlink = 0;
            atomic_long_inc(&inode->i_sb->s_remove_count);
        }
    }
    EXPORT_SYMBOL(clear_nlink);
    ...
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında Linux sistemlerinde bu bağlamda birbirleriyle ilişkili iki kavram vardır: "Çekirdek modülleri (kernel modules)" 
    ve "aygıt sürücüler (device drivers)". Çekirdeğin içerisine yüklenebilen modüllere "çekirdek modülleri" denilmektedir. 
    Eğer bir çekirdek modülüne kullanıcı modundan bir dosya (buna "aygıt dosyası (device file)" denilmektedir) yoluyla 
    erişilip aygıt sürücüye işlemler yaptırılabiliyorsa bu tür çekirdek modüllerine "aygıt sürücü (device drivers)" de 
    denilmektedir. Yani her aygıt sürücü bir çekirdek modülü belirtir ancak her çekirdek modülü bir aygıt sürücü beirtmek 
    zorunda değildir. Sistemdeki yüklü olan çekirdek modülleri "/proc/modules" dosyası yoluyla, yüklü olan aygıt sürücüler 
    ise "/proc/devices" dosyası yoluyla görüntülenebilmektedir.

    Aşağıda iskelet bir çekirdek modülü görüyorsunuz:

    #include <linux/module.h>
    #include <linux/kernel.h>

    MODULE_LICENSE("GPL");

    static int helloworld_init(void);
    static void helloworld_exit(void);

    static int helloworld_init(void)
    {
        printk(KERN_INFO "Hello World...\n");

        return 0;
    }

    static void helloworld_exit(void)
    {
        printk(KERN_INFO "Goodbye World...\n");
    }

    module_init(helloworld_init);
    module_exit(helloworld_exit);

    Bir çekirdek modülü "insmod" komutuylaile yüklendiğinde onun module_init makrosuyla belirtilen fonksiyonu çağrılmaktadır. 
    Çekirdek modülü içerisinde ilk işlemler bu fonksiyonda yapılmaktadır. Çekirdek modülü "rmmod" ile sistemden çıkartılırken 
    de çekirdek modülündeki module_exit makrosunda belirtilen fonksiyonu çağrılmaktadır. Birtakım son işlemler (örneğin geri 
    bırakma işlemleri) bu fonksiyonda yapılmaktadır.

    Çekirdek modülleri içerisinde ekrana birşeyler yazamayız (mümkün olsa da uygun değildir). Eğer çekirdek modülleri 
    içerisinde birtakım mesajlar iletilmek isteniyorsa bu mesajlar bir "log" sistemine "yazdırılmaktadır. Bu log sistemine 
    "kernel ring buffer" da denilmektedir. Bu log sistemine yazdırılan yazılar "dmesg" komutuyla ya da "/var/log/syslog" 
    dosyası ile görüntülenebilmektedir. Çekirdek modülü içerisinde bu log sistemine mesajlar printk isimli çekirdek 
    fonksiyonuyla yazılmaktadır. Örneğin:

    printk(KERN_INFO "Goodbye World...\n");

    Burada KERN_INFO mesajın türünü belirtmektedir. KERN_INFO ile diğer argüman arasında virgül atomunun bulunmadığına 
    dikkat ediniz. KERN_INFO aslında bir bir sembolik sabittir ve bir string açımı yapmaktadır. (C'de yana yana iki 
    string'in birleştirildiğini anımsayınız.) KERN_INFO mesaj türü ile mesaj yazdırmanın daha basit yolu pr_info 
    makrosunu kullanmaktadır. Zaten bu makro da printk açılımı yapmaktadır. Örneğin:

    pr_info("Goodbye World...\n");

    printk fonksiyonun kullanımı büyük ölçüde printf fonksiyonuna benzemektedir. Yani bu fonksiyonla değişkenler 
    içerisindeki değerler de yazdırılabilmektedir.

    Çekirdek modüllerini ve aygıt sürücüleri yazarken build sistemi include dosyaları için aşağıdaki dizini kök dizin 
    kabul etmektedir:

    "/lib/modules/$(shell uname -r)/build/include"

    Burada build dizini aslında bir sembolik bağlantı belirtmektedir. Bu sembolik bağlantı başlık dosyalarının (ya da 
    çekirdek kodlarının bulunduğu) ana dizine referans etmektedir. Dolayısıyla include işlemlerinde kök dizin aslında 
    çekirdek kodlarındaki "include" dizini olmaktadır. Çekirdek modüllerinde ve aygıt sürücülerde bir başlık dosyasını 
    include ederken bu çekirdeğin kaynak kod ağacındaki "include" dizininden itibaren yol belirtilmesi gerekir. Örneğin:

    #include <linux/module.h>

    Burada aslında Linux kaynak kod ağacındaki include dizininin itibaren bir yol belirtilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi aygıt sürücülere bir dosya yoluyla kullanıcı modundan erişilebilmektedir. Aygıt 
    sürücülere erişmekte kullanılan bu özel dosyalara "aygıt dosyaları (device files)" denilmektedir. Aygıt dosyaları 
    bir disk dosyası değildir. Yalnızca bir dizin girişi belirtmektedir. Aygıt dosyası open POSIX fonksiyonuyla açıldığında 
    çekirdek diske değil yüklü olan aygıt sürücüye referans etmektedir. Kullanıcı modundaki programcı aygıt dosyası 
    üzerinde open, read, write, lseek, close gibi klasik dosya işlemlerini yaptığında aslında programın akışı çekirdek 
    moduna (kernel mode) geçerek aygıt sürücü içerisindeki ilgili fonksiyonlar çalıştırılmaktadır. Yani biz bu mekanizmayla 
    aslında normalde kullanıcı modunda yapamayacağımız birtakım işlemleri dolaylı bir biçimde yapabilir duruma gelmekteyiz.

    Linux sistemlerinde aygıt sürücüler (ve dolayısıyla aygıt dosyaları) iki kısma ayrılmaktadır:

    - Karakter Aygıt Sürücüleri (Character Device Drivers)
    - Blok Aygıt Sürücüleri (Block Device Drivers)

    En çok kullanılan aygıt sürücüler karakter aygıt sürücüleridir. Blok aygıt sürücüleri "hard disk, SSD, CDROM, ramdisk" 
    gibi blok blok transferlerin yapıldığı aygıtlar söz konusu olduğunda kullanılmaktadır. Karakter aygıt sürücülerine 
    ilişkin aygıt dosyaları UNIX/Linux sistemlerinde 'c' dosya türü ile , blok aygıt sürücülerine ilişkin aygıt dosyaları 
    ise 'b' dosya türü ile temsil edilmektedir. Örneğin:

    ...
    crw-rw----   1 root disk     21,   0 Ağu 31 10:49 sg0
    crw-rw----+  1 root cdrom    21,   1 Ağu 31 10:49 sg1
    drwxrwxrwt   2 root root          40 Ağu 31 12:30 shm
    crw-------   1 root root     10, 231 Ağu 31 10:49 snapshot
    drwxr-xr-x   3 root root         200 Ağu 31 10:49 snd
    brw-rw----+  1 root cdrom    11,   0 Ağu 31 10:52 sr0
    lrwxrwxrwx   1 root root          15 Ağu 31 10:49 stderr -> /proc/self/fd/2
    lrwxrwxrwx   1 root root          15 Ağu 31 10:49 stdin -> /proc/self/fd/0
    lrwxrwxrwx   1 root root          15 Ağu 31 10:49 stdout -> /proc/self/fd/1
    crw-rw-rw-   1 root tty       5,   0 Ağu 31 11:45 tty
    crw--w----   1 root tty       4,   0 Ağu 31 10:49 tty0
    crw--w----   1 root tty       4,   1 Ağu 31 10:49 tty1
    crw--w----   1 root tty       4,  10 Ağu 31 10:49 tty10
    ...

    UNIX/Linux sistemlerinde aygıt dosyaları genel olarak "/dev" dizininde bulunmaktadır. Ancak böyle bir zorunluluk 
    yoktur. Eskiden bu "/dev" dizini disk tabanlı bir dosya sistemi içerisinde bulunuyordu. Sonra zamanla bu dosya 
    sistemi ramdisk tabanlı hale getirildi. Linux sistemlerinde "udev" denilen bir servis (daemon) ile bu dizinde aygıt 
    dosyalarının bu dizinde yaratılması mümkün hale getirilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Her aygıt sürücünün "majör" ve "minör" numarası vardır. Bu majör ve minör numaralar çekirdeğin aygıt sürücüye erişebilmesi 
    için bir adres görevi görmektedir. Bir aygıt dosyası yaratılırken aygıt dosyasına da bu majör ve minör numara iliştirilmektedir. 
    (Normal (regular) dosyalarda böyle bir majör ve minör numara olmadığına dikkat ediniz.) Örneğin:

    crw--w----   1 root tty       4,   0 Ağu 31 10:49 tty0

    Buradaki karakter aygıt dosyasının majör numarası 4, minör numarası 0'dır. Majör numara aygıt sürücünün ana kodunu temsil 
    eder. Minör numara ise onun örneklerini (instances) temsil etmektedir. Örneğin bir seri port aygıt sürücüsünün tek 
    bir majör numarası vardır. Eğer sistemimizde 4 tane seri port varsa aynı majör numaraya sahip minör numaraları farklı 
    4 aygıt dosyasının bulunması gerekir. Aygıt sürücünün aslında toplamda bir tane kodunun bulunduğuna dikkat ediniz. Aygıt 
    sürücüleri yazanlar zaten birden fazla minör numarayı destekleyecek biçimde kodlarını yazmaktadır.

    Aygıt dosyaları komut satırında "mknod" isimli programla yaratılmaktadır. Tabii bu program da aslında mknod isimli 
    POSIX fonksiyonunu çağırarak yazılmıştır. mknod POSIX fonksiyonu da sys_mknod isimli sistem fonksiyonunu çağırmaktadır. 
    mknod programı ile aygıt dosyası şöyle yaratılmaktadır:

    sudo mknod [-m ya da --mode <erişim_hakları>] <dosya_ismi> <c ya da b> <majör_numara> <minör_numara>

    Örneğin:

    $ sudo mknod -m 666 mydriver c 250 0

    O halde aygıt sürücüyü yazanlar belli bir majör ve minör numara belirleyerek belirledikleri bu majör ve minör numaraya 
    dayalı olarak aygıt sürücülerini yazarlar ve aygıt sürücülerinin kullanıcı modundan kullanılabilmesini sağlamak için 
    de aynı majör ve minör numaraya ilişkin aygıt dosyalarını yaratırlar. (Tabii aygıt sürücüler birden fazla minör numarayı 
    da destekleyebilirler.) Ancak aynı majör ve minör numaraya ilişkin birden fazla aygıt sürücü yüklenememektedir. Bu 
    nedenle aygıt sürücüleri yazanlar "kullanılmayan bir majör numarayı" çekirdekten isteyip aygıt sürücülerine o majör 
    numarayı atayabilmektedir. Tabii bu durumda aygıt sürücünün majör numarası önceden bilinmediği için aygıt dosyası da 
    ancak aygıt sürücü yüklendikten sonra yaratılabilmektedir. İşte tipik olarak sistem programcısı bir "kabuk betiği 
    (shell script)" yazarak bu işlemi otomatize etmeye çalışmaktadır. Bu betik önce "insmod" ile aygıt sürücüyü yükler, 
    "/proc/devices" dosyasına başvurup onun majör numarasını elde eder ve aygıt dosyasını "mknod" komutuyla dinamik bir 
    biçimde bu majör numarayı kullanacak biçimde oluşturur. Biz de denemelerimizde böyle yapacağız. Bunun için aşağıdaki 
    gibi bir "load" betiği kullanacağız:

    #!/bin/bash

    module=$1
    mode=666

    /sbin/insmod ./${module}.ko ${@:2} || exit 1
    major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
    rm -f $module
    mknod -m $mode $module c $major 0

    Bu betik dosyasını oluşturduktan sonra dosyaya "x" hakkını vermeyi unutmayınız:

    $ chmod +x load

    Bu betik aşağıdaki gibi çalıştırılmalıdır:

    $ sudo ./load mydriver

    Bu betik hem aygıt sürücüyü yükleyecek hem de ilgili majör numarayı kullanarak minör 0 olacak biçimde karakter 
    aygıt dosyasını oluşturacaktır. Bu biçimde yüklenen aygıt sürücüyü sistemden çıkardıktan sonra aygıt dosyası da 
    silinmelidir. Bu işlemde bir kabuk betik dosyası ile yapılabilir. Aygıt sürücüyü "rmmod" ile çıkartıp ilgili aygıt 
    dosyasını silen "unload" isimli bir kabuk betiği aşağıdaki gibi yazılabilir:

    #!/bin/bash

    module=$1

    /sbin/rmmod ./${module}.ko || exit 1
    rm -f $module

    Tabii yine bu "unload" betiğine de "x" hakkının verilmesi gerekir:

    $ chmod +x unload

    Bu betik şöyle kullanılır:

    sudo ./unload mydriver
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki aygıt sürücülere neden gereksinim duyulmaktadır? İşte kullanıcı modunda sistemin çökmesine yol açabilecek 
    işlemler "işlemcilerin sağladığı koruma mekanizması" yoluyla engellenmektedir. Bu durumda kullanıcı modundaki programlar 
    aslında yapamayacakaları işlemleri aygıt sürücülere dolaylı olarak yaptırabilir hale gelmektedir. Örneğin ekrana 
    bir yazının yazdırılması aslında doğrudan kullanıcı modundaki programlar tarafından sağlanamamaktadır. Bunun için 
    işletim sistemlerinde "terminal aygıt sürücüsü" denilen aygıt sürücüler bulundurulmaktadır. C'deki stdin ve stdout 
    dosyaları aslında bu aygıt sürücüye ilişkin aygıt dosyalarını temsil etmektedir. Programcı bu stdout dosyasına birşeyler 
    yazdırmak istediğinde programın akışı çekirdek moduna geçer ve aygıt sürücü içerisindeki ilgili fonksiyon çalıştırılır, 
    bu fonksiyon koruma engeli ortadan kalktığı için ekrana yazıları basabilmektedir. Aygıt sürücüler tipik olarak "donanım 
    aygıtlarıyla kullanıcı modundaki programlar arasında arayüz" oluşturma görevini üstlenirler. Tabii zamanla "aygıt 
    sürücü" kavramı daha genelleştirilmiştir. Linux sistemlerinde değişik mekanizmayla çalışan aygıt sürücüler bulunmaktadır. 
    İşlemcilerin "koruma mekanizması (protection mechanism)" kursumuzda izleyen bölümlerde ele alınacaktır.

    Peki çekirdek üzerindeki manipülasyonların hepsi aygıt sürücü yazarak sağlanabilir mi? Hayır, aygıt sürücü çekirdekteki 
    olanakları kullanmaktadır ancak çekirdek kodlarında değişiklik yapamamaktadır. Dolayısıyla bazı çekirdek davranışları 
    basit bir biçimde aygıt sürücü yazarak gözlemlenebilir ve test edilebilir. Ancak aygıt sürücüler çekirdeğin davranışında 
    önemli değişikliklere neden olamamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda aygıt sürücülerin bir dosya gibi kullanıldığını belirtmiştik. Gerçekten de aygıt sürücü üzerinde dosya 
    işlemleri yapıldığında aslında aygıt sürücüyü yazan sistem programcısının belirlediği fonksiyonlar çağrılmaktadır. 
    Örneğin:

    open ---> aygıt sürücüdeki open fonksiyonu çağrılır
    close ---> aygıt sürücüdeki closed fonksiyonu çağrılır
    read ---> aygıt sürücüdeki read fonksiyonu çağrılır
    write ---> aygıt sürücüdeki write fonksiyonu çağrılır
    lseek ---> aygıt sürücüdeki lseek fonksiyonu çağrılır

    Ancak her türlü işlem bir dosya işlemi yoluyla sağlanamamaktadır. Klasik dosya işlemleriyle alakası olmayan aygıt 
    sürücüdeki spesifik bir kod ioctl denilen mekanizmayla da çağrılabilmektedir. Aygıt sürücüyü yazanlar aygıt dosyasını 
    açarlar, ioctl isimli POSIX fonksiyonu bir kod numarasıyla çağırırlar. (Tabii bu POSIX fonksiyonu da aslında arka 
    planda sys_ioctl isimli sistem fonksiyonunu çağırmaktadır.) Böylece aygıt sürücü içerisinde belirlenmiş bir fonksiyon 
    kullanıcı modundan çağrılabilmektedir. Tabii ioctl çağrısı yapıldığında çağrıyı yapan thread kullanıcı modundan 
    çekirdek moduna geçmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda iskelet bir karakter aygıt sürücüsü oluşturulmuştur. Bu karakter aygıt sürücüsü için bir ioctl kodu da 
    hazırlanmıştır. Aygıt sürücü kodu şöyledir:

    /* test-driver.h */

    #ifndef TEST_DRIVER_H_
    #define TEST_DRIVER_H_

    #include <linux/stddef.h>
    #include <linux/ioctl.h>

    #define TEST_DRIVER_MAGIC		't'
    #define IOC_TEST		        _IO(TEST_DRIVER_MAGIC, 0)

    #endif

    /* test-driver.c */

    #include <linux/module.h>
    #include <linux/kernel.h>
    #include <linux/fs.h>
    #include <linux/cdev.h>
    #include "test-driver.h"

    MODULE_LICENSE("GPL");
    MODULE_AUTHOR("Kaan Aslan");
    MODULE_DESCRIPTION("test-driver");

    static int test_driver_open(struct inode *inodep, struct file *filp);
    static int test_driver_release(struct inode *inodep, struct file *filp);
    static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
    static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
    static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

    static long ioctl_test(struct file *filp, unsigned long arg);

    static dev_t g_dev;
    static struct cdev g_cdev;
    static struct file_operations g_fops = {
        .owner = THIS_MODULE,
        .open = test_driver_open,
        .read = test_driver_read,
        .write = test_driver_write,
        .release = test_driver_release,
        .unlocked_ioctl = test_driver_ioctl
    };

    static int __init test_driver_init(void)
    {
        int result;

        printk(KERN_INFO "test-driver module initialization...\n");

        if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
            printk(KERN_INFO "cannot alloc char driver!...\n");
            return result;
        }
        cdev_init(&g_cdev, &g_fops);
        if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
            unregister_chrdev_region(g_dev, 1);
            printk(KERN_ERR "cannot add device!...\n");
            return result;
        }

        return 0;
    }

    static void __exit test_driver_exit(void)
    {
        cdev_del(&g_cdev);
        unregister_chrdev_region(g_dev, 1);

        printk(KERN_INFO "test-driver module exit...\n");
    }

    static int test_driver_open(struct inode *inodep, struct file *filp)
    {
        printk(KERN_INFO "test-driver opened...\n");

        return 0;
    }

    static int test_driver_release(struct inode *inodep, struct file *filp)
    {
        printk(KERN_INFO "test-driver closed...\n");

        return 0;
    }

    static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
    {
        printk(KERN_INFO "test-driver read...\n");

        return 0;
    }

    static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
    {
        printk(KERN_INFO "test-driver write...\n");

        return 0;
    }

    static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
    {
        long result;

        printk(KERN_INFO "test_driver_ioctl...\n");

        switch (cmd) {
            case IOC_TEST:
                result = ioctl_test(filp, arg);
                break;
            default:
                result = -ENOTTY;
        }

        return result;
    }

    long ioctl_test(struct file *filp, unsigned long arg)
    {
        printk(KERN_INFO "ioctl_test...\n");

        return 0;
    }

    module_init(test_driver_init);
    module_exit(test_driver_exit);

    Burada kullanıcı modundan aygıt dosyası açıldığında aygıt sürücünün test_driver_open fonksiyonu, kapatıldığında 
    test_driver_release fonksiyonu, aygıt dosyasından okuma yapılmak istendiğinde test_driver_read fonksiyonu ve aygıt 
    sürücüye yazma yapılmak istendiğinde ise test_driver_write fonksiyonu çağrılacaktır. Bu fonksiyonlar çağrıldığında 
    log mesajları yazdırılmaktadır. Ayrıca örneğimizde bir ioctl kodu da oluşturulmuştur. Bu ioctl kodu hem aygıt sürücüden 
    hem de kullanıcı modundan kullanıldığı için bir başlık dosyasında define edilmiştir:

    /* test-driver.h */

    #ifndef TEST_DRIVER_H_
    #define TEST_DRIVER_H_

    #include <linux/stddef.h>
    #include <linux/ioctl.h>

    #define TEST_DRIVER_MAGIC		't'
    #define IOC_TEST		        _IO(TEST_DRIVER_MAGIC, 0)

    #endif

    İşte kullanıcı modundan biz aşağıdaki getirilebilmekte ioctl işlemi yaptığımızda aygıt sürücümüzdeki ioctl_test 
    fonksiyonu çağrılacaktır. Bu aygıt sürücüyü kullanan örnek bir kullanıcı modu programı şöyle yazılabilir:

    /* app.c */

    #include <stdio.h>
    #include <stdlib.h>
    #include <fcntl.h>
    #include <unistd.h>
    #include <sys/ioctl.h>
    #include "test-driver.h"

    void exit_sys(const char *msg);

    int main(void)
    {
        int fd;

        if ((fd = open("test-driver", O_RDONLY)) == -1)
            exit_sys("open");

        if (ioctl(fd, IOC_TEST) == -1)
            exit_sys("ioctl");

        close(fd);

        return 0;
    }

    void exit_sys(const char *msg)
    {
        perror(msg);

        exit(EXIT_FAILURE);
    }

    Aygıt sürücümüzü şöyle derleyebilirsiniz:

    $ make file=test-driver

    Yükleme işlemini"load" betiği ile şöyle yapabilirsiniz:

    $ sudo ./load test-driver

    Bu işlemle dizin içerisinde "test-driver" isimli bir aygıt dosyası da oluşturulacaktır.

    İlgili C programını derleyerek aşağıdaki gibi çalıştırabilirsiniz:

    $ gcc -Wall -o app app.c
    $ ./app

    "app" programını çalıştırdıktan sonra "dmesg" komutunu kullandığımızda printk fonksiyonu ile yazdırılan log mesajlarının
    aşağıdaki gibi görülmesi gerekir:

    ...
    [  431.787375] test-driver module initialization...
    [  450.631940] test-driver opened...
    [  450.631953] test_driver_ioctl...
    [  450.631958] IOC_test_driver_TEST1
    [  450.631967] test-driver closed...

    Aygıt sürücüyü sistemden kaldırma işlemini de "unload" betiği ile şöyle yapabilirsiniz:

    $ sudo ./unload test-driver

    Bu işlemden sonra "dmesg" komutunu uyguladığımızda aşağıdakine benzer bir çıktı görüntüleceketir:

    ...
    204.225618] test-driver module initialization...
    [ 6220.150945] test-driver opened...
    [ 6220.150971] test_driver_ioctl...
    [ 6220.150985] IOC_test_driver_TEST1
    [ 6220.151003] test-driver closed...
    [ 6240.648236] test-driver module exit...
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST				_IO(TEST_DRIVER_MAGIC, 0)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test(struct file *filp, unsigned long arg);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
	.unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	long result;

	printk(KERN_INFO "test_driver_ioctl...\n");

	switch (cmd) {
		case IOC_TEST:
			result = ioctl_test(filp, arg);
			break;
		default:
			result = -ENOTTY;
	}

	return result;
}

long ioctl_test(struct file *filp, unsigned long arg)
{
	printk(KERN_INFO "IOC_test_driver_TEST1\n");

	return 0;
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

obj-m += ${file}.o

all:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include "test-driver.h"

void exit_sys(const char *msg);

int main(void)
{
	int fd;

	if ((fd = open("test-driver", O_RDONLY)) == -1)
		exit_sys("open");

	if (ioctl(fd, IOC_TEST) == -1)
		exit_sys("ioctl");

	close(fd);

	return 0;
}

void exit_sys(const char *msg)
{
	perror(msg);

	exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
										15. Ders 06/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de görmüş olduğumuz task_struct yapısına ilişkin bağlı listeler üzerinde gezinme işlemlerine örnekler verelim. 
    Daha önceden de belirttiğimiz gibi böyle testler için çekirdeği yeniden derlememize gerek yoktur. Basit bir çekirdek 
    modülü ya da aygıt sürücü yazarak da bu tür testleri yapabiliriz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz task_struct listelerini dolaşırken çekirdek bu listelere eleman eklerse ya da bu listelerden eleman silerse bizim 
    dolaşım yapan kodumuzda "tanımsız davranışlar (undefined behaviours)" oluşabilir. Bu da tüm sistemin çökmesine yol 
    açabilir. Bu tür durumlarda dolaşımın senkronizasyon nesneleri kullanılarak yapılması gerekmektedir. Biz kursumuzda 
    çekirdekte kullanılan senkronizasyon nesnelerini ileride ayrı başlık halinde ele alacağız. Birden fazla thread'in 
    beklemeden okuma işlemi yapabildiği ancak bir yazan thread varsa okuyan ve yazan thread'lerin bekletildiği senkronizasyon 
    nesnelerine "okuma yazma kilitleri (readers-writer locks)" denilmektedir. Eskiden Linux çekirdekleri task_struct 
    nesnelerini dolaşırken tasklist_lock isimli bir okuma yazma kilidini kullanılıyordu. Biz de dolaşımları aynı kilidi 
    kullanarak güvenli bir biçimde yapabiliyorduk. Ancak daha sonraları Linux çekirdeklerinde "kilitsiz (lock-free) 
    senkronizasyon nesneleri kullanılmaya başlandı. Artık belli bir süredir çekirdek task_struct listeleri üzerinde kilitsiz 
    bir biçimde "RCU (Read-Copy-Update)" mekanizmasıyla işlemler yapmaktadır. Dolayısıyla bizim de kendimizi çekirdeğin 
    kullandığı bu RCU mekanizmasına uydurarak dolaşım yapmamız gerekir. tasklist_lock kilidinin dışarıya export edilmesine 
    çekirdeğin 2.6.18 versiyonu ile son verilmiştir. Dolayısıyla bu kilit güncel çekirdeklerde artık çekirdek modülleri 
    ve aygıt sürücüler tarafından kullanılamamaktadır. (Tabii çekirdeğin içerisine gömeceğimiz kodlar ve aygıt sürücüler 
    bu kilidi kullanılabilirler. export işlemi yalnızca çekirdek modülleri ve aygıt sürücüleri etkilemektedir.) tasklist_lock 
    kilidi çekirdek tarafından eş zamanlı yazmaları senkronize etmek için halen kullanılmaktadır.

    RCU mekanizmasının ayrıntılarını ileride ayrı başlıkta ele alacağız. Ancak bu noktada RCU mekanizması ile task_struct 
    listelerinin dolaşılması için bazı temel bilgileri de vereceğiz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    RCU mekanizmasıyla bağlı listeleri dolaşırken işlemin başında rcu_read_lock fonksiyonun, işlemin sonunda da rcu_read_unlock 
    fonksiyonun çağrılması gerekmektedir. Bu fonksiyonların prototipleri şöyledir:

    #include <linux/rcupdate.h>

    void rcu_read_lock(void);
    void rcu_read_unlock(void);

    Bizim dolaşım kodunu bu iki çağrı arasına yerleştirmemiz gerekir:

    rcu_read_lock();
    ...
    rcu_read_unlock();

    RCU mekanizması altında bağlı listelerin bağlarına erişilirken rcu_dereference makrosu kullanılmalıdır. Bu makroya 
    parametre olarak erişeceğiniz bağlı listenin prev ya da next düğümlerini vermelisiniz. Bu durumda örneğin sistemdeki 
    bütün proseslerin ana thread'lerine ilişkin task_struct nesneleri aşağıdaki gibi güvenli bir biçimde dolaşılabilir:

    static void walk_processes(void)
    {
        struct task_struct *ts;
        struct list_head *lh;

        rcu_read_lock();

        lh = rcu_dereference(init_task.tasks.next);
        while (lh != &init_task.tasks) {
            ts = container_of(lh, struct task_struct, tasks);      /* ts = list_entry((lh, struct task_struct, tasks) */
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
            lh = rcu_dereference(ts->tasks.next);
        }

        rcu_read_unlock();
    }

    container_of makrosu ile list_entry makrosunun tamamen eşdeğer olduğunu anımsayınız. printk fonksiyonu ile task_struct 
    içerisindeki pid değerinin yanı sıra comm elemanın da yazıdırıldığına dikkat ediniz. Bu comm elemanında en fazla 
    16 karakterlik task_struct nesnesini betimleyen bir yazı bulunmaktadır. fork işlemi sırasında ya da thread yaratımı 
    sırasında comm elemanındaki yazı bu işlemi yapan thread'in task_struct nesnesindeki comm elemanından kopyalanmaktadır. 
    exec işlemleri sırasında ise bu yazı çalıştırılan programın dosya isminden hareketle değiştirilmektedir. comm elemanındaki 
    yazı istenirse prctl isimli kütüphane fonksiyonu ile istenildiği zaman da değiştirilebilir. Tabii bu fonksiyon da 
    sys_prctl isimli sistem fonksiyonunu çağırmaktadır. Linux sistemlerinde thread'lere pthread_setname_np fonksiyonu ile 
    isimlerde de verilebilmektedir. Bu fonksiyonla thread'e bir isim verildiğinde thread'e ilişkin task_struct nesnesinin 
    comm elemanındaki yazı da değiştirilmektedir. (pthread_setname_np bir POSIX fonksiyonu değildir. libc kütüphanesinde 
    bulunmaktadır.)

    <linux/sched/signal.h> dosyası içerisinde next_task isimli bir makro da vardır. Bu makro bir task_struct nesnesinin 
    adresini parametre olarak alıp RCU mekanizmasına uygun olarak onun tasks.next göstericisinin gösterdiği yerdeki 
    task_struct nesnesininin adresini vermektedir:

    #include <linux/sched/signal.h>

    next_task(p)

    Bu makro kullanılarak walk_process fonksiyonu şöyle de yazılabilir:

    static void walk_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        ts = next_task(&init_task);
        while (ts != &init_task) {
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
            ts = next_task(ts);
        }

        rcu_read_unlock();
    }

    Aşağıda tüm proseslerin ana thread'lerini dolaşan örnek bir çekirdek modülü verilmiştir. Modulü aşağıdaki derleyip 
    yüklediğiniz zaman modülün init fonksiyonu içerisinde yukarıdaki walk_processes fonksiyonu çağrılacaktır. Yükleme 
    işleminden sonda "dmesg" komutunu kullandığınızda proseslerin ana thread'lerine ilişkin task_struct nesnelerini 
    "çekirdeğin ring tamponunda" görüntüleyebilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/* test-module.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/rcupdate.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);
static void walk_processes(void);

static int test_module_init(void)
{
	printk(KERN_INFO "test_module_init...\n");

	walk_processes();

	return 0;
}

static void walk_processes(void)
{
	struct task_struct *ts;
	struct list_head *lh;

	rcu_read_lock();

	ts = next_task(&init_task);
	while (ts != &init_task) {
		printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
		ts = next_task(ts);
	}

	rcu_read_unlock();
}

static void test_module_exit(void)
{
	printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/*----------------------------------------------------------------------------------------------------------------------
    Aslında tasks listesini daha kolay dolaşmak için çekirdek kodlarında <linux/sched/signal.h> başlık dosyasında bulunan 
    for_each_process isimli bir döngü makrosu da kullanılmaktadır. Bu makroya parametre olarak task_struct türünden bir 
    gösterici verilir. Bu makro da her dolaşımda sıradaki task_struct nesnesinin adresini bu göstericiye yerleştirir. 
    for_each_process makrosu kendi içerisinde rcu_dereference işlemini de yapmaktadır. Bu makroyla dolaşım tipik olarak 
    şöyle yapılmaktadır:

    rcu_read_lock();

    for_each_prosess(ts) {
        /* ... */
    }

    rcu_read_unlock();

    Aslında çoğu kez <linux/sched/signal.h> başlık dosyasının include edilmesine gerek kalmamaktadır. Çünkü bu dosya çok 
    temel bir dosya olduğu için zaten <linux/kernel.h> başlık dosyasında include edilmektedir.

    Dolaşımı yapan fonksiyonu şöyle yazabiliriz:

    static void walk_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        for_each_process(ts) {
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Aşağıda örneğin tamamı verilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/* test-module.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);
static void walk_processes(void);

static int test_module_init(void)
{
	printk(KERN_INFO "test_module_init...\n");

	walk_processes();

	return 0;
}

static void walk_processes(void)
{
	struct task_struct *ts;

	rcu_read_lock();

	for_each_process(ts) {
		printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
	}

	rcu_read_unlock();
}

static void test_module_exit(void)
{
	printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/*----------------------------------------------------------------------------------------------------------------------
    Bağlı listeleri RCU mekanizmasına uygun biçimde dolaşan list_for_each_entry_rcu isimli bir döngü makrosu da vardır. 
    Bu makro <linux/rculist.h> dosyasında bulunmaktadır. Bu makrosunun parametrik yapısı şöyledir:

    #include <linux/rculist.h>

    list_for_each_entry_rcu(pos, head, member)

    Makronun birinci parametresi bağın içinde bulunduğu nesnenin adresinin yerleştirileceği göstericiyi, ikinci parametresi 
    kök bağ düğümünün adresini ve üçüncü parametresi de kök bağ düğümün ismini almaktadır. Tüm proseslerin ana thread'lerini 
    dolaşan fonksiyonu bu makroyu kullanarak aşağıdaki gibi de yazabilirdik:

    static void walk_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        list_for_each_entry_rcu(ts, &init_task.tasks, tasks) {
            printk(KERN_INFO "PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Aynı başlık dosyasındaki list_entry_rcu makrosu bir bağın adresini alarak RCU mekanizmasına uygun bir biçimde 
    bağın bulunduğu nesnesin adresi vermektedir. Bu makronun parametrik yapısı da şöyledir:

    <linux/rculist.h>

    list_entry_rcu(ptr, type, member)

    Makronun birinci parametresi bağ düğümünün list_head adresini, ikinci parametresi bağın içinde bulunduğu yapının 
    tür ismini, üçüncü parametresi ise bağ ismini almaktadır. Makro bağın içinde bulunduğu nesnenin adresini vermektedir. 
    Aslında önceki paragraflarda gördüğümüz next_task makrosu da bu makro kullanılarak yazılmıştır:

    #define next_task(p) \
        list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										16. Ders 07/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de thread'lerle ilişkin task_struct nesnelerinin dolaşılmasına örnekler verelim. Anımsanacağı gibi yeni çekirdeklerde 
    belli bir prosesin thread'lerine ilişkin task_struct nesneleri signal nesnesindeki thread_head düğümü kök alınarak 
    thread_node düğümlerinin dolaşılmasıyla elde ediliyordu.

    Belli bir prosesin thread'lerinin dolaşılabilmesi için çekirdek modülü yetersiz kalmaktadır. Çünkü çekirdek modüllerindeki 
    fonksiyonlar dışarıdan prosesler tarafından (yani proseslerin thread'leri tarafından) çağrılamamaktadır. Biz bir çekirdek 
    modülünü yüklediğimizde onun init fonksiyonu çalıştırılır. Biz bu init fonksiyonunda current makrosunu kullanırsak onu 
    yükleyen "insmod" prosesinin ilgili thread'inin (muhtemelen ana thread'inin) task_struct adresine erişiriz. Benzer biçimde 
    çekirdek modülünün exit fonksiyonunda current makrosunu kullanırsak "rmmod" prosesine ilişkin thread'in (muhtemelen ana 
    thread'inin) task_struct nesnesine erişiriz. O halde biz thread dolaşım testlerimizi ancak aygıt sürücü yazarak oluşturabiliriz. 
    Aygıt sürücülerdeki fonksiyonlar kullanıcı modundaki thread'ler tarafından çağrılabilmektedir. Bu fonksiyonlarda current 
    makrosu kullanıldığında ilgili fonksiyonu çağıran thread'in task_struct nesnesinin adresi elde edilecektir.

    Biz daha önce iskelet bir aygıt sürücü yazmıştık. Bu aygıt sürücümüz için bir ioctl kodu da oluşturmuştuk. O halde 
    testimizi bu ioctl fonksiyonu içerisinde yapabiliriz:

    long ioctl_test(struct file *filp, unsigned long arg)
    {
        walk_process_thread();

        return 0;
    }

    void walk_process_thread(void)
    {
        /* ... */
    }

    Şimdi artık walk_process_thread fonksiyonunda current makrosunu kullanırsak ioctl işlemini yapan thread'e ilişkin 
    task_struct nesnesine erişebiliriz.

    Prosesin thread'leri aşağıdaki gibi dolaşılabilir:

    static void walk_process_thread(void)
    {
        struct list_head *lh;
        struct task_struct *ts;

        rcu_read_lock();

        lh = rcu_dereference(current->signal->thread_head.next);

        while (lh != &current->signal->thread_head) {
            ts = container_of(lh, struct task_struct, thread_node);		/* ts = list_entry((lh, struct task_struct, thread_node) */
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
            lh = rcu_dereference(ts->thread_node.next);
        }

        rcu_read_unlock();
    }

    Tabii aynı işlemi daha önce görmüş olduğumuz list_for_each_entry_rcu makrosuyla da yapabilirdik:

    static void walk_process_thread(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        list_for_each_entry_rcu(ts, &current->signal->thread_head, thread_node) {
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Bir task_struct adresini alıp onun thread_node.next göstericisinin gösterdiği yerdeki task_struct adresini veren 
    next_thread isimli bir fonksiyon da vardır. next_thread fonksiyonu şöyle tanımlanmıştır:

    #include <linux/sched/signal.h>

    static inline struct task_struct *next_thread(struct task_struct *p)
    {
        return __next_thread(p) ?: p->group_leader;
    }

    Fonksiyondaki ?: operatöründe gcc eklentisi kullanılmıştır. Biz burada bu fonksiyonun içini açıklamayacağız. Ancak 
    bu fonksiyon ile biz prosesin herhangi bir thread'inden başlayarak tüm thread'lerini dolaşabilmekteyiz. Bu fonksiyon 
    dolaşım sırasında signal yapısındaki thread_head düğümüne gelindiğinde onu atlayabilmektedir. Bu durumda biz prosesin 
    thread'lerini bu fonksiyonu kullanarak aşağıdaki gibi de yapabiliriz:

    static void walk_process_thread(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        ts = current;

        do {
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
            ts = next_thread(ts);
        } while (ts != current);

        rcu_read_unlock();
    }

    Fonksiyonun yazımına dikkat edildiğinde, dolaşım sırasında yeniden signal yapısı içerisindeki thread_head düğümüne 
    gelindiğinde onu atlamak için group_leader düğümüne geçildiği görülmektedir. Buradaki kod bağlı listenin ilk elemanının 
    group_leader olduğu garantisi ile yazılmıştır. (Fonksiyon bu garantiye dayalı olmadan daha anlaşılır biçimde de 
    yazılabilirdi. Böyle yazılmasının bir gerekçesi de olabilir. Bunun için daha fazla incelemenin yapılması gerekir.)

    Aslında prosesin thread'lerini basit bir biçimde dolaşmak için for_each_thread döngü makrosu tercih edilmektedir:

    #include <linux/sched/signal.h>

    for_each_thread(p, t)

    Makronun birinci parametresi prosesin herhangi bir thread'inin task_struct adresini almaktadır. Döngü her yinelendikçe 
    yeni bir thread'e ilişkin task_struct yapının adresi ikinci parametreyle belirtilen task_struct türünden göstericinin 
    içerisine yerleştirilmektedir. Dolaşım her zaman signal yapısındaki thread_head düğümden başlatılmaktadır. (Bu bağlı 
    listenin ilk elemanı ana thread olduğu için aslında dolaşım ana thread'ten başlatılmaktadır.)

    for_each_thread döngü makrosu kullanılarak dolaşım aşağıdaki gibi basit bir biçimde yapılabilmektedir:

    static void walk_process_thread(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        for_each_thread(current, ts) {
            printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Aşağıda prosesin thread'lerini dolaşan örnek aygıt sürücü kodlarının tamamı verilmiştir. Aygıt sürücüyü yükledikten 
    sonra "app" programı çalıştırmalısınız. Bu program önce 10 tane thread yaratıp sonra aygıt sürücüdeki ioctl kodunu 
    çalıştırmaktadır. Ekrana çıkan thread'lere ilişkin pid değerleriyle "dmesg" komutunun çıktısını karşılaştırınız.
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST				_IO(TEST_DRIVER_MAGIC, 0)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test(struct file *filp, unsigned long arg);
static void walk_process_thread(void);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
	.unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	long result;

	printk(KERN_INFO "test_driver_ioctl...\n");

	switch (cmd) {
		case IOC_TEST:
			result = ioctl_test(filp, arg);
			break;
		default:
			result = -ENOTTY;
	}

	return result;
}

long ioctl_test(struct file *filp, unsigned long arg)
{
	walk_process_thread();

	return 0;
}

static void walk_process_thread(void)
{
	struct task_struct *ts;

	rcu_read_lock();

	for_each_thread(current, ts) {
		printk(KERN_INFO "Thread PID = %d, COMM = %s", ts->pid, ts->comm);
	}

	rcu_read_unlock();
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#define _GNU_SOURCE

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <fcntl.h>
#include <unistd.h>
#include <pthread.h>
#include <sys/ioctl.h>
#include "test-driver.h"

#define NTHREADS		10

void exit_sys(const char *msg);
void *thread_proc(void *param);

int main(void)
{
	int fd;
	int result;
	pthread_t tids[NTHREADS];

	for (int i = 0; i < NTHREADS; ++i) {
		if ((result = pthread_create(&tids[i], NULL, thread_proc, NULL)) != 0) {
			fprintf(stderr, "pthread_create: %s\n", strerror(result));
			exit(EXIT_FAILURE);
		}
	}

	if ((fd = open("test-driver", O_RDONLY)) == -1)
		exit_sys("open");

	if (ioctl(fd, IOC_TEST) == -1)
		exit_sys("ioctl");

	close(fd);

	for (int i = 0; i < NTHREADS; ++i)
		pthread_join(tids[i], NULL);

	return 0;
}

void *thread_proc(void *param)
{
	printf("Thread PID: %jd\n", (intmax_t)gettid());

	sleep(120);

	return NULL;
}

void exit_sys(const char *msg)
{
	perror(msg);

	exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
    Peki sistemdeki bütün task_struct nesnelerini dolaşabilir miyiz? Bunu yapmanın en pratik yolu init_task nesnesinden 
    hareketle tüm proseslerin ana thread'lerine ilişkin task_struct nesnelerini elde edip o ana thread'lerden faydalanarak 
    onların diğer thread'lerine ilişkin task_struct nesnelerini elde etmektir. Aslında bunu iç içe iki döngü oluşturarak 
    RCU mekanizması eşliğinde yapan for_each_process_thread isimli bir makro vardır:

    #include <linux/sched/signal.h>

    #define for_each_process_thread(p, t)	        \
        for_each_process(p) for_each_thread(p, t)

    Makroya biz iki task_struct göstericisi veririz. Makro her yinelemede prosesin ana thread'ine ilişkin task_struct 
    nesnesinin adresini birinci parametreyle verilen göstericinin içerisine, o prosesin thread'lerine ilişkin task_struct 
    nesnelerinin adresleri de ikinci parametreyle verilen göstericinin içerisine yerleştirmektedir. O halde tüm thread'lere 
    ilişkin task_struct nesneleri aşağıdaki gibi elde edilebilir:

    static void walk_all_threads(void)
    {
        struct task_struct *tsp;
        struct task_struct *tst;

        rcu_read_lock();

        for_each_process_thread(tsp, tst) {
            printk(KERN_INFO "Process Main Thread PID = %d, Thread PID == %d COMM = %s\n", tsp->pid, tst->pid, tst->comm);
        }

        rcu_read_unlock();
    }

    Tabii proseslerin thread'lerini girinti biçimde göstermek için kodu aşağıdaki gibi de düzenleyebiliriz:

    static void walk_all_threads(void)
    {
        struct task_struct *tsp;
        struct task_struct *tst;

        rcu_read_lock();

        for_each_process_thread(tsp, tst) {
            if (tsp == tst) {
                printk(KERN_INFO "Process Main Thread PID = %d, COMM = %s\n", tsp->pid, tst->comm);
            }
            else {
                printk(KERN_INFO "Thread PID == %d COMM = %s\n", tst->pid, tst->comm);
            }
        }

        rcu_read_unlock();
    }

    "dmesg" çıktısı aşağıdakine benzer biçimde elde edilecektir:

    ...
    [ 6022.928026] Process Main Thread PID = 3494, COMM = kworker/u257:2
    [ 6022.928027] Process Main Thread PID = 3997, COMM = kworker/1:0
    [ 6022.928042] Process Main Thread PID = 4271, COMM = kworker/0:0
    [ 6022.928044] Process Main Thread PID = 4279, COMM = kworker/u259:0
    [ 6022.928045] Process Main Thread PID = 4543, COMM = kworker/3:1
    [ 6022.928048] Process Main Thread PID = 4546, COMM = kworker/5:2
    [ 6022.928049] Process Main Thread PID = 4547, COMM = kworker/u264:2
    [ 6022.928051] Process Main Thread PID = 4565, COMM = bash
    [ 6022.928053] Process Main Thread PID = 4599, COMM = app
    [ 6022.928054]     Thread PID == 4600 COMM = app
    [ 6022.928056]     Thread PID == 4601 COMM = app
    [ 6022.928057]     Thread PID == 4602 COMM = app
    [ 6022.928059]     Thread PID == 4603 COMM = app
    [ 6022.928060]     Thread PID == 4604 COMM = app
    [ 6022.928062]     Thread PID == 4605 COMM = app
    [ 6022.928064]     Thread PID == 4606 COMM = app
    [ 6022.928081]     Thread PID == 4607 COMM = app
    [ 6022.928083]     Thread PID == 4608 COMM = app
    [ 6022.928084]     Thread PID == 4609 COMM = app
    [ 6022.928087] Process Main Thread PID = 4610, COMM = sudo
    [ 6022.928089] Process Main Thread PID = 4611, COMM = sudo
    [ 6022.928090] Process Main Thread PID = 4612, COMM = insmod
    ...

    Aşağıda bu işlemi yapan çekirde modülünün tüm kodları verilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/* test-module.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);
static void walk_all_threads(void);

static int test_module_init(void)
{
	printk(KERN_INFO "test_module_init...\n");

	walk_all_threads();

	return 0;
}

static void walk_all_threads(void)
{
	struct task_struct *tsp;
	struct task_struct *tst;

	rcu_read_lock();

	for_each_process_thread(tsp, tst) {
		if (tsp == tst) {
			printk(KERN_INFO "Process Main Thread PID = %d, COMM = %s\n", tsp->pid, tst->comm);
		}
		else {
			printk(KERN_INFO "Thread PID == %d COMM = %s\n", tst->pid, tst->comm);
		}
	}

	rcu_read_unlock();
}

static void test_module_exit(void)
{
	printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de son olarak bir prosesin alt proseslerini dolaşalım. Anımsanacağı gibi prosesin alt proses listesinin kök 
    düğümü ana prosesin ana thread'ine ilişkin task_struct nesnesinin children elemanında tutuluyordu. Bu children elemanı 
    task_struct içerisindeki sibling düğümlerini dolaşmakta kullanılıyordu. children/sibling listesinde yalnızca alt 
    proseslerin ana thread'lerine ilişkin task_struct nesnelerinin bulunduğunu da anımsayınız.

    Alt prosesleri dolaşan hazır bir döngü makrosu yoktur. Ancak biz yukarıdaki tekniklerle dolaşımı yapabiliriz. Örneğin 
    bunun için RCU mekanizmasıyla bağlı listeyi dolaşan list_for_each_entry_rcu döngü makrosundan faydalanabiliriz:

    static void walk_child_processes(void)
    {
        struct task_struct *ts;

        rcu_read_lock();

        list_for_each_entry_rcu(ts, &current->children, sibling) {
            printk(KERN_INFO "Child Process Main Thread PID = %d, COMM = %s\n", ts->pid, ts->comm);
        }

        rcu_read_unlock();
    }

    Bu kodda current makrosunun belirttiği prosesin alt proses listesi dolaşılmaktadır. Tabii daha önce de belirttiğimiz 
    gibi alt proseslerin dolaşılması prosesin ana thread'inden hareketle yapılmalıdır. Yukarıdaki fonksiyonda current 
    makrosu kullanıldığı için biz dolaşımı basit bir çekirdek modülü ile yapamayız, ancak bir aygıt sürücü yoluyla yapabiliriz. 
    Bir karakter aygıt sürücüsü oluşturarak yukarıdaki fonksiyonu onun ioctl kodu içerisinden çağırabiliriz:

    static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
    {
        long result;

        printk(KERN_INFO "test_driver_ioctl...\n");

        switch (cmd) {
            case IOC_TEST:
                result = ioctl_test(filp, arg);
                break;
            default:
                result = -ENOTTY;
        }

        return result;
    }

    long ioctl_test(struct file *filp, unsigned long arg)
    {
        walk_child_processes();

        return 0;
    }

    Kodu önce çeşitli alt prosesler yarattıktan sonra aygıt sürücü üzerinde ioctl çağrısı yapan bir programla test edebilirsiniz. 
    Biz test işlemini yapan "app.c" programında 10 tane alt proses ve her alt proseste 3 tane thread yarattık. Aygıt 
    sürücüyü yükleyip "app.c" programını derleyip çalıştırdıktan sonra "dmesg" komutunu uygulayarak çıktıyı incelemelisiniz.

    Aşağıda tüm kodları bütünsel olarak veriyoruz.
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST				_IO(TEST_DRIVER_MAGIC, 0)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test(struct file *filp, unsigned long arg);
static void walk_child_processes(void);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
	.unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	long result;

	printk(KERN_INFO "test_driver_ioctl...\n");

	switch (cmd) {
		case IOC_TEST:
			result = ioctl_test(filp, arg);
			break;
		default:
			result = -ENOTTY;
	}

	return result;
}

long ioctl_test(struct file *filp, unsigned long arg)
{
	walk_child_processes();

	return 0;
}

static void walk_child_processes(void)
{
	struct task_struct *ts;

	rcu_read_lock();

	list_for_each_entry_rcu(ts, &current->children, sibling) {
		printk(KERN_INFO "Child Process Main Thread PID = %d, COMM = %s\n", ts->pid, ts->comm);
	}

	rcu_read_unlock();
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#define _GNU_SOURCE

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/wait.h>
#include <pthread.h>
#include <sys/ioctl.h>
#include "test-driver.h"

#define NCHILDS		10
#define NTHREADS	3

void exit_sys(const char *msg);
void *thread_proc(void *param);

int main(void)
{
	int fd;
	int result;
	pid_t pid;

	for (int i = 0; i < NCHILDS; ++i) {
		if ((pid = fork()) == -1) {
			perror("fork");
			exit(EXIT_FAILURE);
		}

		if (pid == 0) {
			pthread_t tids[NTHREADS];


			for (int k = 0; k < NTHREADS; ++k) {
				if ((result = pthread_create(&tids[k], NULL, thread_proc, NULL)) != 0) {
					fprintf(stderr, "pthread_create: %s\n", strerror(result));
					exit(EXIT_FAILURE);
				}
			}

			for (int k = 0; k < NTHREADS; ++k)
				pthread_join(tids[k], NULL);

			_exit(0);
		}
		else {
			printf("Child PID = %jd\n", (intmax_t)pid);
		}
	}

	if ((fd = open("test-driver", O_RDONLY)) == -1)
		exit_sys("open");

	if (ioctl(fd, IOC_TEST) == -1)
		exit_sys("ioctl");

	close(fd);

	for (int i = 0; i < NCHILDS; ++i)
		if (wait(NULL) == -1) {
			perror("fork");
			exit(EXIT_FAILURE);
		}

	return 0;
}

void *thread_proc(void *param)
{
	sleep(60);

	return NULL;
}

void exit_sys(const char *msg)
{
	perror(msg);

	exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin uyguladığı RCU mekanizması tek bir yazan ve birden fazla okuyan akışın bulunduğu durumda beklemeyi ortadan 
    kaldırmaktadır. Ancak veri yapısına birden fazla yazanın (güncellemeyi kastediyoruz) olması durumunda yazan tarafların 
    bir kilit mekanizmasıyla (tipik olarak okuma yazma kilitleri yoluyla) senkronize edilmesi gerekir. İşte güncel 
    çekirdekler hala task_struct bağlı listelerine yazma için daha önce sözünü etmiş olduğumuz tasklist_lock kilidini 
    kullanmaktadır. Daha önceden de belirttiğimiz tasklist_lock okuma yazma kilidi artık çekirdek modülleri ve aygıt 
    sürücüler için export edilmemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										17. Ders 13/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de pid değerleriyle task_struct nesneleri arasındaki ilişkiyi ele alalım. Bilindiği gibi kullanıcı modunda 
    prosesler pid değerleriyle temsil edilmektedir. Bazı POSIX fonksiyonlarının pid parametresi aldığını anımsayınız. 
    pid parametresine sahip tipik POSIX fonksiyonları şunlardır:

    kill
    waitpid
    getpgis
    setpgid
    getsid
    getpriority
    setpriority

    Bu POSIX fonksiyonları genellikle Linux sistemlerinde doğrudan çekirdek içerisindeki sistem fonksiyonlarını çağırmaktadır. 
    Çekirdek de pid değeri ile belirtilen prosesin (ya da thread'in) task_struct nesnesine erişerek işlemlerini bu nesne 
    içerisindeki bilgileri kullanarak yapmaktadır. Örneğin programcı kill POSIX fonksiyonu ile bir prosese sinyal gönderecek 
    olsun. Programcı sinyal göndereceği prosesin pid değerini fonksiyona argüman olarak geçecektir. kill POSIX fonksiyonu 
    Linux çekirdeğindeki sys_kill sistem fonksiyonu çağırmaktadır. Çekirdek de bu pid değerinden hareketle prosese ilişkin 
    task_struct nesnesini elde edip işlemlerini buradaki bilgileri kullanarak yapacaktır. Linux sistemlerinde pid değerleriyle 
    işlem yapan ancak POSIX standartlarında bir karşılığı bulunmayan başka sistem fonksiyonları da vardır. Bu sistem 
    fonksiyonlarının çoğu "libc" kütüphanesi tarafından da sarmalanmıştır.

    O halde çekirdeğin pid değerinden hareketle o pid değerine ilişkin task_struct nesnesini hızlı bir biçimde elde 
    etmesi gerekmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bizim çekirdek geliştiricisi olarak pid değerleriyle ilgili aşağıdaki iki sorunun yanıtını biliyor olmamız gerekir:

    1) Yeni bir proses ya da thread yaratıldığında çekirdek o proses ya da thread'e ilişkin pid değerini nasıl üretmektedir?
    2) Çekirdek belli bir pid değerine ilişkin proses ya da thread'in task_struct nesnesine nasıl ulaşmaktadır?

    Bir task_struct nesnesi çekirdek tarafından yaratıldığında ona atanacak pid değeri eskiden oldukça basit bir biçimde 
    belirleniyordu. Çekirdek last_pid isminde global bir değişken tutuyordu. Boş pid araması bu değerden itibaren yapılıyordu. 
    Tabii henüz bu değer üst limite varmadığında pid tahsisatı hızlı bir biçimde yapılabiliyordu. Ancak bu değer üst limite 
    varıp da yeniden başa döndüğünde boş pid değerinin aranması zaman alıyordu. Yani arama işlemi doğrusal arama (O(N) 
    karmaşıklıkta arama) haline geliyordu. Daha sonra Linux çekirdekleri boş pid değerlerini bir bit dizisinde (buna 
    Linux'ta bitmap veri yapısı deniyor) tutmaya başladı ve işlemcilerin özel makine komutları sayesinde aramaların eskisine 
    göre daha hızlı yapılması sağlandı. Ancak Linux sistemlerinde proses limitlerinin gittikçe yükselmesiyle bu bitmap 
    yöntemi da yetersiz kalmaya başlamıştır. Özellikle container teknolojilerinin desteklenmesi amacıyla çekirdeğe "isim 
    alanı (namespace)" kavramının eklenmesiyle birlikte pid değerinin hızlı elde edilmesi daha da önem kazanmıştır. Bugün 
    mevcut çekirdeklerde boş pid değerleri radix ağaçlarının özelleştirilmiş bir biçimi olan XArray denilen veri yapısı 
    kullanılarak biraz karmaşık bir biçimde elde edilmektedir. Mevcut çekirdeklerde boş pid değerini bu karmaşık yöntemle 
    elde eden alloc_pid isimli yüksek seviyeli bir fonksiyon bulunmaktadır. Bu fonksiyonun prototipi güncel çekirdeklerde 
    şöyledir:

    struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid, size_t set_tid_size);

    Az daha geriye gittiğimizde bu fonksiyonun prototipi şöyleydi:

    struct pid *alloc_pid(struct pid_namespace *ns)

    Bu fonksiyonlar dışarıya export edilmemiştir. Bu fonksiyonların doğrudan bir pid değeri vermediğine dikkat ediniz. 
    Bu fonksiyonlar pid isimli bir yapı nesnesi tahsis edip onun adresini geri döndürmektedir. Bu yapıda da çekirdeğin 
    çeşitli versiyonlarında değişiklikler yapılmıştır. Güncel versiyonlardaki pid yapısı şöyledir:

    struct pid {
        refcount_t count;
        unsigned int level;
        spinlock_t lock;
        struct dentry *stashed;
        u64 ino;
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct hlist_head inodes;
        /* wait queue for pidfd notifications */
        wait_queue_head_t wait_pidfd;
        struct rcu_head rcu;
        struct upid numbers[];
    };

    Linux çekirdekleri belli bir versiyondan sonra her pid değeri için bir pid yapı nesnesi tutmaya başlamıştır. pid 
    üretim mekanizması ve pid değerinden hareketle task_struct nesnesinin elde edilme mekanizması zaman içerisinde 
    karmaşıklaşmıştır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi eski çekirdeklerde boş pid değerinin elde edilmesi oldukça basit bir biçimde yapılıyordu. 
    Linux'un öğrenci ödevi gibi olan ilk 0.01 versiyonu bu bakımdan çok ilkeldi:

    int find_empty_process(void)
    {
        int i;

        repeat:
            if ((++last_pid)<0) last_pid=1;
            for(i=0 ; i<NR_TASKS ; i++)
                if (task[i] && task[i]->pid == last_pid) goto repeat;
        for(i=1 ; i<NR_TASKS ; i++)
            if (!task[i])
                return i;
        return -EAGAIN;
    }

    Bu ilk versiyonda gördüğünüz gibi sistemde tahsis edilebilecek en fazla task_struct nesnesi NR_TASK kadardı:

    #define NR_TASKS 64

    Zaten bu versiyonda 64 tane task_struct yapısı baştan statik olarak tahsis edilmişti:

    struct task_struct * task[NR_TASKS] = {&(init_task.task), };

    Buradaki find_empty_process fonksiyonu tasks dizisi içerisinde boş bir task_struct indeksi ile geri dönmektedir. 
    Bu task_struct indeksi için kullanılacak pid değeri ise last_pid değişkeninde saklanmıştır. task_struct nesnelerinin 
    64 ile sınırlı olduğu olduğu halde pid değerinin 64 ile sınırlı olmadığına dikkat ediniz.

    2.4 çekirdeğinde pid numarasının belirlenmesi işlemi belli bir değerden itibaren aday olan pid değerlerinin herhangi 
    bir task_struct nesnesi tarafından kullanılıp kullanılmadığına bakılarak yapılmıştır:

    static int get_pid(unsigned long flags)
    {
        static int next_safe = PID_MAX;
        struct task_struct *p;
        int pid, beginpid;

        if (flags & CLONE_PID)
            return current->pid;

        spin_lock(&lastpid_lock);
        beginpid = last_pid;
        if((++last_pid) & 0xffff8000) {
            last_pid = 300;		/* Skip daemons etc. */
            goto inside;
        }
        if(last_pid >= next_safe) {
    inside:
            next_safe = PID_MAX;
            read_lock(&tasklist_lock);
        repeat:
            for_each_task(p) {
                if(p->pid == last_pid	||
                p->pgrp == last_pid	||
                p->tgid == last_pid	||
                p->session == last_pid) {
                    if(++last_pid >= next_safe) {
                        if(last_pid & 0xffff8000)
                            last_pid = 300;
                        next_safe = PID_MAX;
                    }
                    if(unlikely(last_pid == beginpid)) {
                        next_safe = 0;
                        goto nomorepids;
                    }
                    goto repeat;
                }
                if(p->pid > last_pid && next_safe > p->pid)
                    next_safe = p->pid;
                if(p->pgrp > last_pid && next_safe > p->pgrp)
                    next_safe = p->pgrp;
                if(p->tgid > last_pid && next_safe > p->tgid)
                    next_safe = p->tgid;
                if(p->session > last_pid && next_safe > p->session)
                    next_safe = p->session;
            }
            read_unlock(&tasklist_lock);
        }
        pid = last_pid;
        spin_unlock(&lastpid_lock);

        return pid;

    nomorepids:
        read_unlock(&tasklist_lock);
        spin_unlock(&lastpid_lock);
        return 0;
    }

    Tabii bu versiyonlarda artık çekirdeğin bir heap sistemi vardır ve task_struct yapıları dinamik olarak bu heap 
    sisteminden tahsis edilmektedir. Bu fonksiyonda last_pid değerinden başlanarak sistemdeki bütün task_struct nesneleri 
    dolaşılmış (o zamanlar bunun için for_each_task makrosu kullanılıyordu) ve boş bir pid değeri doğrusal aramayla 
    tespit edilmeye çalışılmıştır. Ancak pid araması 2.6 çekirdekleriyle birlikte bitmap veri yapısı kullanılarak 
    elde edilmeye başlanmıştır. Bitmap veri yapısı bitleri tutan bir veri yapısıdır. Modern işlemcilerde belli bir 
    adresten itibaren 0 olan ilk bitin yerini veren makine komutları bulunmaktadır. 2.6 çekirdeklerinde bu veri yapısı 
    kullanılarak boş bir pid değeri elde eden fonksiyon şöyle yazılmıştır:

    static int alloc_pidmap(struct pid_namespace *pid_ns)
    {
        int i, offset, max_scan, pid, last = pid_ns->last_pid;
        struct pidmap *map;

        pid = last + 1;
        if (pid >= pid_max)
            pid = RESERVED_PIDS;
        offset = pid & BITS_PER_PAGE_MASK;
        map = &pid_ns->pidmap[pid/BITS_PER_PAGE];
        /*
        * If last_pid points into the middle of the map->page we
        * want to scan this bitmap block twice, the second time
        * we start with offset == 0 (or RESERVED_PIDS).
        */
        max_scan = DIV_ROUND_UP(pid_max, BITS_PER_PAGE) - !offset;
        for (i = 0; i <= max_scan; ++i) {
            if (unlikely(!map->page)) {
                void *page = kzalloc(PAGE_SIZE, GFP_KERNEL);
                /*
                * Free the page if someone raced with us
                * installing it:
                */
                spin_lock_irq(&pidmap_lock);
                if (!map->page) {
                    map->page = page;
                    page = NULL;
                }
                spin_unlock_irq(&pidmap_lock);
                kfree(page);
                if (unlikely(!map->page))
                    break;
            }
            if (likely(atomic_read(&map->nr_free))) {
                do {
                    if (!test_and_set_bit(offset, map->page)) {
                        atomic_dec(&map->nr_free);
                        set_last_pid(pid_ns, last, pid);
                        return pid;
                    }
                    offset = find_next_offset(map, offset);
                    pid = mk_pid(pid_ns, map, offset);
                } while (offset < BITS_PER_PAGE && pid < pid_max);
            }
            if (map < &pid_ns->pidmap[(pid_max-1)/BITS_PER_PAGE]) {
                ++map;
                offset = 0;
            } else {
                map = &pid_ns->pidmap[0];
                offset = RESERVED_PIDS;
                if (unlikely(last == offset))
                    break;
            }
            pid = mk_pid(pid_ns, map, offset);
        }
        return -1;
    }

    Fonksiyon biraz karmaşık olsa da boş pid değerinin elde edildiği yer şurasıdır:

    offset = find_next_offset(map, offset);

    find_next_offset fonksiyonu bitmap'teki ilk boş olan 0 bitinin offset değerini bulan özel makine komutu kullanılarak 
    yazılmıştır. Kursumuzda bitmap veri yapısını ileride ayrı bir başlık altında inceleyeceğiz.

    Çekirdeğin 4.15 versiyonu ile pid tahsisatında bitmap kullanımı bırakılarak ve radix ağaçları kullanılmaya başlanmıştır. 
    güncel çekirdekler ise boş pid değerinin üretimi için artık radix ağaçlarınının özel bir biçimi olan XArray veri 
    yapısını kullanmaktadır. Biz bu konuyu izleyen paragraflarda ele alacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdekleri maksimum pid değeri için bir üst limit kullanmaktadır. Sistemdeki hiçbir pid değeri bu üst limitten 
    daha büyük olamamaktadır. Linux'un öğrenci ödevi gibi olan 0.01 versiyonunda böyle bir üst limit kullanılmamıştır. 
    Yani bu ilk versiyonda pid değeri long türünün sınırları içerisinde herhangi bir değerde olabiliyordu. Çekirdeğin 
    2.2 ve 2.4 versiyonlarında pid değerininin üst sınırı PID_MAX sembolik sabiti ile belirlenmiştir. Bu sistemlerde 
    PID_MAX sembolik sabiti 32768 olarak define edilmişti. Bu sistemlerde en büyük pid değeri bu değerden bir eksik 
    değer olan 32767 olabiliyordu. Daha sonra 2.6 çekirdekleriyle birlikte PID_MAX sembolik sabitinin ismi PID_MAX_LIMIT 
    olarak değiştirildi. 2.6'lı çekirdeklerden itibaren günümüze kadarki çekirdeklerde PID_MAX_LIMIT sembolik sabiti şöyle 
    bildirilmiştir:

    #define PID_MAX_LIMIT (IS_ENABLED(CONFIG_BASE_SMALL) ? PAGE_SIZE * 8 : \
            (sizeof(long) > 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))

    Görüldüğü gibi eğer CONFIG_BASE_SMALL konfigürasyon parametresi 'y' yapılmışsa bu limit PAGE_SIZE * 8 yani 32768 
    değerini, CONFIG_BASE_SMALL konfigürasyon parametresi 'n' yapılmışsa (normal kullanımda bu konfigürasyon parametresi 
    'n' biçimindedir) ve sizeof(long) 4 byte'tan büyükse (64 bit sistemlerde böyledir) bu limit 4 * 1024 * 1024 = 4194304 
    değerini belirtmektedir. Eğer sizeof(long) 4 değerinden büyük değilse (32 bit sistemler böyledir) bu durumda bu 
    PID_MAX_LIMIT değeri PID_MAX_DEFAULT biçimindedir. Bu sembolik sabit de şöyle tanımlanmıştır:

    #define PID_MAX_DEFAULT (IS_ENABLED(CONFIG_BASE_SMALL) ? 0x1000 : 0x8000)

    Buradan 32 bir Linux sistemlerinde PID_MAX_LIMIT değerinin eğer CONFIG_BASE_SMALL konfigürasyon parametresi 'y' 
    yapılmışsa 4096, 'n' yapılmışsa 32768 olduğu görülmektedir.

    PID_MAX_LIMIT kullanan 2.6 ve günümüze kadarki çekirdeklerde bir prosesin ya da thread'in pid değeri en fazla 
    bu PID_MAX_LIMIT değerlerinden bir eksik olabilmektedir.

    Bugün kullandığımız 64 bitlik işlemcilerdeki Linux sistemlerinde maksimum pid değeri 4194304, 32 bit sistemlerde ise 
    32767 biçimindedir. Bu değer proc dosya sisteminde "/proc/sys/kernel/pid_max" dosyasında da belirtilmektedir. 
    Programcı isterse sysctl komutu yoluyla sistem çalışırken bu değeri düşürebilir ancak yükseltemez.

    Peki maksimum pid değeri için neden bir kısıtlama getirilmiştir? İşte bunun en temel nedeni pid aramaları için 
    gereken hash tablosu ya da bitmap'ler gibi veri yapılarının daha iyi performans göstermesini sağlamaktır. Tabii 
    sistemde hiçbir zaman bu maksimum pid değerinden daha fazla sayıda task_struct nesnesi bulunamayacaktır. Aslında 
    maksimum task_struct sayısı (başka bir deyişle maksimum thread sayısı) için başka önemli kısıtlar da vardır. pid 
    kısıtı yalnızca bunlardan biridir. Sisteminizde tahsis edilebilecek maksimum task_struct nesnelerinin sayısını 
    proc dosya sistemindeki "/proc/sys/kernel/threads-max" dosyasından öğrenebilirsiniz. Bu "threads-max" dosyasındaki 
    değer sistem çalışırken de değiştirilebilir. Bu değiştirme işlemi daha resmi olarak "sysctl" komutuyla da yapılabilmektedir. 
    Örneğin:

    $ echo 100000 | sudo tee /proc/sys/kernel/threads-max

    Bu sayıyı ayarlamak için doğrudan bir çekirdek konfigürasyon parametresi yoktur. (Bazı değerler başka değerlere 
    bağlı olabilmektedir. Bu ilişki konfigürasyon parametrelerinde olduğu gibi belli bir sembolik sabite değer vermekle 
    sağlanamamaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarda çekirdeğin yeni bir pid değerini nasıl elde ettiğine yönelik bilgiler verdik. Ancak yeni çekirdeklerde 
    bu süreç karmaşık hale geldiği için henüz bunun ayrıntıları üzerinde durmadık. Şimdi de ikinci soru üzerinde duralım: 
    "Çekirdek belli bir pid değerine ilişkin task_struct adresini nasıl hızlı bir biçimde elde etmektedir?"

    Bir pid değerine ilişkin task_struct nesnesi düz mantıkla sistemdeki bütün task_struct nesneleri dolaşılarak elde 
    edilebilir. Çünkü pid değerleri zaten task_struct nesnelerinin içerisinde bulunmaktadır. Ancak böyle bir arama 
    çekirdek için çok yavaştır. Sistemde binlerce proses ve thread bulunuyor olabilir. Bu biçimdeki sıralı arama çekirdeğin 
    performansını düşürebilir. Bu tür hızlı aramalar için Linux çekirdeğinde genel olarak hash tabloları bazen de arama 
    ağaçları kullanılmaktadır. Linux'un 2.6.24 versiyonuna kadar bunun için hash tabloları kullanılıyordu. Ancak bu 
    versiyondan itibaren bu amaçla "radix ağaçları" da işin içine sokulmuştur. Güncel çekirdeklerdeki arama sistemi 
    radix ağaçlarının özel bir biçimi olan XArray veri yapısı ve hash tablolarının hibrit bir biçimine benzemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										    18. Ders 14/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Arama işlemleri bir anahtar (key) eşliğinde yapılmaktadır. Anahtar bulunması istenen varlığı temsil etmektedir. 
    Örneğin pek çok kişinin bilgileri bir veri yapısında tutuluyor olabilir. Biz de ismini bildiğimiz bir kişiyi bu veri 
    yapısında arayabiliriz. Burada anahtar isimdir. Veri yapıları dünyasında anahtar-değer çiftlerini tutan ve anahtar 
    verildiğinde ona karşı gelen değerin elde edilmesini sağlayan veri yapılarına genel olarak "sözlük (dictionary)" tarzı 
    veri yapıları denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Elemanların sıralı olmadığı liste tarzı veri yapılarında elemanların tek tek gözden geçirilmesi yoluyla yapılan 
    aramalara "sıralı arama (sequential search)" denilmektedir. Sıralı arama oldukça yavaş bir yöntemdir. Sıralı aramada 
    listedeki eleman sayısı N olmak üzere anahtarın bulunması için ortalama N/2 kez karşılaştırmanın yapılması gerekmektedir. 
    Bu tür algoritmaların karmaşıklığına "Big O" notasyonunda "O(N) karmaşıklık" denildiğini anımsayınız. Ancak ne 
    olursa olsun eleman sayısının makul olduğu bir durumda sıralı arama en iyi yöntem haline de gelebilmektedir. Örneğin 
    en fazla 20 civarında elemanın bulunduğu bir durumda bu elemanları diziye yerleştirip sıralı bir biçimde aramak en 
    etkin yöntem haline gelebilmektedir.

    Eğer elemanlar sıralıysa ve herhangi bir elemana erişim çok hızlı (buna rastgele erişim de denilmektedir) yapılabiliyorsa 
    "ikili arama (binary search)" en iyi yöntemdir. İkili aramanın algoritmik karmaşıklığı O(log N) biçimindedir. Aslında 
    ikili aramanın daha genel bir biçimine "enterpolasyon araması (interpolation search)" denilmektedir. Enterpolasyon 
    aramasında bölme ortadan değil daha uygun yerlerden yapılmaktadır. Ancak enterpolasyon araması dizi dağılımının 
    bilindiği ve özellikle de dizinin düzgün dağıldığı durumlarda faydalı bir etki oluşturmaktadır. Diziyi önce sıraya 
    dizip sonra ikili arama uygulamak ise genellikle iyi bir fikir değildir. Çünkü sırayı korumak için araya eleman ekleme 
    ve aradan eleman silme gibi işlemlerde O(N) karmaşıklıkta kaydırmaların yapılması gerekmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki ideal bir anahtar-değer araması nasıl olabilir? Şüphesiz ideal durumda aramanın O(1) karmaşıklıkta yapılması 
    istenir. Anahtarın bir int değer olduğunu ve kişinin numarasını belirttiğini düşünelim. Biz de numarasını bildiğimiz 
    kişinin bilgilerini elde etmek isteyelim. Örneğimizdeki kişilerin bilgileri PERSON isimli bir yapıyla temsil edilmiş 
    olsun:

    struct PERSON {
        ...
    };

    struct PERSON türünden büyük bir dizi açabiliriz:

    struct PERSON people[MAX_SIZE];

    Sonra da kişilerin numaralarını indeks yaparak bu diziye yerleştirebiliriz. Örneğin numarası 123 olan kişinin bilgilerini 
    diziye şöyle yerleştirilebilir:

    people[123] = person_info;

    Artık numarası 123 olan bu kişinin bilgilerini O(1) karmaşıklıkta aşağıdaki gibi elde edebiliriz:

    person_info = people[123];

    Bu yöntem ilk bakışta çok iyi bir yöntem gibi gözükse de genellikle kullanılabilir bir yöntem değildir. Çünkü burada 
    anahtar int türdendir. Ancak uygulamalarda anahtarlar farklı türlerden olabilmektedir. Örneğin anahtar kişinin adı 
    soyadı olabilir. Ad ve soyad gibi yazısal bilgiler indeks belirtmemektedir. Bu yöntemin diğer bir sakıncası da 
    anahtarların yüksek basamaklı sayılardan oluşabildiği durumlarda dizilerin çok fazla yer kaplamasıdır. Örneğin TC 
    kimlik numarasının anahtar yapılarak kişilerin bilgilerinin elde edilmesinin istendiği bir durumu düşünelim. TC 
    kimlik numarası 11 basamaklı bir sayıdır. Yani skalası 100 milyarlık sınırdadır. 100 milyarlık bir yapı dizisini bu 
    amaçla oluşturmak mümkün olmayabilir, mümkün olsa da etkin olmayabilir. Bu yönteme "indeksli arama (index search)" 
    denilmektedir. Ancak çok özel durumlarda bu yöntem uygun bir yöntem olarak kullanılabilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Algoritmik aramalarda en çok kullanılan yöntemlerden biri "hash tabloları (hash tables)" denilen yöntemdir. Hash 
    tabloları aslında yukarıda belirttiğimiz indeksli arama ile sıralı aramanın hibrit bir biçimi gibidir. Yöntemde ismine 
    "hash tablosu (hash table)" denilen makul uzunlukta bir dizi oluşturulur. Sonra anahtarlar ismine "hash fonksiyonu 
    (hash function)" denilen bir fonksiyona sokularak dizi indeksine dönüştürülür. Sonra da dizinin o indeksteki elemanına 
    başvurulur. Örneğin kişilerin bilgilerini TC kimlik numaralarına göre saklayıp geri almak isteyelim. Hash tablomuzun 
    uzunluğu da 1000 olsun. Hash fonksiyonumuzun da "1000'e bölümden elde edilen kalan" değerini veren fonksiyon olduğunu 
    varsayalım. Bu durumda örneğin 2566198712 TC kimlik numarasına sahip kişinin bilgileri hash tablosunun 712'nci 
    indeksteki elemanında saklanacaktır. 72484926820 TC kimlik numarasına sahip kişinin bilgileri de dizinin 820'nci 
    indeksteki elemanında saklanacaktır. Ancak farklı kişilerin TC kimlik numaraları hash fonksiyonuna sokulduğunda aynı 
    indeks değerleri de elde edilebilecektir. Örneğin 6238517712 TC numarasına sahip kişi de dizinin 712'nci indeksteki 
    elemanına yerleşmek isteyecektir. İşte hash tablosu yönteminde bu duruma "çakışma (collison)" denilmektedir. Hash 
    tablosu yöntemi çakışma durumunda izlenecek stratejiye göre çeşitli alt kollara ayrılmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Hash tabloları yönteminde çakışma durumunda bu sorunu çözmek için temel olarak iki alt yöntem grubu kullanılmaktadır: 
    "Ayrı zincir oluşturma (separate chaining)" yöntemi ve "açık adresleme (open addressing)" yöntemi. Açık adresleme 
    yöntemi de kendi aralarında "doğrusal yoklama (linear probing)", "karesel yoklama (quadratic probing)", "çift hash'leme 
    (double hashing)" gibi alt yöntemlere ayrılmaktadır. Ayrı zincir oluşturma ve açık adresleme yöntemlerinin dışında başka 
    çakışma çözümleme stratejileri de vardır. Ancak ağırlıklı olarak bu strateji tercih edilmektedir.

    Linux çekirdeklerindeki tüm hash tablolarında "ayrı zincir oluşturma (separate chaining)" alt yöntemi tercih edilmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Ayrı zincir oluşturma (separate chaining) yönteminde hash tablosu aslında bir bağlı liste dizisi gibi oluşturulur. 
    Yani hash tablosunun her elemanı bağlı listenin ilk elemanını (head pointer) gösteren bir gösterici durumundadır. 
    Anahtar hash fonksiyonuna sokulur, bir indeks elde edilir ve o indeksteki bağlı listenin hemen önüne (ya da duruma 
    göre arkasına) eklenir. Eleman aranırken anahtar yine aynı hash fonksiyonuna sokulur ve dizinin ilgili indeksindeki 
    bağlı listede sıralı arama yapılır.

    Hash tablolarına eleman insert etmek O(1) karmaşıklıktadır. Tabii burada kullanılacak hash fonksiyonu da önemlidir. 
    Küçük döngüler içeren hash fonksiyonları O(1) karmaşıklığı yükseltmemektedir. Elemanın silinmesi de O(1) karmaşıklıkta 
    yapılabilmektedir. Eleman aramanın O(1) karmaşıklıkta yapılabilmesi için bağlı listelerdeki zincir uzunluklarının uzun 
    olmaması gerekir. Örneğin yukarıda 10 kadar eleman için en hızlı arama yönteminin sıralı arama olduğunu söylemiştik. 
    Bu koşul sağlandığında sıralı aramanın O(1) karmaşıkta olduğu söylenebilir. O halde eğer zincirlerdeki ortalama eleman 
    makul bir düzeyde tutulursa arama işlemi de O(1) karmaşıklıkta yapılabilecektir. Ancak hash tablosu küçük fakat tabloya 
    eklenecek eleman fazla ise bu durumda hash tablosu yöntemi artık sıralı arama yöntemine benzer hale gelir. Yani bu 
    yöntemin "en kötü durumdaki (worst case)" karmaşıklığının O(N) olduğu söylenebilir. Tabii hash tablosu yöntemini kullanan 
    kişiler sistem hakkında bazı ön bilgilere sahip olursa tabloyu uygun bir büyüklükte oluşturabilirler. İşletim sistemi 
    gibi yüksek performans isteyen sistemlerde hash tablolarının zincirlerinin ortalama 1 civarında tutulması uygun 
    olabilmektedir. Hash tabloları da duruma göre büyütülebilmektedir. Ancak büyütmenin önemli bir zaman maliyeti vardır. 
    Yeni bir hash tablosunun tahsis edilmesi eski tablodaki elemanların yeniden hash'lenerek yeni tabloya yerleştirilmesi 
    uzun zaman alan bir işlemdir. İşletim sistemlerinin çekirdeklerinde bu biçimde uzun zaman alacak işlemler tercih edilmez. 
    Bu nedenle Linux çekirdeğindeki hash tabloları büyütülmemektedir. Hash tablolarında tablo elemanlarına (zincirlere değil) 
    İngilizce "bucket (kova)", tablodaki toplam eleman sayısının bucket sayısına bölümüne de "yükleme faktörü (load factor)" 
    denilmektedir. İdeal yükleme faktörünün <= 1 olduğunu söyleyebiliriz.

    Sözlük tarzı veri yapılarında genel olarak aynı anahtara ilişkin birden fazla anahtar-değer çifti veri yapısına 
    yerleştirilememektedir. (Bazı kütüphanelerde buna izin verilebilmektedir.) Eğer aynı anahtara ilişkin yeni bir değer 
    insert edilmeye çalışılırsa eski değer yeni değerle yer değiştirilmektedir. Bazı tasarımlar ise aynı anahtara ilişkin 
    insert yapmayı engellemektedir. Yani bu tasarımlarda yalnızca olmayan elemanlar tabloya insert edilebilmektedir. 
    Linux çekirdeğindeki hash tablolarında anahtarlar birbirinden farklı olmak zorundadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki hash tablolarında kullanılacak iyi bir hash fonksiyonu nasıl olmalıdır? İyi bir hash fonksiyonunun "hızlı" 
    olması gerekir. Çünkü insert gibi arama gibi işlemlerde hash fonksiyonu kullanılacaktır. İyi bir hash fonksiyonunun 
    "anahtarlar yanlı bile olsa" tabloya onları iyi bir biçimde yaydırabilmesi gerekir. Örneğin aslında sayısal anahtarlar 
    için "bölümden elde edilen kalan" iyi bir hash fonksiyonu değildir. Hash tablolarında tablonun asal sayı uzunluğunda 
    olması hash fonksiyonlarının daha iyi yaydırmasına yardımcı olmaktadır. (Örneğin tablo uzunluğu için 100 yerine 101 
    değeri tercih edilebilmektedir.) Hash fonksiyonları "sayıyı indekse dönüştüren" ve "yazıyı indekse" dönüştüren 
    fonksiyonlar biçiminde oluşturulabilir. Hash tablolarının 2'nin kuvveti uzunluğunda alınması hash fonksiyonlarının 
    daha hızlı çalışmasına da yol açabilmektedir. (Örneğin bölümden elde edilen kalan yerine bit düzeyinde öteleme 
    işlemleri hız kazancı sağlayabilmektedir.) Linux çekirdeklerinde hash tabloları için genellikle sayfa katlarında 
    (yani 4096'nın katlarında) alanlar tahsis edilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Her ne kadar Linux çekirdeğindeki hash tablolarında "ayrı zincir oluşturma (separate chaining)" stratejisi kullanılıyorsa 
    da burada "açık adresleme (open addressing)" stratejisi hakkında da küçük bir açıklama yapmak istiyoruz.

    Açık adresleme yöntemi de kendi arasında "yoklama (probing)" biçimine göre çeşitli alt yöntemlere ayrılmaktadır. Açık 
    adreslemenin en yaygın ve basit biçimi "doğrusal yoklama (linear probing)" denilen biçimidir.

    Doğrusal yoklama (linear probing) oldukça basit bir fikre dayanmaktadır. Bu yöntemde yine bir hash tablosu oluşturulur. 
    Ancak hash tablosunda bağlı listelerin adresleri tutulmaz bizzat değerlerin kendisi tutulur. Tabloya eleman ekleneceği 
    zaman yine anahtardan bir hash değeri elde edilir. Doğrudan değer tablonun hash ile elde edilen indeksine yerleştirilir. 
    Başka bir anahtar aynı hash değerini verdiğinde (yani çakışma durumu oluştuğunda) o indeksten itibaren boş yer bulunana 
    kadar yan yana indekslere sırasıyla bakılır. Örneğin hash olarak 123 değerini elde etmiş olalım. Tablonun 123'üncü 
    elemanın dolu olduğunu düşünelim. Bu durumda 124'üncü elemanına bakarız. O da doluysa 125'inci elemanına bakarız. 
    Ta ki boş bir indeks bulana kadar. Değeri ilk boş indekse yerleştiririz. Tabii bu durumda nasıl başka bir değer bizim 
    indeksimize yerleşmişse biz de aslında başka bir değerin indeksine yerleşmiş oluruz. Ancak bizim yerleştiğimiz indeks 
    için hash'e sahip olan değer de bizim yaptığımız gibi ilk boş yer bulunana kadar ilerleyecektir. Bu yöntemde arama 
    işlemi de benzer biçimde yapılmaktadır. Yani aranacak elemanın hash değeri elde edilir. O indekse başvurulur. Anahtar 
    o indekste değilse anahtar bulunana kadar ya da boş bir kova (bucket) görülene kadar yan yana diğer indekslere bakılır.

    Anahtara dayalı eleman silme de benzer biçimde yapılmaktadır. Ancak eleman silindiğinde ilgili kovanın (bucket) 
    boşaltılması arama işlemlerinde sorunlara yol açabilecektir. Burada yöntemlerden biri silinen elemana ilişkin kovanın 
    boş yapmayıp silinmenin özel bir değerle belirtilmesidir. Örneğin her kova için bir durum bayrağı tutulabilir. Bu 
    durum bayrağı ilgili kovanın "dolu" olduğunu", "boş" olduğunu ya da "silinmiş" olduğunu belirtebilir. Böylece arama 
    sırasında "silinmiş" kovalar görüldüğünde durulmaz. İlk boş kova görüldüğünde durulur. Tabii silinmiş kovalara yeni 
    elemanlar eklenebilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de Linux çekirdeğindeki hash tablosu gerçekleştirimi üzerinde duralım. Linux kaynak kodlarında hash tablosuna 
    ilişkin yapılar ve fonksiyonlar bağlı listelere ilişkin yapıların ve fonksiyonların bulunduğu başlık dosyasında 
    tanımlanmıştır. Yani hash tabloları için ayrı bir başlık dosyası oluşturulmamıştır. RCU'suz hash tablolarının gerçekleştirimi 
    "include/linux/list.h" dosyası içerisinde RCU'lu hash tablolarının gerçekleştirimi ise "include/linux/rculist.h" 
    dosyası içerisinde bulunmaktadır. Ancak her iki gerçekleştirim de aynı yapıları kullanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Ayrı zincir oluşturma yönteminde hash tablosundaki kovaların (buckets) aslında bağlı listelerin ilk düğümünün yerini 
    tutan göstericilerden oluştuğunu belirtmiştik. Linux çekirdeklerinde bağlı listenin kök düğümü de eleman düğümleri 
    de list_head yapısıyla temsil ediliyordu. Ancak ayrı zincir oluşturmalı hash tablolarındaki kovalarda (buckets) bağlı 
    listenin son düğümünün yerinin tutulmasına gerek yoktur. Elemanlar hemen listenin başına eklenebilir. Arama da listenin 
    başından itibaren yapılabilir. Tabii performans bakımından bağlı liste düğümlerinin yine çift bağlı (doubly linked) 
    olması gerekir. Çünkü adresini bildiğimiz bir düğümü hash tablosundan kolaylıkla çıkartabiliriz. O halde çekirdek 
    tablolarında "kök düğümde tek gösterici, eleman düğümlerinde ise çift gösterici" bulunmalıdır.

    Çekirdekte kullanılan hash tablosu gerçekleştiriminde kök düğüm hlist_head yapısıyla temsil edilmiştir:

    struct hlist_head {
        struct hlist_node *first;
    };

    Görüldüğü gibi yapıda tek bir gösterici vardır. O da zincirdeki ilk elemanı göstermektedir. Zincirdeki bağlı listenin 
    düğümleri de hlist_node yapısıyla temsil edilmiştir:

    struct hlist_node {
        struct hlist_node *next, **pprev;
    };

    Buradaki düğüm yapısını listelerdeki düğüm yapısı ile karşılaştırınız:

    struct list_head {
        struct list_head *next, *prev;
    };

    Hash tablosundaki düğümlerin pprev elemanı önceki düğümün adresini değil önceki düğümdeki next elemanın adresini 
    göstermektedir. Bu nedenle pprev göstericiyi gösteren göstericidir. Hash tablolarındaki düğümlerde geri gitmek 
    için bir neden yoktur. Ancak eleman silme durumunda bu tasarım önceki elemanın next göstericisinin daha kolay 
    güncellenmesine yol açmaktadır. Örneğin p göstericisi bir hlist_node düğümününü gösteriyor olsun. Biz de bu düğümü 
    silecek olalım. Burada önceki düğümün next elemanının sileceğimiz düğümün next elemanındaki düğümü göstermesi 
    sağlanmalıdır. Bu işlem de pratik olarak şöyle yapılabilmektedir:

    *p->pprev = p->next;

    Halbuki düğümler list_head yapısıyla temsil ediliyor olsaydı bu işlem ancak şöyle yapılabilirdi:

    p->prev->next = p->next;

    Görüldüğü gibi bu güncellemede fazladan bir işlem yapılmaktadır. hlist_node tasarımının diğer önemli bir faydası 
    da by tasarımda kök düğüm için özel bir işlemin yapılmasına gerek kalmamasıdır. Yani listeye ilk kez eleman eklerken 
    *p->pprev kök düğümün next göstericisi haline gelecektir. Heterojen yapılarda bu tür güncellemelerin yapılması 
    daha fazla çabayı gerektirmektedir.

    Peki neden bütün çift bağlı listelerde bu teknik kullanılmıyor? hlist_node yapısında olduğu gibi prev göstericisi 
    önceki düğümün başlangıç adresini göstermek yerine neden önceki düğümün next göstericisini göstermiyor? İşte geriye 
    doğru ilerlemenin gereli olabildiği durumlarda bu tasarımda geriye gidiş zorlaşmaktadır. Ancak yukarıda da belirttiğimiz 
    gibi hash tablolarındaki zincirlerde zaten geriye gitmenin bir anlamı yoktur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki biz yukarıdaki hlist_head ve hlist_node yapılarını kullanarak hash tablosunu nasıl oluşturabiliriz? İşte 
    yapılacak ilk şey hlist_head yapısı türünden bir dizi tahsis etmektir. Örneğin biz bu işlemi kullanıcı modunda 
    aşağıdaki gibi simüle edebiliriz:

    #include <stdio.h>
    #include <stdlib.h>

    #define TABLE_SIZE		4096

    struct hlist_head {
        struct hlist_node *first;
    };

    struct hlist_node {
        struct hlist_node *next, **pprev;
    };

    int main(void)
    {
        struct hlist_head *hash_table;

        if ((hash_table = (struct hlist_head *)malloc(sizeof(struct hlist_head) * TABLE_SIZE)) == NULL) {
            fprintf(stderr, "cannot allocate memory!...\n");
            exit(EXIT_FAILURE);
        }

        /* ... */

        free(hash_table);

        return 0;
    }

    Çekirdekteki "list.h" dosyası içerisinde hash tablosuna eleman eklemek için çeşitli basit fonksiyonlar bulundurulmuştur. 
    Tabii hlist_node düğümleri de aslında başka yapıların elemanı durumunda olacaktır. Yine asıl yapı nesnesinin adresi 
    container_of makrosuyla elde edilecektir.

    Başlangıçta hlist_head yapısındaki first elemanı NULL değerinde olmalıdır. Bunun için "list.h" içerisinde makrolar 
    bulundurulmuştur:

    #define HLIST_HEAD_INIT { .first = NULL }
    #define HLIST_HEAD(name) struct hlist_head name = { .first = NULL }
    #define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)

    Örneğin:

    for (int i = 0; i < TABLE_SIZE; ++i)
        INIT_HLIST_HEAD(&hash_table[i]);

    Zincirlerdeki son düğümün next elemanı da NULL değerindedir. Böylece arama NULL görmeyene kadar ilerlenerek yapılabilmektedir.

    Tablodaki zincirlerin önüne eleman eklemek için hlist_add_head fonksiyonu bulundurulmuştur:

    static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
    {
        struct hlist_node *first = h->first;
        WRITE_ONCE(n->next, first);
        if (first)
            WRITE_ONCE(first->pprev, &n->next);
        WRITE_ONCE(h->first, n);
        WRITE_ONCE(n->pprev, &h->first);
    }

    Burada fonksiyonun birinci parametresi kök düğüm nesnesinin adresini, ikinci parametresi yeni düğümün adresini 
    belirtmektedir. Buradaki WRITE_ONCE makrosu çok işlemcili ya da çok çekirdekli sistemlerde bellek bariyeri oluşturarak 
    atama yapmaktadır. Siz WRITE_ONCE(a, b) çağrısını a = b gibi düşünebilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										19. Ders 20/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Hash tablosundan bir düğüm silmek için hlist_del fonksiyonu kullanılmaktadır. Fonksiyon "list.h" içerisinde şöyle 
    tanımlanmıştır:

    static inline void hlist_del(struct hlist_node *n)
    {
        __hlist_del(n);
        n->next = LIST_POISON1;
        n->pprev = LIST_POISON2;
    }

    Burada asıl silme işlemi yapan fonksiyon __hlist_del fonksiyonudur. Düğüm silindikten sonra silinen düğümün next 
    ve pprev göstericilerine güvenlik amacıyla özel değerler atandığını görüyorsunuz. Bu özel değerler geçerli adresler 
    belirtmemektedir. Bu özel değerler kullanılmayan adres değerleri olduğu için eğer silinen düğüm yanlışlıkla kullanılırsa 
    "page fault" oluşmasına yol açacaktır. __hlist_del fonksiyonu da şöyle tanımlanmıştır:

    static inline void __hlist_del(struct hlist_node *n)
    {
        struct hlist_node *next = n->next;
        struct hlist_node **pprev = n->pprev;

        WRITE_ONCE(*pprev, next);
        if (next)
            WRITE_ONCE(next->pprev, pprev);
    }

    Zincirdeki son düğümün next göstericisinde NULL adres bulunmalıdır. Fonksiyonun içerisinde bu durumun kontrol edildiğine 
    ve ilk düğümün silinmesinin özel bir durum oluşturmadığına dikkat ediniz. Çok fazla gereksinim olmasa da zincirdeki 
    belli bir düğümün önüne ve arkasına düğüm insert eden hlist_add_before ve hlist_add_behind fonksiyonları da bulundurulmuştur:

    static inline void hlist_add_before(struct hlist_node *n, struct hlist_node *next)
    {
        WRITE_ONCE(n->pprev, next->pprev);
        WRITE_ONCE(n->next, next);
        WRITE_ONCE(next->pprev, &n->next);
        WRITE_ONCE(*(n->pprev), n);
    }

    static inline void hlist_add_behind(struct hlist_node *n, struct hlist_node *prev)
    {
        WRITE_ONCE(n->next, prev->next);
        WRITE_ONCE(prev->next, n);
        WRITE_ONCE(n->pprev, &prev->next);

        if (n->next)
            WRITE_ONCE(n->next->pprev, &n->next);
    }

    Hash tablosundaki bir hlist_node adresi bilindiğinde onun içinde bulunduğu asıl yapının adresinin container_of 
    makrosu ile elde edilebildiğini biliyorsunuz. Ancak tıpkı bağlı listelerde olduğu gibi hash tablolarında da bunun 
    için container_of makrosu ile aynı işlemi yapana bir entry makrosu bulundurulmuştur:

    #define hlist_entry(ptr, type, member) container_of(ptr,type,member)

    Hash tablosundaki bir zinciri dolaşmak için hlist_for_each döngü makrosu kullanılmaktadır:

    #define hlist_for_each(pos, head) \
        for (pos = (head)->first; pos ; pos = pos->next)

    Makronun ilk parametresi hlist_node türünden bir göstericiyi, ikinci parametresi ise bağlı liste zincirinin başlangıç 
    düğümüne ilişkin hlist_head nesnesinin adresini almaktadır. Bu döngü makrosunun next elemanı NULL olmayana kadar 
    ilerleme sağladığına dikkat ediniz. Döngü her yinelendikçe birinci parametreye girilen göstericinin içerisine zincirdeki 
    sıraki düğümün adresi yerleştirilmektedir. Bu döngü makrosuyla zincir dolaşılırken döngünün her yinelenmesinde asıl 
    yapı nesnesinin değil onun içerisindeki hlist_node nesnesinin adresinin elde edildiğine de dikkat ediniz. Bu adresten 
    hareketle container_of ya da hlist_entry makrolarıyla asıl nesnenin başlangıç adresininin elde edilmesi gerekir. İşte 
    bu iki işlemi aynı anda yapan hlist_for_each_entry isimli bir döngü makrosu da bulundurulmuştur:

    #define hlist_for_each_entry(pos, head, member)				                    \
    for (pos = hlist_entry((head)->first, typeof(*(pos)), member);                  \
        pos;							                                            \
        pos = hlist_entry((pos)->member.next, typeof(*(pos)), member))

    Bu makronun artık birinci parametresi doğrudan asıl yapı türünden bir göstericiyi, ikinci ve üçüncü parametreler 
    sırasıyla bağlı listenin hlist_head adresini ve asıl yapıdaki bağ elemanın ismini almaktadır.

    Yukarıdaki makroda bir noktaya dikkatini çekmek istiyoruz. Hash tablosundaki hlist_head kök düğümünün first elemanında 
    NULL adres varsa yukarıdaki hlist_for_each_entry makrosu tanımsız davranışa yol açar. İşte eğer first elemanı NULL 
    ise bu durumu ele alıp dolaşımı sonlandıran hlist_for_each_entry döngü makrosu da bulundurulmuştur:

    #define hlist_for_each_entry_safe(pos, n, head, member) 		    \
    for (pos = hlist_entry_safe((head)->first, typeof(*pos), member);   \
        pos && ({ n = pos->member.next; 1; });			                \
        pos = hlist_entry_safe(n, typeof(*pos), member))

    Makronun birinci parametresi asıl yapı nesnesi türünden göstericiyi, ikinci parametresi hlist_node türünden göstericiyi, 
    üçüncü parametresi hlist_head türünden kök düğümün adresini, dördüncü parametresi de asıl yapıdaki bağ düğümünün ismini 
    almaktadır. Bu makronun hlist_for_each_entry makrosundan tek farkı zincir boşsa bir soruna yol açmadan dönünün sonlanmasını 
    sağlamasıdır. Döngü makrosunun içerisinde hlist_entry_safe makrosunun kullanıldığına dikkat ediniz. Bu makro zincirin 
    sonundaki NULL adresi de dikkate almaktadır:

    #define hlist_entry_safe(ptr, type, member)                 \
	({ typeof(ptr) ____ptr = (ptr);                             \
	     ____ptr ? hlist_entry(____ptr, type, member) : NULL;     \
	})

    "list.h" dosyası içerisinde hash tablolarına ilişkin başka yararlı fonksiyonlar da vardır. Bu fonksiyonları ileride 
    gerektiğinde açıklayacağız. Bunları siz de inceleyebilirsiniz.

    Bir hash tablosunun tamamen yok edilmesi için yalnızca hash tablosunun değil onun tüm zincirlerdeki elemanlarının da 
    serbest bırakılması gerekir. Çekirdekte genel olarak böyle bir gereksinim yoktur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda "list.h" içerisindeki hash tablosu işlemleri için kullanıcı modunda çalıştırılabilecek bir örnek verilmiştir. 
    Örnekte TABLE_SIZE uzunluğunda her bir elemanı hlist_head türünden olan bir dizi oluşturulmuştur:

    if ((hash_table = (struct hlist_head *)malloc(sizeof(struct hlist_head) * TABLE_SIZE)) == NULL) {
        fprintf(stderr, "cannot allocate memory!...\n");
        exit(EXIT_FAILURE);
    }

    Sonra bu hlist_head elemanlarına ilkdeğer verilmiştir. (Yani onların first göstericilerine NULL adres yerleştirilmiştir):

    for (int i = 0; i < TABLE_SIZE; ++i)
        INIT_HLIST_HEAD(&hash_table[i]);

    Ondan sonra hash tablosuna rastgele 100 eleman eklenmiştir. Ancak bunun yanı sıra belli bir eleman da listeye 
    ayrıca eklenmiştir:

    for (int i = 0; i < 100; ++i) {
        if ((per = (struct PERSON *)malloc(sizeof(struct PERSON))) == NULL) {
             fprintf(stderr, "cannot allocate memory!...\n");
            exit(EXIT_FAILURE);
        }
        if (i == 50) {
            strcpy(per->name, "ALI SERCE");
            per->no = 12345678;
        }
        else
            set_random_person(per);
        hash = hash_func(per->no);
        hlist_add_head(&per->hlink, &hash_table[hash]);
    }

    Sonra liste dolaşılmıştır. Program biterken de tüm hash tablosu zincirleriyle birlikte serbest bırakılmıştır:

    {
        struct PERSON *per, *temp;
        struct hlist_node *node;

        for (int i = 0; i < TABLE_SIZE; ++i) {
            temp = NULL;
            hlist_for_each_entry_safe(per, node, &hash_table[i], hlink) {
                free(temp);
                temp = per;
            }
            free(temp);

        }
        free(hash_table);
    }

    Bağlı liste düğümleri serbest bırakılırken sonraki düğümün adresini saklamak gerekir. NULL adrese free uygulamanın 
    bir soruna yol açmayacağını anımsayınız.
----------------------------------------------------------------------------------------------------------------------*/

#include <stdio.h>
#include <stddef.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#define TABLE_SIZE		4096

struct hlist_head {
	struct hlist_node *first;
};

struct hlist_node {
	struct hlist_node *next, **pprev;
};

#define HLIST_HEAD_INIT { .first = NULL }
#define HLIST_HEAD(name) struct hlist_head name = { .first = NULL }
#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)

#define WRITE_ONCE(a, b)		((a) = (b))		/* bize özgü */
#define LIST_POISON1			(struct hlist_node *)0x00100100
#define LIST_POISON2			(struct hlist_node **)0x00200200

static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
{
	struct hlist_node *first = h->first;

	WRITE_ONCE(n->next, first);
	if (first)
		WRITE_ONCE(first->pprev, &n->next);
	WRITE_ONCE(h->first, n);
	WRITE_ONCE(n->pprev, &h->first);
}

static inline void __hlist_del(struct hlist_node *n)
{
	struct hlist_node *next = n->next;
	struct hlist_node **pprev = n->pprev;

	WRITE_ONCE(*pprev, next);
	if (next)
		WRITE_ONCE(next->pprev, pprev);
}

static inline void hlist_del(struct hlist_node *n)
{
	__hlist_del(n);
	n->next = LIST_POISON1;
	n->pprev = LIST_POISON2;
}

#define container_of(ptr, type, member) ({				\
		void *__mptr = (void *)(ptr);					\
		((type *)(__mptr - offsetof(type, member))); })

#define hlist_entry(ptr, type, member) container_of(ptr,type,member)

#define hlist_entry_safe(ptr, type, member) \
	({ typeof(ptr) ____ptr = (ptr); \
		____ptr ? hlist_entry(____ptr, type, member) : NULL; \
	})

#define hlist_for_each(pos, head) \
		for (pos = (head)->first; pos ; pos = pos->next)

#define hlist_for_each_safe(pos, n, head) \
	for (pos = (head)->first; pos && ({ n = pos->next; 1; }); \
		pos = n)

#define hlist_for_each_entry(pos, head, member)							\
	for (pos = hlist_entry((head)->first, typeof(*(pos)), member);		\
		pos;															\
		pos = hlist_entry((pos)->member.next, typeof(*(pos)), member))

#define hlist_for_each_entry_safe(pos, n, head, member) 		\
	for (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\
		pos && ({ n = pos->member.next; 1; });			\
		pos = hlist_entry_safe(n, typeof(*pos), member))

/* Test code */

struct PERSON {
	char name[32];
	int no;
	struct hlist_node hlink;
};

void set_random_person(struct PERSON *per);
unsigned int hash_func(unsigned int key);

int main(void)
{
	struct hlist_head *hash_table;
	struct PERSON *per;
	unsigned int hash;
	int no;

	srand(time(NULL));

	if ((hash_table = (struct hlist_head *)malloc(sizeof(struct hlist_head) * TABLE_SIZE)) == NULL) {
		fprintf(stderr, "cannot allocate memory!...\n");
		exit(EXIT_FAILURE);
	}

	for (int i = 0; i < TABLE_SIZE; ++i)
		INIT_HLIST_HEAD(&hash_table[i]);

	for (int i = 0; i < 100; ++i) {
		if ((per = (struct PERSON *)malloc(sizeof(struct PERSON))) == NULL) {
			 fprintf(stderr, "cannot allocate memory!...\n");
			exit(EXIT_FAILURE);
		}
		if (i == 50) {
			strcpy(per->name, "ALI SERCE");
			per->no = 12345678;
		}
		else
			set_random_person(per);
		hash = hash_func(per->no);
		hlist_add_head(&per->hlink, &hash_table[hash]);
	}

	{
		struct hlist_node *node;
		struct PERSON *per_find;

		printf("Person no:");
		scanf("%d", &no);

		hash = hash_func(no);

		hlist_for_each(node, &hash_table[hash]) {
			per_find = hlist_entry(node, struct PERSON, hlink);
			if (per_find->no == no) {
				printf("Found: %s, %d\n", per_find->name, per_find->no);
				break;
			}
		}
		if (node == NULL)
			printf("cannot find...\n");
	}

	{
		struct PERSON *per_find;
		struct hlist_node *node;

		hash = hash_func(no);

		hlist_for_each_entry_safe(per_find, node, &hash_table[hash], hlink) {
			if (per_find->no == no) {
				printf("Found: %s, %d\n", per_find->name, per_find->no);
				break;
			}
		}
		if (per_find == NULL)
			printf("cannot find...\n");
	}

	{
		struct PERSON *per, *temp;
		struct hlist_node *node;

		for (int i = 0; i < TABLE_SIZE; ++i) {
			temp = NULL;
			hlist_for_each_entry_safe(per, node, &hash_table[i], hlink) {
				free(temp);
				temp = per;
			}
			free(temp);

		}
		free(hash_table);
	}

	return 0;
}

void set_random_person(struct PERSON *per)
{
	int i;

	for (i = 0; i < 31; ++i)
		per->name[i] = rand() % 26 + 'A';
	per->name[i] = '\0';

	per->no = rand() % 1000000;
}

unsigned int hash_func(unsigned int key)
{
	key = (key ^ 61) ^ (key >> 16);
	key = key + (key << 3);
	key = key ^ (key >> 4);
	key = key * 0x27d4eb2d;
	key = key ^ (key >> 15);

	return key % TABLE_SIZE;
}

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğine kilitsiz (lock-free) RCU mekanizması eklendiğinde tıpkı bağlı listelerde olduğu gibi hash tablolarına 
    da yukarıdaki fonksiyonların sonu _rcu ile biten RCU uyumlu versiyonları eklenmiştir. Bu fonksiyonlar "include/linux/rculist.h" 
    dosyası içerisindedir. Bu dosyada yukarıda gördüğümüz hash tablosu fonksiyonlarının RCU mekanizmalı versiyonları 
    bulunmaktadır. Biz buradaki fonksiyonların yalnızca isimlerini vereceğiz. RCU mekanizması daha önce de belirttiğimiz 
    gibi başka bir başlık altında ele alınacaktır:

    hlist_add_head_rcu
    hlist_del_rcu
    hlist_add_before_rcu
    hlist_add_behind_rcu
    hlist_for_each_entry_rcu
    hlist_for_each_entry_srcu	(safe version)

    hlist_first_rcu(head)
    hlist_next_rcu(node)
    hlist_pprev_rcu(node)

    Bu RCU'lu fonksiyonlar tıpkı bağlı listelerde olduğu gibi birden fazla okuyan ancak tek bir yazan taraf varsa beklemeye 
    yol açmamaktadır. Tabii birden fazla yazan tarafın ayrıca bir senkronizasyon nesnesiyle korunması gerekir. Bu 
    makrolarla dolaşım yapılırken yine bağlı listelerde olduğu gibi ilgili kod bloğunun başına ve sonuna rcu_read_lock 
    ve rcu_read_unlock çağrılarının yerleştirilmesi gerekmektedir.

    Çekirdek tıpkı listelerde olduğu gibi pek çok yerde (ancak her yerde değil) hash tablolarıyla RCU mekanizması eşliğinde 
    işlem yapmaktadır. Çekirdeğin RCU mekanizması eşliğinde işlem yaptığı yerlerde sizin de bu RCU'lu fonksiyonları 
    kullanmanız gerekir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Hash tablolarını da görmüş olduk. Şimdi bir pid değeri verildiğinde çekirdeğin ona ilişkin task_struct nesnesini 
    nasıl bulunduğu üzerinde duralım. pid değerinden hareketle task_struct nesnesinin bulunmasına ilişkin algoritmalar 
    zaman içerisinde çekirdeğin çeşitli versiyonlarında değiştirilmiştir.

    Çekirdeğin öğrenci ödevi gibi olan ilk 0.01 versiyonunda zaten en fazla 64 tane task_struct nesnesi oluşturulabiliyordu. 
    Bu versiyonda pid değerine ilişkin task_struct nesnesinin bulunması için bu task_struct nesnelerinin tutulduğu task 
    isimli global dizide sıralı arama yapılmıştır.

    Çekirdeğin 2.2 versiyonlarında pid değerinden hareketle task_struct nesnesinin bulunması için hash tablosu kullanılmıştır. 
    Bu versiyonlarda bu işi yapan find_task_by_pid fonksiyonu aşağıdaki gibi yazılmıştır:

    extern __inline__ struct task_struct *find_task_by_pid(int pid)
    {
        struct task_struct *p, **htable = &pidhash[pid_hashfn(pid)];

        for(p = *htable; p && p->pid != pid; p = p->pidhash_next)
            ;

        return p;
    }

    Bu versiyonlarda henüz yukarıda açıkladığımız hash tablosu fonksiyonları çekirdekte bulunmuyordu. find_task_by_pid 
    fonksiyonunda önce pid değeri pid_hashfn isimli bir hash fonksiyonuna sokularak bir index değeri elde edilmiş sonra 
    da pidhash dizisinin bu indexteki bağlı liste zincirinde arama yapılmıştır. Buradaki pidhash dizisi şöyle 
    tanımlanmıştır:

    struct task_struct *pidhash[PIDHASH_SZ];

    Görüldüğü gibi hash tablosundaki zincirler doğrudan task_struct nesnelerini tutmaktadır. Bu versiyonlarda bu zincirler 
    için task_struct içerisinde iki link elemanı bulunduruluyordu:

    struct task_struct {
        ...
        struct task_struct *pidhash_next;
        struct task_struct **pidhash_pprev;
        ...
    };

    Çekirdeğin 2.4 versiyonunda da algoritmalarda ve yukarıdaki fonksiyonda bir değişiklik yapılmamıştır. Yani yine bir 
    hash tablosu eşliğinde arama yapılmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										20. Ders 21/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin 2.6'lı versiyonlarında artık her farklı pid değeri için o pid değerine ilişkin bir pid nesnesi (struct pid 
    nesnesi) oluşturulmaya başlanmıştır. Bu versiyonlarda çekirdek her farklı pid değeri için bir pid nesnesi oluşturup 
    bu pid nesnelerini de hash tablolarında saklamaktadır. Her pid nesnesi de izleyen paragraflarda açıklayacağımız gibi 
    bir grup bağlı listenin kök düğümlerini tutmaktadır. Bu versiyonlarda bir pid değerine ilişkin task_struct nesnesini 
    bulmak için çekirdek önce hash tablosundan o pid değerine ilişkin pid nesnesini elde etmekte,sonra o pid nesnesinin 
    içerisindeki bağlı listelerde arama yapmaktadır.

    2.6'lı versiyonlardaki pid yapısı şöyleydi:

    struct pid
    {
        atomic_t count;
        unsigned int level;
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct rcu_head rcu;
        struct upid numbers[1];
    };

    Yukarıda da belirttiğimiz gibi bu versiyondan itibaren çekirdek önce pid değerinden hareketle o pid değerine 
    ilişkin pid yapı nesnesini elde edip daha sonra bu pid nesnesinin içerisindeki ilgili bağlı listeyi dolaşmaktadır. 
    pid yapısında bir tane bağlı liste değil PIDTYPE_MAX kadar bağlı liste bulunmaktadır. Yapının tasks elemanı bu bağlı 
    listelerin kök düğümlerini belirtmektedir. Bu versiyonlardaki durumu şöyle özetleyebiliriz:

    1) Çekirdek her pid için bir pid nesnesi (struct pid nesnesi) oluşturup onu bir hash tablosunda saklamaktadır. Yani 
    pid nesneleri için bir hash tablosu kullanılmıştır.

    2) pid yapısının içerisindeki tasks elemanı o pid değerine ilişkin izleyen paragraflarda açıklayacağımız bağlı 
    liste zincirlerinin kök düğümlerini tutmaktadır.

    3) Bu durumda sistem bir pid değerine ilişkin task_struct nesnesini elde etmek için önce o pid değerine ilişkin pid 
    nesnesini hash tablosundan bulmakta sonra onun içerisindeki ilgili bağlı listede arama yapmaktadır.

    pid değeri ---> pid nesnesi ---> pid nesnesi içerisindeki ilgili bağlı listede arama

    2.6 çekirdekleriyle birlikte Linux'a "isim alanları (name spaces)" kavramı da sokulmaya başlanmıştır. Bu versiyonlar 
    artık pid değerleri için bir isim alanı da kullanmaktadır. pid isim alanı sayesinde sanki çekirdek birden fazla pid 
    dünyasına sahip gibi bir etki oluşturulmaktadır. Bu sayede bir pid isim alanı yaratılıp bu isim alanın diğerlerinden 
    bağımsız bir biçimde kullanılabilmesi sağlanmıştır. Pid isim alanları iç içe de oluşturulabilmektedir. Yani artık bu 
    versiyonlarla birlikte her pid değeri bir pid isim alanı içerisindedir. Tabii sistem açıldığında zaten yaratılmış hazır
    default bir pid isim alanı bulunmaktadır. Artık 2.6 versiyonlarıyla birlikte pid değerleri sistem genelinde tek değil 
    isim alanı genelinde tektir. Linux çekirdeğine eklenen bu isim alanları özelliği "docker" gibi container teknolojilerinin 
    gelişmesine katkı sağlamıştır. Linux üzerinde bu ortamlar işletim sisteminin kendi çekirdek kaynaklarını kullanarak 
    sanallaştırmayı daha kolay bir biçimde gerçekleştirilebilmektedir.

    2.6'lı çekirdeklerden itibaren pid işlemleri "kernel/pid.c" dosyası içerisinde gerçekleştirilmiştir. 2.6'lı çekirdeklerde 
    pid değeri verildiğinde task_struct nesnesinin elde edilmesi için "kernel/pid.c" dosyası içerisinde aşağıdaki fonksiyonlar 
    yazılmıştır:

    struct task_struct *find_task_by_vpid(pid_t vnr)
    {
        return find_task_by_pid_ns(vnr, current->nsproxy->pid_ns);
    }

    struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns)
    {
        rcu_lockdep_assert(rcu_read_lock_held());
        return pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);
    }

    struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
    {
        struct hlist_node *elem;
        struct upid *pnr;

        hlist_for_each_entry_rcu(pnr, elem,
                &pid_hash[pid_hashfn(nr, ns)], pid_chain)
            if (pnr->nr == nr && pnr->ns == ns)
                return container_of(pnr, struct pid,
                        numbers[ns->level]);

        return NULL;
    }

    struct task_struct *pid_task(struct pid *pid, enum pid_type type)
    {
        struct task_struct *result = NULL;
        if (pid) {
            struct hlist_node *first;
            first = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),
                            rcu_read_lock_held() ||
                            lockdep_tasklist_lock_is_held());
            if (first)
                result = hlist_entry(first, struct task_struct, pids[(type)].node);
        }
        return result;
    }

    Burada en ana fonksiyonun find_task_by_vpid (buradaki "v" harfi "virtual" sözcüğünden gelmektedir) olduğunu görüyorsunuz. 
    Bu fonksiyon o anda prosesin çalıştığı pid isim alanında arama yapmaktadır. Fonksiyonların çağırma grafı şöyledir:

    find_task_by_vpid ---> find_task_by_pid_ns ---> find_pid_ns ---> pid_task ---> task_struct

    Burada find_task_by_vpid fonksiyonu çağrıyı yapan prosesin pid isim alanında aramayı başlatmaktadır. find_task_by_pid_ns 
    fonksiyonu yalnızca belli bir isim alanında aramayı sağlar. find_pid_ns fonksiyonu ise ilgili isim alanında pid yapı nesnesini 
    bulur. pid_task fonksiyonu da o pid nesnesinin içerisindeki bağlı zincirinde arama yaparak task_struct nesnesinin adresini 
    elde eder. findd_task_vpid fonksiyonu çağrıyı yapana prosesin pid isim alanına task_struct yapısı içerisindeki nsproxy->pid_ns 
    elemanı yoluyla erişmektedir.

    2.6 çekirdeklerinde pid ve isim alanından hareketle pid nesnelerinin bulunması için tek bir hash tablosu oluşturulmuştur. 
    Yani bütün isim alanlarındaki pid nesneleri aynı hash tablosu içerisindedir. Hash tablosunda pid nesnesini arayan 
    find_pid_ns fonksiyonu şöyle yazılmıştır:

    struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
    {
        struct hlist_node *elem;
        struct upid *pnr;

        hlist_for_each_entry_rcu(pnr, elem,
                &pid_hash[pid_hashfn(nr, ns)], pid_chain)
            if (pnr->nr == nr && pnr->ns == ns)
                return container_of(pnr, struct pid,
                        numbers[ns->level]);

        return NULL;
    }

    Burada hash zincirlerindeki düğümler upid isimli yapıyla temsil edilmiştir. Gördüğünüz gibi arama yapılırken hem 
    pid değerine hem de pid isim alanına birlikte bakılmıştır. Buradaki hash tablosu üzerinde bir açıklama yapalım. 
    Ana hash tablosu pid_hash dizisi biçimindedir. Bu dizinin tahsisatı çekirdek ilkdeğerlenirken (initialize edilirken) 
    yapılmaktadır. Burada hlist_node elemanı upid yapısının içerisinde, upid yapısı da pid yapısının içerisindedir. 
    Bunlar arasındaki içerme ilişkisini daha iyi anlayabilmeniz için bu iki yapıyı alt alta veriyoruz:

    static struct hlist_head *pid_hash;
    ...

    struct upid {
        /* Try to keep pid_chain in the same cacheline as nr for find_vpid */
        int nr;
        struct pid_namespace *ns;
        struct hlist_node pid_chain;
    };

    struct pid
    {
        atomic_t count;
        unsigned int level;
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct rcu_head rcu;
        struct upid numbers[1];
    };

    Burada hlist_node bağının uid nesnesinin içerisinde olduğuna, uid nesnesinin de pid nesnesinin içerisinde olduğuna 
    dikkat ediniz.

    2.6'lı çekirdek versiyonlarında hash tablosundan pid nesnesi elde edildikten sonra artık task_struct nesnesi bu pid 
    yapısının içerisindeki bağlı listelerde aranmaktadır. pid yapısının tasks elemanının bir grup bağlı liste zincirinin 
    kök düğümlerini tuttuğunu belirtmiştik. Buradaki tasks dizisinin PIDTYPE_MAX kadar elemana sahip olduğunu görüyorsunuz. 
    Aslında PIDTYPE_MAX aşağıdaki gibi bir enum türünün elemanıdır:

    enum pid_type
    {
        PIDTYPE_PID,
        PIDTYPE_PGID,
        PIDTYPE_SID,
        PIDTYPE_MAX
    };

    Burada PIDTYPE_MAX değerinin 3 olduğunu görüyorsunuz. Ancak güncel çekirdeklerde bu değer 4'tür. Güncel çekirdeklerdeki
    pid_type enum türü şöyledir:

    enum pid_type {
        PIDTYPE_PID,
        PIDTYPE_TGID,
        PIDTYPE_PGID,
        PIDTYPE_SID,
        PIDTYPE_MAX,
    };

    Bu diziye sonraları PIDTYPE_TGID ile temsil edilen bir bağlı listenin daha eklendiğine dikkat ediniz. Peki pid 
    değerini saklamak için neden birden fazla bağlı liste kullanılmaktadır? Bir pid nesnesinin tek bir pid değeri belirtmesi 
    gerekmez mi? İşte 2.6'lı çekirdeklerle birlikte bu tarz aramalarda hız kazancı sağlamak için tasarım değiştirilmiştir. 
    Buradaki bağlı listelerin hangi task_struct nesnelerini tuttuğunu tek tek açıklayalım:

    - PIDTYPE_PGID bağlı listesi aynı proses grubuna ilişkin proseslerin ana thread'lerinin task_struct nesnelerini tutmaktadır. 
    Yani aranan pid değeri bir proses grup liderine ilişkin pid belirtiyorsa bu zincirde o proses grubundaki tüm proseslerin 
    ana thread'lerinin task_struct nesnelerinin adresleri bulunmaktadır. Çekirdeğe biz bir proses grup id değeri verdiğimizde 
    çekirdek o proses grubunun içerisindeki tüm proseslerin (proseslerin ana thread'lerinin) task_struct adreslerini bu 
    zinciri dolaşarak elde edebilmektedir.

    - PIDTYPE_SID bağlı listesinde belli bir oturuma (session) ilişkin tüm proseslerin (onların ana thread'lerinin) task_struct 
    nesnelerinin adresleri tutulmaktadır. Yani biz çekirdeğe bir oturum liderinin pid değerini verdiğimizde çekirdek o 
    oturumdaki tüm proseslerin task_struct adreslerini bu listeyi dolaşarak elde edebilmektedir.

    - PIDTYPE_TGID bağlı listesi ise belli bir prosesin thread'lerini dolaşmak için kullanılmaktadır. Bu bağlı listedeki 
    tüm task_struct nesneleri aynı prosesin thread'lerini oluşturmaktadır. Yani biz çekirdeğe bir prosesin ana thread'ine 
    ilişkin bir pid değeri verdiğimizde çekirdek bu bağlı liste zincirini dolaşarak o prosesin tüm thread'lerinin 
    task_struct adreslerini elde edebilmektedir.

    - PIDTYPE_PID bağlı liste zincirinde aslında tek bir eleman vardır. Amacımız yalnızca belli bir pid değerine ilişkin 
    task_struct nesnesinin adresinin bulunmasıysa hemen bu listenin ilk elemanını alabiliriz.

    Yukarıda sözünü ettiğimiz pid_task fonksiyonu pid yapısının içerisindeki bir bağlı liste zincirinin tamamını değil 
    yalnızca ilk elemanını vermektedir:

    struct task_struct *pid_task(struct pid *pid, enum pid_type type)
    {
        struct task_struct *result = NULL;
        if (pid) {
            struct hlist_node *first;
            first = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),
                            rcu_read_lock_held() ||
                            lockdep_tasklist_lock_is_held());
            if (first)
                result = hlist_entry(first, struct task_struct, pids[(type)].node);
        }
        return result;
    }

    2.6 versiyonlarında ilgili task_struct pid değerine ilişkin pid nesnesinin adresi ayrıca task_struct yapısının 
    içerisinde tutulmuyordu. Ancak çekirdeğin 4.20 versiyonuyla birlikte artık task_struct pid değeri doğrudan task_struct 
    yapısının thread_pid elemanında da saklanmaya başlamıştır.

    Peki 2.6 öncesindeki yukarıda sözünü ettiğimiz işlemler nasıl yapılıyordu? Örneğin bir proses grubunun pid değeri 
    verildiğinde çekirdek o gruptaki tüm proseslerin (onların ana thread'lerinin) task_struct adreslerini nasıl elde 
    ediyordu? İşte o çekirdeklerde tek bir hash tablosu vardı. Bu nedenle proses grubundaki proseslerin task_struct 
    nesneleri ancak tüm task_struct nesneleri dolaşılarak elde edilebiliyordu. O versiyonlarda bir prosesin thread'lerine 
    ilişkin task_struct nesneleri için de önce ana thread'in task_struct nesnesi, sonra o ana thread'in task_struct 
    nesnesinden hareketle prosesin thread'lerine ilişkin task_struct nesneleri elde ediliyordu. Halbuki 2.6'daki bu 
    tasarımla bu biçimdeki dolaşımlar bu hash tablosu ve bağlı listeler yoluyla daha hızlı yapılabilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek versiyonu 4.20'lere geldiğinde yukarıdaki mekanizmada bazı değişiklikler yapılmıştır. En önemli değişiklik 
    pid ve isim alanından hareketle pid nesnesinin bulunması işleminde hash tablosu yerine radix ağacının kullanılmaya 
    başlanmasıdır. Aslında 2.5 ile birlikte çekirdekte başka alt sistemlerde radix ağacı kullanılmaya başlanmıştı. Sonra 
    bu radix ağaçlarının 4.20 versiyonuyla birlikte XArray isimli yeni bir gerçekleştirimi yapılmıştır. İşte çekirdeğin 
    4.20 versiyonuna gelindiğinde bu radix ağacının XArray gerçekleştirimi pid alt yapısında da kullanılmıştır.

    4.20 ve sonrasındaki pid yapısındaki (struct pid içerisindeki) bağlı listelerde bir farklılık yoktur. Yalnızca pid 
    ve isim alanı bilgisinden hareketle pid yapı nesnesinin elde edilmesinde hash tablosu yerine radix ağacı (XArray 
    gerçekleştirimi) kullanılmaktadır. Bu bağlamda pid mekanizmasında radix ağacının hash tablosuna tercih edilmesinin 
    tipik nedenleri şunlardır:

    - Radix ağacındaki arama hash tablolarından yavaş gözükse bile aslında pratikte durum tam böyle değildir. 
    Buradaki radix ağacı seyrek tutulmaktadır. Bu yüzden arama hızlı yapılır.

    - pid’leri artan sırayla gezmek, boş pid bulmak (en küçük kullanılmayan PID’i seçmek) çok daha kolaydır.

    - Ağaçta yalnızca kullanılan düğümler tutulmaktadır. Bu da bellek kazancı sağlamaktadır.

    - pid sayısı arttıkça hash tablosunun performansı düşme eğilimi gösterdiği halde radix ağacının performansı hash 
    tablosuna kıyasla düşmez.

    - Radix ağacı yalnızca pid araması için değil, aynı zamanda yeni yaratılan task_struct nesneleri için pid tahsis 
    edilmesinde de kullanılabilmektedir. Bu tasarımda yeni bir pid numarası eskisine göre daha hızlı elde edilebilmektedir.

    Özetle yalnızca pid araması için hash tablosu tasarımına devam edilebilirdi. Ancak pid tahsisatı gibi diğer işlemler 
    için bu radix ağacı tasarımı toplamda daha iyi performans sağlamaktadır. Güncel çekirdeklerde hem pid aramaları hem 
    de boş pid değerlerinin tespit edilmesi işlemleri bu XArray gerçekleştirimi ile sağlanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz radix arama ağacını ve XArray gerçekleştirimini daha sonra başka bir bölümde ele alacağız. Burada yalnızca radix 
    arama ağacının temel mekanizması üzerinde açıklamalar yapacağız. Radix arama ağaçları için "dijital arama ağaçları 
    (digital search tree)", "önek arama ağacı (prefix search tree)", "sıkıştırılmış arama ağaçları (compresses search 
    trie)" gibi isimler de kullanılmaktadır. Bu arama ağaçlarında düğümlerde anahtar olarak anahtarın hepsi değil onun 
    ön kısımları kullanılmaktadır. Böylece ağaç bir leksigrafik sıralama ağacı gibi de kullanılabilmektedir. Tipik 
    anlatım sayısal değerlerden oluşan ve öneklerin bit belirttiği anahtarlar üzerinde yapılmaktadır. Ağacın kökünden 
    girilir. Her kademede bir bit daha anahtara eklenir. Örneğin 4 bitlik sayıların radix ağacına yerleştirilmek istendiğini 
    düşünelim. Bu sayılar şunlar olsun:

    1100
    1010
    0101
    1011
    0010
    0111
    0000
    1000

    Ağacın kökünün boş olduğunu düşünelim. Aslında ağaca yerleştirilecek ilk değer de kök yapılabilir. İlk değer için 
    ilk bit bite bakılır. 0 olan bitler için sola, 1 olan bitler için sağa yerleştirme yapılmaktadır:

                                Kök
                                        1100

    Şimdi ikinci değeri ağaca yerleştirelim. Bu durumda 1010 değeri kökün sağına ancak 1100 deüğümn soluna yerleştirilir:

                                Kök
                                             1100
                                      1010

    Üçüncü değer olan 0101 kökün soluna yerleşecektir:

                                Kök
                   0101                           1100
                                      1010

    Dördüncü değer olan 1011'i yerleştirdikten sonra ağaç şu görünümde olacaktır:

                                 Kök
                   0101                            1100
                                      1010
                                            1011

    Şimdi de 0010 değerini yerleştirelim:

                                    Kök
                        0101                          1100
                0010                1010
                                          1011

    Şimdi de 0111 değerini yerleştirelim:

                                    Kök
                         0101                          1100
                   0010        0111       1010
                                                1011

    Şimdi de 0000 değerini yerleştirelim:

                                      Kök
                        0101                         1100
                0010          0111        1010
         0000                                   1011

    Nihayet 1000 değerini de ağaca yerleştirelim:

                                      Kök
                        0101                            1100
                0010         0111            1010
         0000                          1000       1011

    Peki bu ağaçta arama nasıl yapılır? İşte arama için kökten girilir. Her bit pozisyonu için bir aşağıya inilir. 
    Örneğin 0000 değerini arayacak olalım. İlk bit 9 olduğu için kökten sola ineriz. Oradaki değer 0101'dir. Bulamadığızdan 
    dolayı ikinci bite bakarız. İkinci bit de 0 olduğu için oradan da sola gideriz. Oradaki değer 0010'dır. Bulamadığımız 
    için üçüncü bite bakarız. Üçüncü bit de 0 olduğu için sola gideriz. Artık değeri buluruz.

    Burada biz yüksek anlamlı bitten hareketle dallanmaları yaptık. Ancak bunun tersini de yapabilirdik. Radix ağaçlarını 
    enlemesine (breadt-first gibi) dolaştığımızda anahtarların sıralı bir biçimde elde edilebildiğine dikkat ediniz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										21. Ders 27/09/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki anlatımda düğümlerde anahtarların turulduğunu varsaydık. Aslında düğümlerde anahtarların tutulmasına 
    da gerek yoktur. Zaten her kademede bir bit ilerlendiğinde göre arama yaprağa gelindiğinde sonlandırılabilir. Bir 
    yaprağa gelindiğinde ya ilgili değer bulunmuştur ya da ilgili değer bulunamamıştır. Eğer bütün bitler tüketilerek 
    yaprağa gelinmişse ilgili anahtar bulunmuş, bütün bitler tüketilmeden yaprağa gelinmişse ilgili değer bulunamamıştır.

    Kök
    ├── 0
    │    ├── 1
    │    │    ├── 0
    │    │    │    └── 1 (0101 son)
    │    │    └── 1
    │    │         └── 1 (0111 son)
    │    └── 0
    │         ├── 1
    │         │    └── 0 (0010 son)
    │         └── 0
    │              └── 0 (0000 son)
    └── 1
        ├── 1
        │    └── 0
        │         └── 0 (1100 son)
        └── 0
            ├── 1
            │    ├── 0 (1010 son)
            │    └── 1 (1011 son)
            └── 0
                    └── 0 (1000 son)

    Aynı ağacı şöyle de gösterebiliriz:

                     ┌───────────────────────────┐
                     |           Kök             |
                     └───────────────────────────┘
                    /                             \
                 ┌───┐                            ┌───┐
                 | 0 |                            | 1 |
                 └───┘                            └───┘
              /        \                       /         \
        ┌───┐           ┌───┐             ┌───┐           ┌───┐
        | 0 |           | 1 |             | 0 |           | 1 |
        └───┘           └───┘             └───┘           └───┘
       /    \          /      \          /     \            |
   ┌───┐    ┌───┐    ┌───┐    ┌───┐    ┌───┐   ┌───┐      ┌───┐
   | 0 |    | 1 |    | 0 |    | 1 |    | 0 |   | 1 |      | 0 |
   └───┘    └───┘    └───┘    └───┘    └───┘   └───┘      └───┘
     |        |        |        |       |      /   \        |
   ┌───┐    ┌───┐    ┌───┐    ┌───┐   ┌───┐  ┌───┐ ┌───┐  ┌───┐
   | 0 |    | 0 |    | 1 |    | 1 |   | 0 |  | 0 | | 1 |  | 0 |
   └───┘    └───┘    └───┘    └───┘   └───┘  └───┘ └───┘  └───┘

    Görüldüğü gibi bu tasarımda hiç anahtarlar saklanmadan yaprak görülene kadar ilerlenirse ya ilgili anahtar bulunmuş 
    olur ya da bulunmamış olur. Peki her düğümde anahtarı tutmakla yalnızca yapraklarda değeri (anahtarı değil) tutmanın 
    hangisi daha iyi bir yöntemdir? İşte her iki yöntemin de avantajları ve dezavantajları vardır:

    1. Her Düğümde Anahtar Saklama Yöntemi

    Avantajları:
    • Erken durabilme: Sorgu sırasında ara bir düğümde tam eşleşme varsa yaprağa kadar inmek zorunda kalınmaz.
    • Prefix aramalar daha kolay: “Bu prefix ile başlayan anahtarlar var mı?” soruları doğal olarak çözülebilir.

    Dezavantajları:
    • Veri yapının karmaşıklığı artar; hem iç düğümlerde hem de yapraklarda anahtar tutmak gerekir.
    • Bellek kullanımı artar.

    2. Yalnızca Yapraklarda Değer Saklama Yöntemi

    Avantajları:
    • Basit tasarım: İç düğümler yalnızca yönlendirme (ayrıştırma) bilgisi tutar.
    • Bellek daha düzenli kullanılır, çünkü anahtar yalnızca tek yerde saklanır.
    • Arama, ekleme, silme algoritmaları daha etkin yapılabilir.

    Dezavantajları:
    • Her zaman yaprağa kadar inmek gerekir, dolayısıyla tam eşleşme ara düğümde olsa bile ek adımlar atılır.
    • Prefix tabanlı aramalarda ek iş yapılması gerekir.

    Linux'un klasik radix ve XArray gerçekleştirimlerinde değerler her zaman yapraklarda tutulmaktadır. Ara düğümlerde 
    anahtarlar tutulmamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Radix ağacınının ikili ağaç olması zorunlu değildir. Dallanma bit bit yapıldığı zaman yukarıdaki örnekte olduğu gibi 
    radix ağacı ikili ağaç durumunda olur. Ancak dallanma n bit n bit de yapılabilir. Bu durumda ağaç da n'li ağaç olacaktır. 
    Örneğin 32 bitlik pid değerlerini böyle bir ağaçta tutmak isteyelim. Ama dallanmayı bit bit değil de 6 bit 6 bit yapalım. 
    Ağacın kademelerini de düşük anlamlı bitlerden yüksek anlamlı bitlere doğru oluşturalım. 6 bit 2^6 = 64 farklı dallanmaya 
    yol açacaktır. Bu durumda ağacımızın her düğümünde 64 gösterici bulunacaktır:

                                                 Kök
    000000   000001  000010  000011  ...                ...  111100  111101  111110 111111

    İşte bu kökten itibaren her düğümde 64 dallanma olacaktır. Yapraklara varabilmek için en fazla 6 kademe ilerlenecektir. 
    Güncel sürümlerde Linux'un pid araması için kullandığı XArray ağacı bu örnekteki gibi altışar bitlidir.

    Bu biçimdeki XArray ağacının performansını 2.6'lı versiyonlardan itibaren kullanılmaya başlanmış olan hash tablolarının 
    performansıyla kıyasladığımızda ilk bakışta hash tablolarının daha iyi performans gösterebileceğini sanabilirsiniz. Ancak 
    boş pid değerlerinin aranmasında da aynı ağaçtan faydalanıldığı göz önüne alındığında XArray ağacının toplamda daha yüksek 
    performans sağladığı görülebilmektedir. Aynı zamanda bu ağaç yapısı iç içe pid isim alanlarında da hızlı aramaya olanak 
    sağlamaktadır. XArray ağacının kullanıldığı çekirdeklerde boş pid değerinin elde edilmesi için yine son pid değerini 
    tutan bir değişkenden başlanarak ağaçta arama yapılır. Ancak pid değerleri doluysa ilk boş pid değerinin bulunması bu veri 
    yapısında daha hızlı gerçekleşebilmektedir. Boş bir pid değerinin bulunmasının amacı zaten yeni bir pid nesnesinin (struct 
    pid nesnesinin) tahsis edilerek ağaca iliştirilmesidir. XArray ağaçlarında bu iki işlem birlikte yapılmaktadır. 
    Bu işlemler güncel çekirdeklerde çu çağrılar eşliğinde yapılmaktadır:

    alloc_pid ---> idr_alloc_cyclic ---> idr_alloc_u32 ---> idr_get_free

    pid değerinden pid yapı nesnesinin elde edilmesi sürecinde hash tablosu ile XArray gerçekleştimi arasındaki avantaj ve 
    dezavantajlar şöyle sıralanabilir:

    Hash Tablosu İle PID Arama

    - Arama karmaşıklığı: Ortalama O(1), yani sabit zamanlı.
    - Bellek kullanımı: Tablo önceden belli bir boyutta ayrılır. pid üst lmiti yüksekse ama kullanılan pid sayısı göreli olarak 
    azsa, büyük miktarda alan boşa gider.
    - Çakışmalar (Collisions): Birden fazla pid aynı kovaya düşerse arama yavaşlayabilir en kötü durumda O(n) haline gelebilir.
    - Ölçeklenebilirlik: Tablo boyutunu başlangıçta iyi seçmek gerekir. Aksi takdirde performans düşebilir.
    - pid numaralarına göre sıralı gezinme: Tüm pid’leri sırayla dolaşmak yavaştır. Çünkü hash tablosunda doğal bir sıralama 
    yoktur.

    XArray/Radix Ağacı ile PID Arama

    - Arama karmaşıklığı: O(logₖ(n)), burada k = dallanma sayısıdır (tipik olarak Linux'ta 64). pid aramasında bit gruplarıyla 
    seviye seviye aşağıya inilir. Tipik derinlik çok küçüktür (ör. 32-bit PID için ortalama 2–3 seviye).
    - Bellek kullanımı: Yalnızca kullanılan pid aralıkları için düğümler açılır (sparse allocation). pid alanı seyrek 
    kullanılıyorsa çok verimlidir.
    - Çakışma sorunu yoktur: Her pid tek bir yolu takip eder, hash gibi aynı kovaya düşme ihtimali yok.
    - Ölçeklenebilirlik: pid alanı büyüse bile ağaç yapısı uyum sağlar; yeniden boyutlandırma gerekmez.
    - XArray/Radix Ağacı doğası gereği indeks sırasını korur. Tüm pid'leri sıralı biçimde dolaşmak çok hızlıdır.

    Özet olarak da şunları söyleyebiliriz:

    - Tekil arama için hash tablosu genellikle biraz daha hızlıdır.
    - Büyük ve seyrek pid isim alanlarında XArray çok daha verimli bellek kullanır.
    - Sıralı gezinme gerektiğinde XArray üstün gelir, hash tablosunda bu durum mümkün değildir.
    - Çakışmasız, deterministik performans açısından XArray daha öngörülebilirdir, hash tablosunda ise çakışma durumunda 
    kötüleşmeler yaşanabilir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz çekirdeğin 2.6 versiyonlarıyla birlikte pid'ler için "pid isim alanlarının" da oluşturulduğunu söylemiştik. 
    İşte çekirdeğin 4.20 versiyonlarıyla hash tablosu bırakılıp XArray ağacının kullanılmaya başlanmasıyla birlikte her 
    pid isim alanı için ayrı bir XArray ağacı oluşturulmaya başlanmıştır. Anımsanacağı gibi 2.6'lı versiyonlarda hash 
    tablolarına geçildiğinde tüm pid isim alanları aynı hash tablosunda bulunuyordu. Halbuki güncel çekirdeklerde bir pid 
    nesnesi aranacaksa o nesne belli bir pid isim alanındaki XArray ağacında aranmaktadır. 
 
    Güncel çekirdeklerde pid araması yapan yüksek seviyeli fonksiyonlardan biri find_vpid fonksonudur. Fonkiyon şöyle
    tanımlanmıştır:

    struct pid *find_vpid(int nr)
    {
        return find_pid_ns(nr, task_active_pid_ns(current));
    }
    EXPORT_SYMBOL_GPL(find_vpid);

    Bu fonksiyon belli bir pid değerine ilişkin pid nesnesini çağrıyı yapan prosesin bulunduğu pid isim alanı içerisindeki 
    XArray ağacında aramaktadır. Peki bu fonksiyon o anda çalışmakta olan prosesin pid isim alanını nasıl bulmaktadır? 
    2.6'lı versiyonlarda prosesin ilişkin olduğu pid isim alanının bilgilerine task_struct yapısının nsproxy elemanının 
    gösterdiği nsproxy yapısının pid_ns elemanı yoluyla erişiliyordu. Ancak güncel versiyonlarda artık prosesin ilişkin olduğu
    pid isim alanının bilgilerine pid nesnesi yoluyla erişilmektedir. 4.20 versiyonlarından itibaren task_struct pid değerinin 
    artık doğurdan task_struct yapısının thread_pid elemanında da tutulduğunu önceki paragraflarda belirtmiştik:

    struct task_struct {
        /* ... */

        pid_t pid;
        struct pid *thread_pid;     

        /* ... */
    };

    İşte güncel versiyonlarda ilgili task_struct pid değerine ilişkin pid nesnesine bu thread_pid elemanı ile erişilmektedir. 
    Güncel versiyonlardaki pid yapısı şöyledir: 

    struct pid {
        refcount_t count;
        unsigned int level;
        spinlock_t lock;
        struct {
            u64 ino;
            struct rb_node pidfs_node;
            struct dentry *stashed;
            struct pidfs_attr *attr;
        };
        /* lists of tasks that use this pid */
        struct hlist_head tasks[PIDTYPE_MAX];
        struct hlist_head inodes;
        /* wait queue for pidfd notifications */
        wait_queue_head_t wait_pidfd;
        struct rcu_head rcu;
        struct upid numbers[];
    };

    Çekirdeğin 4.20 versiyonundan itibaren prosesin ilişkin olduğu pid isim alanına pid yapısı içerisindeki numbers dizisinin 
    ilgili elemanından erişilmektedir. numbers dizisinin elemanlarının upid isimli yapı türünden olduğuna dikkat ediniz. upid 
    yapısı da şöyle bildirilmiştir:

    struct upid {
        int nr;
        struct pid_namespace *ns;
    };  

    Şimdi find_vpid fonksiyonuna geri dönelim:

    struct pid *find_vpid(int nr)
    {
        return find_pid_ns(nr, task_active_pid_ns(current));
    }

    Bu fonksiyon önce task_active_pid_ns fonksiyonu ile yukarıda belirttiğimiz biçimde prosesin içerisinde bulunduğu pid
    isim alanınının bilgilerini aşağıdaki çağrı zinciriyle elde etmektedir:

    task_active_pid_ns ---> ns_of_pid --> task_pid 

    Çağrı zincirindeki fonksiyonların tanımlamalarını veriyoruz:

    struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)
    {
        return ns_of_pid(task_pid(tsk));
    }
    EXPORT_SYMBOL_GPL(task_active_pid_ns);

    static inline struct pid *task_pid(struct task_struct *task)
    {
        return task->thread_pid;
    }

    static inline struct pid_namespace *ns_of_pid(struct pid *pid)
    {
        struct pid_namespace *ns = NULL;
        if (pid)
            ns = pid->numbers[pid->level].ns;
        return ns;
    }

    İşte find_vpid fonksiyonu prosesin içinde bulunduğu pid isim alanının bilgilerini elde ettikten sonra daha genel olan, 
    belli bir pid isim alanındaki XArray ağacında arama yapan find_pid_ns fonksiyonunu çağırmaktadır:

    struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
    {
        return idr_find(&ns->idr, nr);
    }
    EXPORT_SYMBOL_GPL(find_pid_ns);

    Ağaçta arama yapan asıl fonksiyon idr_find isimli fonksiyondur.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    XArray ağaçları üzerinde ekleme, silme arama gibi işlemler için aşağı seviyeli fonksiyonlar bulundurulmuştur. XArray 
    veri yapısının aşağı seviyeli gerçekleştirimi "include/linux/xarray.h" dosyası içerisinde yapılmıştır. XArray ağacı 
    bu dosyadaki xarray isimli yapıyla temsil edilmiştir:

    struct xarray {
        spinlock_t	xa_lock;
        /* private: The rest of the data structure is not to be used directly. */
        gfp_t		xa_flags;
        void __rcu *	xa_head;
    };

    Bu ağaç üzerinde işlem yapan xa_ öneki ile başlayan bir grup fonksiyon vardır:

    void *xa_load(struct xarray *, unsigned long index);
    void *xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);
    void *xa_erase(struct xarray *, unsigned long index);
    void *xa_store_range(struct xarray *, unsigned long first, unsigned long last, void *entry, gfp_t);
    bool xa_get_mark(struct xarray *, unsigned long index, xa_mark_t);
    void xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);
    void xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);
    void *xa_find(struct xarray *xa, unsigned long *index, unsigned long max, xa_mark_t) __attribute__((nonnull(2)));
    void *xa_find_after(struct xarray *xa, unsigned long *index, unsigned long max, xa_mark_t) __attribute__((nonnull(2)));
    unsigned int xa_extract(struct xarray *, void **dst, unsigned long start, unsigned long max, unsigned int n, xa_mark_t);
    void xa_destroy(struct xarray *);

    Bu fonksiyonların tanımlamaları "lib/xarray.c" dosyasında yapılmıştır.

    Güncel çekirdeklerin yukarıda belirttiğimiz aşağı seviyeli fonksiyonlarının yanı sıra aynı zamanda bunları kullanan 
    yüksek seviyeli IDR fonksiyonları da bulunmaktadır. Çekirdek kodları incelendiğinde genellikle bu yüksek seviyeli idr_ 
    önekli fonksiyonların kullanıldığı görülmektedir. Yukarıda da belirttiğimiz gibi bu fonksiyonlar xa_ önekli aşağı 
    seviyeli XArray fonksiyonlarını çağırmaktadır:

    +-------------------------------------------------------+
    |          Cekirdek Kodları ve Aygıt Sürücüler          |
    |   (sürücüler, alt sistemler, idr_* çağrıları yapan)   |
    +-------------------------------------------------------+
                            │
                            ▼
    +-------------------------------------------------------+
    |                  IDR Katmanı (idr_*)                  |
    |   Basit integer→pointer eşlemeleri için sarma fonk.   |
    |                                                       |
    |   Örnekler:                                           |
    |     idr_alloc()   → çağırır xa_alloc()                |
    |     idr_find()    → çağırır xa_load()                 |
    |     idr_remove()  → çağırır xa_erase()                |
    |     idr_replace() → çağırır xa_store()                |
    |     idr_for_each  → çağırır xa_for_each()             |
    +-------------------------------------------------------+
                            │
                            ▼
    +-------------------------------------------------------+
    |                XArray Katmanı (xa_*)                  |
    |   Gerçek veri yapısı: radix ağaç tabanlı,             |
    |   concurrency-aware, lock/RCU destekli.               |
    |                                                       |
    |   Fonksiyonlar: xa_alloc, xa_load, xa_store, xa_erase |
    |                 xa_find, xa_for_each, vb.             |
    +-------------------------------------------------------+
                            │
                            ▼
    +-------------------------------------------------------+
    |         Çekirdek Alt Veri Yapısı / Radix Tree         |
    |   (XArray'in düşük seviye radix implementasyonu)      |
    +-------------------------------------------------------+

    XArray gerçekleştirimi hakkında ileride başka konularda ek bilgiler vereceğiz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz son derslerde task_struct nesnelerinin birbirleriyle bağlantısını ve pid değerleri ile task_struct nesneleri 
    arasındaki ilişkileri gördük. Şimdi dikkatimizi bir süre dosya sistemine ilişkin çekirdek veri yapıları üzerine 
    yönelteceğiz. Bilindiği UNIX/Linux sistemlerinde pek çok kavram kullanıcıya bir dosya gibi gösterilmektedir. Biz bu 
    bölümde belli bir derinliğe kadar çekirdeğin dosya işlemleri için oluşturduğu organizasyon üzerinde duracağız. Daha 
    sonra başka bir bölümde dosya sistemine ilişkin aşağı seviyeli ayrıntıları ele alacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinin "dosya sistemi (file system)" denilen alt sistemlerinin iki tarafı vardır:

    1) Disk tarafı
    2) Bellek Tarafı

    Dosya bilgileri disk üzerindeki bloklarda tutulmaktadır. (Bu bloklara Microsoft dünyasında "cluster" da denilmektedir.) 
    Hangi dosyaların diskin hangi bloklarında tutulduğu, dosyaların metadata bilgilerinin diskte nasıl saklandığı gibi 
    belirlemeler dosya sisteminin disk tarafını, diskteki dosya sisteminin çekirdekteki temsilinin oluşturulması ve 
    işletim sisteminin açılan dosyalar için yaptığı düzenlemeler ise dosya sisteminin bellek tarafını oluşturmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Disk terimi genel bir terimdir. Bir süre önceye kadar disk olarak ağırlıklı biçimde "hard disk" denilen elektromekanik 
    birimler kullanılıyordu. Ancak bir süredir artık disk olarak yarı iletken teknolojiler kullanılarak oluşturulmuş SSD 
    (Solid State Disk) denilen diskler kullanılmaktadır. Bugün ağırlıklı olarak kullandığımız SSD disklerin herhangi 
    bir mekanik parçası yoktur. SSD'ler "NAND Flash" denilen bellek teknolojisini kullanmaktadır. SSD'ler hard disklere 
    göre oldukça hızlıdır. Ancak onların en önemli handikapları belli bir yazma ömrünün olmasıdır. SSD'lerde aynı bölgeye 
    belli sayıdan daha fazla yazma yapıldığında artık SSD'nin o bölgesi bozulabilmektedir. Tabii teknoloji bu bakımdan 
    da ilerleme içerisindedir. SSD teknolojisi ile USB yuvalarına taktığımız flash belleklerin teknolojisi birbirine 
    benzemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Kullandığımız disk birimi ister "hard disk" olsun isterse SSD olsun disk ile bilgisayarımızın RAM'i arasındaki 
    transferler sektör (sector) denilen byte grupları ile yapılabilmektedir. Sektör bir diskten okunabilecek ya da 
    bir diske yazılabilecek en küçük birimdir. Bir sektör genellikle 512 byte'tır. Diskte byte düzeyinde erişim yoktur. 
    Sektörel erişim vardır. Örneğin diskteki bir sektörde bulunan bir byte üzerinde değişiklik ancak şöyle yapılabilmektedir:
    Önce o byte'ın içinde bulunduğu sektör RAM'e okunur. Sonra byte RAM üzerinde değiştirilir. Sonra aynı sektör yeniden 
    diske yazılır.

    Diskteki her sektörün ilk sektör 0 olmak üzere bir mantıksal numarası vardır. Hard disklerde ardışıl numaralı mantıksal 
    sektörler disk üzerinde de fiziksel olarak peşi sıra bulunmaktadır. Mekanik hard disklerde bilgiler "track" denilen 
    yollara yazılmaktadır. Ardışıl sektörler aynı track'te bulunurlar. Dolayısıyla hard disklerde diskin kafası bir 
    kez konumlandırıldığında ardışıl sektörlere daha hızlı okuma yazma yapılabilmektedir. SSD'ler mekanik öğe barındırmadığı
    için rastgele erişimlidir. Yani her sektörden okuma eşit hızda yapılma ve her sektöre yazma eşit hızda yapılmaktadır.

    Modern bilgisayar sistemlerinde disk birimine doğrudan erişilmez. Disk erişimlerinde bu işleme aracılık eden ismine 
    "disk denetleyicisi (disk controller)" denen yerel bir işlemcinden faydalanılmaktadır. Yani sistem programcısı ya da 
    işletim sistemlerini yazanlar disk denetleyicisini programlar, disk denetleyicisi isteği elektriksel olarak disk 
    birimine iletir, okuma yazma işlemleri de disk birimi tarafından yapılır:

    ┌─────────────────┐
    │ İşletim Sistemi │
    └───────┬─────────┘
            │
            ▼
    ┌───────────────┐
    │   CPU / RAM   │
    └───────┬───────┘
            │
            ▼
    ┌─────────────────┐
    │ Disk Denetleyici│
    │  (Controller)   │
    └───────┬─────────┘
            │
            ▼
    ┌───────────────┐
    │     Disk      │
    │ (HDD / SSD)   │
    └───────────────┘

    Bugün PC'lerde SATA ve NVMe denetleyicileri en çok kullanılan disk denetleyicileridir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
										    22. Ders 28/09/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki disk denetleyicisi işletim sistemi tarafından "falanca sektörleri oku" ya da "falanca sektörlere yaz" biçiminde 
    programlandıktan sonra aktarım nasıl yapılmaktadır? Aktarım CPU tarafından tek tek byte'ların denetleyiciden alınarak 
    RAM'e yerleştirilmesi yoluyla yapılmamaktadır. (Çok eskiden ilk PC mimarisinde aktarım böyle de yapılabiliyordu.) Çünkü 
    CPU'nun bu işle meşgul olması önemli bir zaman kaybı oluşturmaktadır. Bu tür disk ile RAM arasındaki aktarımlar için 
    "DMA (Direct Memory Access)" denilen yardımcı denetleyiciler kullanılmaktadır. Tipik olarak CPU'da çalışan kod (yani 
    işletim sistemi) disk denetleyicisine transfer isteğini ve aktarımda kullanılacak bellek alanlarının aresini bildirir. 
    disk denetleyicisi de disk birimini ve DMA'yı elektirksel düzeyde programlayarak aktarımın DMA üzerinden doğrudan RAM'e 
    yapılmasını sağlar. Aktarım sırasında artık CPU bu işle meşgul olmaz, işletim sistemi de CPU'yu başka bir thread'i 
    çalıştırması için bağlamsal geçişe (task switch) sokar. Tabii aktarım işlemi bittiğinde disk denetleyicisi CPU'yu bir 
    donanım kesmesi yoluyla durumdan haberdar etmektedir. Yani disk ile RAM arasındaki aktarım işlemleri tipik olarak şöyle 
    yapılmaktadır:

    1) İşletim sistemi disk denetleyicisine aktarılacak sektörlere ilişkin bilgileri ve transfer adreslerini CPU yoluyla 
    elektriksel olarak iletir.
    2) Okuma söz konusuysa disk denetleyicisi disk birimine elektriksel düzeyde komutlar göndererek sektörlerin okunmasını 
    ve DMA yoluyla bunların RAM'de uygun yerlere aktarılmasını sağlar. Eğer yazma söz konusuysa RAM'de belirtilen adresteki 
    bilgiler yine DMA yoluyla disk birimine iletilerek yazma gerçekleştirilir.
    3) Aktarım işlemi bittiğinde disk denetleyicisi bir donanım kesmesi yoluyla CPU'yu durumdan haberdar eder.
    4) CPU aktarım için gereken kodları çalıştırdıktan sonra aktarım bitene kadar meşgul bir döngüde beklemez.
    Başka thread'ler çalıştırılabiliyorsa "bağlamsal geçiş (context switch)" yapılarak CPU'nun boşta kalması engellenir.

    Yukarıdaki süreci bir diyagramla aşağıdaki gibi özetleyebiliriz:

     ┌──────────────┐        ┌──────────────────┐        ┌─────────────────────┐
     │   Uygulama   │        │  İşletim Sistemi │        │ Blok Aygıt Sürücüsü │
     └──────┬───────┘        └─────────┬────────┘        └────────┬────────────┘
            │                          │                          │
            │   read()/write() isteği  │                          │
            └────────────────────────► │                          │
                                       │   I/O isteği hazırla     │
                                       └────────────────────────► │
                                                                  │
                                                                  │ Komutu
                                                                  │ denetleyiciye
                                                                  │ gönder
                                                                  ▼
                                                        ┌────────────────────┐
                                                        │ Disk Denetleyicisi │
                                                        └───┬───────────┬────┘
                                                            │           │
                                        DMA tabloları ◄───┘             │
                                                            │           │
                                                Kesme (IRQ) ◄───────────┘
                                                            │
                                            ┌───────────────┴────────────────┐
                                            │         Disk Donanımı          │
                                            │ (HDD: kafa hareketi, SSD: FTL) │
                                            └────────────────────────────────┘

    Kullandığımız masaüstü bilgisayarlarda zaman içerisinde DMA denetleyicileri de geliştirilmiştir. Eskiden Intel 
    tabanlı PC mimarisinde ISA bus kullanıldığı zamanlarda tek bir merkezi DMA denetleyicisi (Intel 8237) vardı. 
    Ancak daha sonra Intel tabanlı PC mimarisinde PCI bus kullanılmaya başlanmasıyla birlikte artık transfer 
    yapabilen her donanım birimi kendi DMA denetleyicisini de içermeye başladı. Bugün Intel tabanlı ve Apple Silicon 
    tabanlı bilgisayar mimarilerinde disk denetleyicisi kendi içerisindeki DMA denetleyicisini programlayarak transferi 
    gerçekleştirmektedir. Disk denetleyicilerinin programlanması ise artık uzunca bir süredir "bellekten tabanlı IO 
    (memory-mapped IO)" tekniği ile yapılmaktadır.

    Hard disklerde disk birimi içerisinde bir önbellek (cache) de bulundurulmaktadır. Böylece disk denetleyicisi aynı 
    sektörleri disk biriminden istediği zaman disk birimi eğer daha önce ilgili sektörler önbellek içerisindeyse kafa 
    hareketleri yapmadan onları doğrudan kendi önbelleğinden vermektedir. Bugün örneğin 1 TB'lık hard disklerde 64MB, 
    128, 256 MB civarında önbellek kullanılmaktadır. SSD'ler de bir önbellek sistemi vardır. Bu önbellek sistemi 
    özellikle yazma işlemlerinde hız kazancı sağlamakta ve aynı sektörlere sürekli yazım yapıldığında o bölgenin 
    yıpranmamasını (wear leveling) sağlamaktadır. Tabii bu ö önbellek stemleri tamamen disk birimleri tarafından içsel 
    olarak işletilmektedir. Bu önbellek sistemleri işletim sistemleri tarafından erişilebilir değildir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi bir disk biriminde transfer edilecek en küçük birime sektör denilmektedir. Bir sektör 
    tipik olarak 512 byte uzunluğundadır. Ancak aslında sektör uzunlukları da disk üreticilerine bağlı olarak değişebilir 
    bir niceliktir. 512 byte sektör uzunlukları bugün için standart bir uzunluktur. Tabii zaman geçtikçe diskler büyüdüğü 
    için sektör uzunluklarının da büyüyebileceğini söylemek istiyoruz. Nitekim 4K uzunluğunda sektörlere sahip olan diskler 
    özellikle büyük sistemlerde gittikçe yaygınlaşmaktadır. Disk birimi her sektöre ilk sektör 0 olmak üzere mantıksal 
    bir numara vermektedir. Yani adeta disk üzeründeki her sektörün bir adresi vardır. Disk denetleyicisi disk birimine 
    transfer edilecek sektörlerin numaralarını elektriksel düzyde iletmektedir. (Bu biçimde mantıksal sektör numaraları 
    kullanılmadan önce 80'lerde ve 90'ların ilk yarısında sektörlerin yerleri "fiziksel koordinat sistemi" denilen "hangi 
    yüz (head)", "hangi track", "hangi sektör dilimi" biçiminde üç parametreyle belirtiliyordu.)

    Sektör kavramı aslında dosya sistemleri için küçük bir depolama birimidir. İşletim sistemleri bir dosyanın parçası 
    olabilecek minimum disk alanı için sektör yerine "blok (block)" ya da "cluster" denilen daha büyük birimleri kullanmaktadır. 
    Blok terimi daha çok UNIX/Linux sistemlerinde kullanılmaktadır. Microsoft ise blok yerine "cluster" terimini kullanmaktadır. 
    Bir blok ardışıl n tane sektörden oluşmaktadır. Uygulamada bu n değeri 2'nin bir kuvveti olur. Ardışıllık hard disklerde 
    önemli bir unsurdur. Çünkü hard disklerde en önemli zaman kaybı mekanik bir birim olan disk kafasının track hizasına 
    çekilmesinde yaşanmaktadır. Disk kafası track hizasına çekildiğinde disk dönerken artık ardışıl sektörler hiç kafa 
    hareketi yapılmadan okunup yazılabilmektedir. Peki neden işletim sistemi dosyalar söz konusu olduğunda bir dosyanın 
    parçası olabilecek en küçük birim için sektör değil de ardışıl n tane sektör kullanmaktadır? İşte bunun birkaç nedeni 
    vardır:

    1) Dosyaların parçaları disk üzerinde ardışıl yerlerde olmak zorunda değildir. Dosyaların parçaları üzerinde yayılmış 
    bir biçimde bulunabilmektedir. Eğer dosyalar çok fazla parçadan oluşursa hard disklerde (ve kısmen de olsa SSD'lerde 
    de) bu parçalar disk üzerinde daha fazla yayılmış olur, bunlara erişmek için gereken zaman artar.

    2) Eğer dosyanın parçaları sektör gibi küçük birimlerden oluşsaydı bu parçaların diskteki yerlerine ilişkin "meta 
    data" tabloları büyürdü. Bu da hem disk alanını hem de işletim sisteminin bellekte yaptığı düzenlemede alan verimsizliği 
    oluştururdu.

    3) CPU'ların kullandığı sayfalama mekanizmasında genellikle 4K uzunluklar kullanılmaktadır. Dosya parçalarının 4K 
    uzunluğun katlarında olması dosya sistemi ile sayfalama sistemi arasında daha iyi bir uyumun ortaya çıkmasına yol 
    açmaktadır. Bu uyum da dolaylı biçimde performans artışı sağlamaktadır.

    Peki bu durumda işletim sistemleri blok denilen dosyanın parçası olabilecek en küçük birim için hangi uzunluğu 
    kullanmaktadır? İşte genellikle bu karar disk formatlanırken diskin (disk bölümünün) büyüklüğüne bakılarak verilmektedir. 
    Dosyaların son bloklarında kalan kullanılmayan alanların oluşmasına "içsel bölünme (internal fragmentation)" 
    denilmektedir. Küçük disklerde (disk bölümlerinde) içsel bölünmenin etkisi daha büyük olacağından blokların 1K gibi 
    küçük uzunluklarda alınması uygun olabilir. Ancak orta büyüklükte disklerde içsel bölünmenin etkisi göreli olarak 
    azalacağı için bloklar 4k gibi bir değerde seçilebilmektedir. Büyük disklerde ise 8K, 16K blok büyüklükleri tercih 
    edilmektedir. Aslında blok büyüklükleri ilgili disk bölümü formatlanırken (Linux sistemlerinde mkfs.xxx programlarıyla 
    formatlama yapılmaktadır) belirlenmektedir. Yani kullanıcı isterse kendisi bu programda kendi tercih ettiği blok 
    uzunluğunu kullanabilir. Ancak kullanıcılar genellikle böyle bir belirleme yapmazlar. Bu durumda bu programlar disk 
    bölümünün büyüklüğüne bağlı olarak yukarıda açıkladığımız gibi uygun bir blok büyüklüğünü seçerler.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    UNIX/Linux sistemlerinde dosya sistemi için tek bir kök vardır. Blok aygıtları (örneğin hard diskler, flash bellekler 
    vs) belli bir dizine mount edilmektedir. Mount işlemi bir dizin üzerine uygulanır. Mount işlemi sonucunda o dizinin 
    içeriği görünmez, artık mount edilen dosya sisteminin kök dizini mount dizininde gözükür. Dolayısıyla bu sistemlerde 
    aslında farklı dizinler farklı blok büyüklüklerine ilişkin dosya sistemlerinin içerisinde olabilmektedir. Anımsanacağı 
    gibi stat POSIX fonksiyonu ya da komut satırından uygulanan "stat" komutu belli bir dosyanın bilgilerini verirken 
    o dosyanın içinde bulunduğu dosya sisteminin blok uzunluğunu da vermektedir. Linux sistemlerinde bu bilgi doğrudan 
    dosya sistemine ilişkin blok aygıtı üzerinde dumpe2fs programıyla da elde edilebilmektedir. Windows sistemlerinde 
    de "cluster" adı altında blok sistemi kullanılmaktadır. O sistemlerde blok uzunluklarını "chkdsk" programı ile ya 
    da "fsutil" programı ile komut satırından elde edebilirsiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri işlemlerini kolaylaştırmak için her bloğa bir numara da vermektedir. Örneğin bir bloğun 4K (tipik 
    olarak 8 sektör) olduğunu düşünelim. Artık işletim sistemi için ilgili disk (aslında disk bölümü ya da genel olarak 
    blok aygıtı da diyebiliriz) bloklardan oluşmaktadır. Örneğin diskin (disk bölümünün) ilk 8 sektörü artık 0'ıncı bloktur. 
    Sonraki 8 sektöre 1'inci bloktur. İşletim sistemi içsel olarak artık ilgili diski bloklardan oluşan ve her bloğun bir 
    numarasının olduğu mantıksal bir depolama alanı gibi ele almaktadır. Yani işletim sistemi için yalnızca sektörlerin 
    değil aynı zamanda dosya sistemine ilişkin blokların da numaraları vardır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemleri son okunan ya da yazılan disk bloklarını RAM'de bir önbellek sisteminde saklamaktadır. Bu önbellek 
    (cache) sisteminde genel olarak "işletim sisteminin disk önbellek sistemi" denilmektedir. Linux dünyasında eskiden bu 
    önbellek sistemine "buffer cache" deniliyordu. Sonra bu önbellek sistemi iyileştirildi ismi de "page cache" olarak 
    değiştirildi. İşletim sistemlerinin bu disk önbellek sistemleri disk erişimini ciddi boyutta azaltmakta ve sistem 
    performansı üzerinde en önemli olumlu etkilerden birini oluşturmaktadır. Eğer işletim sistemlerinde böyle bir disk 
    önbellek sistemi olmasaydı sistemler çok yavaş çalışırdı.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz UNIX/Linux sistemlerinde bir dosyadan okuma yapmak için ya da bir dosyaya yazma yapmak için read/write POSIX 
    fonksiyonlarını kullanmış olalım. Bu fonksiyonlar çekirdek içerisindeki sys_read ve sys_write sistem fonksiyonlarını 
    çalıştırmaktadır. İşletim sistemi bellek tarafında yaptığı organizasyonla okunacak ya da yazılacak bilginin ilgili 
    blok aygıtının (disk bölümünün) kaç numaralı bloğuna ve sektörüne ilişkin olduğunu belirleyebilmektedir. Ancak işletim 
    sisteminin sys_read ve sys_write gibi sistem fonksiyonları hemen diske yönelmez. Bu fonksiyonlar önce dosyanın ilgili 
    bölümünün RAM'de oluşturulmuş bir önbellek sistemi içerisinde olup olmadığına bakmaktadır. Eğer ilgili bölüm bu önbellek 
    sisteminin içerisinde varsa bu fonksiyonlar diske hiç erişmeden dolayısıyla da hiç bloke olmadan bu okuma yazma işlemini 
    gerçekleştirmektedir. Eğer dosyadan okunacak ya da dosyaya yazılacak kısım RAM'de oluşturulmuş olan bu önbellek sisteminde 
    yoksa bu durumda gerçek disk okuması ya da yazması izleyen paragrafta açıklayacağımız biçimde yürütülmektedir. Linux 
    sistemlerinde bu önbellek sistemine eskiden "buffer cache" dendiğini ancak sonraları isminin "page cache" olarak 
    değiştirildiğini belirtmiştik. Biz artık Linux temelli anlatımlarımızda bu önbellek sistemine "sayfa önbelleği 
    diyeceğiz. Çizimlerde doğrudan İngilizce "page cache" ismini de kullanacağız.

    read/write POSIX fonksiyonları çağrıldığında yapılan işlemleri aşağıdaki şekille özetleyebiliriz:

    ┌─────────────────────────────────────┐
    │    read/write POSIX Fonksiyonları   │
    └──────────────┬──────────────────────┘
                   │
                   ▼
    ┌──────────────────────────────┐
    │    sys_read / sys_write      │
    └──────────────┬───────────────┘
                   │
                   ▼
    ┌──────────────────────────────┐
    │   Page Cache'te bilgi var?   │
    └───────┬─────────────┬────────┘
            │             │
        Evet           Hayır
            │             │
            ▼             ▼
    ┌──────────────┐  ┌─────────────────────────┐
    │  Veriyi      │  │ Gerçek disk I/O başlat  │
    │  kopyala     │  │  (okuma / yazma)        │
    └──────────────┘  └───────────┬─────────────┘
                                  │
                                  ▼
                        ┌────────────────────────┐
                        │  Page Cache güncellenir│
                        └────────────────────────┘
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki dosyadaki okunacak ya da yazılacak kısım sayfa önbelleğinde (page caceh) yoksa gerçek transfer nasıl yapılmaktadır? 
    İşte işletim sistemlerinde bu tür transferler aslında başka bir birime devredilmektedir. Linux sistemlerinde bu 
    transferlerin yapıldığı birime "blok aygıt sürücüleri (block device drivers)" denilmektedir. Bir Linux sistemi 
    kurulduğunda zaten temel disk denetleyicileri üzerinden transfer yapabilen blok aygıt sürücüleri çekirdeğin içerisine 
    gömülmüş durumda olur. Ancak sistem programcısının kendisi de blok aygıt sürücüleri yazabilir. Örneğin bir 
    gömülü Linux sisteminde yeni bir SD kart birimi için bir blok aygıt sürücüsü yazmak zorunda kalabilirsiniz. Blok 
    aygıt sürücülerinin yazımı aygıt sürücülerinin yazılmasına ilişkin konuların bir bölümünü oluşturmaktadır.

    Aslında işletim sistemi sayfa öneblleğinde (page cache) olmayan kısımların transfer edilmesini blok aygıt sürücülerinden 
    istemektedir. Disklere ilişkin (bunlara genel olarak blok aygıtları denilmektedir) birtakım okuma yazma işlemleri 
    aslında işletim sistemleri tarafından çizelgelemeye sokulmaktadır. Çünkü çok sayıda farklı proses aynı disk sektörlerini 
    okuyacak ya da o sektörlere yazacak olabilir. İşletim sistemi bu nedenle hemen IO isteğini blok aygıt sürücüsüne 
    göndermez. Önce onları sıraya dizer, mümkünse birleştirir, bu biçimdeki iyileştirme işleminden sonra istekleri blok 
    aygıt sürücüsüne gönderir. Bu sürece işletim sistemlerinin "IO çizelgelemesi (IO scheduling)" denilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        23. Ders 04/10/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    O halde bir dosya okuması ya da yazması sonucunda gelişen olayları şöyle özetleyebiliriz:

    1) Kullanıcı modunda çalışan program (yani proses) read ya da write POSIX fonksiyonlarını çağırır. (UNIX/Linux 
    sistemlerindeki C derleyicilerinin standart dosya fonksiyonları da eğer okunacak ya da yazılacak kısım kendi tamponlarında 
    yoksa zaten bu POSIX fonksiyonlarını çağırmaktadır.)
    
    2) read ve write POSIX fonksiyonları Linux'ta sys_read ve sys_write isimli sistem fonksiyonlarını çağırır. Artık 
    akış kullanıcı modundan (user mode) çekirdek moduna (kernel mode) geçmiştir. Çekirdekteki kodlar çalışmaktadır.

    3) sys_read ve sys_write fonksiyonları önce okunacak ya da yazılacak yerin Linux'un' disk önbellek sistemi olan "sayfa 
    önbelleğinde ("page cache", eski ismiyle "buffer cache") sisteminde olup olmadığına bakar. Eğer ilgili disk blokları 
    sayfa önbelleğinde varsa akış hiç bloke olmadan sayfa önbelleği içerisinden karşılanır. Ancak eğer söz konusu bloklar 
    sayfa önbelleğinde yoksa bu durumda Linux çekirdeği isteği "IO çizelgeleyicisi (IO scheduler)" denilen çekirdek birimine 
    iletir. read/write çağrısını yapan thread bloke edilir.

    4) IO çizelgeleyicisi istekleri çizelgeler. Okuma işlemi söz konusuyse sayfa önbelleğinde transfer edilecek önbellek 
    bloklarını tahsis eder. Yazma işlemi söz konusuysa diske transfer edilecek önbellek bloklarını belirler. (Bu konuda 
    bazı ayrıntılar da vardır.) Blok aygıt sürücüsüne transfer edilecek sektörleri iletir. 

    5) Gerçek transfer işlemi blok aygıt sürücüsü tarafından yapılmaktadır. İşletim sistemi blok aygıt sürücüsüne "hangi 
    sektörlerin sayfa önbelleğindeki hangi adreslere transfer edileceğini ya da sayfa önbelleği içerisindeki hangi adresteki
    bilgilerin diskteki hangi sektörlere transfer edileceğini" bir kuyruk sistemi yardımıyla iletmektedir. 
    
    6) Blok aygıt sürücüsü diskten istenen sektörleri sayfa önbelleği içerisinde belirtilen adrese ya da sayfa önbelleğinde
    belirtilen adresteki bilgileri diskin belirtilen sektörlerine transfer eder. 

    7) Artık okuma söz konusuysa okunan bilgi sayfa önbelleği içerisindedir. İşlemi başlatan thread'in blokesi çözülür. 
    sys_read sistem fonksiyonu bunu sayfa önbelleği içerisinden programcının kullanıcı modundaki adresine kopyalar.

    Tabii bugün kullandığımız Linux sistemlerinde aslında transfleri yapan blok aygıt sürücüleri zaten çekirdek imajı 
    içerisine gömülmüş bir biçimde bulunmaktadır. Ancak nadiren de olsa sistem programcısının yeni birtakım aygıtlar için 
    blok aygıt sürücüleri yazması gerekebilmektedir. 

    Linux sistemlerinde yukarıda özetlediğimiz olaylar silsilesi zaman içerisinde değişikliklere uğratılarak ve sürekli
    geliştirilerek bugünkü durumuna getirilmiştir. 

    Yukarıda maddeler halinde açıkladığımız süreci bir şekille de özetleyebiliriz:

    ┌───────────────────────────────────────────────────┐
    │ Kullanıcı Modu (User Mode)                        │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 1) Program read() / write() çağırır               │
    │ (veya stdio tamponları dolduysa/boşaldıysa)       │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 2) POSIX fonksiyonları sys_read / sys_write       │
    │    sistem çağrılarını çağırır                     │
    │    → Geçiş: User Mode ➜ Kernel Mode              │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 3) sys_read / sys_write, Page Cache'i             │
    │    (disk cache) kontrol eder:                     │
    │  - Veri Page Cache'te varsa:                      │
    │    → Bloke olmaz, doğrudan bellekten okunur veya  │
    │      yazılır                                      │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 4) Bilgi Page Cache'te yoksa:                     │
    │    → İstek I/O çizelgeleyicisine gönderilir,      │
    │     thread bloke edilir                           │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 5) I/O Çizelgeleyicisi isteği çizelgeler          │
    │    - Page Cache içinde yer ayırır                 │
    │    - Blok aygıt sürücüsüne iletir                 │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 6) Blok aygıt sürücüsü                            │
    │    - Hangi sektörlerin okunacağını/               │
    │      yazılacağını bilir                           │
    │    - Page Cache adresleriyle eşleştirir           │
    │    - İsteği donanıma iletir                       │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 7) Gerçek transfer donanım tarafından yapılır     │
    │ (ör. disk → page cache veya tersi)                │
    │ - Tamamlanınca çekirdeğe kesme gönderir           │
    │ - Bilgi artık page cache içindedir                │
    │ - İşlemi başlatan thread'in blokesi çözülür       │
    └───────────────────────────────────────────────────┘
                            │
                            ▼
    ┌───────────────────────────────────────────────────┐
    │ 8) Kernel Mode → User Mode dönüşü                 │
    │ - Okuma tamamlandıysa veri kullanıcıya kopyalanır │
    │ - Yazma tamamlandıysa geri dönüş yapılır          │
    └───────────────────────────────────────────────────┘
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Buradaki süreçte yazma olayı söz konusu olduğunda bazı ayrıntılar da devreye girmektedir. Kullanıcı modundaki 
    program write POSIX fonksiyonunu çağırıp bu fonksiyon da sys_write fonksiyonunu çağırdığında bu sistem fonksiyonu 
    yazılmak istenen bilgiler diske yazılana kadar write işlemini yapan thread'i bloke etmez. Yazma işlemi her zaman 
    Linux'un RAM'deki sayfa önbelleğine yapılmaktadır. sys_write fonksiyonu yazmayı sayfa önbelleği içerisine yaptıktan 
    sonra hemen "başarılı" olarak geri dönmektedir. Yani gerçek transferi beklememektedir. Bu bakımdan write işlemi
    aslında seknron yapılmamaktadır. Sistem programlama terminolojisinde IO işlemlerinde "senkron" terimi "fonksiyon geri 
    döndüğünde tüm işlemlerin yapılıp bitmiş olması" anlamına gelmektedir. Asenkron terimi ise "işlemin başlatılması, 
    fonksiyonun geri dönmesi ancak işlemin aslında arka planda devam etmesi" anlamına gelmektedir. Görüldüğü gibi 
    modern işletim sistemlerinde diske yazma işlemi aslında senkron bir işlem değildir. Çünkü gerçek aktarım yapılmadan 
    yazan fonksiyon hemen başarıyla geri dönmektedir. 

    Peki yazma işleminde sayfa önbelleğine yazılan bilgiler çekirdek tarafından ne zaman gerçek aygıta aktarılmakatdır? 
    İşte işletim sistemi kasten bu tür durumlarda araya belli bir gecikme koymaktadır. Böylece peşi sıra yapılan write 
    işlemlerinin tek tek gereksiz biçimde aygıta yapılması engellenir, bunlar biriktirilerek ve çizelgelenerek blok aygıt 
    sürücüsüne aktarılır. Bu biçimdeki aktarmaya işletim sistemleri dünyasında "gecikmeli yazım (delayed write)" da denilmektedir. 
    Tabii işletim sisteminin IO çizelgleyicisi çizelgelemeyi yaptıktan sonra yukarıda da belirttiğimiz gibi transferi kendisi 
    yapmamaktadır. Transfer blok aygıt sürücüsüne istek olarak gönderilir. Transferden asıl sorumlu birim blok aygıt 
    sürücüleridir. 

    Buradaki süreci aşağıdaki gibi özetleyebiliriz:

    Kullanıcı Modundayız
            │
            ▼
            write()
            │
            ▼
            sys_write()  (Çekirdek moduna geçiliyor) ───────────────┐
            │                                                       │
            ▼                                                       │
            Page Cache'e kopyala                                    │
            (sayfa "kirlenmiş (dirty)" olarak işaretlenir)          │
            │                                                       │
            ▼                                                       │
            Geri dönüş (user mode) ◄────────────────────────────────┘
            │
            ▼
            (Asenkron olarak)
            Flusher thread → I/O cizelgeleyici → Blok aygıt sürücüsü → Disk denetleyicisi → Disk donanımı

    Peki işletim sistemi transfer işlemlerini ne kadar süre bekletmektedir? Eğer transfer çok uzun süre bekletilirse 
    elektrik kesilmesi gibi durumlarda kayıplar fazlalaşır. İşte modern işletim sistemlerinde kirlenmiş sayfaların flush 
    edilmesi "çekirdek thread'leri (kernel threads)" tarafından yapılmaktadır. Örneğin Linux sistemlerinde bu işlemlerden 
    "flush" isimli çekirdek thread'leri sorumludur. Eskiden Linux çekirdeklerinin 2.6.32 versiyonuna kadar bu işlemler 
    "pdflush" isimli tek bir çekirdek thread tarafından yapılıyordu. Bu versiyondan sonra artık her blok aygıt sürücüsü 
    için (disk bölümü için değil, diskin bütünü için) ayrı bir flush thread'i oluşturulmaya başlandı. Bu thread'leri 
    komut satırında aşağıdaki gibi görüntüleyebilirsiniz:

    $ s -aux | grep flush
    
    flush thread'leri arka planda sürekli olarak sayfa önbelleğini izler. Orada "kirlenmiş (dirty)" olan sektörleri 
    ilgili blok aygıt sürücüsüne gönderir. Peki bu işleyişte yazma gecikmesi takriben kaç saniye civarında olmaktadır? 
    Aslında bu gecikme süresi başka faktörlere de bağlı olarak değişebilmketedir Burada fikir vermek amacıyla istersek 
    modern Linux sistemleri için bu sürenin ortalama 5 saniye civarında olduğunu söyleyebiliriz. Ancak bu değerler de 
    değiştirilebilmektedir. flush thread'lerinin parametreleri hakkında aşağıda tabloda özet bir bilgi veriyoruz:

    ┌────────────────────────────────┬────────────────────┬─────────────────────────────────────────────────────────────┐
    │ Parametre                      │ Varsayılan değer   │ Anlamı                                                      │
    ├────────────────────────────────┼────────────────────┼─────────────────────────────────────────────────────────────┤
    │ dirty_writeback_centisecs      │ 500                │ Flusher thread’in periyodik olarak çalıştığı aralık         │
    │                                │                    │ (santi saniye cinsinden). 500 cs = 5 saniye.                │
    │                                │                    │ Bu aralıkta çekirdek "dirty" sayfaları kontrol eder.        │
    ├────────────────────────────────┼────────────────────┼─────────────────────────────────────────────────────────────┤
    │ dirty_expire_centisecs         │ 3000               │ Bir "dirty" sayfa, en fazla bu kadar süre (santi saniye     │
    │                                │                    │ cinsinden) RAM’de kalabilir. 3000 cs = 30 saniye sonra      │
    │                                │                    │ “süresi dolmuş” sayılır ve flush edilir.                    │
    ├────────────────────────────────┼────────────────────┼─────────────────────────────────────────────────────────────┤
    │ dirty_ratio /                  │ %20 / %10 civarı   │ RAM’in ne kadarı "dirty" sayfalarla dolarsa flush işleminin │
    │ dirty_background_ratio         │                    │ başlatılacağını belirler (bellek baskısı durumunda          │
    │                                │                    │ zaman beklenmez).                                           │
    └────────────────────────────────┴────────────────────┴─────────────────────────────────────────────────────────────┘

    Bu değerler proc dosya sisteminden görüntülenebilmektedir. Örneğin:

    $ cat /proc/sys/vm/dirty_writeback_centisecs
    $ cat /proc/sys/vm/dirty_expire_centisecs

    "sysctl" komutu ile de bu değerler değerler aşağıdaki örnekte gibi de değiştirilebilmektedir:

    $ sudo sysctl -w vm.dirty_writeback_centisecs=100 

    "sysctl" komutu zaten kendi içerisinde "/proc/sys" dizinindeki dosyalar üzerinde güncelleme işlemleri yamaktadır.

    flush thread'lerinin çalışması daha aytıntılı olarak "sayfa önbelleği (page cache)" konusunun ele alındığı bölümde 
    açıklanacaktır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki "gecikmeli yazım (delayed write)" işleminin gerekçeleri nelerdir? Yukarıda da belirttiğimiz gibi en önemli
    gerekçe peşi sıra yapılan yazma işlemlerinin tek hamlede aygıta yansıtılmasıdır. Bu sayede yazma işlemini yapan 
    thread bloke olmaz ve toplamda bu işlemler paralel yürütüldüğü içim sistem performasnı yükselir. Aynı zamanda flash 
    belleklerde ve SSD'lerde bu gecikme aynı zamanda sürekli yazım sonucunda belleğin eskimesini de kısmen engellemektedir.
    (Tabii "bu eskime" sorunu aslında asıl olarak flash belleklerdeki ve SSD içerisindeki önbellekler yardımıyla
    azaltılmaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında sayfa önbelleğinde "kirlenmiş (dirty)" hale gelmiş olan sayfalar bazı durumlarda işletim sisteminin "tazaleyici
    ("flusher") thread'lerini beklemeden de diske aktarılabilmektedir. Örneğin bir dosya kapatıldığında artık bu işlem 
    arka planda dosyanın kirlenmiş sayfalarının da diske yazılmasına yol açmaktadır. UNIX/Linux sistemlerinin çoğunda bazı 
    özel sistem fonksiyonları yoluyla ya da open fonksiyonundaki bayraklarla da bu duruma müdahale edilebilmektedir. Örneğin 
    sync isimli POSIX fonksiyonu çağrıldığında o anda dosya sistemine ilişkin "kirlenmiş (dirty olan)" olan sayfaların 
    hepsinin flush edilmesini sağlamaktadır:

    #include <unistd.h>

    void sync(void); 

    sync fonksiyonu asenkron biçimde çalışmaktadır. Yani fonksiyon geri döndüğünde tüm blokların flush edilmiş olma garantisi
    yoktur. fsync POSIX fonksiyonu ise belli bir dosyaya ilişkin kirlenmiş sayfaların flush edilmesi için kullanılmaktadır:

    #include <unistd.h>

    int fsync(int fildes); 

    fsync fonksiyonu senkronize (synchronous) çalışmaktadır. Yani fonksiyon geri döndüğünde sayfa önbelleğindeki kirlenmiş 
    sayfaların flush edilmiş olması garanti edilmektedir. 

    Bir dosya açılırken open POSIX fonksiyonunda kullanılan konuyla ilgili üç bayrak vardır. Konuyla ilgili olduğu için 
    bu bayrakların da işlevlerini burada açıklamak istiyoruz.

    O_DSYNC Bayrağı: Bu bayrak POSIX'in "Base Definitions" bölümündeki "Synchronized I/O Data Integrity Completion" 
    başlığında açıklanan yazma koşullarının sağlanacağını belirtmektedir. Bu bayrak kullanıldığında aşağıdaki iki durumun 
    çekirdek tarafından sağlanması garanti edilmektedir:

    - Dosyaya yazdırılan bilgilerin write fonksiyonu geri döndüğünde hedefe transfer edilmiş olması.
    - Yazılan bilginin dosyadan okunabilmesi için gereken metadata bilgilerinin hedefe transfer edilmiş olması. 
    (Tüm metadata bilgilerinin hedefe transfer edilmiş olması gerekmemektedir. Yazılanların okunabilmesi için gerekli 
    metadata bilgilerinin transfer edilmesi yeterlidir.)

    O_SYNC Bayrağı: Bu bayrak POSIX'in "Base Definitions" bölümündeki "Synchronized I/O File Integrity Completion" 
    başlığında açıklanan koşulların sağlanacağını belirtmektedir. O_SYNC bayrağı O_DSYNC bağrağını kapsamaktadır. 
    Fakat bu bayrak write fonksiyonu geri dönmeden önce tüm metadata bilgilerinin hedefe transfer edilmiş olmasını 
    zorunlu tutmaktadır.
    
    O_RSYNC: Bu okuma işlemi ile ilgilidir. Tek başına değil O_DSYNC ya da O_SYNC bayraklarıyla birlikte kullanılır. 
    Eğer O_RSYNC bayrağı O_DSYNC bayrağı ile birlikte kullanılırsa read işlemini etkileyecek olan daha önce yapılmış 
    write işlemleri varsa read fonksiyonu geri dönmeden önce bu write işlemleri için O_DSYNC bayrağında belirtilen semantik 
    uygulanmaktadır. Eğer bu bayrak O_SYNC ile birlikte kullanılırsa read işlemini etkileyecek olan daha önce yapılmış 
    write işlemleri varsa read fonksiyonu geri dönmeden önce bu write işlemleri için O_SYNC bayrağında belirtilen semantik 
    uygulanmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki C'nin standart dosya fonksiyonları bu süreçte nerede yer almaktadır? C'nin dosya fonksiyonları aslında neticede
    POSIX fonksiyonlarını çağırmaktadır. Yukarıda da belirtitğimiz gibi POSIX fonksiyonları da sistem fonksiyonlarını 
    çağırıp çekirdek içerisindeki kodların çalışmasını sağlamaktadır. Ancak diğer kurslardan da anımsayacağınız gibi C'nin 
    standart dosya fonksiyonları işletim sisteminin okuma yazma fonksiyonlarını daha az çağırmak için "kullanıcı alanında 
    (user space)" her dosya için bir önbellek de oluşuturmaktadır. Bu önbellek sistemine genellikle önbellek yerine 
    "tamponlama (buffering)" sistemi, burada kullanılan önbelleğe de "tampon (buffer)" denilmektedir. Örneğin Linux'ta 
    biz C'nin getc gibi dosya fonksiyonunu çağırmış olalım. Standart C kütüphanesi fgetc ile 1 byte okumak istediğimizde 
    read POSIX fonksiyonu ile 1 byte okumamaktadır. read POSIX fonksiyonu ile <stdio.h> dosyasında belirtilen BUFSIZ kadar
    byte'ı bir tampona okumakta ve oradan 1 byte'ı programcıya vermektedir. Böylece sonradan okunanacak byte'lar için hiç 
    read fonksiyonu çağrılmayacak ve istek hemen bu tampondan karşılanacaktır. Aynı durum yazma için de söz konusudur. 
    Bu nedenle C'nin standart fonksiyonlarına "tamponlı IO (buffered IO)" fonksiyonları da denilmektedir. Buradaki önbellek 
    sisteminin POSIX fonksiyonlarını dolayısıyla da sistem fonksiyonlarını daha az çağırmak için oluşturulduğuna dikkat 
    ediniz. O halde C'nin standart dosya fonksiyonlarıyla yapılan tipik bir okuma işlemi şöyle gerçekleşmektedir:

    C'deki okuma fonksiyonu ---> read POSIX fonksiyonu ---> sys_read sistem fonksiyonu ---> ....

    Tabii biz kursumuzda okuma ve yazma süreçlri üzerinde dururken olaylar silsilesini standart C fonksiyonlarından 
    başlatmayacağız. POSIX fonksiyonlarından ta da sistem fonksiyonlarından başlatacağız. 

    C'deki bu tamponlama yani önbellek mekanizması C'ye özgü değildir. Diğer prgramlama dillerinin de standart kütüphanelerinde
    benzer biçimde tamponlamalar yapılmaktadır. Örneğin C++'taki <iostream> kütüphanesi, C#'ta kullanılan .NET kütüphaneleri
    Java'da kullanılan temel kütüphanelerde hep kullanıcı alanında C'de olduğu gibi tamponlama yapmaktadır. Ancak bunların 
    hepsi neticede Linux sistemlerinde POSIX fonksiyonlarını, onlar da sistem fonksiyonlarını çağırmaktadır. 

    POSIX dosya fonksiyonlarının Linux'taki işletim sisteminin sistem fonksiyonlarını çağırdığını belirtmiştik. İşletim 
    sisteminin sistem fonksiyonları bilgi önbellekte olsa bile belli bir yavaşlık oluşturmaktadır. Programın akışının 
    kullanıcı modundan çekirdek moduna geçirilmesi ve akışın ilgili sistem fonksiyonuna aktarılması göreli bir zaman 
    kaybına yol açmaktadır. Bu nedenle ayrıca bu kütüphanelerin kullanıcı modunda tamponlama yapması önemli olmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz şimdiye kadar blok ve sektör düzeyinde okuma yazmaların kabaca nasıl gerçekleştirildiğini açıkladık. Ancak 
    çekirdeğin açık dosyalar için oluşturuğu organizasyon hakkında bilgi vermedik. Şimdi sürecin bu yönü üzerinde 
    duracağız. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi UNIX/Linux sistemlerinde dosyalar open isimli POSIX fonksiyonuyla açılmaktadır. open POSIX fonksiyonu
    başarı durumunda ismine "dosya betileyicisi (file descriptor)" denilen bir handle değeri vermektedir. read, write, 
    lseek, close gibi POSIX'in diğer dosya fonksiyonları bu dosya betimleyicisini parametre olarak alıp hangi dosya 
    üzerinde işlem yapılacağını bu betimleyiciden hareketle belirlemektedir. 

    open, read, write, lseek, close gibi POSIX'in temel dosya fonksiyonları Linux sistemlerinde aslında neredeyse doğrudan 
    Linux'un ilgili sistem fonksiyonlarını çağırmakatadır:

    open ---> sys_open
    read ---> sys_read
    write ---> sys_write
    lseek ---> sys_lseek
    close ---> sys_close

    Bu nedenle birtakım ayrıntıları da göz ardı edersek biz Linux sistemlerinde dosya işlemlerini yapan temel POSIX 
    fonksiyonlarının aslında doğrudan sys_xxx sistem fonksiyonlarını çağırdığını varsayabiliriz. Bir sistem fonksiyonu 
    çağrıldığında ileride de ele alacağımız gibi akış otomatik olarak kullanıcı modundan (user mode) çekirdek moduna 
    (kernel mode) geçirilmektedir. Çekirdek içerisindeki sistem fonksiyonları çalıştığı sürece artık koruma mekanizması 
    (yani bellek ve komut koruması) ortadan kalkmakta bu çekirdek kodları her makine komutlarını kullanabilmektedir. 

    UNIX/Linux sistemlerinde kullanılan standart C kütüphaneleri aynı zamanda POSIX fonksiyonlarını da içermektedir. 
    Bilindiği gibi bugün masaüstü Linux sistemlerinde en fazla kullanılan standart C kütüphanesi GNU'nun "libc" kütüphanesidir. 
    Bu kütüphane pek çok sistem için ortak kodlar içerdiğinden dolayı biraz karmalık bir kod yapısına sahiptir. Eğer standart 
    C fonksiyonlarının ve POSIX fonksiyonlarının nasıl yazılıdığını merak ediyorsanız gömülü Linux sistemleri için daha 
    minimalist biçimde yazılmış olan kütüphanelerin kaynak kodlarını inceleyebilirsiniz. Bunun için iki alternatif "musl" 
    ve "uclibc" kütüphaneleridir. "uclibc" kütüphanesine "Mikro C kütüphanesi de denilmektedir. Bu kütüphanelerin kaynak 
    kodlarını yine "elixir.bootlin.com" sitesinden inceleyebilirsiniz. Bu site yalnızca Linux çekirdekleri için değil 
    başka projeler için de kodlar üzerinde gezinme olanağı de sunmaktadır. Biz sadeliği nedeniyle "musl" kütüphanesini
    incelemenizi salık veriririz. Kütüphanenin kodları üzerinde gezinebilmek için aşağıdaki bağlantıdan faydalanabilirsiniz:

    https://elixir.bootlin.com/musl/v1.2.5/source
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    task_struct yapısı (proses kontrol blok) içerisinde proseslere ilişkin dosya işlemleri için kullanılan iki önemli 
    eleman bulunmaktadır:

    struct task_struct {
        /* ... */
        
        /* Filesystem information: */
        struct fs_struct		*fs;

        /* Open file information: */
        struct files_struct		*files;
        
        /* ... */
    };

    Bu iki eleman çok uzun süredir task_struct yapısı içerisinde bulunmaktadır. Ancak buradaki fs_struct ve files_struct 
    yapılarının içeriğinde çekirdeğin versiyonları ilerledikçe çeşitli değişiklikler de yapılmıştır. Yuradaki fs_struct 
    yapısı açık dosyalara ilişkin yapılan organizasyonla ilgili değildir. Prosesin kök dizini ve çalışma dizini gibi dosya 
    sistemine ilişkin proses bilgileri burada tutulmaktadır. Mevcut çekirdeklerde fs_struct yapısı  "/include/linux/fs_struct.h"
    dosyası içerisinde şöyle bildirilmiştir:

    struct fs_struct {
        int users;
        spinlock_t lock;
        seqcount_spinlock_t seq;
        int umask;
        int in_exec;
        struct path root, pwd;
    } __randomize_layout;

    Buradaki root ve pwd elemanları sırasıyla prosesin kök dizinini ve çalışma dizinini (current working directory)
    tutmaktadır. umask elemanı ise prosesin umask değerini tutmaktadır. Buradaki path yapısı da şöyle bildirilmiştir:

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;
        
    vfsmount ve dentry yapıları çeşitli başka bölümlerde ele alınacaktır. Eskiden bu yapı biraz daha küçüktü. Örneğin 
    çekirdeğin 2.2'li versiyonlarında şöyleydi:

    struct fs_struct {
        atomic_t count;
        int umask;
        struct dentry * root, * pwd;
    };

    Çekirdeğin 2.4'te şu hale getirildi:

    struct fs_struct {
        atomic_t count;
        rwlock_t lock;
        int umask;
        struct dentry * root, * pwd, * altroot;
        struct vfsmount * rootmnt, * pwdmnt, * altrootmnt;
    };

    2.6'da da şu ise hale getirildi:

    struct fs_struct {
        int users;
        spinlock_t lock;
        seqcount_t seq;
        int umask;
        int in_exec;
        struct path root, pwd;
    };

    Çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu yapı yoktu. Bu yapıdaki bilgiler doğrudan task_struct 
    içerisinde bulunmaktaydı:

    struct task_struct {
        /* ... */

        unsigned short umask;
        struct m_inode * pwd;
        struct m_inode * root;
        unsigned long close_on_exec;

        /* ... */
    };

    Ayrıntıları göz ardı edersek bu fs_struct yapısındaki en önemli elemanlar "prosesin kök dizinin yeri", "prosesin 
    çalışma dizinin yeri" ve "prosesin umask değeri" dir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        24. Ders 05/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    task_struct içerisindeki files isimli gösterici prosesin açmış olduğu dosyalara ilişkin bilgilerin tutulduğu files_struct 
    türünden yapı nesnesini göstermektedir. files_struct yapısı da zaman içerisinde değişiklere uğratılmıştır. Güncel 
    çekirdeklerde bu yapı "include/linuc/fdtable.h" dosyası içerisinde şöyle bildirilmiştir:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    2.6'lı çekirdeklerde bu yapı şöyleydi:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        int next_fd;
        struct embedded_fd_set close_on_exec_init;
        struct embedded_fd_set open_fds_init;
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    2.4'lü çekirdeklerde şöyleydi:

    struct files_struct {
        atomic_t count;
        rwlock_t file_lock;	/* Protects all the below members.  Nests inside tsk->alloc_lock */
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;	/* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
    };

    2.2 versiyonunda yapı bir eleman dışında aşağı yukarı aynıydı:

    struct files_struct {
        atomic_t count;
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;	/* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
    };

    Çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu bilgiler doğrudan task_struct içerisinde bulunuyordu:

    struct task_struct {
        /* ... */

        unsigned short umask;
        struct m_inode * pwd;
        struct m_inode * root;
        unsigned long close_on_exec;

        /* ... */
    };
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux sistemlerinde ne zaman open POSIX fonksiyonuyla bir dosya açılsa sys_open sistem fonksiyonu açılan dosya 
    için file isimli (struct file türünden) bir yapı türünden nesne tahsis edip dosya işlemleri için gereken bilgileri 
    bu yapı nesnesinin içerisine yerleştirmektedir. sys_read, sys_write, sys_lseek, sys_close gibi sistem fonksiyonları
    da dosya üzerinde işlem yapabilmek için bu file yapısındaki bilgileri kullanmaktadır. İşletim sistemlerinde bu amaçla 
    kullanılan nesnelere "dosya nesnesi (file object)" de denilmektedir. Biz de kursumuzda file yapısı türünden nesneye 
    "dosya nesnesi" de diyeceğiz. Tabii sistem fonksiyonları ve çekirdek bu dosya nesnelerine task_struct nesnesinden 
    hareketle erişmektedir. İşte zaten files_struct yapısı bu erişme ilişkin bilgileri de içermektedir. Biz aşağıdaki 
    gibi bir dosya açmış ollaım:

    fd = open(...);

    sys_open sistem fonksiyonu açılmak istenen dosyanın diskteki yerini ve metadata bilgilerini bulur O bilgilerden 
    hareketle bir dosya nesnesi (file object) oluşturur. O dosya nesnesinin adresini de izleyen paragrafta açıklayacağımız
    gibi files_struct nesnesinin içerisine yerleştirir. Böylece sys_read, sys_write, sys_lseek, sy_close gibi sistem 
    fonksiyonları task_struct nesnesindne hareketle bu dosya nesnesine erişebilmektedir. Güncel çekirdeklere file yapısı 
    "include/linuc/fs.h" dosyasının içerisinde şöyle bildirilmiştir:

    struct file {
        spinlock_t			            f_lock;
        fmode_t				            f_mode;
        const struct file_operations	*f_op;
        struct address_space		    *f_mapping;
        void				            *private_data;
        struct inode			        *f_inode;
        unsigned int			        f_flags;
        unsigned int			        f_iocb_flags;
        const struct cred		        *f_cred;
        struct fown_struct		        *f_owner;
        /* --- cacheline 1 boundary (64 bytes) --- */
        struct path			            f_path;
        union {
            /* regular files (with FMODE_ATOMIC_POS) and directories */
            struct mutex		        f_pos_lock;
            /* pipes */
            u64			                f_pipe;
        };
        loff_t				            f_pos;
    #ifdef CONFIG_SECURITY
        void				            *f_security;
    #endif
        /* --- cacheline 2 boundary (128 bytes) --- */
        errseq_t			            f_wb_err;
        errseq_t			            f_sb_err;
    #ifdef CONFIG_EPOLL
        struct hlist_head		        *f_ep;
    #endif
        union {
            struct callback_head	    f_task_work;
            struct llist_node	        f_llist;
            struct file_ra_state	    f_ra;
            freeptr_t		            f_freeptr;
        };
        file_ref_t			            f_ref;
        /* --- cacheline 3 boundary (192 bytes) --- */
    } __randomize_layout
    __attribute__((aligned(4)));	/* lest something weird decides that 2 is OK */

    Eskiden bu yapının içeriği daha küçüktü. Zaman içerisinde bu yapıda da değişilikler ve eklemeler yapılmıştır.  
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi bir dosya sys_open sistem fonksiyonuyla açıldığında kabaca dosya nesnesinin ve dosya betimleycisinin (file 
    decriptor) nasıl saklandığına ilişkin bilgileri edinelim. Güncel çekirdeklerde bu veri yapısı biraz ayrıntılıdır. 
    Biz bu ayrıntılardan bahsedeceğiz. Ancak önce çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu süreci açıklayalım. 
    Bu ilkel versiyonda henüz struct_files biçiminde bir yapı yoktu. Açık dosya bilgileri doğrudan task_struct içerisinde 
    bulunan aşağıdaki elemalarda saklanıyordu:

    struct task_struct {
        /* ... */

        unsigned long close_on_exec;
	    struct file *filp[NR_OPEN];

        /* ... */
    };

    Burada filp isimli dizinin struct file * türünden olduğuna dikkat ediniz. Yani filp diisi file nesneleriin adreslerini 
    tutan bir gösterici dizisidir. Uzunluğu da NR_OPEN kadardır. Bu versiyonda NR_OPEN şöyle define edilmiştir:

    #define NR_OPEN 20

    İzleyen paragraflarda da anlayacağınız üzere bu ilkel versiyonda bir proses en fazla 20 dosyayı açık durumda tutabiliyordu. 
    UNIX/Linux dünyasında dosya nesnelerinin adreslerini tutan bu gösterici dizilerine "dosya betimleyici tablosu (file 
    descriptor table)" denilmektedir. Yukarıda da belirttiğimiz gibi dosya betimleyici tablosu dosya nesnelerinin adreslerini 
    tutan bir gösterici dizisi biçimindedir. Bunu 0.01 çekirdeği için şöyle bir şekille şöyle gösterebiliriz:
    
    Dosya Betimleyici Tablosu 
    ┌───┐
    │ 0 │ ───────────────► Dosya Nesnesi 
    │ 1 │ ───────────────► Dosya Nesnesi 
    │ 2 │ ───────────────► Dosya Nesnesi 
    │ 3 │ ───────────────► Dosya Nesnesi 
    │ . │                         
    │ . │                         
    │ . │                         
    │18 │ (boş)                   
    │19 │ (boş)                   
    └───┴

    Buradaki sayılar dizinin indekslerini belirtmektedir. Tabii zamanla dosyalar kapanınca bu dizinin elemanları boşa
    düşecektir. Boş elemanlara NULL adres yerleştirilmektedir. İşte open POSIX fonksiyonunun (yani sys_open sistem 
    fonksiyonun) verdiği "dosya betimleyicisi (file descriptor)" aslında dosya betimleyici tablosundaki dizide bir 
    indeks belirtmektedir. open POSIX fonksiyonun (dolayısıyla sys_open sistem fonksiyonunun) dosya betimleyici tablosundaki 
    en düşük boş indeksi vereceği POSIX standartlarında garanti altına alımıştır. Dosya betimleyici tablosunun (yani 
    struct file *) dizisinin uzunluğunun "aynı anda açık tutulabilecek" dosya sayısını da belirttiğine dikkat ediniz. 

    Linux çekirdeğindeki sys_read, sys_write, sys_lseek, sys_close gibi sistem fonksiyonları önce task_struct yapından 
    hareketle dosya betimleyici tablosuna erişmekte, parametre olarak aldıkları betimleyici değerini bu diziye indeks 
    yaparak asıl dosya nesnesine erişmektedir. 

    Dosya betimelyici tablosunun "prosese özgü" olduğuna dikkat ediniz. Bir proseste açılmış olan dosyaya ilişkin dosya 
    nesnesinin adresi o prosesteki dosya betimleyici tablosuna yazılmaktadır. Dosya betimleyicileri sistem genelinde 
    bir değer belirtmemektedir, dosya betimleyici değerleri yalnızca ilgili proses için anlamlıdır. Örneğin 12 numaralı 
    betimleyici bir proseste bir dosyayı belirtirken diğer bir proseste başka bir dosyayı belirtiyor olabilir. Dolayısıyla 
    biz bir proseste bir dosya açıp elde ettiğimiz dosya betimleyicisini başka bir prosese prosesler arası haberleşme 
    yöntemleriyle iletsek o proseste o betimleyicinin hiçbir anlamı olmaz. Ancak anımsanacağı gibi özel bir durum olarak 
    üst proses fork işlemi yaptığında üst prosesin dosya betimleyici tablosu alt prosese sığ kopyalanmaktadır. Böylece 
    üst proses ile alt proses aynı dosya üzerinde işlem yapabilmektedir. (trace işlemlerinde Linux sistemlerine özgü 
    kullanılmak üzere bulundurulmuş olan sys_pidfd_getfd isimli bir sistem fonksiyonu vardır. Ayrıca başka bir prosesin
    açmış olduğu dosyanın başka bir proseste açılmasına olanak sağlayan sys_pidfd_open isimli bir sistem fonksiyonu da 
    Linux çekirdeğinde bulunmaktadır.)

    Yukarıdaki 0.01 versiyonunda konuyla ilgili unsigned long türden close_on_exec isimli bir elemanın da bulunduğunu
    görüyorsunuz. Bu elemanın her biti bir betimleyicinin "close-on-exec" drumunu belirtmektedir. Söz konusu bit 1 ise 
    ilgili betimleyici exec işlemleri sırasında close edilir, 0 ise close edilmez. POSIX standartlarında bir dosya 
    açıldığında close-on-exec bayrağının default durumda 0 olduğu belirtilmiştir. (Yani default durumda exec işlemlerinde 
    dosya kapatılmamaktadır.) Bu ilkel versiyonda zaten bir prosesin maksimum açık tutacağı dosya sayısı 20'dir. O 
    zamanlarda long türü 32 bitti. Yani bu unsigned long eleman bütün dosya betimleyicilerinin close-on-exec bayraklarını 
    tutmak için yeterliydi. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    sys_open sistem fonksiyonu öncelikle dosya betimleyici tablosundaki ilk boş betimleyiciyi bulmaya çalışır. Çünkü 
    dosya betimleyici tablosu tamamen doluysa zaten bir dosya nesnesinin oluşturulup işlemlere devam edilmesinin de bir 
    anlamı olmayacaktır. Peki dosya betimleyici tablosundaki ilk boş betimleyici nasıl bulunmaktadır? Düz mantıkla 
    "mademki dosya betimleyici tablosundaki boş indekslerde NULL adres var o zaman ilk NULL adres görülene kadar bir 
    döngü ile sıralı arama yapılabilir" diye düşünebilirsiniz. Eğer dosya betimleyici tabloları 0.01 versiyonundaki gibi 
    çok küçük olsaydı sıralı arama yapmanın önemli bir sakıncası olmayabilirdi. Gerçekten de 0.01 versiyonunda boş 
    betilmeyici şöyle bulunmuştur:

    int sys_open(const char * filename,int flag,int mode) {
        /* ... */

    	for(fd=0 ; fd<NR_OPEN ; fd++)
            if (!current->filp[fd]) 
                break;
        
        /* ... */
    }

    Görüldüğü gibi bu ilkel versiyonda dosya betimleyici tablosu üzerinde tek tek sıralı arama yapılmış, ilk boş 
    betimleyici (yani NULL adres içeren ilk dizi elemanının indeksi) elde edilmiştir. Ancak uzunca bir süredir proseslerin 
    default dosya betimleyici tablolarının default uzunlukları 1024'tür ve bu uzunluk da büyütülebilmektedir. 1024 elemanlı 
    bir tabloda sıralı arama ile ilk NULL olan dizi elemanının indeksinin bulunması yavaş bir işlemdir. İşte bir süre 
    sonra Linux çekirdeklerinde bu arama işlemi bit düzeyinde aramayla hızlandırılmıştır. Bit düzeyinde arama yönteminde 
    dosya betimleyici tablosunun uzunluğu kadar bit dizisi oluşturulur. Sonra o bit dizisindeki ilk 0 olan bitin indeksi 
    bulunmaya çalışılır. Bu bit dizisindeki 0 olan bitler betimleyici tablosundaki boş elemanları 1 olan bitler dolu olan 
    elemanları belirtmektedir. İşlemcilerde belli bir yazmaçtaki (ya da Intel işlemcileri söz konusuysa bellek adresindeki) 
    "ilk 0 olan bitin indeksini veren özel makine komutları" bulunmaktadır. Tabii işlemci 32 bit ise bu makine komutları 
    32 bitlik yani 4 byte'lık bir veri üzerinde, 64 bit ise 64 bitlik yani 8 byte'lık bir veri üzerinde işlem yapabilmektedir. 
    Örneğin elimizdeki işlemcinin 64 bit olduğunu düşünelim. Bu işlemcilerdeki C derleyicilerinde unsigned long türü 
    8 byte yani 64 bittir. Bu durumda örneğin 1024 eleman uzunluğundaki dosya betimleyici tablosu için 16 elemanlı bir 
    unsigned long dizi bitmap olarak kullanılabilir. Tabii bu sistemlerde ilk 0 bitini bulan makine komutları zaten 64 
    bitlik bir bilgi üzerinde bu işi yapabilmektedir. O halde çekirdek tasarımcısı 16 elemanlı bir döngü kullanıp dizinin 
    her elemanı için bu özel makine komutunu kullanarak işlemleri hızlandırabilir. Ancak belli bir süreden sonra bu 
    yöntem de biraz daha geliştirilerek arama işlemi biraz daha hızlandırılmıştır. Bu ikinci hızlandırma yönteminde ikinci 
    bir bit dizisi kullanılmaktadır. Ancak ikinci bir dizisinin her biti ilk bit dizisindeki unsigned long elemanın tüm 
    bitlerinin 0 olup olmadığını tutmaktadır. Bu durumda güncel çekirdeklerde önce bu ikinci bit dizisindeki ilk 1 olan 
    bit bulunur. Sonra bu bitin indeksi birinci bit dizisine indeks yapılarak oradaki unsigned long değer içerisinde ilk 
    0 olan bit elde edilir. Bu yöntemde örneğin ilk bit dizisinin aşağıdaki gibi olduğunu varsayalım:
    
    1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 - 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 -
    1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 - 1111 1101 1111 1111 1111 1111 1111 1111 1111 1111 - ....

    Burada ilk bit dizisi unsigned long dizi biçimindedir. Görüldüğü gibi bu dizinin ilk üç elemanında hiç 0 olan bit 
    yoktur. İlk 0 olan bit 3'üncü indekstedir. Bu durumda ikinci bir dizisi de aşağıdaki gibi olacaktır:

    0001.....

    Bu hızlandırma mantığında önce kinci bit dizisindeki ilk 1 olan bitin indeksi elde edilir. Örneğimizde bu 3'tür. 
    Sonra birinci bit disizinin 3'üncü indeksteki unsigned long elemanında ilk 0 olan bitin indeksi bulunur. Bu yöntemde 
    birkaç makine komutuyla istenen bilginin elde edilebildiğine dikkat ediniz. 

    Çekirdek dokümantasyonunda her dosya betimleyicisinin boş mu dolu mu olduğunu tutan bitmap'e "birinci düzey bitmap"
    bu bitmap'teki ilk boş unsigned long elemanın dizi indeksini veren ikinci bitmap'e ise "ikinci düzey bitmap" 
    denilmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    2.6 çekirdeğine kadar (bu çekirdek de dahil) bit dizileri için fd_set isimli bir yapı kullanılıyordu. Sonraları bu 
    fd_set yapısı bırakıldı. Örneğin çekirdeğin 2.2 ve 2.4 versiyonundaki "include/linux/sched.h" içerisindeki files_struct 
    yapısı şöyleydi (2.2 versiyonunda file_lock elemanı yoktu):

    struct files_struct {
        atomic_t count;
        rwlock_t file_lock;	        /* Protects all the below members.  Nests inside tsk->alloc_lock */
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;	/* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
    };

    Burada görmüş olduğunuz fd_set yapısı "bit dizilerini" temsil etmektedir. Nu yapı şöyle bildirilmiştir:

    typedef __kernel_fd_set		fd_set;

    typedef struct {
        unsigned long fds_bits [__FDSET_LONGS];
    } __kernel_fd_set;

    Buradaki __FDSET_LONGS sembolik sabiti 32 bit sistemlerde 32 değerini 64 bit sistemlerde 16 değerini vermektedir. 
    Yani bu yapının içerisindeki fds_bits elemanı toplamda 1024 biti tutan unsigned long türünden dizidir. 2.6 çekirdeği 
    dahil olmak üzere bit dizisi anlamında çekirdekte bu fd_set yapısı kullanılmıştır. Ancak bu fd_set temsilinin tasarımında
    da aslında kusurlar vardır. Bu temsilde bit dizisi büyütülmek istendiğinde artık bu fd_set temsili işe yaramaz 
    hale gelmektedir. Bu nedenle artık güncel çekirdeklerde fd_set yerine doğrudan unsigned long * türünden bir gösterici 
    tutulup bu göstericinin gösterdiği yer için belli uzunlukta unsşigned long dizi tahsis edilmektedir. Aslında uzun 
    süre kulanılmış olan bu fd_set temsilinden vazgeçilmesi iyi olmuştur. Yukarıdaki çekirdeğin 2.4 versiyonundaki 
    files_struct yapısında dosya betimleyici tablosunun uzunluğu yapının max_fds elemanında tutulmaktadır. Çünkü işin 
    başında bu tablo 1024 elemanlık olsa da daha sonra büyütülebilmektedir. Bu versiyonda dosya betimelyeici tablosunun 
    adresinin de fd elemanında tutulduğuna dikkat ediniz. Dosyaların close-on-exec bayrakları da yine yapının close_on_exec 
    elemanında tutulmaktadır. 
    
    Yukarıdaki files_struct yapısı biraz kafanızı karıştırabilir. Sanki size bu yapıda aynı amaçla kullanılan birden 
    fazla eleman varmış gibi gelebilir. Konuya açıklık getirmek amacıyla bu versiyondaki yapı elemanlarının hepsinin 
    işlevlerini tek tek açıklayalım:

    - Bir proses yaratıldığında işin başında dosya betimleyici tablosu için, boş betimleyici tespit etmek için ve 
    close-on-exec bayrakları için files_struct yapısı içerisinde alanlar ayrılmıştır:

    struct files_struct {
        /* ... */

        fd_set close_on_exec_init;                  /* close-on-exec bayrakları için kullanılan static bitmap */
        fd_set open_fds_init;                       /* açık dosya betimleyicilerini tutan statik bitmap */
        struct file * fd_array[NR_OPEN_DEFAULT];    /* dosya betimleyici tablosu için ayrılmış statik dizi */

        /* ... */
    };

    Burada NR_OPEN_DEFAULT 32 bit sistemlerde 32, 64 bit sistemlerde 64 değerini vermektedir. Eğer proses dosya betimleyici 
    tablosunu genişletmezse zaten bu tablolar ve bitmap'ler files_struct yapısı içerisinde hazır bir biçimde tutulmaktadır. 
   
    - Çekirdek her zaman dosya tablosunun yerini fd göstericisinin gösterdiği yerde, açık dosya betimleyicilerinin bitmap'ini 
    open_fds göstericisinin gösterdiği yerde, close-on-exec bayraklarına ilişkin bitmap'i ise close_on_exec göstericisinin 
    gösterdiği yerde aramaktadır. Yapının bu elemanalrına dikkat ediniz:

    struct files_struct {
        /* ... */

        struct file ** fd;	                    /* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;

        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];

        /* ... */
    };

    İşin başında default durumda fd göstericisi fd_array elemanını, close_on_exec göstericisi close_on_exec_init elemanını
    ve open_fds göstericisi de open_fds_init elemanını göstermektedir. 

    current göstericisinden hareketle fdx betimeleyicisinin gösterdiği yerdeki dosya nesnesine (struct file) 
    current->files->fd[fdx] ifadesiyle erişilebilir. Bu erişimi kolaylaştırmak için 2.2 ve 2.4 çekirdeklerinde fcheck
    isimli çekirdek fonksiyonu bulundurulmuştur:

    static inline struct file * fcheck(unsigned int fd)
    {
        struct file * file = NULL;
        struct files_struct *files = current->files;

        if (fd < files->max_fds)
            file = files->fd[fd];
        return file;
    }

    Ancak bu fonksiyon export edilmemiştir. Yani aygıt sürücüler tarafından kullanılamamaktadır. Aslında çekirdekte bir 
    dosya betimleyicisinden hareketle dosya nesnesini elde etmek için daha yüksek seviyeli fget fonksiyonu kullanılmaktadır.
    Bu fonksiyon 2.4 ve 2.6 versiyonlarında aşağıdaki yazılmıştır:

    struct file fastcall *fget(unsigned int fd)
    {
        struct file * file;
        struct files_struct *files = current->files;

        read_lock(&files->file_lock);
        file = fcheck(fd);
        if (file)
            get_file(file);
        read_unlock(&files->file_lock);
        return file;
    }

    Bu fonksiyonun fcheck fonksiyonu kullanılarak yazıldığını görüyorsunuz. Ancak bu fonksiyon ileride göreceğimiz gibi 
    dosya nesnesi içerisindeki (struct file yapısındaki) sayacı da güvenli bir biçimde artırmaktadır. fget fınksşyonu 
    da bu versiyonlarda export edilmemiştir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        25. Ders 11/10/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin 2.6 versiyonlarına gelindiğinde files_struct yapısının içerisi fdtable isimli bir yapı ile biraz daha 
    derli toplu fakat biraz daha karmaşık hale getirilmiştir. 2.6'lı versiyonlardaki files_struct yapısı şöyledir:
  
    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        int next_fd;
        struct embedded_fd_set close_on_exec_init;
        struct embedded_fd_set open_fds_init;
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    fdtable yapısı da şöyledir:

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd;      /* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        struct rcu_head rcu;
        struct fdtable *next;
    };

    Artık bu yapılar da "include/linux/fdtable.h" isimli dosya oluşturularak oraya taşınmıştır. By versiyonlarda 
    çekirdek her zaman fdt gösterisicinin gösteriği yerden işlemine başlamaktadır. fdt göstericisi işin başında yapı 
    içerisindeki fdtab yapı nesnesini göstermektedir. fdtab yapı nesnesinin içerisinde de önceki versiyonlarda olduğu
    gibi fd, close_on_exec, open_fds göstericileri vardır. Bu göstericiler de işin başında files_struct içerisindeki 
    fd_array, close_on_exec_init ve open_fds_init elemanlaırnı göstermektedir. Ancak ileride aslında files_struct 
    içerisindeki fdt göstericisi başka bir fdtable nesnesini, fdtable nesnesinin içerisindeki göstericiler de büyütülmüş
    başka nesneleri gösterir hale gelebilmektedir. 

    Bu versiyonlarda current göstericisinden hareketle fdx betimelyicisinin gösterdiği yerdeki dosya nesnesine (struct file) 
    current->files->fdt->fd[fdx] ifadesiyle erişilebilir. Bu versiyonalarda da bu erişimi bazı kontrollerle sağlayan 
    ayrı fonksiyonlar ve makrolar da bulundurulmuştur. Örneğin fcheck_files fonksiyonu şöyle tanımlanmıştır:

    static inline struct file * fcheck_files(struct files_struct *files, unsigned int fd)
    {
        struct file * file = NULL;
        struct fdtable *fdt = files_fdtable(files);

        if (fd < fdt->max_fds)
            file = rcu_dereference_check_fdtable(files, fdt->fd[fd]);
        return file;
    }

    #define files_fdtable(files)        \
		    (rcu_dereference_check_fdtable((files), (files)->fdt))

    #define fcheck(fd)	fcheck_files(current->files, fd)

    Yani çekirdek içerisinde fcheck makrosuyla fd numaralı betimleyiciye ilişkin dosya nesnesi elde edilebilmektedir. 
    Ancak fcheck_files fonksiyonu da export edilmemiştir. Yine 2.6'lı çekirdeklerde de dosya betimleyicisinden hareketle 
    dosya nesnesi içerisindeki sayacı artırarak dosya nesnesini elde eden daha yüksek seviyeli fget isimli bir fonksiyon 
    da bulunmaktadır:

    struct file *fget(unsigned int fd)
    {
        return __fget(fd, FMODE_PATH);
    }
    EXPORT_SYMBOL(fget);

    Biz burada bu fonksiyonun çağırdığı fonksiyonları gözden geçirmeyeceğiz. Ancak bu fonksiyonun artık export edildiğine
    dikkat ediniz. Yani bu versiyondan itibaren aygıt sürücüler de dosya betimleyicisinden hareketle dosya nesnesine bu 
    fonksiyon yoluyla erişebilmektedir. Çekidekteki nesnenin sayacını artırarak erişim sağlayan fonksiyonlar genel olarak 
    get soneki ile sayacı eksilten fonksiyonlar da put soneki ile isimlendirilmiştir. fget fonksiyonuyla elde edilen 
    dosya nesnesi fput fonksiyonuyla geri bırakılmaktadır:

    void fput(struct file *file)
    {
        if (atomic_long_dec_and_test(&file->f_count))
            __fput(file);
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Belli bir zamandan sonra artık bit dizisi oluşturmak için fd_set yapısının kullanılmasından vazgeçilmiştir. Güncel 
    çekirdeklerdeki açık dosyalara ilişkin veri yapısı 2.6 ile çok benzerdir. Ancak yukarıda da belirttiğimiz gibi artık
    fd_set yapısı kullanılmamaktadır. Güncel çekirdeklerdeki files_struct yapısı şöyledir:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    fdtable yapısı da şöyledir:

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd;      /* current fd array */
        unsigned long *close_on_exec;
        unsigned long *open_fds;
        unsigned long *full_fds_bits;
        struct rcu_head rcu;
    };

    Görüldüğü gibi artık bit dizileri fd_set yerine doğrudan unsigned long türden bir dizi biçiminde oluşturulmaktadır. 
    Yine bu versiyonlarda da fdx numaralı dosya onlar betimleyicisinin gösterdiği yerdeki dosya nesnesine current->files->fdt->fd[fdx]
    ifadesiyle erişilmektedir. Fakat artık güncel versiyonlarda fcheck biçiminde bir makro ve fcheck_files isimli bir 
    fonksiyon yoktur. Ancak yine güncel versiyonlarda dosya betimleyicisi yoluyla dosya nesnesine erişimi referans sayacını 
    artırarak yapan fget fonksiyonu bulunmaktadır:
    
    struct file *fget(unsigned int fd)
    {
        return __fget(fd, FMODE_PATH);
    }
    EXPORT_SYMBOL(fget);

    Yine referans sayacını eksliterek nesneyi bırakmak için fput fonksiyonu kullanılmaktadır:

    void fput(struct file *file)
    {
        if (unlikely(file_ref_put(&file->f_ref)))
            __fput_deferred(file);
    }
    EXPORT_SYMBOL(fput);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Mevcut çekirdeklerde dosya betimleyici tablosundaki ilk boş betimleyicinin bulunmasının bit dizilerinde "ilk 0 olan 
    bitin bulunması" problemi biçiminde ele alındığını belirtmiştik. Bunun için güncel çekirdeklerde iki düzey bitmap 
    kullanılıyordu. Güncel çekirdeklerdeki fdtable yapısının içerisinde bulunan open_fds birinci düzey bitmap'i full_fds_bits  
    ise ikinci düzey bitmap'i belirtmektedir. Tüm dosya betimleyicilerinin dolu boş mu olduğu open_fds bitmap'inde tutulmaktadır. 
    full_fds_bits bitmap'i ise open_fds bitmap'indeki tüm bitleri 0 olmayan ilk unsigned long elemanın indeksinin bulunmasında 
    kullanılmaktadır. files_struct ve fdtable yapılarını aşağıda yeniden veriyoruz:

    struct files_struct {
    /*
    * read mostly part
    */
        atomic_t count;
        bool resize_in_progress;
        wait_queue_head_t resize_wait;

        struct fdtable __rcu *fdt;
        struct fdtable fdtab;
    /*
    * written part on a separate cache line in SMP
    */
        spinlock_t file_lock ____cacheline_aligned_in_smp;
        unsigned int next_fd;
        unsigned long close_on_exec_init[1];
        unsigned long open_fds_init[1];
        unsigned long full_fds_bits_init[1];
        struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd;      /* current fd array */
        unsigned long *close_on_exec;
        unsigned long *open_fds;
        unsigned long *full_fds_bits;
        struct rcu_head rcu;
    };
    
    Uzun süredir bir bit dizisi içerisindeki ilk 0 olan bitin indeksini elde etmek için find_next_zero_bit isimli 
    bir çekirdek fonksiyonu kullanılmaktadır. Tabii bu fonksiyon nihayetinde yukarıda da bahsettiğimiz gibi işlemciye 
    özgü makine komutlarını kullanmaktadır. Çekirdeğin güncel versiyonlarında sys_open sistem fonksiyonundan başlanarak
    ilk boş dosya betimleyicisinin bulunması için yapılan çağrılar şöyledir:

    sys_open --> do_sys_open ---> do_sys_openat2 ---> __get_unused_fd_flags ---> alloc_fd ---> find_next_fd ---> 
    find_next_zero_bit

    Bu çağrı zincirinde bir dizisi içerisinde ilk 0 olan bitin bulunması işlemini find_next_zero_bit fonksiyonu yapmaktadır. 
    İlk 0 olan bitin bulunması aslında baştan başlanarak yapılmamaktadır. files_struct yapısı içerisindeki next_fd elemanı 
    aramanın başlatılacağı yeri belirtmektedir. Yani buradaki next_fds elemanının belirttiği değerden küçük tüm dosya 
    betimleyicileri doludur. Dolayısıyla arama full_fds_bits dizisinin hemen başından başlatılmamaktadır. Tabii eğer bu 
    next_fds elemanın belirttiği dosya betimeleyicisinden daha küçük bir betimleyici kapatılırsa çekirdek zaten bu next_fds 
    elemanını güncellemektedir. 

    Güncel çekirdeklerdeki find_next_fd fonksiyonu şöyle yazılmıştır:

    static unsigned int find_next_fd(struct fdtable *fdt, unsigned int start)
    {
        unsigned int maxfd = fdt->max_fds; /* always multiple of BITS_PER_LONG */
        unsigned int maxbit = maxfd / BITS_PER_LONG;
        unsigned int bitbit = start / BITS_PER_LONG;
        unsigned int bit;

        /*
        * Try to avoid looking at the second level bitmap
        */
        bit = find_next_zero_bit(&fdt->open_fds[bitbit], BITS_PER_LONG,
                    start & (BITS_PER_LONG - 1));
        if (bit < BITS_PER_LONG)
            return bit + bitbit * BITS_PER_LONG;

        bitbit = find_next_zero_bit(fdt->full_fds_bits, maxbit, bitbit) * BITS_PER_LONG;
        if (bitbit >= maxfd)
            return maxfd;
        if (bitbit > start)
            start = bitbit;
        return find_next_zero_bit(fdt->open_fds, maxfd, start);
    }

    Fonksiyonun birinci parametresi fdtable nesnesinin adresini, ikinci parametresi ise aramanın başlatılacağı betimleyicinin 
    numarasını belirtmektedir. Fonksiyon önce ikinci düzey birtmap'te arama yapmadan ilk düzey bitmap'te, dizinin hemen 
    aramanın yapılacağı indeksinde hızlı bir arama yapar. Eğer bu aramadan sonuç elde edilemeze önce ikinci düzey bitmap'te 
    birinci düzey bitmap için dizi indeksini elde eder, sonra birinci düzey bitmap'te arama yapar. find_next_zero_bit 
    fonksiyonu da güncel çekirdeklerde şöyle tanımlanmıştır:

    unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
				 unsigned long offset)
    {
        if (small_const_nbits(size)) {
            unsigned long val;

            if (unlikely(offset >= size))
                return size;

            val = *addr | ~GENMASK(size - 1, offset);
            return val == ~0UL ? size : ffz(val);
        }

        return _find_next_zero_bit(addr, size, offset);
    }

    Buradaki small_const_nbits fonksiyonu find_next_fd fonksiyonundaki ilk hızlı aramanın ve birinci düzey bitmap'taki 
    aramanın yapılabilmesi için kontrol sağlamaktadır. Yani arama tek bir dizi elemanı üzerinde yapılacaksa bu if deyiminin 
    doğruysa kısmı çalıştırılacaktır. Eğer arama birden fazla dizi elemanı üzerinde yapılacaksa bu durumda arama 
    _find_next_zero_bit fonksiyonuna yaptırılmaktadır. Bu fonksiyon da şöyle tanımlanmıştır:

    unsigned long _find_next_zero_bit(const unsigned long *addr, unsigned long nbits,
					 unsigned long start)
    {
        return FIND_NEXT_BIT(~addr[idx], /* nop */, nbits, start);
    }

    #define FIND_NEXT_BIT(FETCH, MUNGE, size, start)				        \
    ({										                                \
        unsigned long mask, idx, tmp, sz = (size), __start = (start);		\
                                                                            \
        if (unlikely(__start >= sz))						                \
            goto out;							                            \
                                                                            \
        mask = MUNGE(BITMAP_FIRST_WORD_MASK(__start));				        \
        idx = __start / BITS_PER_LONG;						                \
                                                                            \
        for (tmp = (FETCH) & mask; !tmp; tmp = (FETCH)) {			        \
            if ((idx + 1) * BITS_PER_LONG >= sz)				            \
                goto out;						                            \
            idx++;								                            \
        }									                                \
                                                                            \
        sz = min(idx * BITS_PER_LONG + __ffs(MUNGE(tmp)), sz);			    \
    out:										                            \
        sz;									                                \
    })

    Burada dizi elemanlarında arama gördüğünüz gibi FIND_NEXT_BIT makrosuyla yapılmıştır. Tabii bu makro içerisindeki 
    döngü ancak birinci düzey bitmap aramasında çalıştırılacaktır. 

    Burada şöyle bir özet yapmak istiyoruz:

    1) Çekirdek hemen ikinci düzey bitmap'e (full_fds_bits) yönelmez. Önce open_fds dizisinde tek bir unsigned long 
    elemanda hızlı bir arama yapar.

    2) Eğer yukarıdaki arama başarısız olursa bu durumda önce ikinci düzey bitmap'te (full_fds_bits) ilk 0 olan bitin 
    indeksi elde edilir. Birinci düzey bitmap'te (open_fds) yalnızca bu indeksteki unsigned long dizi elemanında arama 
    yapılır. 

    3) find_next_zero_bit fonksiyonu unsigned long dizisinin yalnızca tek bir elemanında mı yoksa belli bir elemandan 
    itibaren dizinin geri kalan tüm elemanlarında mı aramaya yapılacağına small_const_nbits çağrısıyla karar 
    vermektedir. 

    Peki yukarıdaki kodlarda bitmap dizisinin belli bir unsigned long elemanında işlemcinin özel makine komutlarıyla
    arama işlemi tam nerede yapılmaktadır? İşte yukarıdaki kodlar incelnirse makine dili düzeyinde aramanın ffz fonksiyonunda
    ve __ffs fonksiyonunda yapıldığı görülecektir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    POSIX sistemlerinde dolayısıyla da Linux çekirdeğinde thread'lerin ayrı dosya betimleyici tabloları yoktur. Dosya
    betimleyicileri ve dosya betimleyici tablosu prosese özgüdür. Yani siz bir prosesin hangi thread'inde dosya açmış 
    olursanız olun bu dosya bilgisi prosese özgüdür, siz prossin herhangi bir thread'inde bu dosyaya erişebilirsiniz. 
    Bir thread yaratıldığında thread'e ilişkin task_struct nesnesinin fs gibi files gibi elemanları onu yaratan thread'in
    task_struct nesnesinden sığ kopyalamayla (yani gösterici elemanları söz konusu olduğunda yalnızca göstericilerin 
    içerisindeki adreslerin kopyalamasıyla) kopyalanmaktadır. Dolayısıyla aslında prosesin bütün thread'leri açık 
    dosyalara ilişkin aynı veri yapısı nesnelerini kullanmaktadır. task_struct yapısının ilgili kısmına dikkat ediniz:

    struct task_struct {
        /* ... */
        
        /* Filesystem information: */
        struct fs_struct		*fs;

        /* Open file information: */
        struct files_struct		*files;
        
        /* ... */
    };

    Burada yeni task_struct nesnesi yaratılıp diğer task_struct nesnesinden yeni yaratılan task_struct nesnesine 
    sığ kopyalama yapıldığında files göstericisinin de aslında aynı files_struct nesnesini göstereceğine dikkat ediniz. 
    Yani toplamda aslında proses için tek bir files_struct nesnesi bulunmaktadır. Dolayısıyla bir prosesin tüm thread'leri 
    aslında aynı bilgilere erişip onları kullanmaktadır.  

    Anımsanacağı gibi fork fonksiyonuyla alt proses yaratılırken alt proses üst prosesle aynı açık dosyaları görebiliyordu. 
    Peki bu güncel çekirdeklerde nasıl sağlanmaktadır? Örneğin üst proses open fonksiyonuyla bir dosya açmış olsun. 
    Açılan dosyanın da dosya betimleyicisi 3 olsun. Şimdi bu proses fork yaptığında bu prosesin tamamen özdeş bir kopyası 
    oluşturulacaktır. Ancak alt proses 3 numaralı betimleyiciyi de fork işleminden sonra kullanabilecektir. fork işleminden 
    sonra Üst prosesin 3 numaralı betimleyicisi ile alt prosesin 3 numaralı betimleyicisi aynı dosya nesnesini gösterecektir. 
    Özetle fork işlemi sırasında üst prosesin açmış olduğu dosyalar da adeta alt prosese aktarılmış gibi olmaktadır. 
    Peki bu çekirdek veri yapısında nasıl sağlanmaktadır? Anımsanacağı gibi fork işlemi sonrasında artık prosesler 
    birbirinden bağımsızdır. Yani fork işleminden sonra artık birinin açtığı dosya diğeri tarafından görülemez. İşte 
    fork işlemi sırasında tamamen alt proses için yeni bir files_struct nesnesi ve yeni bir dosya betimleyici tablosu 
    (fd dizisi) yaratılmaktadır. Ancak üst prosesin dosya betimleyici tablosundaki adresler yeni yaratılan alt posesteki 
    dosya betimleeyici tablosuna kopyalanamktadır. Böylece üst prosesin dosya betimleyici tablosunun aynı mumaralı 
    betimleyicileriyle alt prosesin dosya betimleyici tablosunun aynı numaralı betimleyicileri aynı dosya nesnelerini 
    gösteriyor durumda olur. Tabii artık üst ve alt proseslerin yeni açacağı dosyalar onlara özgü olacaktır. Paylaşılan 
    dosya nesneleri yalnızca fork öncesinde açılmış olanlardır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            26. Ders 12/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz şimdiye kadar açık dosyalara ilişkin çekirdeğin oluşturduğu veri yapıları hakkında şu bilgileri edindik:

    - Açık dosyalara ilişkin task_struct içerisindeki veri yapıları.
    - Dosya betimleyicilerinin anlamı ve dosya betimleyicisi yoluyla dosya nesnelerine nasıl erişildiği.
    - En düşük boş betimleyicinin elde edilme biçimi.

    Şimdi dosya sisteminin diğer önemli veri yapıları üzerinde duracağız ve temel dosya sistem fonksiyonlarının 
    gerçekleştirimleri hakkında temel bilgileri edineceğiz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdeğin açılmış olan dosyalara ilişkin bilgileri dosya nesnesi dediğimiz bir yapı içerisinde tuttuğunu belirtmiştik. 
    Güncel çekirdeklerde bu yapı "include/linux/fs.h" dosyasında aşağıdaki gibi bildirilmiştir:

    struct file {
        spinlock_t			            f_lock;
        fmode_t				            f_mode;
        const struct file_operations	*f_op;
        struct address_space		    *f_mapping;
        void				            *private_data;
        struct inode			        *f_inode;
        unsigned int			        f_flags;
        unsigned int			        f_iocb_flags;
        const struct cred		        *f_cred;
        struct fown_struct		        *f_owner;
        /* --- cacheline 1 boundary (64 bytes) --- */
        struct path			            f_path;
        union {
            /* regular files (with FMODE_ATOMIC_POS) and directories */
            struct mutex		        f_pos_lock;
            /* pipes */
            u64			                f_pipe;
        };
        loff_t				            f_pos;
    #ifdef CONFIG_SECURITY
        void				            *f_security;
    #endif
        /* --- cacheline 2 boundary (128 bytes) --- */
        errseq_t			            f_wb_err;
        errseq_t			            f_sb_err;
    #ifdef CONFIG_EPOLL
    struct hlist_head		            *f_ep;
    #endif
        union {
            struct callback_head	    f_task_work;
            struct llist_node	        f_llist;
            struct file_ra_state	    f_ra;
            freeptr_t		            f_freeptr;
        };
        file_ref_t			            f_ref;
        /* --- cacheline 3 boundary (192 bytes) --- */
    } __randomize_layout
    __attribute__((aligned(4)));	/* lest something weird decides that 2 is OK */

    Yapı bildirimindeki bazı elemanların bazı konfigürasyon seçenekleri seçildiğinde yapıya dahil edildiğine dikkat ediniz. 
    Eskiden file yapısı daha az elemana sahipti. Zaman içerisinde bu yapıda değişikler ve eklemeler yapılmış yapı bugünkü 
    durumuna gelmiştir. Örneğin çekirdeğin öğrenci ödevi gibi olan 0.01 versiyonunda bu yapı şöyleydi:

    struct file {
        unsigned short f_mode;
        unsigned short f_flags;
        unsigned short f_count;
        struct m_inode * f_inode;
        off_t f_pos;
    };

    Çekirdğein 2.2'li versiyonlarında yapı şöyle bildirilmişti:

    struct file {
        struct file		        *f_next, **f_pprev;
        struct dentry		    *f_dentry;
        struct file_operations	*f_op;
        mode_t			        f_mode;
        loff_t			        f_pos;
        unsigned int 		    f_count, f_flags;
        unsigned long 		    f_reada, f_ramax, f_raend, f_ralen, f_rawin;
        struct fown_struct	    f_owner;
        unsigned int		    f_uid, f_gid;
        int			            f_error;

        unsigned long		    f_version;

        /* needed for tty driver, and maybe others */
        void			        *private_data;
    };

    Çekirdeğin 2.4 versiyonunda file yapısı şöyleydi:

    struct file {
        struct list_head	    f_list;
        struct dentry		    *f_dentry;
        struct vfsmount         *f_vfsmnt;
        struct file_operations	*f_op;
        atomic_t		        f_count;
        unsigned int 		    f_flags;
        mode_t			        f_mode;
        loff_t			        f_pos;
        unsigned long 		    f_reada, f_ramax, f_raend, f_ralen, f_rawin;
        struct fown_struct	    f_owner;
        unsigned int		    f_uid, f_gid;
        int			            f_error;

        size_t			        f_maxcount;
        unsigned long		    f_version;

        /* needed for tty driver, and maybe others */
        void			        *private_data;

        /* preallocated helper kiobuf to speedup O_DIRECT */
        struct kiobuf		    *f_iobuf;
        long			        f_iobuf_lock;
    };

    Çekirdeğin 2.6 versiyonundaki file yapısı güncel versiyonlara daha fazla benzemektedir:

    struct file {
        /*
        * fu_list becomes invalid after file_free is called and queued via
        * fu_rcuhead for RCU freeing
        */
        union {
            struct list_head	        fu_list;
            struct rcu_head 	        fu_rcuhead;
        } f_u;
        struct path		                f_path;
    #define f_dentry	                f_path.dentry
    #define f_vfsmnt	                f_path.mnt
        const struct file_operations	*f_op;
        spinlock_t		                f_lock;  /* f_ep_links, f_flags, no IRQ */
    #ifdef CONFIG_SMP
        int			                    f_sb_list_cpu;
    #endif
        atomic_long_t		            f_count;
        unsigned int 		            f_flags;
        fmode_t			                f_mode;
        loff_t			                f_pos;
        struct fown_struct	            f_owner;
        const struct cred	            *f_cred;
        struct file_ra_state	        f_ra;

        u64			f_version;
    #ifdef CONFIG_SECURITY
        void			                *f_security;
    #endif
        /* needed for tty driver, and maybe others */
        void			                *private_data;

    #ifdef CONFIG_EPOLL
        /* Used by fs/eventpoll.c to link all the hooks to this file */
        struct list_head	            f_ep_links;
    #endif /* #ifdef CONFIG_EPOLL */
        struct address_space	        *f_mapping;
    #ifdef CONFIG_DEBUG_WRITECOUNT
        unsigned long f_mnt_write_state;
    #endif
    };

    file yapısının tüm elemanlarının f_ öneki ile başlatılarak isimlendirildiğine de dikkat ediniz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıda çekirdeğin çeşitli versiyonlarına ilişkin file yapılarını verdik. Peki bu yapının elemanları nelerdir 
    ve ne amaçla bu yapıda yer almaktadır? Aslında file yapısında çekirdeğin bir dosya üzerinde işlem yapabilmesi için 
    gerekli bilgiler bulunmaktadır. file yapısındaki elemanlar farklı konulara ilişkin olduğu için bu noktada bu elemanların 
    hepsini tek tek açıklamayacağız. Ancak file yapısının dosya işlemleri için kritik önemdeki bazı elemanları hakkında 
    açıklamalar yapmak istiyoruz. 

    Açık bir dosyanın open POSIX fonksiyonunda (yani sys_open sistem fonksiyonunda) kullanılan açış bayrakları (O_ ile 
    başlyan bayrakları kastediyoruz) file yapısının f_flags elemanında saklanamktadır. Örneğin open fonksiyonu ile dosya 
    şöyle açılmış olsun:

    fd = open("test.txt", O_RDWR|O_APPEND);

    Buradaki O_RDWR ve O_APPEND bayrakları file yapısının f_flags elemanında saklanmaktadır. Bu f_flags elemanının daha 
    çabuk işleme sokulabilecek yeniden düzenlenmiş hali yapının f_mode elemanında saklanmaktadır. (Bu f_mode elemanının 
    inode yapısındaki i_mode elemanıyla doğrudan bir ilgisi yoktur.) Okuma yazma işlemlerinin "dosya göstericisi (file 
    pointer)" denilen bir offset'en itibaren yapıldığını anımsayınız. İşte dosya göstericisinin konumu da file yapısının 
    f_pos elemanında tutulmaktadır. Dosya nesnelerini birden fazla betimleyici gösterebilmektedir. Örneğin dup ve dup2 
    POSIX fonksiyonları aynı dosya nesnesini gösteren farklı bir betimleyicinin oluşturulmasına yol açmaktadır. Benzer 
    biçimde fork işlemi sonrasında üst prosesin dosya betimleyici tablosununun betimleyicileri ile alt prosesin dosya 
    betimleyici tablosunun aynı numaralı betimleyicileri aynı dosya nesnesini gösteriyor durumda olur. Bu durumda close 
    fonksiyonu ile dosya kapatıldığında dosya hemen silinmez. Çünkü onu kullanan başka betimleyiciler de bulunuyor olabilir. 
    İşte file yapısının içerisindeki f_count elemanı o dosya nesnesinin kaç betimelyici tarafından gösterildiği bilgisini 
    tutmaktadır. Her betimleyici close fonksiyonu ile kapatıldığında bu sayaç 1 eksiltilir. Sayaç 0'a düştüğünde dosya 
    nesnesi silinir. İşte bu referans sayacı file yapısının içerisinde uzunca bir süredir f_count ismiyle bulunyordu. 
    Ancak çekirdeğin 6'lı güncel çekirdeklerde artık bu elemanın ismi f_ref biçimindedir. Aşağıda file yapısının gördüğümüz 
    önemli elemanlarının listesini veriyoruz:

    struct file {
        /* ... */
        	
        fmode_t				f_mode;
        unsigned int		f_flags;
        loff_t				f_pos;
        atomic_long_t		f_count;            /* file_ref_t f_ref */

        /*... */
    };
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki çekirdek açık bir dosya üzerinde read/write gibi işlemleri yaparken bir biçimde dosyanın diskteki bloklarına
    erişeceğine göre ve dosyanın son okunma tarihi, son güncelleme tarihi, dosya uzunluğu gibi bilgileri de güncelleyeceğine 
    göre bu bilgilere nasıl erişmektedir? İşte aslında dosyaların "uzunluk gibi, erişim hakları gibi, tarih-zaman bilgisi 
    gibi" metadata bilgileri diskte tutulmaktadır. (Ext dosya sistemlerinde bu bilgilerin diskte tutulduğu yere inode 
    blok denilmektedir.) Çekirdek dosya açılırken dosyanın bu metadata bilgilerini diskten okuyarak bellekte inode 
    isimli bir yapı nesnesinin içerisine yerleştirmektedir. Biz dosyaların metadata bilgilerinin tutulduğu bu inode yapısı 
    türünden nesnelere "inode nesnesi" de diyeceğiz. inode yapısı dosyanın diskteki bilgilerini bellekte temsil eden bir 
    veri yapısıdır. Yani çekirdek dosyanın metadata bilgilerine erişmek için sürekli diske başvurmamaktadır. O bilgileri 
    dosya açılırken diskten çekip inode nesnesi biçiminde bellekta saklamaktadır. Bu noktada hikayeye "inode nesnesi" 
    denilen yeni bir aktörün daha katıldığına dikkat ediniz.
    
    Peki bir dosya açıldığında dosyanın dosya sistemindeki yeri (örneğin yol ifadesi) çekirdek tarafından nasıl 
    saklanmaktadır? İşte çekirdek her dosya açıldığında o dosyanın dizin girişi bilgilerini (yani dosya sisteminde nerede 
    olduğu bilgisini ve bazı diğer bilgileri) ismine "dentry" denilen bir yapı nesnesine yerleştirmektedir. Açılmış olan 
    dosyanın dosya sistemindeki yerine ilişkin bu nesnelere biz "dentry nesneleri" diyeceğiz. Bu noktada hikayaye "dentry" 
    isimli başka bir aktörün daha katıldığını görüyorsunuz. 
    
    Şimdi dosya sistemine ilişkin nesneler hakkında bir özet yapalım:

    Dosya Nesnesi (struct file): Açılmış dosyalar üzerinde işlem yapmak için gereken tüm bilgilerin tutulfuğu nesne.
    Inode Nesnesi (struct inode): Dosyanın diskteki metadata bilgilerini tutan nesne. 
    Dentry Nesnesi (struct dentry) : Dosyanın dosya sisteni üzerinde yerini ve buna ilişkin bazı bilgileri tutan nesne.

    Peki dosyanın ilişkin olduğu inode nesnesine ve dentry nesnesine dosya nesnesi yoluyla nasıl erişilmektedir? 
    Uzunca bir süre (2.6'ya kadar ve 2.6'lı versiyonlar da dahil olmak üzere) dosya nesnesinin (file yapısının) içerisinde 
    dentry nesnesinin adresi, dentry nesnesinin içerisinde de o dizin girişinin inode nesnesinin adresi tutuluyordu:

    ┌──────────┐        ┌──────────┐         ┌──────────┐
    │  file    │ -----> │ dentry   │ ----->  │  inode   │
    └──────────┘        └──────────┘         └──────────┘

    Ancak daha sonraları dosya nesnesinden hareketle inode nesnesine daha kolay bir erişimin sağlanabilmesi için dosya
    nesnesinin içerisinde de doğrudan inode nesnesinin adresi tutulmaya başlanmıştır.

    ┌──────────┐        ┌──────────┐         ┌──────────┐
    │  file    │ -----> │ dentry   │ ----->  │  inode   │
    │          │        │          │         │          │
    │          │ --------------------------> │          │
    └──────────┘        └──────────┘         └──────────┘

    UNIX/Linux sistemlerinde bir dosya sistemi kök dizinde bir yere maount edilebilmektedir. Yani aslında bir yol ifadesine 
    ilişkin dosya ile diğer bir yol ifadesine ilişkin dosya farklı fiziksel aygıtlarda bulunuyor olabilir. Çekirdeğin bazı 
    durumlarda bir dosyanın hangi dosya sisteminin içerisinde bulunduğunu anlaması da gerekebilmektedir. Bu bilgilere dosyanın 
    "mount" bilgileri denilmektedir. Eskiden çekirdeğin 2.2 versiyonunda dosyanın mount bilgileri dentry nesnesi içerisinde 
    tutuluyordu. 2.4 ile birlikte dosyanın mount bilgileri daha düzenli bir biçimde file yapısının (dosya nesnesinin) vfsmount 
    isimli yapı türünden f_vfsmnt elemanında tutulmaya başlandı. 2.6 çekirdeklerinden itibaren de dosyanın dentry nesnesinin
    adresiyle vfsmount bilgileri path isimli bir yapıya yerleştirilmiş ve file yapısının içerisindeki f_path elemanında tutulmaya 
    başlanmıştır. 2.6 ve sonrasına ilişkin çekirdeklerdeki durum şöyledir:

    struct file {
        /* ... */
        	
        fmode_t				f_mode;
        unsigned int		f_flags;
        loff_t				f_pos;
        atomic_long_t		f_count;            /* file_ref_t f_ref */
        struct path			f_path;
        struct inode		*f_inode;

        /*... */
    };

    path yapısı da şöyle bildirilmiştir:

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;

    Buradaki __randomize_layout belirleyicisi sonraları eklenmiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz aynı dosyayı birden fazla kez open fonksiyonuyla açmış olalım. Örneğin:

    fd1 = open("test.txt", O_RDONLY);
    fd2 = open("test.txt", O_RDONLY);

    Bu durumda ne olacaktır? İşte çekirdek farklı dosya da olsa aynı dosya da olsa her açılan dosya için ayrı bir dosya 
    nesnesi (yani struct file nesnesi) oluşturmaktadır. (Çünkü örneğin aynı dosyayı birden fazla kez açtığımızda bu 
    dosyaların hepsinin dosya göstericileri faklı olmaktadır. Dosya göstericilerinin de dosya nesnesi içerisinde saklandığını 
    anımsayınız.) Ancak örneğimizdeki iki dosya neticide aslında diskte aynı dosyayı belirtmektedir. O halde bu iki dosya 
    için ayrı dentry ve inode nesneslerinin oluşturulmasına gerek yoktur. Kaldı ki farklı prosesler de aynı dosyayı açmış 
    olabilirler. Bunlar için de ayrı dentry ve inode nesnelerinin oluşturulumasına gerek yoktur. O halde her açılan yeni 
    dosya için çekirdek yeni bir dosya nesnesi yarattığı halde aynı dosyalar açıldığında bu dosyalar için tek bir dentry 
    ve inode nesnesi oluşturmaktadır. Bunun için tabii open fonksiyonuyla bir dosya açıldığında çekirdeğin "bu dosyaya 
    ilişkin dentry nesnesi ve inode nesnesi daha önce yaratılmış mı" diye bir araştırma yapması gerekmektedir. Bu araştırmayı
    yapabilmesi için de çekirdeğin bir biçimde bütün yaratılmış olan dentry ve inode nesnelerini bir yerde tutması gerekir. 

    İşte çekirdek dentry ve inode neneleri için ayrı önbellek (cache) sistemleri oluşturmaktadır. Bunlara Linux sistemlerinde 
    "dentry önbelleği (dentry cache)" ve "inode önbelleği (inode cache)" denilmektedir. Bir dosya açıldığında çekirdek 
    o dosyaya ilişkin dentry ve inode nesneleri zaten bu önbellek sistemlerinde varsa hiç diske gitmeden doğrudan bu 
    önbelleklerden onları alıp kullanmaktadır. Bir dosyayı onu açmış olan bütün prosesler kapatmış olsa bile o dosyaya 
    ilişkin dentry ve inode nesneleri bu önbellek sistemlerinde kalmaya devam edebilir. Çünkü dosyalar (özellikle bazı merkezi 
    dosyalar) bir kere değil farklı prosesler tarafından defalarca açılıp kullanılabilmektedir. (Örneğin "/etc/passwd" 
    dosyası pek çok proses tarafından dolaylı bir biçimde açılıp kullanılabilmektedir.) Bu tür durumlarda onların bu 
    önbellekler içerisinde biriktirilmesi sistem performansını oldukça iyileştirmektedir. Tabii bu önbellek sistemlerinin 
    de belli bir büyüklüğü vardır. Bu önbellek sistemleri dolduğunda dentry ve inode nesnelerinin bazıları bu önbelleklerden 
    atılmaktadır. Linux'taki bu tür önbellek sistemlerinde önbellekten çıkarma için genel olarak "LRU (Least Recently 
    Used)" denilen "önbellek yer değiştirme (cache replacement) algoritması" işletilmektedir. Yani önbellekten toplamda 
    en az kullanılanlar değil "son zamanalarda en az kullanılanlar" çıkartılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bu noktada "dentry önbelleği (dentry cache)" ve "inode önbelleği (inode cache)" ile "sayfa önbelleği (page cache)"  
    arasındaki ilişkiye de değinelim. Sayfa önbelleği disk blokları temeleninde organize edilen ve her türlü disk bloğunun 
    transfer edilmesinde kullanılan aşağı seviyeli bir önbellek sistemidir. Halbuki dentry ve inode önbellekleri yalnızca
    dentry ve inode elemanalarından oluşan önbelleklerdir. Bir dosya açıldığında eğer o dosyaya ilişkin dentry ve inode 
    nesneleri dentry ve inode önbelleklerinde yoksa diskten elde edilmektedir. Tabii bu amaçla disk okuması yapılmadna 
    önce bu disk bloklarının sayfa önbelleğinde olup olmadığına da bakılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Geldiğimiz noktaya kadarki konuları dikkate aldığımızda open fonksiyonuyla bir dosyanın açılması durumunda sırasıyla 
    şunların yapıldığını söyleyebiliriz:

    1) Prosesin dosya betimleyici tablosunda boş bir betimleyici hızlı bir biçimde bulunur.
    2) Bir dosya nesnesi (struct file nesnesi) yaratılır ve elemanalarına gerekli ilkdeğerler verilir.
    3) Dosyaya ilişkin dentry nesnesi ve inode nesnesi dentry önbelleğinde ve inode önbelleğinde yoksa diskte arama 
    yapılarak yaratılır ve bunların adresleri dosya nesnesine yerleştirilir.
    4) Dosya nesnesinin adresi dosya beyimeleyici tablosuna (fd dizisine) yerleştirilir ve yerleştirilen dizi indeksi 
    dosya betimleyicisi olarak geri döndürülür.

    Ancak dosya açılırken henüz ele almadığımız başka süreçler de işin içerisine girmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir prosesin "kök dizininin (root directory)" ve "çalışma dizinin (current working directory)" proses kontrol 
    bloğunda yani task_struct nesnesinde tutulduğunu belirtmiştik. Güncel çekirdeklerde bu bilgi şöyle tutulmaktadır:

    struct task_struct {
        /* ... */

        struct fs_struct *fs;

        /* ... */
    };

    struct fs_struct {
        /* ... */
        struct path root, pwd;

        /* ... */  
    } __randomize_layout;

    path yapısını daha önce vermiştik:

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;

    Görüldüğü gibi çekirdek prosesin kök dizinin ve çalışma dizininini yol ifadesi biçiminde değil dentry nesnesi 
    biçiminde tutmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            27. Ders 19/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi dosya sisteminin temel veri yapıları üzerinde görmüş olduğumuz konulara ilişkin bazı testler yapalım. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------  
    İlk örneğimizde bir dosya betimleyicisine ilişkin dosya nesnesini elde ederek onun f_pos elemanını yazdırmaya çalışalım. 
    Dosya nesnesindeki f_pos elemanının dosya göstericisinin değerini tuttuğunu anımsayınız. Bu işlemi bir aygıt sürücü 
    yazarak ioctl yoluyla gerçekleştirebiliriz. (Kursumuzda kullandığımız Linux makinedeki çekirdek sürümü 6.9.2'dir.) 
    Bunun şçin oluşturduğumuz ioctl fonksiyonuna dikkat ediniz:

    static long ioctl_test1(struct file *filp, unsigned long arg)
    {
        struct file *f;
        int fd = (int) arg;

        if (fd < 0 || fd >= current->files->fdt->max_fds) {
            printk(KERN_INFO "file descriptor is not valid!..\n");
            return -EBADF;
        }

        f = current->files->fdt->fd[fd];
        if (f == NULL) {
            printk(KERN_INFO "file descriptor is not valid!..\n");
            return -EBADF;
        }

        printk(KERN_INFO "File pointer offset: %lld\n", (long long)f->f_pos);

        return 0;
    }

    Burada daha önceden de belirttiğimiz gibi dosya betimleyicisinden hareketle dosya nesnesine current->files->fdt->fd[fd]
    ifadesiyle erişilmiştir. Ancak erişmeden önce fd ile belirtilen betimleyicinin o anda çekirdek tarafından tahsis 
    edilmiş olan dosya betimeleyici tablosunun uzunluğundan büyük olup olmadığına da bakılmıştır:

    if (fd < 0 || fd >= current->files->fdt->max_fds) {
        printk(KERN_INFO "file descriptor is not valid!..\n");
        return -EBADF;
    }

    Burada önemli bir nokta üzerinde durmak istiyoruz. Biz test amacıyla dosya betimleyicisinden hareketle dosya 
    nesnesine current->files->fdt->fd[fd] ifadesiyle eriştik. Ancak bu erişim aslında güvenli değildir. Çünkü tam bu 
    erişim yapılırken eğer dosya betimleyici tablosu büyütülürse ya da bu betimleyici üzerinde işlem yapılırsa bizim 
    kodumuz kararsız bir durumla karşı karşıya kalır. Tabii çekirdek durup dururken dosya betimleyici tablomuz üzerinde 
    işlem yapmaz. Çekirdek ancak biz bir dosya işlemi yaptığımızda bizim dosya betimleyici tablomuza erişmektedir. Çünkü 
    dosya betimleyici tablosu prosese özgüdür. O halde biz ioctl çağrısı yaparken aynı zamanda başka bir thread'te 
    (ya da alt proseste) dosya işlemi yapmıyorsak bir sorun da oluşmayacaktır. Ancak genel olarak çekirdeğin uyguladığı 
    senkronizyan mekanizmasına uygun hareket etmek gerekir. Linux çekirdeklerinin bir süredir "kilitsiz (lock-free)" 
    veri yapılarından olan RCU mekanizmasının kullandığını belirtmiştik. Biz bu veri yapısı üzerindeki ayrıntıları 
    "çekirdeğin senkronizasyon mekanizmalarını" anlattığımız bölümde açıklayacağız. 

    Burada diğer bir nokta da nesnelerin referans sayaçlarıyla ilgilidir. Daha önceden de belirttiğimiz gibi çekirdek 
    nesneleri farklı amaçlarla birden fazla kaynak tarafından kullanılabilmektedir. Bu tür çekirdek nesnelerinde bu 
    nedenle hep bir sayaç tutulmaktadır. Bir çekirdek nesnesi üzerinde işlem yapacak kişi eğer bu sayacı artırmazsa 
    çekirdek onu boşaltabilir. İşlem yapan kişi de tanımsız davranışla karşılaşabilir. Tabii yukarıdaki gibi prosese
    özgü test işlemlerinde çekirdek bizim programımız talep etmedikten sonra kapatma işlemleri yapmayacaktır. Ancak 
    genel olarak bu tür durumlarda "çekirdek kaynağı boşaltmasın diye" nesnenin referans sayacı artırılmalı, kullanım 
    bittikten sonra da azaltılmalıdır. Daha önceden de  belirttiğimiz gibi Linux çekirdeklerinde sayacı artırarak nesneyi 
    elde eden fonksiyonlar "get" soneki ile sayacı azaltarak nesneyi bırakan fonksiyonlar ise "put" soneki ile isimlendirilmiştir. 
    Örneğin fget isimli yüksek seviyeli çekirdek fonksiyonu dosya betimelicisinden hareketle bize dosya nesnesini 
    RCU mekanizmasını kullanarak dosya nesnesinin sayacını da artırarak vermektedir. fput fonksiyonu da sayacı azaltarak
    dosya nesnesini geri bırakmaktadır. fget ve fput fonksiyonlarının prototipleri şöyledir:

    struct file *fget(unsigned int fd);
    void fput(struct file *);

    Bu fonksiyonların prototipleri "include/linux.file.h" dosyası içerisindedir. Güncel çekirdeklerde fget fonksiyonun 
    tanımlaması "fs/file.c" dosyası içerisinde fput fonksiyonun tanımlaması ise "fs/file_table.c" içerisinde yapılmıştır. 
    fget ve fput fonksiyonları export edildiği için aygıt sürücüler içerisinde de kullanılabilmektedir. Bu durumda biz 
    yukarıdaki testi daha basit bir biçimde fget ve fput fonksiyonlarını kullanarak aşağıdaki gibi de yapabiliriz:

    static long ioctl_test2(struct file *filp, unsigned long arg)
    {
        struct file *f;
        int fd = (int) arg;

        if ((f = fget(fd)) == NULL) {
            printk(KERN_INFO "file descriptor is not valid!..\n");
            return -BADF;
        }
        
        printk(KERN_INFO "File pointer offset: %ld\n", (long)f->f_pos);

        fput(f);

        return 0;
    }
        
    Güncel çekirdeklere fget ve fput işlemlerini daha etkin gerçekleştirmek için fdget ve fdput isimli fonksiyonlar da
    eklenmiştir. Bu fonksiyonların ptototipleri şöyledir:

    static struct fd fdget(unsigned int fd);
    static void fdput(struct fd fd);

    Kursun yapıldığı makinede bulunan 6.9.2 çekirdeğinde struct fd yapısı şöyle bildirilmiştir:

    struct fd {
        struct file *file;
        unsigned int flags;
    };
    
    Ancak en yeni çekirdeklerde fd yapısı şöyledir:

    struct fd {
        unsigned long word;
    };

    En yeni çekirdeklerde dosya nesnesine erişim için yalnızca fdget değil fd_file fonksiyonun da kullanılması gerekmektedir. 
    Örneğin:

    struct fd f = fdget(fd);
	struct file *file = fd_file(f);

    Güncel çekirdeklerde artık fdget ve fdput fonksiyonları export edilmiştir. Ancak kurusumuzun yapıldığı 6.9.2 
    çekirdeklerinde bu fonksiyonlar export edilmemişti. 

    Her ne kadar yeni çekirdeklerde fdget ve fdput daha hızlı çalışıyorsa da fget ve fput basit arayüzü ve kullanım 
    kolaylığı ve eskiden beri aynı biçimde bulunması nedeniyle tercih edilebilir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        28. Ders 25/10/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dup POSIX fonksiyonun işlevini yerine getiren (yani sys_dup sistem fonksiyonu gibi çalışan) bir fonksiyonu 
    kendimiz yazalım. Çekirdeği yeniden derlememek için bu işlemi de bir aygıt sürücüye yaptırabiliriz. Bunun için bir 
    ioctl kodu oluşturabiliriz. Bu ioctl kodu kullanıcı modundan çağrılırken ioctl fonksiyonunun üçüncü parametresine
    aşağıdaki gibi bir yapı nesnesinin adresini geçebiliriz:

    struct FDDUP {
        int fd;
        int fd_dup;
    };

    Burada yapının fd elemanı çiftlenecek dosya betimleyicisini belirtmektedir. fd_dup elemanı ise çiftleme sonucunda 
    elde edilecek yeni betimleyiciyi belirtmektedir. (Yani biz fonksiyonu çağırmadan önce yapının fd elemanına çiftlenecek 
    betimleyiciyi yerleştireceğiz, fonksiyon da bunu çiftleyip yeni betimleyiciyi yapının fd_dup elemanına yerleştirecek.)
    Bu elemanların dup fonksiyonu ile ilişkisini şöyle temsil edebiliriz:

    fd_dup = dup(fd);
    
    Dosya betimleyicisini çiftleyen ioctl kodu düz bir biçimde test amaçlı olarak (yani bazı kusurlarla) şöyle 
    yazılabilir:

    static long ioctl_test3(struct file *filp, unsigned long arg)
    {
        struct file *f;
        struct fdtable *fdt;
        struct file **fd_table;
        struct FDDUP fddup;
        int i;

        f = NULL;

        if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
            return -EFAULT;

        fdt = current->files->fdt;
        fd_table = fdt->fd;

        if (fddup.fd < 0 || fddup.fd >= fdt->max_fds)
            return -EBADF;
        if (fd_table[fddup.fd] == NULL)
            return -EBADF;

        for (i = 0; i < fdt->max_fds; ++i)
            if (fd_table[i] == NULL) {
                f = fd_table[fddup.fd];	
                fd_table[i] = f;
                ++f->f_count.counter;			/* atomic_long_inc(&f->f_count); */
                fddup.fd_dup = i;
                break;
            }
        if (f == NULL)
            return -EMFILE;

        if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0)
            return -EFAULT;
        
        return 0;
    }

    Fonksiyonumuzda önce kullanıcı modundaki fddup nesnesini çekirdek moduna copy_from_user fonksiyonu ile kopyaldık:

    if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
        return -EFAULT;

    Sonra fonksiyonumuzda çiftlenecek dosya betimleycisinin geçerli bir betimleyici olup olamdığına baktık:

    fdt = current->files->fdt;
    fd_table = fdt->fd;

    if (fddup.fd < 0 || fddup.fd >= fdt->max_fds)
        return -EBADF;
    if (fd_table[fddup.fd] == NULL)
        return -EBADF;

    Bu işlemden sonra fonksiyonumuzda bir döngü içerisinde ilk boş betimleyiciyi bulup, çiftlenecek olan betimeleyicinin
    gösterdiği dosya nesnesinin adresini bu boş betimleyiciye yerleştirdik. Bu tür işlemlerde dosya nesnesinin sayacının
    artırılması gerektiğini anımsayınız:

	for (i = 0; i < fdt->max_fds; ++i)
		if (fd_table[i] == NULL) {
			f = fd_table[fddup.fd];	
			fd_table[i] = f;
			++f->f_count.counter;			/* atomic_long_inc(&f->f_count); */
			fddup.fd_dup = i;
			break;
		}
	if (f == NULL)
		return -EMFILE;

    Kursumuzun yapıldığı 6.9.2 çekirdeğinde dosya nesnesinin sayacının file yapısının içerisinde şöyle tutulduğunu yeniden
    anımsatmak istiyoruz:

    struct file {
        /* ... */

        atomic_long_t		f_count;

        /* ... */
    };

    Burada aslında atomic_long_t türü bir yapı biçiminde bildirilmiştir:

    typedef struct {
        long counter;
    } atomic_long_t;

    Neden bu sayacın doğrudan long bir yapı elemanında tutulmayıp başka bir yapının elemanı yapıldığını merak edebilirsiniz. 
    Bunun amacı aslında bu elemanın gizlenmek istenmesidir. Bu atomic_long_t türüyle belirtilen değerler üzerinde işlemler
    atomik düzeyde özel fonksiyonlarla yapılmaktadır. Aslında atomic_long_t türünün atomic_t isimi int türden versiyonu da 
    vardır. Eskiden çekirdeklerde atomic_long_t yerine yalnızca atomic_t türü bulunuyordu. Sonra atomic_long_t türü de eklendi. 
    atomic_long_t türü üzerinde atomik işlem yapan fonksiyonların bizim için bu konu bağlamında önemli olan birkaçını 
    aşağıda veriyoruz:

    void atomic_long_set(atomic_long_t *v, long i);
    long atomic_long_read(const atomic_long_t *v);
    void atomic_long_inc(atomic_long_t *v);
    void atomic_long_dec(atomic_long_t *v);

    atomic_t ve atomic_long_t türleri hakkında ayrıntıları başka bir başlıkta ele alacağız. 

    Fonksiyonumuzda en sonunda yeniden çekirdek modundaki fddup nesnesi kullanıcı modundaki prosesin fddup nesnesine
    copy_to_user fonksiyonu ile kopyalanmıştır:

    if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0)
            return -EFAULT;

    Fonksiyonumuzdaki döngüde aslında mantıksal bir kusur da vardır. Biz döngüde yalnızca o andaki dosya betimleyici 
    tablosunda arama yaptık. Halbuki çekirdek aslında başlangıçta küçük bir dosya betimleyci tablosunu tutarken daha 
    sonra bunu gerektiğinde proses limitinin izin verdiği kadar yükseltebilmektedir. Halbuki bizim kodumuz bunu göz 
    ardı etmiştir. Yani aslında işin başında anımsayacağınız gibi max_fds 64 bit sistemlerde 64 elemanlı bir dosya 
    betimeleyici tablousunun uzunluğunu tutmaktadır. Halbuki default durumda prosesin dosya betimleyici tablosu 1024 
    geçerli uzunluğa sahiptir. O halde aslında bizim max_fds ile belirtilen dosya betimleyici tablosu elemanlarının 
    hepsi doluysa dosya betimleyici tablosunu prosesin kaynak limitlerinde belirtilen değere (default durumda 1024) 
    büyütüp arama işlemini oradan devam ettirmemiz gerekirdi. Ancak bunu biz yapmadık.

    Fonksiyonumuzda ilk boş betimleyicinin aranmasının düz mantıkla bir for döngüsü içerisinde -tıpkı çekirdeğin 
    ilkel versiyonlarındaki gibi- yapıldığına dikkat ediniz. Aslında testi yaptığımız makinedeki Linux çekirdeğinde 
    anımsayacağınız gibi ilk boş betimleyiciyi hızlı bir biçimde bulabilmek için iki düzey bitmap kullanılıyordu. 
    Bu versiyonlarda ilk boş betimleyiciyi bulan get_unused_fd_flags isimli daha yüksek seviyeli bir çeekirdek 
    fonksiyonu bulunmaktadır. Bu fonksiyon export edildiği için aygıt sürücülerden de kullanılabilmektedir. Fonksiyon 
    şöyle yazılmıştır:

    int get_unused_fd_flags(unsigned flags)
    {
        return __get_unused_fd_flags(flags, rlimit(RLIMIT_NOFILE));
    }
    EXPORT_SYMBOL(get_unused_fd_flags);

    Fonksiyonun başka bir fonksiyonu çağırdığını görüyorsunuz. Fonksiyonun flags parametresi boş dosya betimleycisini 
    bulurken aynı zamanda dosyaya ilişkin O_CLOEXEC bayrağının da set edilmesini sağlayabilmektedir. Tabii böyle bir 
    şeyi istemiyorsanız bu parametreye 0 geçebilirsiniz. (Yani bu fonksiyonun parametresi ya O_CLOEXEC biçiminde ya da
    0 biçiminde geçilebilmektedir.) get_unused_fd_flags fonksiyonu aynı zamanda "gerektiğinde dosya betimleyici tablosunu 
    büyütme işlemini de" kendisi yapmaktadır. Dolayısıyla bu yüksek seviyeli fonksiyon hem aygıt sürücüler tarafından 
    hem de çekirdek kodları üzerinde değişilik yapacak kişiler tarafından bu tür durumlarda tercih edilmektedir. 
    get_unused_fd_flags fonksiyonu başarı durumunda yeni boş betimleyiciye başarısızlık durumunda ise -EMFILE ya da -ENOMEM 
    değerlerinden birine geri dönmektedir. Eskiden (örneğm çekirdeğin 2.2, 2.4 ve 2.6 versiyonlarında) get_unused_fd_flags 
    fonksiyonu yerine flags'siz get_unused_fd fonksiyonu bulunuyordu:

    int get_unused_fd(void);

    get_unused_fd_flags fonksiyonu çekirdeğin 2.6.23 versiyonunda eklenmiştir. 

    Aslında çekirdek içerisinde RCU mekanizması eşliğinde files_struct yapısının adresini alarak current->files->fdt 
    gösterisini elde eden files_fdtable isimli bir çekirdek fonksiyonu da bulunmaktadır. Bu durumda dosya betimleyici 
    tablosuna şöyle de erişebilirdik:

    fdt = files_fdtable(current->files);
    fd_table = fdt->fd;

    RCU mekanizması eşliğinde dosya nesnesini (strcut file nesnesini) dosya betimelyeici tablosuna yerleştiren 
    fd_install isimli export edilmiş yüksek seviyeli bir çekirdek fonksiyonu da bulunmaktadır:

    void fd_install(unsigned int fd, struct file *file);

    Eğer işlemler RCU mekanizmasına uyumlu olacaksa dosya betimeleyicisini dosya betimleyici tablosuna yerleştirilmesi 
    için fd_install fonksiyonu tercih edilmelidir. 

    Böylece yukarıdaki fonksiyonu şu hale getirebiliriz:
 
    static long ioctl_test4(struct file *filp, unsigned long arg)
    {
        struct file *f;
        struct FDDUP fddup;
        
        if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
            return -EFAULT;

        if ((f = fget(fddup.fd)) == NULL)
            return -EBADF;
            
        if ((fddup.fd_dup = get_unused_fd_flags(0)) < 0) {
            fput(f);
            return fddup.fd_dup;
        }

        if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0) {
            fput(f);
            return -EFAULT;
        }

        fd_install(fddup.fd_dup, f);

        /*
        fd_table = current->files->fdt->fd;
        fd_table[fddup.fd_dup] = f;	
        */

        return 0;
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıdaki test işlemlerine ilişkin aygıt sürücü kodunu bir bütün olarak aşağıda veriyoruz. 
----------------------------------------------------------------------------------------------------------------------*/

/* test-driver.h */

#ifndef TEST_DRIVER_H_
#define TEST_DRIVER_H_

#include <linux/stddef.h>
#include <linux/ioctl.h>

struct FDDUP {
    int fd;
    int fd_dup;
};

#define TEST_DRIVER_MAGIC		't'
#define IOC_TEST1		        _IOR(TEST_DRIVER_MAGIC, 0, int)
#define IOC_TEST2		        _IOR(TEST_DRIVER_MAGIC, 1, int)
#define IOC_TEST3		        _IOWR(TEST_DRIVER_MAGIC, struct FDDUP)
#define IOC_TEST4		        _IOWR(TEST_DRIVER_MAGIC, 3, struct FDDUP)

#endif

/* test-driver.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/cdev.h>
#include <linux/fdtable.h>
#include <linux/file.h>
#include "test-driver.h"

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("test-driver");

static int test_driver_open(struct inode *inodep, struct file *filp);
static int test_driver_release(struct inode *inodep, struct file *filp);
static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off);
static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off);
static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);

static long ioctl_test1(struct file *filp, unsigned long arg);
static long ioctl_test2(struct file *filp, unsigned long arg);
static long ioctl_test3(struct file *filp, unsigned long arg);
static long ioctl_test4(struct file *filp, unsigned long arg);

static dev_t g_dev;
static struct cdev g_cdev;
static struct file_operations g_fops = {
	.owner = THIS_MODULE,
	.open = test_driver_open,
	.read = test_driver_read,
	.write = test_driver_write,
	.release = test_driver_release,
    .unlocked_ioctl = test_driver_ioctl
};

static int __init test_driver_init(void)
{
	int result;

	printk(KERN_INFO "test-driver module initialization...\n");

	if ((result = alloc_chrdev_region(&g_dev, 0, 1, "test-driver")) < 0) {
		printk(KERN_INFO "cannot alloc char driver!...\n");
		return result;
	}
	cdev_init(&g_cdev, &g_fops);
	if ((result = cdev_add(&g_cdev, g_dev, 1)) < 0) {
		unregister_chrdev_region(g_dev, 1);
		printk(KERN_ERR "cannot add device!...\n");
		return result;
	}

	return 0;
}

static void __exit test_driver_exit(void)
{
	cdev_del(&g_cdev);
	unregister_chrdev_region(g_dev, 1);

	printk(KERN_INFO "test-driver module exit...\n");
}

static int test_driver_open(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver opened...\n");

	return 0;
}

static int test_driver_release(struct inode *inodep, struct file *filp)
{
	printk(KERN_INFO "test-driver closed...\n");

	return 0;
}

static ssize_t test_driver_read(struct file *filp, char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver read...\n");

	return 0;
}

static ssize_t test_driver_write(struct file *filp, const char *buf, size_t size, loff_t *off)
{
	printk(KERN_INFO "test-driver write...\n");

	return 0;
}

static long test_driver_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
    long result;
	
    printk(KERN_INFO "test_driver_ioctl...\n");

    switch (cmd) {
        case IOC_TEST1:
            result = ioctl_test1(filp, arg);
            break;  
		case IOC_TEST2:
            result = ioctl_test2(filp, arg);
            break;  
		case IOC_TEST3:
            result = ioctl_test3(filp, arg);
            break;  
		case IOC_TEST4:
            result = ioctl_test4(filp, arg);
            break;  
        default:
            result = -ENOTTY;
    }

    return result;
}

static long ioctl_test1(struct file *filp, unsigned long arg)
{
	struct file *f;
	int fd = (int)arg;

	if (fd < 0 || fd >= current->files->fdt->max_fds) {
		printk(KERN_INFO "file descriptor is not valid!..\n");
		return -EBADF;
	}

	f = current->files->fdt->fd[fd];
	if (f == NULL) {
		printk(KERN_INFO "file descriptor is not valid!..\n");
		return -EBADF;
	}

	printk(KERN_INFO "File pointer offset: %ld\n", (long)f->f_pos);

    return 0;
}

static long ioctl_test2(struct file *filp, unsigned long arg)
{
	struct file *f;
	int fd = (int)arg;

	if ((f = fget(fd)) == NULL) {
		printk(KERN_INFO "file descriptor is not valid!..\n");
		return -EBADF;
	}
	
	printk(KERN_INFO "File pointer offset: %ld\n", (long)f->f_pos);

	fput(f);

    return 0;
}

static long ioctl_test3(struct file *filp, unsigned long arg)
{
	struct file *f;
	struct fdtable *fdt;
	struct file **fd_table;
	struct FDDUP fddup;
	int i;

	f = NULL;

	if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
		return -EFAULT;

	fdt = current->files->fdt;
	fd_table = fdt->fd;

	if (fddup.fd < 0 || fddup.fd >= fdt->max_fds)
		return -EBADF;
	if (fd_table[fddup.fd] == NULL)
		return -EBADF;

	for (i = 0; i < fdt->max_fds; ++i)
		if (fd_table[i] == NULL) {
			f = fd_table[fddup.fd];	
			fd_table[i] = f;
			++f->f_count.counter;			/* atomic_long_inc(&f->f_count); */
			fddup.fd_dup = i;
			break;
		}
	if (f == NULL)
		return -EMFILE;

	if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0)
		return -EFAULT;
	
    return 0;
}

static long ioctl_test4(struct file *filp, unsigned long arg)
{
	struct file *f;
	struct FDDUP fddup;
	
	if (copy_from_user(&fddup, (struct FDDUP *)arg, sizeof(fddup)) != 0)
		return -EFAULT;

	if ((f = fget(fddup.fd)) == NULL)
		return -EBADF;
		
	if ((fddup.fd_dup = get_unused_fd_flags(0)) < 0) {
		fput(f);
		return fddup.fd_dup;
	}

	if (copy_to_user((struct FDDUP *)arg, &fddup, sizeof(fddup)) != 0) {
		fput(f);
		return -EFAULT;
	}

	fd_install(fddup.fd_dup, f);

	/*
	fd_table = current->files->fdt->fd;
	fd_table[fddup.fd_dup] = f;	
	*/

	return 0;
}

module_init(test_driver_init);
module_exit(test_driver_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

#!/bin/bash

module=$1
mode=666

/sbin/insmod ./${module}.ko ${@:2} || exit 1
major=$(awk "\$2 == \"$module\" {print \$1}" /proc/devices)
rm -f $module
mknod -m $mode $module c $major 0

/* unload (bu satırı dosyaya kopyalamayınız ) */

#!/bin/bash

module=$1

/sbin/rmmod ./${module}.ko || exit 1
rm -f $module

/* app.c */

#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include "test-driver.h"

void exit_sys(const char *msg);

int main(int argc, char *argv[])
{
    int fd_dev;
    int fd;
    struct FDDUP fddup;
    off_t pos;
    
    if (argc != 2) {
        fprintf(stderr, "wrong number of arguments!..\n");
        exit(EXIT_FAILURE);
    }
       
    if ((fd_dev = open("test-driver", O_RDONLY)) == -1)
        exit_sys("open");
   
    if ((fd = open(argv[1], O_RDONLY)) == -1)
        exit_sys("open");
    lseek(fd, 100, 0);
   
    fddup.fd = fd;
    if (ioctl(fd_dev, IOC_TEST4, &fddup) == -1)
        exit_sys("ioctl");

    pos = lseek(fddup.fd_dup, 0, 1);
    printf("%jd\n", (intmax_t)pos);

    close(fd);
    close(fddup.fd);
    close(fd_dev);

    return 0;
}

void exit_sys(const char *msg)
{
    perror(msg);

    exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi dosyanın açılması ve kapatılması sürecini temel düzeyde inceleyelim. Bir dosya açılırken çekirdek kabaca neler 
    yapmaktadır? Bilindiği gibi kullanıcı modundan open POSIX fonksiyonuyle bir dosya açılmak istendiğinde open fonksiyonu 
    sys_open sistem fonksiyonunu çağırmaktadır. Dosya açma işlemi sys_open fonksiyonu tarafından yapılmaktadır. 2.6 
    çekirdeğinden itibaren sys_open sistem fonksiyonun kodlarında önemli bir değişiklik olmamıştır. Güncel çekirdeklerde 
    sys_open fonksiyonu "fs/open.c"" dosyası içerisinde şöyle tanımlanmıştır:

    SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)
    {
        if (force_o_largefile())
            flags |= O_LARGEFILE;
        return do_sys_open(AT_FDCWD, filename, flags, mode);
    }

    2.6 çekirdeğinden bu yana asıl açım işlemleri do_sys_open fonksiyonu tarafından yapılmaktadır. Ancak fonksiyonların 
    kodları üzerinde zamanla çeşitli değişiklikler yapılmıştır. do_sys_open fonksiyonu export edilmemiştir. Fonksiyonun 
    prototiği şöyledir:

    int do_sys_open(int dfd, const char __user *filename, int flags, mode_t mode);
    
    do_sys_open fonksiyonu aynı zamanda openat fonksiyonu (yani sys_openat sistem fonksyonu) tarafından da çağrılan ortak 
    bir fonksiyondur. Bu nedenle fonksiyonun birinci parametresi openat fonksiyonundaki "göreli yol ifadeleri için orijin 
    olarak kullanılacak dosya betimleyicisini" belirtmektedir. do_sys_open fonksiyonun birinci parametresine dikkat eediniz. 
    Bu parametreye AT_FDCWD özel değeri geçilmiştir. openat fonksiyonunun birinci parametresindeki betimleyiciye AT_FDCWD 
    özel değeri geçilirse fonksiyonun tamamen open gibi gibi davrandığını anımsayınız. sys_openat sistem fonksiyonu da 
    güncel çekirdeklerde "fs/open.c" dosaysında şöyle tanımlanmıştır:

    SYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags,
		umode_t, mode)
    {
        if (force_o_largefile())
            flags |= O_LARGEFILE;
        return do_sys_open(dfd, filename, flags, mode);
    }

    Görüldüğü gibi openat sistem fonksiyonu da aslında do_sys_open fonksiyonunu çağırmaktadır.

    Şimdi çağrıu zincirin dvam edelim. Güncel çekirdeklerde do_sys_open fonksiyonu "fs/open.c" dosyasında şöyle 
    tanımanmıştır:

    long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)
    {
        struct open_how how = build_open_how(flags, mode);
        return do_sys_openat2(dfd, filename, &how);
    }

    Fonksiyonda önce build_open_how fonksiyonu çağrılmıştır. Bu fonksiyon açış bayraklarını ve erişim haklarını bazı 
    kontroller uygulayarak ek bir yapı içinde toplamaktadır. Fonksiyonun bu işlemden sonra do_sys_openat2 fonksiyonunu 
    çağırdığını görüyorsunuz. Güncel çekirdeklerde bu fonksiyon da "fs/open.c" dosyasında şöyle tanımlanmıştır:

    static long do_sys_openat2(int dfd, const char __user *filename,
			   struct open_how *how)
    {
        struct open_flags op;
        int fd = build_open_flags(how, &op);
        struct filename *tmp;

        if (fd)
            return fd;

        tmp = getname(filename);
        if (IS_ERR(tmp))
            return PTR_ERR(tmp);

        fd = get_unused_fd_flags(how->flags);
        if (fd >= 0) {
            struct file *f = do_filp_open(dfd, tmp, &op);
            if (IS_ERR(f)) {
                put_unused_fd(fd);
                fd = PTR_ERR(f);
            } else {
                fd_install(fd, f);
            }
        }
        putname(tmp);
        return fd;
    }

    Fonksiyondaki bazı ayrıntıları göz ardı edersek birkaç noktaya dikkatinizi çekmek istiyoruz. build_open_flags fonksiyonu 
    open fonksiyonunda verilen bayrakları çekirdeğin işleyebilmesi için daha uygun bir hale getirmektedir. Bu fonksiyon 
    tarafından daha uygun hale getirilen açış bayrakları open_flags isimli bir yapı nesnesinde saklanmaktadır. getname 
    fonksiyonu ise kullanıcı modundaki yol ifadesini çekirdek moduna kopyalayarak çekirdek bir yapıya yerleştirmektedir. 
    Bundan sonra yukarıda görmüş olduğumuz get_unused_fd_flags fonksiyonu çağrılarak ilk boş betimleyici elde edilmiştir. 
    (POSIX standartlarına göre open fonksiyonunun dosya betimleyici tablosundaki ilk boş betimleyiciyi vermek zorunda 
    oldupuna dikkat ediniz.) Bundan sonra bütün geri kalan önemli işlemler do_filp_open fonksiyonu tarafından yapılmaktadır. 
    do_sys_open ve do_sys_openat2 fonksiyonları aygıt sürücüler için export edilmemiştir. Ancak do_filp open fonksiyonu 
    export edilmiş bir fonksiyondur. Güncel çekirdeklerdeki bu çağrı silsilesini şöyle gösterebiliriz:

    ┌────────────┐
    │  sys_open  │
    └───────┬────┘
            │
            ├──► ┌──────────────┐
            │    │ do_sys_open  │
            │    └──────┬───────┘
            │           │
            │           ├──► ┌─────────────────┐
            │           │    │ do_sys_openat2  │
            │           │    └────────┬────────┘
            │           │             │
            │           │             ├──► ┌───────────────┐
            │           │             │    │ do_filp_open  │
            │           │             │    └───────────────┘
            │           │             │
            └───────────┴─────────────┘

    Akış do_filp_open fonksiyonuna geldiğinde artık dosya betimleyici tablosunda boş betimleyici bulunmuştur. "filp"
    sözcüğü Linux çekirdeklerinde dosya nesnesini (yani struct file nesnesi) gösteren göstericiler için kullanılmaktadır. 
    (Yani "filp" sizcüğü struct file * anlamında kullanılmaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            29. Ders 26/10/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    do_filp_open fonksiyonu güncel çekirdeklerde "fs/namei.c" dosyasında şöyle tanımlanmıştır:

    struct file *do_filp_open(int dfd, struct filename *pathname,
		const struct open_flags *op)
    {
        struct nameidata nd;
        int flags = op->lookup_flags;
        struct file *filp;

        set_nameidata(&nd, dfd, pathname, NULL);
        filp = path_openat(&nd, op, flags | LOOKUP_RCU);
        if (unlikely(filp == ERR_PTR(-ECHILD)))
            filp = path_openat(&nd, op, flags);
        if (unlikely(filp == ERR_PTR(-ESTALE)))
            filp = path_openat(&nd, op, flags | LOOKUP_REVAL);
        restore_nameidata();
        return filp;
    }
    
    Fonksiyon kabaca şu bilgileri parametre yoluyla almaktadır:

    - Göreli yol ifadelerinin nereden itibaren çözüleceğine ilişkin dosya betimleyicisini
    - Açılacak dosyanın yol ifadesini 
    - Dosya açış bayraklarını

    do_flip_open fonksiyonu çekirdek modülleri ve aygıt sürücüler için export edilmemiştir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeklerinde çok eskiden beri çekirdek modülleri aygıt sürücüler tarafından kullanım için tasarlanmış 
    filp_open isimli bir fonksiyon da bulunmaktadır. do_filp_open fonksiyonu çekirdek tarafından kullanılan bir fonksiyondur. 
    Halbuki filp_open fonksiyonu aygıt sürücüler tarafından kullanılabilsin diye tasarlanmıştır.do_filp_open foksiyonu 
    yol ifadesini kullanıcı alanından çekerken filp_open fonksiyonunda yol ifadesi zaten çekirdek alanı içerisinde bulunmak 
    zorundadır. Tabii filp_open fonksiyonu da nihayetinde do_filp_open fonksiyonunu çağırmaktadır. Güncel çekirdeklerde 
    filp_open fonksiyonu "fs/open.c" dosyasında şöyle tanımlanmıştır:

    struct file *filp_open(const char *filename, int flags, umode_t mode)
    {
        struct filename *name = getname_kernel(filename);
        struct file *file = ERR_CAST(name);

        if (!IS_ERR(name)) {
            file = file_open_name(name, flags, mode);
            putname(name);
        }
        return file;
    }
    EXPORT_SYMBOL(filp_open);

    filp_open fonksiyonunun parametrelerinin open fonksiyonuna (dolayısıyla sys_open sistem fonksiyonuna) benzediğinde 
    dikkat ediniz. filp_open fonksiyonunun birinci parametresi olan yol ifadesi kullanıcı modunda bir adres değil çekirdek 
    modunda bir adres belirtmektedir. Buradaki file_open_name fonksiyonu da "fs/open.c" dosyasında şöyle tanımlanmıştır:

    struct file *file_open_name(struct filename *name, int flags, umode_t mode)
    {
        struct open_flags op;
        struct open_how how = build_open_how(flags, mode);
        int err = build_open_flags(&how, &op);
        if (err)
            return ERR_PTR(err);
        return do_filp_open(AT_FDCWD, name, &op);
    }

    Görüldüğü gibi bu fonksiyon da aslında çekirdek alanındaki yol ifadesini uygun biçime dönüştürerek do_filp_open 
    fonksiyonunu çağırmaktadır. do_filp_open fonksiyonun hangi fonksiyonlar tarafından çağrıldığını aşağdaki şeekille 
    görselleştirmek istiyoruz:

    ┌─────────────────────────────────────────────────────────────┐
    │           Kullanıcı Alanı (User Space)                      │
    │                                                             │
    │     open()                    openat()                      │
    │       │                          │                          │
    └───────┼──────────────────────────┼──────────────────────────┘
            │                          │
            │ (syscall)                │ (syscall)
            │                          │
    ┌─── ───▼──────────────────────────▼───────────────────────────┐
    │           Çekirdek Alanı (Kernel Space)                      │
    │                                                              │
    │    sys_open()              sys_openat()                      │
    │       │                         │                            │
    │       └──────────┬──────────────┘                            │
    │                  │                                           │
    │                  ▼                                           │
    │          do_sys_open()                                       │
    │                  │                                           │
    │                  ▼                                           │
    │          do_sys_openat2()                                    │
    │                  │                                           │
    │                  │                                           │
    │       ┌──────────┴──────────┐                                │
    │       │                     │                                │
    │       ▼                     ▼                                │
    │  filp_open()         ┌─────────────────┐                     │
    │       └─────────────►│ do_filp_open()  │  ◄─── Ortak Nokta   │
    │                      └─────────────────┘                     │
    │                              │                               │
    │                              ▼                               │
    │                    [Dosya Açma İşlemleri]                    │
    └──────────────────────────────────────────────────────────────┘  
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek modülleri ve aygıt sürücüler için bulundurulmuş olan filp_open fonksiyonu çağrıldığında bir dosya nesnesi 
    oluşturulup o dosya nesnesinin adresi (yani struct file nesnesinin adresi) geri dönüş değeri olarak verilmektedir. 
    filp_open fonksiyonunun parametrik yapısına dikkat ediniz:

    struct file *filp_open(const char *filename, int flags, umode_t mode);

    filp_open fonksiyonunun herhngi bir prosesle ilişkili bir dosya açmamaktadır. Yani bu fonksiyon oluşturduğu bu dosya 
    nesnesinin adresini herhangi bir prosesin dosya betimleyici tablosuna yazmamaktadır. Açılan dosya herhangi bir prroses 
    tarafından kullanılabilecek bir dosya değildir. Bu dosya yalnızca çekirdek modülleri ve aygıt sürücüler tarafından 
    kullanılabilmektedir. filp_open fonksiyonuyla açılmış olan dosyalar filp_close fonksiyonuyla kapatılabilirler. 
    Güncel çekirdeklerde filp_close fonksiyonu da "fs/open.c"" içerisinde şöyle tanımlanmıştır:

    int filp_close(struct file *filp, fl_owner_t id)
    {
        int retval;

        retval = filp_flush(filp, id);
        fput_close(filp);

        return retval;
    }
    EXPORT_SYMBOL(filp_close);

    filp_open fonksiyonu ile çekirdek modülü ya da aygıt sürücü bir dosyayı açtığında artık o dosyadan okuma ve o dosyaya 
    yazma işlemleri güncel çekirdeklerde "fs/read_write.c" dosyası içerisineki kernel_read ve kernel_write fonksiyoları 
    yoluyla yapılmaktadır. Bu fonksiyonların da prototipleri şçyledir:

    ssize_t kernel_read(struct file *file, void *buf, size_t count, loff_t *pos);
    ssize_t kernel_write(struct file *file, const void *buf, size_t count, loff_t *pos);

    Fonksiyonların parametrik yapılarının read ve write POSIX fonksiyonlarını andırdığına dikkat ediniz. Ancak bu fonksiyonlar 
    dosya betimeleyicisini değil dosya nesnesinin adresini parametre olarak almaktadır. Ayrıca fonksiyonalar dosya göstericisinin 
    gösterdiği yerden itibaren değil son parametreleri ile belirtilen offset'ten işlemlerini yapmaktadır. Bu fonksiyonlardaki 
    aktarım adresleri kullanıcı modundaki adresler değil çekirdek modundaki adreslerdir. Her ne kadar bu fonksiyonlar zaten 
    dosya göstericisini de parametre olarak alıyorsa da dosya göstericisini konumlandırmak için ayrıca vfs_llseek isimli 
    bir fonksiyon da bulunmaktadır:

    loff_t vfs_llseek(struct file *file, loff_t offset, int whence);

    kernel_read, kernel_write ve vfs_llseek fonksiyonlarrının hepsi export edilmiş durumdadır. 

    Aşağıda çekirdek modülü içerisinde dosya işlemlerinin yapılmasına yönelik basit bir örnek veriyoruz. İşlemler çekirdek 
    modülünün init fonksiyonunda yaoılmaktadır:

    static int test_module_init(void)
    {
        struct file *f;
        char *buf;
        ssize_t nread;
        loff_t pos;

        if ((f = filp_open("/etc/hostname", O_RDONLY, 0)) == NULL) {
            printk(KERN_INFO "cannot open file!...\n");
            return PTR_ERR(f);
        }

        if ((buf = kzalloc(100 + 1, GFP_KERNEL)) == NULL) {
            printk(KERN_INFO "cannot allocate memory!...\n");
            filp_close(f, NULL);
            return -ENOMEM;
        }

        pos = 0;
        if ((nread = kernel_read(f, buf, 100, &pos)) < 0) {
            kfree(buf);
            filp_close(f, NULL);
            return (int)nread;
        }

        buf[nread] = '\0';
        printk(KERN_INFO "%s\n", buf);

        kfree(buf);
        filp_close(f, NULL);

        return 0;
    }

    Burada dosyanın filp_open fonksiyonuyla açıldığını, kernel_read fonksiyonuyla dosyadan okuma yapıldığını ve dosyanın
    filp_close fonksiyonuyla kaapatıldığını görüyorsunuz. kzalloc fonksiyonu çekirdek alanında içi 0'larla dolu dinamik
    tahsisat yapmaktadır. Modülü yükledikten sonra okunanları görmek için "dmesg" komutunu çalıştırmalınınız.
----------------------------------------------------------------------------------------------------------------------*/

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");

static int test_module_init(void);
static void test_module_exit(void);

static int test_module_init(void)
{
    struct file *f;
    char *buf;
    ssize_t nread;
    loff_t pos;

    if ((f = filp_open("/etc/hostname", O_RDONLY, 0)) == NULL) {
        printk(KERN_INFO "cannot open file!...\n");
        return PTR_ERR(f);
    }

    if ((buf = kzalloc(100 + 1, GFP_KERNEL)) == NULL) {
        printk(KERN_INFO "cannot allocate memory!...\n");
        filp_close(f, NULL);
        return -ENOMEM;
    }

    pos = 0;
    if ((nread = kernel_read(f, buf, 100, &pos)) < 0) {
        kfree(buf);
        filp_close(f, NULL);
        return (int)nread;
    }

    buf[nread] = '\0';
    printk(KERN_INFO "%s\n", buf);

    kfree(buf);
    filp_close(f, NULL);

    return 0;
}

static void test_module_exit(void)
{
    printk(KERN_INFO "test_module_exit...\n");
}

module_init(test_module_init);
module_exit(test_module_exit);

# Makefile

obj-m += ${file}.o

all:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
    make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/* load (bu satırı dosyaya kopyalamayınız) */

/*----------------------------------------------------------------------------------------------------------------------
    Peki prosesin sys_open sistem fonksiyonuyla açtığı bir dosya nasıl kapatılmaktadır? Dosya kapatılırken çekirdeğin  
    dosya betimleyici tablosunda ilgili betimleyicinin boşaltması, dosyanın blokları page cache içerisinde kirlenmiş 
    (dirty) olarak bulunuyorsa onların da flush etmesi gerekir. Ancak dosya nesnesi başka prosesler tarafından da 
    (alt prosesleri kastediyoruz) kullanılabilidiği için ve dup gibi fonksiyonlarla çiftlenmiş olabileceği için doğrudan 
    serbest bırakılmamakta yalnızca sayacı eksiltilmektedir. Tabii dosya betimleyicisi serebest bırakıldığında aynı 
    zamanda artık bu dosya betimleyicisinin boş olduğunu gösteren ilgili bitmap'lerde de güncellemeler yapılmaktadır. 
    Kapatılan dosyaların dentry ve inode nesnesi üzerinde de bazı güncellemeler yapılmaktadır. Güncel çekirdeklerde 
    sys_close fonksiyonu "fs/open.c" dosyasında şöyle tanımlanmıtır:

    SYSCALL_DEFINE1(close, unsigned int, fd)
    {
        int retval;
        struct file *file;

        file = file_close_fd(fd);
        if (!file)
            return -EBADF;

        retval = filp_flush(file, current->files);

        /*
        * We're returning to user space. Don't bother
        * with any delayed fput() cases.
        */
        fput_close_sync(file);

        if (likely(retval == 0))
            return 0;

        /* can't restart close syscall because file table entry was cleared */
        if (retval == -ERESTARTSYS ||
            retval == -ERESTARTNOINTR ||
            retval == -ERESTARTNOHAND ||
            retval == -ERESTART_RESTARTBLOCK)
            retval = -EINTR;

        return retval;
    }

    Bu fonksiyonda kabaca şunlar yapılmaktadır:

    - file_close_fd fonksiyonu ile prosesin dosya betimleyici tablosundaki ilgili betimleyiciye ilişkin adrese NULL 
    yerleştirilir ve betimeleyici için boş betimleyicilerin hızlı bulunmasını sağlayan için iki düzey bitmap'te 
    güncellenemeler yapılır. Bu fonksiyon aynı zamanda betimleyiciye ilişkin dosya nesnesini adresini de geri 
    döndürmektedir. 

    - filp_flush fonksiyonu dosyanın page cache içerisinde bulunan kirlenmiş bloklarını flush etmektedir. Dosyanın 
    metadata bilgileri de bu sırada flush edilmektedir. 

    - fput_close_sync fonksiyonu dosya nesnesinin sayacını azaltıp dosyaya ilişkin dentry ve inode nesneleri üzerinde 
    ayrıntıları daha sonra anlatılacak olan bazı güncellemeleri yapmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi yeniden çekirdeğim öğrenci ödevi gibi olan ilkel 0.01 versiyonundaki sys_open fonksiyonuna göz gezdirelim:

    int sys_open(const char * filename,int flag,int mode)
    {
        struct m_inode * inode;
        struct file * f;
        int i,fd;

        mode &= 0777 & ~current->umask;
        for(fd=0 ; fd<NR_OPEN ; fd++)
            if (!current->filp[fd])
                break;
        if (fd>=NR_OPEN)
            return -EINVAL;
        current->close_on_exec &= ~(1<<fd);
        f=0+file_table;
        for (i=0 ; i<NR_FILE ; i++,f++)
            if (!f->f_count) break;
        if (i>=NR_FILE)
            return -EINVAL;
        (current->filp[fd]=f)->f_count++;
        if ((i=open_namei(filename,flag,mode,&inode))<0) {
            current->filp[fd]=NULL;
            f->f_count=0;
            return i;
        }
    /* ttys are somewhat special (ttyxx major==4, tty major==5) */
        if (S_ISCHR(inode->i_mode))
            if (MAJOR(inode->i_zone[0])==4) {
                if (current->leader && current->tty<0) {
                    current->tty = MINOR(inode->i_zone[0]);
                    tty_table[current->tty].pgrp = current->pgrp;
                }
            } else if (MAJOR(inode->i_zone[0])==5)
                if (current->tty<0) {
                    iput(inode);
                    current->filp[fd]=NULL;
                    f->f_count=0;
                    return -EPERM;
                }
        f->f_mode = inode->i_mode;
        f->f_flags = flag;
        f->f_count = 1;
        f->f_inode = inode;
        f->f_pos = 0;
        return (fd);
    }

    Bu ilkel versiyonda sistemde neredeyse modern işletim sistemlerinin sahip olduğu hiçbir cache sistemi ve hızlandırıcı 
    unsur yoktur. Fonksiyon işlemine şöyle başlatılmıştır:

    mode &= 0777 & ~current->umask;
    
    Burada önce porsesin umask değeri ile dosya açış modu maskelenmiştir. Sonra dosya betimeleyici tablosundaki ilk 
    boş betimleyici bir döngü ile elde edilmiştir:

    for(fd=0 ; fd<NR_OPEN ; fd++)
        if (!current->filp[fd])
            break;
    if (fd>=NR_OPEN)
		return -EINVAL;

    Bu ilkel versiyonda zaten prosesin açabileceği dosya sayısı (NR_OPEN değeri) 20 idi. Bundan sonra prosesin close-on-exec 
    bayrağı reset edilmiştir:

    current->close_on_exec &= ~(1<<fd);

    Tüm dosyaların close-on-exec bayrakları task_struct içerisindeki unsigned long türden  close_on_exec elemanında 
    tutulmaktadır. Zaten bu versiyonda prosesin açabileceği dosya sayısı 20 ile kısıtlıdır. 

    Bu ilkel versiyonda henüz çekirdekte bir heap sistemi yoktu. Dolayısıyla bütün dosya nesneleri baştan statik düzeyde 
    bir dizide saklanmıştır:

    struct file file_table[NR_FILE];

    Bütün dosya nesnelerinin toplam sayısı da (NR_FILE değeri) 64'tü. Fonksiyonda daha sonra bu 64 dosya betimleyicisi 
    dolaşılarak sıralı arama ile boş olan bir tanesi elde edilmiştir:

    f=0+file_table;
    for (i=0 ; i<NR_FILE ; i++,f++)
        if (!f->f_count) break;
    if (i>=NR_FILE)
        return -EINVAL;

    Dosya nesnesinin o zamanlarda da f_count isimli bir sayaç elemanı bulunmaktaydı. Burada sayacı 0 olan dosya nesnesinin 
    elde edildiğine dikkat ediniz. Bundan sonra bu nesnein adresi dosya betimleeyici tablosndaki ilgili betimleyicinin
    belirttiği dizi elemanına yerleştirilmiştir:
    
    (current->filp[fd]=f)->f_count++;

    Bu noktada henüz dosya nesnesinin elemanlarına atamalar yapılmamıştır. Bu versiyonda dosya açım işleminin büyük kısmı 
    open_nami isimli fonksiyon tarafından yapılmaktadır:

    if ((i=open_namei(filename,flag,mode,&inode))<0) {
		current->filp[fd]=NULL;
		f->f_count=0;
		return i;
	}

    open_namei fonksyionu isimsel olarak uzun süre kullanılmıştır. Nihayet bu ilkel versiyonda dosya nesnesinin elemanlarrına 
    ilkdeğerler verilerek fonksiyon dosya betimleyicisiyle geri döndürülmüştür:

    f->f_mode = inode->i_mode;
    f->f_flags = flag;
    f->f_count = 1;
    f->f_inode = inode;
    f->f_pos = 0;
    return (fd);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dosya açım işleminde kullanılan çekirdek mekanizmaları üzerinde duracağız. Yukarıda da belirttiğimiz gibi 
    bu ana işlevler güncel çeekirdeklerde do_filp_open fonksiyonu tarafından ilk versiyonlarda da open_namei fonksiyonu 
    tarafından yapılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    İşletim sistemlerinde bir yol ifadesi verildiğinde çekirdek yol ifadesine ilişkin dosya ya da dizinin ve o dosya ya 
    da dizinin içinde bulunduğu dizininin dentry ve inode nesnelerini bulmaya çalışır. (Bir dizindeki dosya ya da dizin 
    üzerinde işlem yapıldığında o dosya ya da dizinin içersinde bulunduğu dizinler üzerinde de işlemlerin yapılması 
    gerekebilmektedir.) Bu sürece işletim sistemlerinde "yol ifadesinin çözümlenmesi (pathname resolution)" ya da "yol 
    ifadesinin aranması (pathname lookup)" denilmektedir. Örneğin open fonksiyonuyla aşağıdaki gibi bir dosya açmak 
    isteyelim:

    fd = open("/home/kaan/Study/LinuxKernel/sample.c", O_RDONLY);

    İşletim sistemi "/home/kaan/Study/LinuxKernel/sample.c" biçiminde bir yol ifadesini aldığında öncelikle bu yol 
    ifadesinin hedefi olan dosya ya da dizinin (burada "sample.c") ve o dosya ya da dizinin içinde bulunduğu dizinin 
    ("burada "LinuxKernel") inode ve dentry nesnelerini bulmaya çalışır. Çekirdek bu işlemi bir döngü içerisinde yol 
    ifadesini "yol bileşenlerine (path component)" ayırarak adım adım gerçekleştirmektedir. Yukarıda yol ifadesinin yol 
    bileşenleri şunlardır:

    "home"
    "kaan"
    "Study"
    "LinuxKernel"
    "sample.c"

    Tabii çekirdeğin bu yol bileşenlerini tek tek doğrulaması da gerekmektedir. Örneğin burada "kaan"" yol bileşeninin 
    bir dizin belirtmesi gerekir. Eğer "kaan" dizin girişi "home" dizinin altında varsa ama bir dosya belirtiyorsa (yani 
    bir dizin belirtmiyorsa) çözümleme işlemine devam etmenin bir anlamı yoktur. Ayrıca çekirdeğin yol ifadesini çözümlerken 
    ilgili dizinler için erişim haklarını da kontrol etmesi gerekmektedir. Bir yol ifadesindeki tüm dizinlerin "x" hakkına 
    sahip olması gerektiğini anımsayınız. 

    Yol ifadelerinin çözümlenmesi (pathname resolution) sırasında yol bileşenleri öncelikle "dentry önbelliğinde (dentry
    cache)" aranmaktadır. Örneğin "kökün altında home dizini var mı?" araması yapılırken. "home" dizininine ilişkin 
    dentry nesnesi dentry önbelleğinden hızlı bir biçimde elde edilebilmektedir. İzleyen paragraflarda da açıklayacağımız 
    gibi dentry önbelleği içerisindeki bir dentry nesnesi "negatif değilse" zaten ona ilişkin inode nesnesi de inode 
    önbelleği içerisinde bulunmaktadır. Böylece pek çok yol ifadesi aslında hiç diske başvurulmadan dentry ve inode 
    önbellek mekanizması sayesinde hızlı bir biçimde çözülebilmektedir. Tabii bir yol bileşeni bu önbelleklerde yoksa 
    bu durumda artık disk okumaları yapılıp ilgili yol bileşenine ilişkin dentry ve inode nesneleri önbelleklere alınacaktır. 
    Daha önceden de belirttiğimiz gibi aslında diskte son okunan bloklar da "sayfa önbelliğinin (page cache)" içerisindedir. 
    Disk okuması sırasında hemen diske yönelinmemekte önce bu sayfa önbelleğine bakılmaktadır. Şimdi yol ifadesinin bu
    önbelleklek sistemleri kullanılarak nasıl çözüldüğüne ilişkin örnek verelim. Yol ifadesi aşağıdaki gibi olsun:

    "/home/kaan/Study/LinuxKernel/sample.c"

    Çekirdek önce dentry önbelleğinde üst dizini kök olan "home" isminde dizin belirten bir giriş var mı diye arama 
    yapar. Bu girişin bulunduğunu düşünelim. Bundan sonra çekirdek bu sefer üst dizini "home" olan "kaan" isminde dizin 
    belirten bir giriş var mı diye bakar. Aramalar böyle devam eder. Örneğin çekirdeğin üst dizini "LinuxKernel" olan 
    "sample.c" isimli girişi dentry önbelleğinde bulamadığını varsayalım. Bu durumda artık gerçekten bu bilgiler 
    diskten alınacak ve "sample.c" dosyasına ilişkin dentry nesnesi ve inode nesnesi ilgili önbelleklere yerleştirilecektir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        30. Ders 01/11/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yol ifadeleri çözümlenirken her yol bileşeni önce "dentry önbelleğinde (dentry cache)" aranmaktadır. Eğer yol bileşeni 
    dentry önbelleğinde  varsa doğrudan oradan alınarak çözümleme işlemine devam edilir. Eğer yol bileşeni dentry önbelleğinde
    yoksa disk okuması yapılarak bir dentry nesnesi oluşturulur ve tabii yeni oluşturulan nesne de dentry önbelleğine 
    yerleştirilir. Örneğin aşağıdaki gibi bir yol ifadesi olsun:

   "/home/kaan/Study/LinuxKernel/sample.c"

    Burada kökün altında "home" isimli yol bileşeni dentry önbelleğinde varsa hemen oradan alınır. Benzer biçimde "home" 
    dizininin altında "kaan" isimli dizin için dentry önbelleğinde bir dentry nesnesi zaten varsa hemen bu nesne oradan 
    alınacaktır. Ancak örneğin "Study" yol bileşenine ilişkin bir dentry nesnesi dentry önbelleğinde yoksa artık bu yol 
    bileşeni için dentry nesnesi disk okuması yapılarak elde edilecek ve dentry önbelleğine yerleştirilecektir. Böylece 
    çok kullanılan yol bileşenleri zamanla dentry önbelleği içerisinde biriktirilmiş olmaktadır. Bu da yol ifadelerinin
    çözümlenmesini oldukça hızlandırmaktadır. Dentry önbellek sistemi Linux çekirdeklerine 2'li versiyonlarla eklenmiştir. 
    Bunun öncesinde dentry nesneleri bir önbellek sisteminde biriktirilmiyordu. Her zaman tekrar tekrar bu bilgiler 
    disk işlemi yapılarak (tabii "sayfa önbelleğine (page cache)" de bakılarak) elde ediliyordu.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi dentry nesnelerinin genel yapısı üzerinde duralım. Dentry önbellek sistemi dentry nesnelerinden oluşmaktadır.
    Dentry nesneleri de çekierdekte dentry isimli yapıyla temsil edilmiştir. Güncel çekirdeklerdeki dentry yapısı 
    "include/linux/dcache.h" dosyası içerisinde şöyle bildirilmiştir:

    struct dentry {
        /* RCU lookup touched fields */
        unsigned int d_flags;		    /* protected by d_lock */
        seqcount_spinlock_t d_seq;	    /* per dentry seqlock */
        struct hlist_bl_node d_hash;	/* lookup hash list */
        struct dentry *d_parent;	    /* parent directory */
        struct qstr d_name;
        struct inode *d_inode;		    /* Where the name belongs to - NULL is
                                        * negative */
        union shortname_store d_shortname;
        /* --- cacheline 1 boundary (64 bytes) was 32 bytes ago --- */

        /* Ref lookup also touches following */
        const struct dentry_operations *d_op;
        struct super_block *d_sb;	    /* The root of the dentry tree */
        unsigned long d_time;		    /* used by d_revalidate */
        void *d_fsdata;			        /* fs-specific data */
        /* --- cacheline 2 boundary (128 bytes) --- */
        struct lockref d_lockref;	    /* per-dentry lock and refcount
                                        * keep separate from RCU lookup area if
                                        * possible!
                                        */

        union {
            struct list_head d_lru;		/* LRU list */
            wait_queue_head_t *d_wait;	/* in-lookup ones only */
        };
        struct hlist_node d_sib;	    /* child of parent list */
        struct hlist_head d_children;	/* our children */
        /*
        * d_alias and d_rcu can share memory
        */
        union {
            struct hlist_node d_alias;	                /* inode alias list */
            struct hlist_bl_node d_in_lookup_hash;	    /* only for in-lookup ones */
            struct rcu_head d_rcu;
        } d_u;
    };

    dentry yapısı eskiden çok daha sade idi. Zaman içerisinde yapıya yeni elemanlar eklenerek yapı iyice büyüdü. 
    Linux'un öğrenci ödevi gibi olan ilk 0.01 versiyonunda dentry yapısı dir_entry ismiyle aşağıdaki gibi bildirilmişti:

    struct dir_entry {
        unsigned short inode;
        char name[NAME_LEN];
    };

    2.2 ve 2.4 çekirdeklerinde dentry yapısı şöyleydi:

    struct dentry {
        int d_count;
        unsigned int d_flags;
        struct inode  * d_inode;	/* Where the name belongs to - NULL is negative */
        struct dentry * d_parent;	/* parent directory */
        struct dentry * d_mounts;	/* mount information */
        struct dentry * d_covers;
        struct list_head d_hash;	/* lookup hash list */
        struct list_head d_lru;		/* d_count = 0 LRU list */
        struct list_head d_child;	/* child of parent list */
        struct list_head d_subdirs;	/* our children */
        struct list_head d_alias;	/* inode alias list */
        struct qstr d_name;
        unsigned long d_time;		/* used by d_revalidate */
        struct dentry_operations  *d_op;
        struct super_block * d_sb;	/* The root of the dentry tree */
        unsigned long d_reftime;	/* last time referenced */
        void * d_fsdata;		    /* fs-specific data */
        unsigned char d_iname[DNAME_INLINE_LEN];    /* small names */
    };

    2.6 çekirdeklerinde ise dentry yapısı şöyleydi:

    struct dentry {
        /* RCU lookup touched fields */
        unsigned int d_flags;		    /* protected by d_lock */
        seqcount_t d_seq;		        /* per dentry seqlock */
        struct hlist_bl_node d_hash;	/* lookup hash list */
        struct dentry *d_parent;	    /* parent directory */
        struct qstr d_name;
        struct inode *d_inode;		    /* Where the name belongs to - NULL is
                                        * negative */
        unsigned char d_iname[DNAME_INLINE_LEN];	/* small names */

        /* Ref lookup also touches following */
        unsigned int d_count;		    /* protected by d_lock */
        spinlock_t d_lock;		        /* per dentry lock */
        const struct dentry_operations  *d_op;
        struct super_block *d_sb;	    /* The root of the dentry tree */
        unsigned long d_time;		    /* used by d_revalidate */
        void *d_fsdata;			        /* fs-specific data */

        struct list_head d_lru;		    /* LRU list */
        /*
        * d_child and d_rcu can share memory
        */
        union {
            struct list_head d_child;	/* child of parent list */
            struct rcu_head d_rcu;
        } d_u;
        struct list_head d_subdirs;	    /* our children */
        struct list_head d_alias;	    /* inode alias list */
    };

   Güncel çekirdeklerdeki dentry yapısı içerisinde ilk dikkat çeken elemanlar şunlardır:

    struct dentry {
        /* ... */

        struct qstr d_name;
        union shortname_store d_shortname;
        struct dentry *d_parent;
        struct hlist_node d_sib;	    /* child of parent list */
        struct hlist_head d_children;	/* our children */
        struct inode *d_inode;	
        struct lockref d_lockref;       /* per-dentry lock and refcount
					                     * keep separate from RCU lookup area if
					                    * possible!
					                    */
        struct super_block *d_sb;
        struct hlist_bl_node d_hash;	/* lookup hash list */
        
        /* ... */
    };

    Yapının qstr elemanı yol bileşeninin (yani dizin girişinin) ismini tutmaktadır. Buradaki qstr yapısı güncel 
    çekirdeklerde şöyle bildirilmiştir:

    struct qstr {
        union {
            struct {
                HASH_LEN_DECLARE;
            };
            u64 hash_len;
        };
        const unsigned char *name;
    };

    2.2, 2.4 ve 2.6 çekirdeklerinde ise qstr yapısı şöyleydi:

    struct qstr {
        unsigned int hash;
        unsigned int len;
        const unsigned char *name;
    };

    Günümüz çekirdeklerinde dentry önbelleği için kullanılan hash algoritması biraz karmaşık hale getirilmiştir. 
    Bu durum qstr yapısının da değişmesine yol açmıştır. Dentry nesnesi içerisindeki qstr yapısında ilgili dizin 
    girişinin ismi (name elemanını kastediyoruz), bunun byte uzunluğu (len elemanını kastediyoruz) ve ilgili ismin 
    hash değeri tutulmaktadır. Burada ismin başlangıç adresinin yanı sıra neden aynı zamanda onun byte uzunluğunun 
    da tutulduğunu merak edebilirsiniz. Linux'un dosya sistemi farklı dosya sistemlerini destekleyecek biçimde genel 
    oluşturulmuştur. Bazı dosya sistemlerinde ismin sonunda null karakterin bulundurulması uygun olmayabilmektedir. 
    Bu nedenle buradaki ismin sonunda null karakter olmak zorunda olmadığı için ayrıca ismin uzunluğu da saklanmıştır. 
    Aynı zamanda isim uzunluğunun saklanması hash tablosunda arama yapılırken de performans kazancı sağlamaktadır.  

    qstr içerisindeki hash değeri dentry nesnesi için oluşturulan bütünsel hash değeri değil yalnızca buradaki ismin 
    hash değeridir. Dentry nesnelerinin bütünsel hash değerleri o dizin girişinin içinde bulunduğu dizine ilişkin dentry 
    nesnesinin adresi de kullanılarak oluşturulmaktadır. Bu konuyu izleyen paragraflarda açacağız. 
    
    İki dizin girişinin isimlerinin karşılaştırılmak istendiğini varsayalım (çekirdekte bu durumla sık karşılaşılmaktadır). 
    Bildiğiniz gibi iki yazının karşılaştırılması bir döngü içerisinde onların karşılıklı karakterlerinin karşılaştırılması 
    yoluyla yapılmaktadır. Oysa karşılaştırma işleminden önce bunların hash değerlerine ve uzunluklarına bakılarak bu 
    işlem hızlandırılabilir. Yani önce isimlerin hash değerleri, sonra isimlerin uzunlukları karşılaştırılabilir. Eğer 
    isimlerin hash değerleri ve uzunluklar aynıysa karşılıklı karakterlerin karşılaştırması aşamasına geçilebilir. Örneğin 
    "mehmet.c" dizin girişi ismi ile "mehtap.c" dizin girişi isminin karşılaştırılacağını düşünelim. Bunların önce hash 
    değerlerine, sonra uzunluklarına bakılır. Bunların hash değerleri ve uzunlukları aynıysa karşılıklı karakterler 
    karşılaştırılır. Tabii isimden hash değerinin elde edilemsi de bir çaba gerektirmektedir. Fakat çekirdek çoğu durumda 
    zaten karşılaştırılacak isim için hash değerini elde zorundadır. Ancak karşılaştırılacak isim için hash değerinin elde 
    edilmesi gerekmiyorsa hash karşılaştırması yapılmadan isimlerin uzunluklarına bakılıp, uzunluklar eşit değilse karşılıkla 
    karakterlerin karşışaltırılması işlemine geçilir.

    Güncel çekirdeklerdeki qstr yapısı özü aynı kalmak üzere biraz daha kompakt hale getirilmiştir:

    struct qstr {
        union {
            struct {
                HASH_LEN_DECLARE;
            };
            u64 hash_len;
        };
        const unsigned char *name;
    };
      
    Buradaki birliğin anonim birlik (anonymous union) olduğuna dikkat ediniz. Buradaki HASH_LEN_DECLARE makrosu şöyle 
    define edilmiştir:

    #define HASH_LEN_DECLARE u32 hash; u32 len

    Dolayısıyla aslında burada hash_len elemanı bütünü, hash ve len elemanları ise bütünün parçalarını belirtmektedir. 
    Yani aslında bu yapının eski çekirdeklerdeki qstr yapısı ile organizasyonel bir farklılığı yoktur. Bu yapıda yalnızca 
    hash değeri ile uzunluk 64 bit içerisinde birleştirilmiştir. Böylece hash değeri ile uzunluk bilgisi 64 bit sistemlerde 
    tek hamlede (tek bir makine komutuyla) karşılaştırılabilmektedir. 

    Bir dentry nesnesinin isminin tutulacağı alan çekirdeğin heap sisteminde tahsis edilmektedir. Ancak bu konuda bir 
    zaman kazancı sağlayabilmek amacıyla kısa dizin girişi isimleri doğrudan dentry yapısının içerisindeki küçük 
    bir dizide saklanmaktadır. Güncel çekirdeklerde yapının d_shortname elemanı bu diziyi barındırmaktadır:

    union shortname_store {
        unsigned char string[DNAME_INLINE_LEN];
        unsigned long words[DNAME_INLINE_WORDS];
    };

    Buradaki string elemanı DNAME_INLINE_LEN uzunluğundadır. DNAME_INLINE_LEN ise güncel çekirdeklerde şöyle define 
    edilmiştir:
    
    #define DNAME_INLINE_LEN (DNAME_INLINE_WORDS*sizeof(unsigned long))

    #ifdef CONFIG_64BIT
    # define DNAME_INLINE_WORDS 5
    #else
    # ifdef CONFIG_SMP
    #  define DNAME_INLINE_WORDS 9 
    # else
    #  define DNAME_INLINE_WORDS 11 
    # endif"
    #endif

    Bugünkü 64 bit Linux sistemlerinde bu dizi 40 byte uzunluğunda, 32 bit sistemlerde ise 36 byte ya da 44 byte uzunluğundadır. 
    Bu durumda çekirdek her zaman dizin girişi ismini qtr yapısının name eleanının gösterdiği yerde arar. İşin başında 
    bu name elemanı zaten dentry içerisindeki bir diziyi göstermektedir. Ancak eğer bu dizinin uzunluğu isim için yetmezse 
    çekirdeğin heap alanında isim için tahsisat yapılır, isim o tahsis edilmiş alana kopyalanır ve yapının bu name elemanı 
    da o alanı gösterir hale getirilir. (Biz bu temayı dosya betiemleyici tablosu ve close-on-exec dizisisin tahsis edilmesi 
    konusunda da görmüştük.) Eski çekirdeklerde bu küçük dizi doğrudan dentry yapısının d_iname elemanında tutuluyordu. 

    Bir dentry nesnesine ilişkin üst dizinin dentry nesnesi de genellikle dentry önbelleğinde bulunmaktadır. Ancak 
    bunun bir garantisi yoktur. Yol ifadesi çözümlenirken üst dizinlere ilişkin dentry nesneleri de dentry önbelleğine 
    yerleştirilmektedir. (Ancak daha sonra üst dizine ilişkin dentry nesnesi az kullanıldığından dolayı dentry önbelleğinden 
    atılmış da olabilir.) İşte dentry yapısının d_parent elemanı ilgili dizin girişinin içinde bulunduğu dizinin dentry 
    nesnesinin adresini tutmaktadır. O halde çekirdek bir dentry nesnesini dentry önbelleğinde bulursa onun üst dizinine 
    ilişkin dentry nesnesine hızlı bir biçimde erişebilmektedir. Aynı durum yapının d_sib ve d_children elemanları için 
    de beznerdir. Yapının d_sib elemanı aynı dizindeki dizin girişlerine ilişkin kardeş dentry nesnelerinin tutulduğu 
    bağlı liste düğümüdür. Kardeş dentry nesnelerinin kök düğümü de yapının d_children elemanında saklanmaktadır. Burada 
    bağlı liste için list_head yerine hlist_node yapısı kullanılmış olsa da aslında bu elemanaların hash tablosuyla bir 
    ilgisi yoktur. Bu elemanlar bağlı liste düğümleri belirtmektedir. Çekirdek bir dizinin altındaki girişleri gözden 
    geçirmek istediğinde dizine ilişkin dentry nesnesini bulup onun d_children bağlı listesini dolaşmaktadır. Tabii bu 
    bağlı listede dizindeki üm dizin girişlerine ilişkin dentry nesnelerinin hepsi dentry önbelleğinde bulunmak zorunda 
    değildir. Ancak arama sırasında olmayanlar zaten diskten okunup önbelleğe alınmaktadır. 

    Dentry nesnelerinin ilişkin olduğu inode nesnelerinin adresleri dentry yapısının d_inode elemanında tutulmaktadır. 
    Dentry nesnelerinin yalnızca yol ifadelerinin çözümlenmesi gibi işlemlerde kullanıldığına dikkat ediniz. Dosya 
    üzerinde işlem yapabilmek için o dosyaya ilişkin inode nesnesinin elde edilmesi gerekmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Pek çok çekirdek nesnesinde olduğu gibi dentry nesnelerinde de sayaç vardır. Güncel çekirdeklerde bu sayaç yapının 
    lockref yapısı türünden d_lockref elemanında tutulmaktadır. Eski çekirdeklerde bu sayaç doğrudan int ya da unsigned 
    int türünden d_count elemanında tutuluyordu. Güncel çekirdeklerde bu sayaç lockref yapısı içerisindeki spinlock nesnesi 
    ile korunmaktadır:

    struct lockref {
        union {
    #if USE_CMPXCHG_LOCKREF
            aligned_u64 lock_count;
    #endif
            struct {
                spinlock_t lock;
                int count;
            };
        };
    };

    Eski çekirdeklerde RCU mekanizması bu kadar yoğun kullanılmıyordu. Bu nedenle bu sayaç dentry yapısının içerisindeki 
    nesnenin diğer elemanları da koruyan d_lock nesnesi yoluyla korunuyordu.

    Her dentry nesnesi bir dosya sisteminin içerisindedir. UNIX/Linux sistemlerinde tek bir dizin ağacının bulunduğunu 
    anımsayınız. Farklı dosya sistemleri aynı ağaca mount edilmektedir. Ağacın bir dizini bir dosya dosya sisteminin
    içerisindeyken diğer bir dizini başka bir dosya sisteminin (ya da disk bölümünün) içerisinde olabilir. İşte her 
    dentry nesnesinin hangi dosya sisteminin (ya da disk bölümünün) içerisinde olduğu dentry elemanının super_block 
    yapısı türünden d_sb elemanında tutulmaktadır. super_block yapısı bir dosya sisteminin bilgilerini tutan  bir 
    yapıdır. Her mount işleminde -eğer daha önce yaratılmadıysa- bir super_block nesnesi yaratılmaktadır. super_block 
    yapısı ileride ayrıntılı bir biçimde ele alınacaktır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aslında sizin de gördüğünüz gibi çekirdeğin dosya sistemi "nesne yönelimli" bir tasarımı andırmaktadır. Ancak işletim 
    sisteminin çekirdekleri C++ gibi C'den daha yüksek seviyeli nesne yönelimli dillerle kodlanmazlar. Bu dillerdeki yüksek
    seviye kontrol kaybına yol açabilmektedir. Rust ise daha önceden de belirttiğimiz gibi yeni çekirdeklerde halen çok 
    çok küçük bir oranda kullanılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dentry önbelleğinin nasıl organize edildiği üzerinde duralım. (Dentry önbelleğine İngilizce kısaca "dcache"
    de denilmektedir.) Dentry önbelliği bir hash tablosu biçiminde oluşturulmuştur. Bu hash tablosunun adresi güncel 
    çekirdeklerde "fs/dcache.c" dosyası içerisinde dentry_hashtable global değişkeninde tutulmaktadır:

    static struct hlist_bl_head *dentry_hashtable __ro_after_init __used;

    Tanımladaki __ro_after_init __used belirleyicileri gcc'ye özgüdür. 2.2 ve 2.4 çekirdeklerinde dentry hash tablosu 
    şöyle tanımlanmıştı:

    static struct list_head *dentry_hashtable;

    2.6 çekirdeklerindeki tanımlama ise şöyleydi: 

    static struct hlist_head *dentry_hashtable __read_mostly;

    2.6 çekirdeklerinin son versiyonlarında bu tanımlama değiştirilerek güncel çekirdeklerdekine benzer hale geitirlmiştir. 

    Güncel çekirdeklerdeki hlist_bl_head hash tablosunun bağlı liste zincirlerini tutan bir yapıdır. Bu yapı şöyle 
    bildirilmiştir:

    struct hlist_bl_head {
        struct hlist_bl_node *first;
    };

    Görüldüğü gibi aslında hlist_bl_head yalnızca hlist_bl_node türünden bir adres tutmaktadır. hlist_bl_node yapısı 
    da şöyle bildirilmiştir:

    struct hlist_bl_node {
        struct hlist_bl_node *next, **pprev;
    };

    Tabii kursumuzun başında da gördüğümüz gibi aslında bu yapı başka bir yapının içerisine (burada dentry yapısı) 
    gömülmüş durumdadır. Güncel çekirdeklerde hash tablosuna ilişkin zincirler dentry yapısının d_hash elemanlarını 
    göstermektedir. Başka bir deyişle hash tablosu zincirleri aslında dentry yapısı içerisindeki d_hash elemanlarını 
    birbirine bağlamaktadır. 

    Biz daha önce hash tablolarının çekirdekte hlist_head ve hlist_node yapılarıyla oluşturulduğunu görmüştük. Ancak 
    buradaki yapılar elemanları aynı olsa da hlist_bl_head ve hlist_bl_node biçimindedir. İsimlerdeki bl harfleri "bit 
    lock" sözcüklerinden kısaltılmıştır. Çünkü dentry hash tablosunda hlist_bl_head yapısındaki first isimli göstericinin 
    düşük anlamlı biti spinlock kilidi için kullanılmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            31. Ders 02/11/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Dentry hash tablosu için dentry nesnesinden hash değeri oluştururken hem dizin girişinin ismi hem de onun üst 
    dizinine ilişkin dentry nesnesinin adresi kullanılmaktadır. Arama sırasında hash değeri yine dizini girişinin isminden 
    ve üst dizine ilişkin dentry nesnesinin adresiden elde edilmeketdir. Ancak çakışma durumunda zincirde arama yapılırken 
    hem dizin girişinin ismine hem de onun üst dizinine ilişkin dentry nesnesinin adresine bakılmaktadır. Örneğin 
    "/home/kaan/Study" yol ifadesinde "study" girişinin hash tablosunda arandığını düşünelim. Bu noktaya gelindiğine göre 
    çekirdeğin zaten "kaan" dizininin dentry nesnesini bulmuş olması gerekir. O halde "kaan" dizininin dentry nesnesinin 
    adresinden ve "Study" isminden hash değeri oluşturularak hash tablosunun o indeksindeki bağlı listede arama yapılacaktır. 
    Tabii bu arama yapılırken dizin girişinin ismiyle üst dizinin dentry nesnesinin adresi uyuşuyor mu diye bakılacaktır. 
    Yukarıda da belirttiğimiz gibi isimler hemen karşılaştırılmamakta önce onların hash değerleri ve uzunlukları karşılaştırılmaktadır. 
    Buradan da görüldüğü gibi dentry hahsh tablosunda "Study" isminde bir giriş aranmamaktadır. "üst dizini falanca olan 
    "Study" isminde bir giriş aranmaktadır. Burada bir noktaya dikkat ediniz. Üst dizini "kaan" olan pek çok "Study" dizin 
    girişi olabilir. Bu nedenle zincirdeki karşılaştırmada üst dizinin ismi değil üst dizinin dentry nesnesinin adresi 
    kullanılmaktadır. Çünkü isimler aynı olsa da her farklı dizin girişi için ayrı bir dentry nesnesi oluşturulmaktadır. 

    Şimdi yeniden yol ifadesinin nasıl çözümlendiğine dikkat edelim. Yol ifadesi aşağıdaki gibi olsun:

    fd = open("/home/kaan/Study/LinuxKernal/sample.c", O_RDONLY);

    Burada çekirdek önce yol ifadesinin mutlak mı göreli mi olduğuna bakar. Böylece arama işlemi için ilk dentry 
    nesnesini çekirdek yol ifadesini veren prosesin task_struct yapısı içerisinden elde eder. (Prosesin task_struct
    yapısı içerisinde prosesin kök dizininin ve çalışma dizinin dentry nesne adreslerinin tutulduğunu anımsayınız.)
    Örneğimizdeki yol ifadesi mutlaktır. Böylece çekirdek kök dizine ilişkin dentry nesnesinin adresini ve "home" 
    ismini kullanarak hash değerini hesaplar ve hash tablosunda "home" girişini arar. Eğer bulursa sonraki yol bileşnine 
    geçer, bulamazsa önce onu diskten elde edip omdan sonra yoluna devam eder. Biz "home" girişinin dentry hash tablosunda 
    bulunduğunu varsayalım. Bu sefer çekirdek aynı biçimde "kaan" ismiyle "home" girişinin dentry nesne adresini kullanarak 
    hash değerini elde edecek ve "kaan" girişini benzer biçimde arayacaktır. İşlemler bu biçimde bir döngü içerisinde 
    devam edecektir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki dentry hash tablosu hangi uzunluktadır? dentry hash tablosunun kova (bucket) uzunluğu (yani kaç tane dizi elemanından 
    oluştuğu) sistem boot edilirken eldeki fiziksel RAM'e bakılarak belirlenmektedir. Hash tablosunun tahsisatı bu süreç 
    içerisinde güncel çekirdeklerde "mm/mm_init.c" dosyasındaki alloc_large_system_hash fonksiyonu tarafından yapılmaktadır.
    Tahsisatın yapılmasında kullanılan çağrı şöyledir:

    dentry_hashtable =
		alloc_large_system_hash("Dentry cache",
					sizeof(struct hlist_bl_head),
					dhash_entries,
					13,
					HASH_EARLY | HASH_ZERO,
					&d_hash_shift,
					NULL,
					0,
					0);
	d_hash_shift = 32 - d_hash_shift;

    Bu çağrı ile tahsisat yapıldıktan sonra d_cache_shift değeri elde edilmektedir. Gördüğünüz gibi bu değer de 32'den 
    çkartılarak saklanmıştır. Yani sonuç olarak d_hash_shift değişkeninde hash tablosunun uzunluğu "sola öteleme miktarı 
    ile" başka bir deyişle 2'nin kuvveti ile tutulmaktadır. Aslında bu tür tabloların uzunlukları genellikle çekirdek 
    parametreleriyle de ayarlanabilmektedir. Dentry hash tablosunun uzunluğu çekirdeğin dhash_entries parametresiyle 
    boot işleminde ayarlanabilmektedir. Bunun için boot parametresi aşağıdaki örnekteki gibi oluşturulabilir:

    dhash_entries=262144 

    Çekirdek bu biçimde boot edilirse d_hash_shift değeri 18 olacaktır. d_hash_shift değişkeni "fs/dcache.c" dosyası 
    içerisinde tanımlanmıştır, ancak export edilmemiştir. Çekirdek boot işlemiyle birlikte kendini initialize ederken 
    printk fonksiyonu ile çeşitli log mesajlarını da çekirdeğin ring tamponuna yazdırmaktadır. Çalışan çekirdekte dentry 
    hash tablosunun uzunluğu aşağıdaki komutla elde edilebilir:

    $ dmesg | grep "Dentry cache"

    Kursun yapıldığı makideneki çıktı şöyledir:

    [    0.461843] Dentry cache hash table entries: 1048576 (order: 11, 8388608 bytes, linear)

    Burada tablonun toplam 1048576 elemandan (yani kovadan) oluştuğu belirtilmiştir. "Order: 11" bilgisi bu hash tablosu 
    için toplam 2 üzeri kaç sayfanın ayrıldığını belirtmektedir. Bir sayfa 4K (4096 byte) olduğundan hash tablosu için 
    toplam 2^11 * 4096 = 8388608 byte yer ayrılmıştır. Tablonun her elemanı bir gösterici tuttuğuna göre ve göstericiler 
    de 64 bit Linux sistemlerinde 8 byte uzunlukta olduğuna göre tablonun toplam eleman sayısı 2^11 * 4096 / 8 = 1048576 
    kadardır. 
    
    Dentry hash tablosunun uzunluğunu (yani kova sayısını) ayarlamak için bir çekirdek konfigürasyon parametetresi 
    bulunmamaktadır. 

    Burada bir noktaya dikkat ediniz. Yukarıda açıkladığımız uzunluk hash tablosunun uzunluğudur. Her dentry nesnesi 
    yaratıldıkça bu tabloya eklenmektedir. Zamanla dentry nesnelerinin sayısı artabilir ve dentry nesneleri çekirdek 
    alanında fazla yer kaplar hale gelebilir. Çekirdek bellek baskısı oluştuğunda bazı alt sistemlere yönelerek orada 
    boşaltımlar yapabilmektedir. İşte bellek baskısı altında çekirdek "son zamanlarda en az başvurulan kullanılmayan 
    dentry nesnelerini1 bellekten atarak onların kapladığı alanı geri alabilmektedir. (Ancak çekirdek hiçbir zaman 
    initialize aşamasında yarattığı dentry hash tablosunu küçültmemektedir. Yalmızca tablodaki dentry nesnelerini
    gerektiğinde boşaltmaktadır.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Dentry önbelleğindeki dentry nesneleri üç gruba ayrılmaktadır:

    1) Kullanımda olan dentry nesneleri. Bunlara pozitif dentry nesneleri de denilmektedir. Bunların referans sayaçları 
    0'dan büyüktür. 
    2) Kullanımda olmayan dentry nesneleri. Bunların referans sayaçları 0'dır.
    3) Negatif dentry nesneleri. Bunların da referans sayaçları 0'dır.

    Hem kullanımda olmayan dentry nesnelerinin hem de negatif dentry nesnelerinin referans sayaçları 0 olduğuna göre 
    bunların arasında ne farklılık vardır? Kullanımda olan ve kullanımda olmayan dentry nesnelerinin ilişkin olduğu 
    inode nesneleri inode önbelleğinde bulunmaktadır. Yani örneğin bir dentry nesnesinin referans sayacı 0'a düşmüş 
    olsa bile ona ilişkin inode nesnesi o anda inode önbelleğinde tutulmaya devam etmektedir. Ancak negatif dentry 
    nesnelerine ilişkin inode nesneleri inode önbelleğinde bulunmamaktadır. O halde referans sayacı 0 olan dentry 
    nesneleri kendi aralarında iki gruba ayrılmaktadır: Inode nesneleri inode önbelleğinde bulunanlar ve inode nesneleri 
    inode önbelleğinde bulunmayanlar. İşte referans sayacı 0 olan ancak o girişe ilişkin inode nesneneleri inode 
    önbelleğinde bulunmayan dentry nesnelerine "negatif dentry nesneleri" denilmektedir. Negatif dentry nesnelerinin 
    d_inode göstericileri NULL durumundadır. Negatif dentry nesneleri tipik olarak "aslında olmayan bir dosya üzerinde 
    işlem yapılmak istendiğinde" oluşturulmaktadır. Örneğin open fonksiyonu ile "xxx.txt" isimli bir dosyayı O_RDONLY 
    modunda açmaya çalışalım. Ancak dosya diskte mevcut olmasın. Bu durumda çekirdek "xxx.txt" dosyasına ilişkin dizin 
    girişi için dentry nesnesini yine de oluşturup dentry önbelleğine yerleştirmektedir. Ancak bu girişe ilişkin bir 
    inode nesnesi bulunmamaktadır. Şimdi siz "mademki dosya diskte yok bu durumda neden onun için bir dentry nesnesi 
    oluşturulup dentry önbelleğinde saklanıyor" diye düşünebilirsiniz. İşte olmayan dosyalara erişmeye çalışmak aslında 
    çok karşılaşılan durumlardandır. Bazen dosyalar üzerinde "var mı yok mu" biçiminde senkronizasyon amaçlı kontrol 
    işlemleri yapılabilmektedir. Bazı olmayan dosyaların ya da dizinlerin aranması da sanıldığından çok daha fazla 
    olabilmektedir. (Örneğin bir dizin silinmiş olabilir. Ancak PATH çevre değişkeninde o dizin hala bulunyor olabilir. 
    Bu durumda exec işlemlerinde exec fonksiyonları o dizinleri üzerinde başarısız açma işlemleri yapacaktır.) Deneyimlere 
    göre yol fadesinin çözümlenme işlemlerinde %30-50 oranındaki işlemler bir dizin girişinin var olmaması nedeniyle 
    başarısızlıkla sonuçlanmaktadır. İşte bu tür durumlarda başarısız aramalara ilişkin dizin girişlerine ilişkin 
    dentry nesnelerinin "bir sonraki sefer başarısızlık daha kolay anlaşılsın diye" dentry önbelleğinde saklanması 
    performansı artırmaktadır. 

    Burada "negatif dentry" tanımlamasıyla ilgili bir nokta üzerinde de durmak istiyoruz. Negatif dentry nesnelerine
    ilişkin bir inode nesnesinin inode önbelleğinde bulunmadığını belirtmiştik. Bu nedenle bu dentry nesnelerinin 
    d_inode göstericileri NULL durumundaydı. Ancak aslında negatif dentry nesnelerinin referans sayaçları geçici olarak
    0'dan büyük hale gelebilmektedir. Örneğin başarısız aramalar yapılırken dentry nesnesinin referans sayacı artırılıp 
    geçici bir süre 0'dan büyük hale gelebilmemektedir. Ancak kararlı durumda onların referans sayaçları 0 olur.

    Yukarıdaki üç grup dentry nesnesi de dentry önbelliğinde yani dentry hash tablosunda tutulmaktadır. Pekiyi sistem 
    bellek baskısı altında hangi dentry nesnelerini önbellekten atmaktadır? İzleyen paragrafta bunun üzerinde duracağız. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bellek baskısı altında çekirdek her zaman referans sayacı 0 olan dentry nesnelerini dentry önbelleğinden (yani 
    dentry hash tablosundan) atmaktadır. Kullanımda olan yani referans sayacı 0'dan büyük olan dentry nesneleri dentry 
    önbelleğinden atılmamaktadır. Pekiyi çok fazla sayıda referans sayacı 0 olan dentry nesnesi denry önbelleğinde 
    bulunuyorsa ve çekirdek bunların bir kısmını önbellekten atmak istiyorsa ne olacaktır? İşte çekirdek genel olarak 
    bu tür durumlarda "son zamanlarda en az kullanılan nesneleri" bu tür önbelleklerden önbelleklerden atma yoluna 
    gitmektedir. Buna İngilizce "LRU (Least Recently Used)" politikası denilmektedir. Genellikle LRU politikası bir 
    bağlı liste ile gerçekleştirilmektedir. Bu gerçekleştirimde aday nesneler bir bağlı listede tutulur. Bunlara 
    erişilince erişilen eleman bağlı listede öne alınır. Böylece son zamanalarda az erişilenler bağlı listenin sonunda 
    kalmış olur. İşte bellek baskısı oluştuğunda da bağlı listenin sonundan atma yapılmaktadır. 

    Eskiden referans sayacı 0 olan (yani referans sayacı 0'a düşmüş olan ve negatif olan) dentry nesneleri toplamda 
    tek bir LRU bağlı listesinde tutuluyordu. Çekirdeğin 2.6.18 versiyonuyla birlikte her dosya sistemi ya da disk 
    bölümü için ayrı bir LRU bağlı listesi tutulmaya başlandı. Güncel çekirdeklerde bu nedenle dentry önbelleği içinde
    LRU bağlı listeleri "include/linux/fs.h" super_block yapısı içerisine taşımıştır:

    struct super_block {
        /* ... */

        struct list_lru		s_dentry_lru;

        /* ... */
    };

    Buradaki list_lru yapısı da "include/linux/list_rlu.h" dosyası içerisinde şöyle bildirilmiştir:

    struct list_lru {
        struct list_lru_node	*node;
    #ifdef CONFIG_MEMCG
        struct list_head	    list;
        int			            shrinker_id;
        bool			        memcg_aware;
    struct xarray		        xa;
    #endif
    #ifdef CONFIG_LOCKDEP
        struct lock_class_key	*key;
    #endif
    };

    Burada bazı konfigürasyon parametrelerine bağlı olarak bazı elemanların yapıya dahil edildiğini görüyorsunuz. 
    yapının node elemanında list_lru_node isimli bir yapı türünden dizinin adresi tutulmaktadır. (Dizinin uzunluğu 
    çekirdek tarafından biliniyor durumdadır.) list_rlu_node yapısı da yine "include/linux/list_rlu.h" dosyası 
    içerisinde şöyle bildirilmiştir:

    struct list_lru_node {
        /* global list, used for the root cgroup in cgroup aware lrus */
        struct list_lru_one	lru;
        atomic_long_t		nr_items;
    } ____cacheline_aligned_in_smp;

    Buradaki list_lru_one yapısı da yine "include/linux/list_rlu.h" şöyle bildirilmiştir:

    struct list_lru_one {
        struct list_head	list;
        /* may become negative during memcg reparenting */
        long			nr_items;
        /* protects all fields above */
        spinlock_t		lock;
    };

    Bu yapılar neden bu kadar karmaşıktır? İşte zaman içerisinde dentry önbelleği için LRU bağlı listeleri NUMA 
    mimarisinde her NUMA bank'ı için ayrı bir tane olacak biçimde çoklanmıştır. Eğer söz konusu bilgisayar NUMA 
    değil de SMP kullanıyorsa aslında burada bir tane bağlı liste bulunmaktadır. NUMA (Non-uniform Memory Access)
    ve SMP (Symmetric Multiprocessor) mimariler kursumuzun bellek yönetimi kısmında ele alınacaktır. Bugün kullandığımız 
    bilgisayar kartlarının çok büyük bölümü SMP mimarisine uygun tasarlanmıştır. Ancak çekirdek kodları hem NUMA 
    hem de SMP mimarilerini destekleyecek biçimde genel oluşturulmuştur. Yukarıdaki veri yapısının özeti şöyle ifade 
    edilebilir: "super_block yapısı içerisinde bir tane değil NUMA bank'larının (node'larının) sayısı kadar LRU 
    listesi vardır. Bu LRU listeleri bir dizi biçiminde tutulmaktadır." 

    Dentry önbelleğindeki LRU listelerinde dentry nesneleri dentry yapısının d_lru elemanı yoluyla turulmaktadır:

    struct dentry {
        /* ... */

        struct list_head d_lru;	        

        /* ... */
    };

    Eskiden dentry önbelleğine ilişkin LRU listesi NUMA mimarisi için özel bir organizasyona sahip değildi. Dolayısıyla 
    çok daha basitti. Dentry önbellek sisteminin zaten Linux çekirdeklerine 2'li versiyonlarla eklendiğini belirtmiştik. 
    Çekirdeğin 2.2 versiyonlarında dentry önbelleği olmasına karşın ayrı bir LRU listesi bulunmuyordu. Referans sayacı 
    0 olan dentry nesneleri için LRU listesi çekirdeğin 2.4 versiyonuyla eklenmiştir. O zamanalar yukarıda da belirttiğimiz 
    gibi zaten tek bir liste vardı ve NUMA mimarisi için özel bir destek düşünülmemişti. 2.4 ve 2.6 versiyonlarının 
    sonlarına kadar dentry önbelleğine ilişkin LRU listesi "fs/dcache.c" dosyasında aşağıdaki gibi tanımlanmıştır:

    static LIST_HEAD(dentry_unused);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            32. Ders 09/11/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz bellek baskısı oluştuğunda LRU listesinin sonunda kalanların dentry önbelleğinden atıldığını belirttik. Ancak 
    aslında dentry önbelleğinden atma sürecinde LRU listesinin sonundaki dentry nesneleri bazı kontrollere sokularak 
    dentry önbelleğinden atılmaktadır. Güncel çekirdeklerde yapılan kontroller şunlardır:

    - Eğer dentry nesnesi kilitlenmişse önbellekten atılmamaktadır.
    -  Eğer dentry nesnesi bir dizine ilişkinse ve bu dizinin altında başka önbellekte bulunan başka dentry nesneleri 
    varsa (başka bir deyişle d_subdirs listesi boş değilse) bu dentry önbellekten atılmaz.
    - Çekirdek dentry nesnesine eriştiğinde dentry yapısı içerisindeki d_flags elemanının DCACHE_REFERENCED bitini 1 yapar. 
    Bellek baskısı oluşup çekirdek "shrink" işlemi için devreye girdiğinde LRU bağlı listesinin sonundan başlayarak başa 
    doğru dentry nesnelerinin DCACHE_REFERENCED bitlerini kontrol eder. Bu biti 1 olan dentry nesnelerini dentry önbelleğinden 
    atmaz bu biti 0 yaparak onu LRU listesinin başına alır. Eğer bu bit zaten 0 ise ilgili dentry nesnesini önbellekten atar.

    Aşağıda güncel çekirdeklerde bellek baskısı oluştuğunda çağrılan fonksiyonları özet olarak veriyoruz. Bu konuyu ileride 
    bellek yönetimini ele aldığımız bölümde açıklayacağız:

            .....
            shrink_slab()  ← Tüm slab cache'ler için shrinker'ları çağırır
                    ↓
     ┌──────────────────────────────┐
     │   DENTRY SHRINKER ÇAĞRISI    │
     └──────────────────────────────┘
                    ↓
        super_cache_scan()  ← dentry cache shrinker
                    ↓
        prune_dcache_sb(sb, sc->nr_to_scan)
                    ↓
                    ├─> freed = list_lru_shrink_walk(
                    │       &sb->s_dentry_lru,
                    │       sc,
                    │       dentry_lru_isolate,  ← Callback fonksiyon
                    │       &dispose)
                    │
                    └─> LRU listesini tara
                            ↓
        ┌───────────────────────────────────────┐
        │   HER DENTRY İÇİN ÇAĞRILIR            │
        └───────────────────────────────────────┘
                            ↓
        dentry_lru_isolate(item, lru, lru_lock, &dispose)
                            ↓
        ┌───────────────────────────────────────┐
        │  DCACHE_REFERENCED KONTROLÜ BURADA!   │
        └───────────────────────────────────────┘
                            ↓
            spin_lock(&dentry->d_lock)
                            ↓
            if (d_flags & DCACHE_REFERENCED) {
                d_flags &= ~DCACHE_REFERENCED;  ← Bit temizle
                return LRU_ROTATE;              ← Başa döndür, silme!
            }
                            ↓
            d_lru_shrink_move(lru, dentry, &dispose);
            return LRU_REMOVED;  ← Silinecekler listesine ekle
                            │
        ┌───────────────────┘
        │
        └─> shrink_dentry_list(&dispose)        ← Seçilmiş dentry'leri sil
                    ↓
            while (!list_empty(&dispose)) {
                dentry = list_entry(...)
                    ↓
                lock_for_kill(dentry)            ← Güvenli kilitleme
                    ↓
                shrink_kill(dentry)
                    ↓
                __dentry_kill(dentry)
                    ↓
                dentry_unlink_inode()           ← inode ilişkisini kes
                    ↓
                __d_free()                      ← Belleği serbest bırak
            }
            .....

    Şimdi dentry LRU listesinin çalışmasının nasıl çalıştığını yeniden gözden geçirelim. Çekirdek yol ifadesinden 
    hareketle dosya araması arması (name lookup) yaparken eriştiği her dentry nesnesini aslında LRU bağlı listesinin 
    başına atmamakta yalnızca onların DCACHE_REFERENCED bitini 1 yapmaktadır. Bellek baskısı oluşup dentry cache 
    sistemlerinden dentry nesneleri atılmak istendiğinde ise çekirdek listenin sonundan başına doğru ilerlemekte eğer 
    bir dentry nesnesinin DCACHE_REFERENCED biti 0 ise ve yukarıda belirttiğimiz koşullar da sağlanıyorsa onu önbellekten 
    atmaktadır. Ancak dentry nesnesinin DCACHE_REFERENCED biti 1 ise onu listenin başına çekip bu biti 0'lamaktadır 
    ve o dentry nesnesini önbellekten atmamaktadır. 

    Son olarak dentry önbellek sistemi için bir özet yapmak istiyoruz:

    - Toplamda tek bir dentry önbellek sistemi vardır.
    - Her super_block nesnesinde ayrı dentry LRU listeleri tutulmaktadır. Her LRU listelerindeki dentry nesnelerinin 
    bir biti isim aramasında set edilmekte sonra reclaim işlemi sırasıda o biti 1 olanlar listednin başına alınmaktadır.
    - dentry hash tablosuna dentry elemanları üst dizinin dentry nesnesi ve dizin giriş ismi anahtar yapılarak 
    yerleştirilmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bilindiği gibi çekirdek çalışma sırasında bazı bilgileri kullanıcı modundan erişilebilsin diye "proc" dosya sistemi 
    içerisindeki çeşitli dosyalara yazmaktadır. Dentry önbellek sistemi için önemli olacak birkaç proc dosyası vardır. 
    Bunlar hakkında kısaca bilgiler vermek istiyoruz:

    - "/proc/sys/fs/dentry-state" dosyasında dentry önbelleğinin genel durumu hakkında bilgiler nulunmaktadır. Bu dosyanın
    içeriği aşağıdakine benzer biçimdedir:

    $ cat /proc/sys/fs/dentry-state 
    188967	139941	45	0	25807	0

    Buradaki bilgiler şöyledir:   

    ┌─────────────────────┬──────────────────────────────────────────────┐
    │ Alan Adı            │ Açıklama                                     │
    ├─────────────────────┼──────────────────────────────────────────────┤
    │ nr_dentry           │ Toplam dentry sayısı                         │
    │ nr_unused           │ Kullanılmayan (LRU listesinde) dentry sayısı │
    │ age_limit           │ Yaşlandırma eşiği (saniye)                   │
    │ want_pages          │ Reclaim sırasında istenen sayfa sayısı       │
    │ shrink_list_len     │ Shrinker listesi uzunluğu                    │
    │ unused_ratio        │ Kullanılmayan oran (bazı sürümlerde)         │
    └─────────────────────┴──────────────────────────────────────────────┘

    Buradaki "nr_dentry" dentry önbelleğindeki toplam dentry nesne sayısı belirtilmektedir. "nr_unused" LRU listesindeki 
    toplam nesne sayısı (yani referans sayacı 0 olan) belirtilmektedir. Buradaki "age_limit" çekirdeklerde hiçbir zaman 
    kullanılmamıştır. Dolayısıyla buradaki 45 saniye belirlemesine ilişkin çekirdeğin herhangi bir yerinde bir gerçekleştirim 
    bulunmamaktadır. "unused_ratio" ise tüm dentry nesneleri ile kullanılmayanların oranını belirtmektedir. Ancak bu bilgi 
    her çekirdekte güncellenmemektedir. 

    - "/proc/slabinfo" dosyası dilimli tahsisat sistemindeki öğelerin izlenmesi için kullanılan genel bir dosyadır. Dentry 
    nesneleri de (tabii inode nesneleri de) dilimli tahsisat sistemiyle tahsis edildiği için dentry nesnelerinin tahsisatları
    hakkında bu dosyaya başvurularak bir bilgi edinilebilir. Dosyadaki "dentry dilimli tahsisat sistemi" hakkında örnek 
    bilgi aşağıdakine benzer bir biçimde rapor edilmektedir. 

    $ sudo cat /proc/slabinfo | grep dentry

    dentry            189187 189231    192   21    1 : tunables    0    0    0 : slabdata   9011   9011      0

    Buradaki sütunların başlıkları şöyledir:

    name  <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> 
    <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail>

    Biz kurusumuzda "dilimli tahsisat sistemini (slab allocator)" çekirdeğin heap sisteminin anlatıldığı bölümde 
    ayrıntılarıyla inceleyeceğiz. Buraki "aktif nesne sayısı (örneğimizdeki 189187 değeri)" tahsis edilmiş olan ve 
    henüz iade edilmemiş olan dentry nesnelerinin sayısını, "toplam nesne sayısı (örneğimizdeki 189231 değeri)" ise 
    çekirdek tarafından dilimli tahsisat sistemi yoluyla tahsis edilmiş nesnelerin toplam sayısını belirtmektedir. 

    Ayrıca "sys" dosya sistemi içerisinde dilimli tahsisat sistemine ilişkin "/sys/kernel/slab/dentry" dizini bulundurulmaktadır. 
    Bu dizinin içeriği şöyledir:

    $ ls
    aliases      ctor            object_size      poison                    shrink             store_user
    align        destroy_by_rcu  objects_partial  reclaim_account           skip_kfence        total_objects
    cache_dma    hwcache_align   objs_per_slab    red_zone                  slabs              trace
    cpu_partial  min_partial     order            remote_node_defrag_ratio  slabs_cpu_partial  usersize
    cpu_slabs    objects         partial          sanity_checks             slab_size          validate

    Burada görüldüğü gibi dentry dilimli tahsisat sistemi hakkında daha ayrıntılı bilgiler yer almaktadır. Bu konu 
    ileride ele alınacaktır. 

    Güncel çekirdeklere "/proc/dentry-state" dosyasındaki dentry önbelleğine ilişkin bilgiler çekirdek tarafından 
    "fs/dcache.c" dosyası içerisindeki dentry_stat_t türünden dentry_stat isimli bir yapı nesnesinde tutulmaktadır:

    struct dentry_stat_t {
        long nr_dentry;
        long nr_unused;
        long age_limit;		/* age in seconds */
        long want_pages;	/* pages requested by system */
        long nr_negative;	/* # of unused negative dentries */
        long dummy;		/* Reserved for future use */
    };

    static struct dentry_stat_t dentry_stat = {
        .age_limit = 45,
    };

    Aslında bu yapı nesnesinin içeriği doğrudan "/proc-dentry-state" dosyasına yansıtılmaktadır. Çekirdek bu yapıyı 
    işlemler sırasında güncellemektedir. İlgii "proc" dosyaları okunduğunda da değerler bu yapıdan alınarak verilmektedir.
    "UNIX/Linux Sistem Programlama" kurslarından da anımsayacağınız gibi aslında "proc" ve "sys" dosya sistemleri 
    içeirindeki dosyalar gerçek birer dosya değilidir. Bu dosyalar okunmak istendiğinde çekirdek modüllerinin ilgili 
    fonksiyonları çağrılmakta çekirdek modülleri de bu bilgileri sağlayıp o anda talep eden prosese vermektedir. Yani 
    örneğin biz "proc/dentry-state" dosyasını okumak istediğimizde çekirdek içeisindeki aşağıdaki fonksiyon çağrılıp 
    bilgiler çekirdekteki fonksiyonlar youyla elde edilip talep eden prosese verilmektedir:

    static int proc_nr_dentry(const struct ctl_table *table, int write, void *buffer,
			  size_t *lenp, loff_t *ppos)
    {
        dentry_stat.nr_dentry = get_nr_dentry();
        dentry_stat.nr_unused = get_nr_dentry_unused();
        dentry_stat.nr_negative = get_nr_dentry_negative();
        return proc_doulongvec_minmax(table, write, buffer, lenp, ppos);
    }

    Biz burada bellek baskısı altında dentry önbelleğinden dentry nesnelerinin nasıl atıldığı konusunun ayrıntıları
    üzerinde durmayacağız. Bu konu aslında birincil olarak "bellek yönetimi (memory management)" ile ilgilidir. 
    Dolayısıyla "bellek yönetimi (memory management)" konusu içerisinde ele alınacaktır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz dentry nesneleri hakkında belli bir derinliğe kadar bilgiler edindik. Şimdi de inode nesneleri hakkında belli 
    bir derinliğe kadar bilgiler edineceğiz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir dosyaya ilişkin tüm bilgiler (yani "stat" fonksiyonuyla elde ettiğimiz tüm bilgiler) aslında diskte bir metadata 
    alanda tutulmaktadır. inode temelli dosya sistemlerinde bu bu metadata alanına "inode blok (inode block)" denilmektedir. 
    Ayrıntıları bir yana bırakırsak tipik bir ext ailesi (ext-2, ext-3, ext-4) inode tabanlı dosya sisteminin disk tarafında 
    disk bölümü üç bölgeye ayrılmaktadır: Süper blok, inode blok ve data block. Bunu şekilsel olarak şöyle gösterebiliriz:

    ┌─────────────┐
    │ Super Blok  │
    ├─────────────┤
    │ inode Blok  │
    ├─────────────┤
    │             │
    │ Data Block  │
    │             │
    └─────────────┘

    Disk bölümünün tüm metadata alanlarına yönelik parametrik bilgiler süper blokta tutulmaktadır. inode blok dosyalara 
    ilişkin metadata bilgilrinin tutulduğu bölümdür. Dosyaların paraçaları ise data blokta tutulmaktadır. inode blok inode 
    elemanlarından oluşmaktadır:

            inode Blok
        ┌────────────────┐
    0   │ inode elemanı  │ 
        ├────────────────┤
    1   │ inode elemanı  │ 
        ├────────────────┤
    2   │ inode elemanı  │ 
        ├────────────────┤
        │      ...       │
        ├────────────────┤
    xxx │ inode elemanı  │ 
        ├────────────────┤
        │      ...       │
        └────────────────┘

    İşte disk bölümündeki her dosya için inode blok içerisinde bir inode elemanı bulunmaktadır. ext4 dosya sisteminde
    inode elemanları aşağıdaki yapıyla temsil edilmiştir:

        struct ext4_inode {
        __le16  i_mode;
        __le16  i_uid;
        __le32  i_size_lo;
        __le32  i_atime;
        __le32  i_ctime;
        __le32  i_mtime;
        __le32  i_dtime;
        __le16  i_gid;
        __le16  i_links_count;
        __le32  i_blocks_lo;
        __le32  i_flags;
        union {
            struct {
                __le32  l_i_version;
            } linux1;
            struct {
                __u32   h_i_translator;
            } hurd1;
        } osd1;
        __le32  i_block[15];
        __le32  i_generation;
        __le32  i_file_acl_lo;
        __le32  i_size_high;
        __le32  i_obso_faddr;
        union {
            struct {
                __le16  l_i_blocks_high;
                __le16  l_i_file_acl_high;
                __le16  l_i_uid_high;
                __le16  l_i_gid_high;
                __le16  l_i_checksum_lo;
                __le16  l_i_reserved;
            } linux2;
            struct {
                __u8    h_i_frag;
                __u8    h_i_fsize;
                __u16   h_i_mode_high;
                __u16   h_i_uid_high;
                __u16   h_i_gid_high;
                __u32   h_i_author;
            } hurd2;
        } osd2;
        __le16  i_extra_isize;
        __le16  i_checksum_hi;
        __le32  i_ctime_extra;
        __le32  i_mtime_extra;
        __le32  i_atime_extra;
        __le32  i_crtime;
        __le32  i_crtime_extra;
        __le32  i_version_hi;
        __le32  i_projid;
    };

    Bir dosyaya ilişkin tüm bilgiler inode bloktaki dosyaya ilişkin inode elemanında tutulmaktadır. Bu bilgilerden bazıları 
    şunlardır:

    - Dosyanın türü ve erişim hakları 
    - Dosyanın kullanıcı ve grup id değerleri
    - Dosyanın tarih ve zaman bilgileri
    - Dosyanın hard link sayacı
    - Dosyanın uzunluğu
    - Dosya bloklarının diskte hangi bloklarda tutulduğu bilgisi

    Linux çekirdeği bir dosya üzerinde işlem yapmadan önce o dosyaya ilişkin inode bilgilerini elde etmek zorundadır. 
    Disk bölümündeki inode blokta her inode elamanının ilk eleman 0 olmak üzere bir numarası vardır. Buna ilgili 
    dosyanın "inode numarası" denilmektedir. Dosyaların inode numaraları ls komutunda "-i" seçeneğiyle görüntülenmektedir. 
    Örneğin:

    $ ls -i
    2392296 app             2392301 mydisk.dat       2392275 test-driver.ko     2392645 test-module.mod.c
    2392265 app.c           2389502 sample           2392266 test-driver.mod    2392647 test-module.mod.o
    ...

    ext2, ext3, ext4 gibi inode tabanlı dosya sistemlerinin disk organizasyonu hakkında çeşitli ayrıntılar vardır. 
    Biz bu kısmı daha sonra ele alacağız. Burada yalnızca "inode" kavramını açıklayabilmek için bu girişi yaptık. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeği diskteki bir dosyaya erişmeden önce (dizinler de birer dosya gibidir) o dosyanın diskteki inode 
    bilgilerini elde ederek bellekte bir inode nesnesi oluşturmaktadır. Inode nesneleri diskteki inode elemanlarındaki 
    bilgilerle birtakım gerekli diğer bilgileri tutmaktadır. Inode nesneleri inode isimli bir yapı ile temsil edilmiştir 
    Güncel çekirdeklerde inode yapısı "/include/fs.h" dosyası içerisinde şöyle bildirilmiştir:

    struct inode {
        umode_t                                  i_mode;
        unsigned short                           i_opflags;
        kuid_t                                   i_uid;
        kgid_t                                   i_gid;
        unsigned int                             i_flags;

    #ifdef CONFIG_FS_POSIX_ACL
        struct posix_acl                        *i_acl;
        struct posix_acl                        *i_default_acl;
    #endif

        const struct inode_operations           *i_op;
        struct super_block                      *i_sb;
        struct address_space                    *i_mapping;

    #ifdef CONFIG_SECURITY
        void                                    *i_security;
    #endif

        /* Stat data, not accessed from path walking */
        unsigned long                            i_ino;
        /*
        * Filesystems may only read i_nlink directly.  They shall use the
        * following functions for modification:
        *
        *    (set|clear|inc|drop)_nlink
        *    inode_(inc|dec)_link_count
        */
        union {
            const unsigned int               i_nlink;
            unsigned int                     __i_nlink;
        };
        dev_t                                    i_rdev;
        loff_t                                   i_size;
        time64_t                                 i_atime_sec;
        time64_t                                 i_mtime_sec;
        time64_t                                 i_ctime_sec;
        u32                                      i_atime_nsec;
        u32                                      i_mtime_nsec;
        u32                                      i_ctime_nsec;
        u32                                      i_generation;
        spinlock_t                               i_lock;                 /* i_blocks, i_bytes, maybe i_size */
        unsigned short                           i_bytes;
        u8                                       i_blkbits;
        enum rw_hint                             i_write_hint;
        blkcnt_t                                 i_blocks;

    #ifdef __NEED_I_SIZE_ORDERED
        seqcount_t                               i_size_seqcount;
    #endif

        /* Misc */
        u32                                      i_state;
        /* 32-bit hole */
        struct rw_semaphore                      i_rwsem;

        unsigned long                            dirtied_when;           /* jiffies of first dirtying */
        unsigned long                            dirtied_time_when;

        struct hlist_node                        i_hash;
        struct list_head                         i_io_list;              /* backing dev IO list */
    #ifdef CONFIG_CGROUP_WRITEBACK
        struct bdi_writeback                     *i_wb;                  /* the associated cgroup wb */

        /* foreign inode detection, see wbc_detach_inode() */
        int                                      i_wb_frn_winner;
        u16                                      i_wb_frn_avg_time;
        u16                                      i_wb_frn_history;
    #endif
        struct list_head                         i_lru;                  /* inode LRU list */
        struct list_head                         i_sb_list;
        struct list_head                         i_wb_list;              /* backing dev writeback list */
        union {
            struct hlist_head                i_dentry;
            struct rcu_head                  i_rcu;
        };
        atomic64_t                               i_version;
        atomic64_t                               i_sequence;             /* see futex */
        atomic_t                                 i_count;
        atomic_t                                 i_dio_count;
        atomic_t                                 i_writecount;
    #if defined(CONFIG_IMA) || defined(CONFIG_FILE_LOCKING)
        atomic_t                                 i_readcount;            /* struct files open RO */
    #endif
        union {
            const struct file_operations    *i_fop;                  /* former ->i_op->default_file_ops */
            void                            (*free_inode)(struct inode *);
        };
        struct file_lock_context                 *i_flctx;
        struct address_space                     i_data;
        union {
            struct list_head                 i_devices;
            int                              i_linklen;
        };
        union {
            struct pipe_inode_info          *i_pipe;
            struct cdev                     *i_cdev;
            char                            *i_link;
            unsigned                         i_dir_seq;
        };

    #ifdef CONFIG_FSNOTIFY
        __u32                                    i_fsnotify_mask;        /* all events this inode cares about */
        /* 32-bit hole reserved for expanding i_fsnotify_mask */
        struct fsnotify_mark_connector __rcu    *i_fsnotify_marks;
    #endif

    #ifdef CONFIG_FS_ENCRYPTION
        struct fscrypt_inode_info               *i_crypt_info;
    #endif

    #ifdef CONFIG_FS_VERITY
        struct fsverity_info                    *i_verity_info;
    #endif

        void                                    *i_private;              /* fs or device private pointer */
    } __randomize_layout;

    Görüldüğü gibi yapıya çekirdek konfigürasyon parametrelerine bağlı olarak bazı elemanlar eklenip çıkartılmaktadır.
    Bu yapı 2.6 çekirdeklerinde şöyledi:

    struct inode {
        struct hlist_node                    i_hash;
        struct list_head                     i_list;
        struct list_head                     i_sb_list;
        struct list_head                     i_dentry;
        unsigned long                        i_ino;
        atomic_t                             i_count;
        unsigned int                         i_nlink;
        uid_t                                i_uid;
        gid_t                                i_gid;
        dev_t                                i_rdev;
        unsigned long                        i_version;
        loff_t                               i_size;
    #ifdef __NEED_I_SIZE_ORDERED
        seqcount_t                           i_size_seqcount;
    #endif
        struct timespec                      i_atime;
        struct timespec                      i_mtime;
        struct timespec                      i_ctime;
        unsigned int                         i_blkbits;
        blkcnt_t                             i_blocks;
        unsigned short                       i_bytes;
        umode_t                              i_mode;
        spinlock_t                           i_lock;                     /* i_blocks, i_bytes, maybe i_size */
        struct mutex                         i_mutex;
        struct rw_semaphore                  i_alloc_sem;
        const struct inode_operations       *i_op;
        const struct file_operations        *i_fop;                      /* former ->i_op->default_file_ops */
        struct super_block                  *i_sb;
        struct file_lock                    *i_flock;
        struct address_space                *i_mapping;
        struct address_space                 i_data;
    #ifdef CONFIG_QUOTA
        struct dquot                        *i_dquot[MAXQUOTAS];
    #endif
        struct list_head                     i_devices;
        union {
            struct pipe_inode_info      *i_pipe;
            struct block_device         *i_bdev;
            struct cdev                 *i_cdev;
        };
        int                                  i_cindex;

        __u32                                i_generation;

    #ifdef CONFIG_DNOTIFY
        unsigned long                        i_dnotify_mask;             /* Directory notify events */
        struct dnotify_struct               *i_dnotify;                  /* for directory notifications */
    #endif

    #ifdef CONFIG_INOTIFY
        struct list_head                     inotify_watches;            /* watches on this inode */
        struct mutex                         inotify_mutex;              /* protects the watches list */
    #endif

        unsigned long                        i_state;
        unsigned long                        dirtied_when;               /* jiffies of first dirtying */

        unsigned int                         i_flags;

        atomic_t                             i_writecount;
    #ifdef CONFIG_SECURITY
        void                                *i_security;
    #endif
        void                                *i_private;                  /* fs or device private pointer */
    };

    2.2 ve 2.4 çekirdeklerinde ise inode yapısı şöyleydi:

    struct inode {
        struct list_head	i_hash;
        struct list_head	i_list;
        struct list_head	i_dentry;

        unsigned long		i_ino;
        unsigned int		i_count;
        kdev_t			i_dev;
        umode_t			i_mode;
        nlink_t			i_nlink;
        uid_t			i_uid;
        gid_t			i_gid;
        kdev_t			i_rdev;
        off_t			i_size;
        time_t			i_atime;
        time_t			i_mtime;
        time_t			i_ctime;
        unsigned long		i_blksize;
        unsigned long		i_blocks;
        unsigned long		i_version;
        unsigned long		i_nrpages;
        struct semaphore	i_sem;
        struct semaphore	i_atomic_write;
        struct inode_operations	*i_op;
        struct super_block	*i_sb;
        struct wait_queue	*i_wait;
        struct file_lock	*i_flock;
        struct vm_area_struct	*i_mmap;
        struct vm_area_struct	*i_mmap_shared;
        struct page		    *i_pages;
        struct dquot		*i_dquot[MAXQUOTAS];

        unsigned long		i_state;

        unsigned int		i_flags;
        unsigned char		i_pipe;
        unsigned char		i_sock;

        int			        i_writecount;
        unsigned int		i_attr_flags;
        __u32			    i_generation;
        union {
            struct pipe_inode_info		pipe_i;
            struct minix_inode_info		minix_i;
            struct ext2_inode_info		ext2_i;
            struct hpfs_inode_info		hpfs_i;
            struct ntfs_inode_info          ntfs_i;
            struct msdos_inode_info		msdos_i;
            struct umsdos_inode_info	umsdos_i;
            struct iso_inode_info		isofs_i;
            struct nfs_inode_info		nfs_i;
            struct sysv_inode_info		sysv_i;
            struct affs_inode_info		affs_i;
            struct ufs_inode_info		ufs_i;
            struct efs_inode_info		efs_i;
            struct romfs_inode_info		romfs_i;
            struct coda_inode_info		coda_i;
            struct smb_inode_info		smbfs_i;
            struct hfs_inode_info		hfs_i;
            struct adfs_inode_info		adfs_i;
            struct qnx4_inode_info		qnx4_i;
            struct usbdev_inode_info	usbdev_i;
            struct socket			    socket_i;
            void				        *generic_ip;
        } u;
    };

    Çekirdeğin öğrenci ödevi gibi olan ilkel 0.01 versiyonunda yapının ismi m_inode biçimindeydi ve içeriği de 
    şöyleydi:

    struct m_inode {
        unsigned short i_mode;
        unsigned short i_uid;
        unsigned long i_size;
        unsigned long i_mtime;
        unsigned char i_gid;
        unsigned char i_nlinks;
        unsigned short i_zone[9];
    /* these are in memory also */
        struct task_struct * i_wait;
        unsigned long i_atime;
        unsigned long i_ctime;
        unsigned short i_dev;
        unsigned short i_num;
        unsigned short i_count;
        unsigned char i_lock;
        unsigned char i_dirt;
        unsigned char i_pipe;
        unsigned char i_mount;
        unsigned char i_seek;
        unsigned char i_update;
    };

    Gördüğünüz gibi bu ilkel versiyonda m_inde yapısı temel bilgileri tutan minimalist bir biçimde oluşturulmuştur. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                      33. Ders 15/11/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    UNIX/Linux sistemlerinde kullanılan inode tabanlı dosya sistemlerinde dizinlerle (directories) dosyaların (regular 
    files) disk organizasyonu tamamen aynıdır. Yani diskte bir dosya nasıl tutuluyorsa bir dizin de aynı biçimde tutulmaktadır. 
    Bir dosyanın normal bir dosya mı yoksa bir dizin mi belirttiği inode elemanındaki i_mode değişkeninin bir bitinde 
    saklanmaktadır. (Bu dosya sistemleri aramaları hızlandırmak için zamanla dosyanın hangi türden olduğu bilgisini 
    dizin girişlerinde de tutmaya başlamıştır.) Bir dizini siz "dizin girişlerinden (directory entries)" oluşan bir 
    dosya gibi düşünebilirsiniz. Bir dizinin içeriği temsili olarak şöyledir:

    Dizin Dosyasının İçeriği

    dizin_girişi 
    dizin_girişi
    dizin_girişi
    .....

    Biz bu bağlamda dizinlerin de birer dosya gibi organize edildiğini vurgulamak için "dizin" yerine "dizin dosyası" 
    terimini de kullanacağız. (UNIX/Linux dünyasında "dizin dosyası" biçiminde bir terim yoktur.) Dizin dosyası 
    içerisindeki dizin girişleri inode tabanlı dosya sistemlerinde eşit uzunlukta değildir. Dosya isimleri uzun 
    olabileceği için eşit uzunluktaki girişlerin çok yer kaplayacağı düşünülmüştür. Tabii dizin girişleri eşit 
    uzunlukta olmadığı için girişlere indeksli biçimde ancak sıralı biçimde erişilebilmektedir. Zaten dizin girişlerinde 
    indeksli erişimenin gerektiği bir durum yoktur. Örneğin ext2, ext3 ve ext4 dosya sistemlerindeki dizin girişlerinin 
    formatı şöyledir:

    ┌──────────────┬──────────┬──────────┬────────────┬────────────────────────────┐
    │    inode     │  rec_len │ name_len │ file_type  │           name[]           │
    │   (4 byte)   │ (2 byte) │ (1 byte) │  (1 byte)  │     (değişken uzunluk)     │
    └──────────────┴──────────┴──────────┴────────────┴────────────────────────────┘

    Bu format "fs/ext4/ext4.h" dosyasında aşağıdaki yapıyla da temsil edilmiştir:

    struct ext4_dir_entry_2 {
        __le32 inode;        // 0: Bu girişin işaret ettiği inode numarası
        __le16 rec_len;      // 4: Bu girişin toplam uzunluğu (byte)
        __u8   name_len;     // 6: Dosya adının uzunluğu (byte)
        __u8   file_type;    // 7: Dosya tipi (regular, dir, symlink vs.)
        char   name[];       // 8: Dosya adı (null terminate yok!)
    };

    Aynı yapı "fs/ext2/ext2.h" dosyası içerisinde de ext2_dir_entry_2 ismiyle de bildirilmiştir.

    Buradaki ilk alan (inode alanı) dosyanın bilgilerinin disk bölümünün inode bloğundaki kaçıncı inode elemanında 
    tutulduğunu belirtmektedir. inode numarası aslında diskin inode blok denilen metadata bölümünde bir indeks 
    belirtmektedir. (Dizin girişine ilişkin inode numarasının "ls" komutunda "-i" seçeneği ile de görüntülendiğini 
    anımsayınız.) rec_len alanı dizin girişinin kaç byte yer kapladığını, name_len alanı ise dosya isminin byte 
    uzunluğunu belirtmektedir. ext-4 dosya sisteminde bir dizin girişinin isminin uzunluğu en fazla 255 byte olabilmektedir. 
    Dizin girişinin file_type elemanında dzin girişinin türü tutulmaktadır. Aslında bu bilgi inode elemanın içerisinde 
    de vardır. Ancak dosyanın türünün inode elemanına gidilmeden daha hızlı bir biçimde elde edilmesi için bu bilginin 
    ayrıca dizin girişlerinde de tutulması uygun görülmüştür. Nihayaet dizin girişinin sonunda değişken uzunlukta dizin 
    girişinin ismi tutulmaktadır. Burada hem rec_len alaının hem de name_len alanın ayrı ayrı tutulmasının nedenini 
    merak edebilirsiniz. Ne de olsa yukarıdaki yapıda değişken olan tek alan dosya isminin uzunluğudur. İşte bir dizin 
    girişi bitttiğinde diğeri hemen onun bittiği yerden başlamak zorunda değildir. Arada hizalama boşlukları (padding) 
    bulundurulabilmektedir. (ext dosya sistemleri dizin girişlerini 4 byte'a hizalamaktadır.) Dosya türü file_type alanında 
    aşağıdaki değerlele tutulmaktadır.

    ┌───────┬────────────┬─────────────────────────────────────┐
    │ Değer │   Binary   │              Tür                    │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   0   │ 0000 0000  │ Bilinmiyor                          │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   1   │ 0000 0001  │ Normal dosya (regular file)         │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   2   │ 0000 0010  │ Dizin                               │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   3   │ 0000 0011  │ Character device                    │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   4   │ 0000 0100  │ Block device                        │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   5   │ 0000 0101  │ FIFO (named pipe)                   │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   6   │ 0000 0110  │ Socket                              │
    ├───────┼────────────┼─────────────────────────────────────┤
    │   7   │ 0000 0111  │ Symlink                             │
    └───────┴────────────┴─────────────────────────────────────┘

    UNIX/Linux sistemlerinde birden fazla dizin girişinde aynı inode mumarası varsa bu duruma bu sistemlerde "hard link" 
    denilmektedir. Hard link sayesinde farklı dizin girişleri aslında aynı dosyayı belirtiyor durumda olur. Bilindiği 
    gibi hard link'ler komut satırında "ln" komutuyla oluşturulmaktadır. Örneğin:

    $ ln x.txt y.txt 

    Burada "x.txt" girişi hangi hangi inode elemanına referans ediyorsa yaratılan "y.txt" girişi de aynı inode elemanına 
    referans eder hale getirilmiştir. Dosya bilgileri inode elemanında saklandığı için bu dizin girişlerine "ls -l" ile 
    bakıldığında tüm bilgileri aynı förünecektir:

    $ ls -li x.txt y.txt
    2392600 -rw-rw-r-- 2 kaan kaan 15 Kas 15 11:04 x.txt
    2392600 -rw-rw-r-- 2 kaan kaan 15 Kas 15 11:04 y.txt

    Bu durumda bir dizin girişi silindiğinde işletim sisteminin hemen ona karşı gelen inode elemanını silmemesi gerekir. 
    Çünkü o inode elemanını kullanan başka dizin girişleri de olabilmektedir. İşte bunu sağlamak için inode elemanın 
    içerisinde bir sayaç tutulmaktadır. Her hard link oluşturulduğunda bu sayaç 1 artırılır. Her hard link dizin girişi 
    silindiğinde bu sayaç 1 eksiltilir. inode elemanı sayaç 0'a düştüğünde gerçek anlamda silinmektedir. inode elemanlarındaki
    hard link sayacının "ls -l" komutunda da görüntülendiğini anımsayınız. Yukarıdaki "ls" komutunda görüntülenen iki 
    girişin de hard link sayacının 2 olduğuna dikkat ediniz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz önceki paragrafta diskteki bir inode elemanının brden fazla dizin girişi tarafından gösterilebildiğini belirttik. 
    Aynı durum tabii dosya sisteminin bellek tarafında da söz konusu olmaktadır. Yani dentry önbelleğindeki birden fazla 
    dentry nesnesi aynı inode nesnesini gösterebilmektedir. İşte çekirdek aynı zamanda bir inode elemanının hangi dentry 
    nesneleri tarafından gösterildiğini de bir bağlı listede tutmaktadır. (Ancak disk tarafındaki organizasyonda inode 
    elemanının içerisinde ona referans eden dizin girişleri tutulmamaktadır. Bu nedenle bir dosyanın tüm hard link'leri 
    ancak tüm dizin ağacının gözden geçirilmesiyle elde edilebilmektedir.) Inode nesnesine referans eden dentry 
    nesneleri Linux çekirdeklerinde uzun süredir inode yapısının i_dentry bağlı liste elemanında tutulmaktadır.
    Güncel çekirdeklerde bu eleman şöyle bildirilmiştir:

    struct inode {
        /* ... */

        union {
            struct hlist_head	i_dentry;
            struct rcu_head		i_rcu;
        };

        /* ... */
    };

    2.2 ve 2.4 çekirdeklerinde yukarıdaki gibi bir birlik (union) yoktu. i_dentry bağlı listesi doğrudan inode yapısının 
    içerisindeydi. İlk ilkel versiyon olan 0.01 versiyonunda m_inde yapısında böyle bir eleman zaten bulunmuyordu. 
    (Bu eleman ilk kez 0.11 versiyonunda eklenmiştir.)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeği inode nesnelerini de "inode önbelleği (inode cache)" denilen bir önbellekte tutmaktadır. Nasıl 
    erişilen dizin girişleri dentry önbelleğinde tutuluyorsa erişilen inode nesneleri de inode önbelleğinde tutulmaktadır. 
    Linux çekirdeğinde temel işlem yönü dentry nesnelerinden inode nesnelerine doğrudur. Dolayıyla eğer bir dentry nesnesi 
    dentry önbelleğinde bulunuyorsa her zaman ona ilişkin inode nesnesi de inode önbelleğinde bulunmak zorundadır. Tabii 
    istisna olarak eğer dentry nesnesi var olan bir dizin girişini göstermiyorsa (bu durum olmayan bir dosyaya ilişkin 
    dosya araması (name lookup) yapıldığında oluşabilmektedir) bu durumda dentry nesnesine ilişin bir inode nesnesi 
    inode önbelleğinde bulunmayacaktır. Bunlara "negatif dentry nesneleri" dendiğini anımsayınız. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'ta inode önbelleğinin organizasyonu zaman içerisinde değişikliklere uğratılmıştır. Güncel çekirdeklerde inode 
    nesneleri için tek bir hash tablosu kullanılmaktadır. Bu hash tablosunun adresi adresi de "fs/inode.c" içerisindeki 
    global inode_hashtable değişkeninde tutulmaktadır. Hash tablosunun büyüklüğü de aynı dosyadaki i_hash_shift değişkeninde 
    saklanmaktadır. Bu değişken hash tablosunun büyüklüğünü 2'nin kuvveti olarak tutar. (Yani örneğin bu değer 16 ise 
    tablo 2^16 = 65536 kova uzunluğundadır.) Ayrıca çekirdek hash fonksiyonunda indeks elde etmede kullanmak için 
    i_hash_shift^2 - 1 değerini de i_hash_mask global değişkeninde de tutmaktadır. (Yani bu i_nash_mask değeri toplam 
    kova sayısının 1 eksik değeridir.) Inode hash tablosunun büyüklüğü (yani kova sayısı) güncel çekirdeklerde sistemdeki 
    RAM miktarı dikkate alınarak aşağıdaki formülle hesaplanmaktadır:

    Tolam kova sayısı ≈ (toplam_ram_miktarı / 8192)

    Örneğin 8GB RAM bulunan bir sistemdeki inode hash tablosunun kova uzunluğu 1M kadar olacaktır. Hash tablosunun 
    uzunluğu (yani kova sayısı) sistem boot edilirken ihash_entries isimli çekirdek parametresiyle de değiltirilebilmektedir. 
    Ancak bunun için bir çekirdek konfigürasyon parametresi bulunmamaktadır. Çekirdeğin inode hash tablosu için belirlediği 
    uzunluk dmesg komutuyla da aşağıdaki gibi elde edilebilmektedir:

    dmesg | grep -i "inode-cache"

    Buradan aşağıdakine benzer çıktı görüntülenecektir:

    ```
    Inode-cache hash table entries: 131072 (order: 8, 1048576 bytes)
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Inode hash tablosuna inode nesneleri super_block nesnesi ve inode numarası anahtar yapılarak yerleştirilmektedir. 
    Arama da buna göre yapımaktadır. Çünkü UNIX/Linux sistemlerinde farklı dosya sistemlerindeki dosyaların inode 
    numaraları aynı olabilmektedir. Yani inode numaraları tüm dizin ağacında tek (unique) olmak zorunda değildir yalnızca 
    belli bir dosya sisteminde tek olmak zorundadır.
    
    Tıpkı dentry önbellek sisteminde olduğu gibi inode önbellek sisteminde de son zamanlarda en az kullanılan inode 
    nesnelerinin önbellekte tutulmasını sağlamak amacıyla bir LRU bağlı listesi oluşturulmuştur. Ancak bu LRU bağlı 
    listesi toplamda bir tane değil dentry LRU listelerinde olduğu gibi her super_block nesnesi için ayrı bir tane olarak 
    bulundurulmaktadır. inode önbelleğinin LRU listesi super_block yapısı içerisinde s_ilru elemanında tutulmaktadır:

    struct super_block {
        /* ... */

        struct list_lru		s_inode_lru;

        */ ... */
    };

    Biz dentry LRU listelerini incelerken zaten list_lru yapısını ele almıştık. Anımsanacağı gibi LRU listeleri NUMA 
    mimarisinde bir tane değil NUMA bank'larının sayısı kadardı. Güncel çekirdeklerde inode LRU listelerinde de dentry 
    LRU listelerinde olduğu gibi "son zamanlarda kullanılanları işaretlemek için" bir bit mekanizması kullanılmaktadır.
    Çekirdek her inode nesne sayacını azalttığında (iput fonksiyonunda) inode yapısının i_state elemanının I_REFERENCED 
    bitini 1 yapmakta, bellek baskısı oluştuğunda da bu bite bakarak bu biti 1 olanları LRU listesinin başına alarak 
    bu biti 0'lamaktadır.

    Çekirdek ayrıca super_block yapısının içerisinde o süper bloğa ilişkin disk bölümünün önbelleğinde bulunan bütün 
    inode nesnelerini de s_inodes isimli bir bağlı listede ayrıca tutulmaktadır:

     struct super_block {
        /* ... */

        struct list_lru		s_inode_lru;
        struct list_head	s_inodes;	/* all inodes */

        */ ... */
    };

    Bu durumda mevcut çekirdeklerdeki inode önbellek sistemini şöyle özetleyebiliriz:

    - Toplamda tek bir inode önbellek sistemi vardır.
    - Her super_block nesnesinde ayrı LRU listeleri tutulmaktadır.
    - Bir disk bölümündeki tüm inode nesneleri de ayrıca super_block yapısı içerisinde tutulmaktadır.
    - inode nesneleri inode hash tablosuna  super_block nesne adresi ve inode numarası anahtar yapılarak yerleştirilmektedir.

    2.6 çekirdeklerinde de inode hash tablosunun genel organizasyonu güncel çekirdeklerde aynıydı. Yine toplamda tek bir 
    hash tablosu vardı. Ancak LRU listeleri her super_block nesnesinde bulunmuyordu. Toplamda global düzeyde "fs/inodec" 
    dosyaısnda iki inode listesi bulunduruluyordu:

    LIST_HEAD(inode_in_use);
    LIST_HEAD(inode_unused);
    static struct hlist_head *inode_hashtable __read_mostly;

    Buradaki inode_unused listesi referans sayaıcı 0'a düşmüş olan inode nesnelerinin bulunduğu listeydi. Bu liste teknik 
    olarak bir LRU listesi değildi. Ancak inode nesnelerinin referans sayacı 0'a düştüğünde çekirdek bu nesneleri inode_unused 
    listesinin başına yerleştiriyordu. Bellek baskısı oluştuğunda reclaim işlemi yine sondan başa doğru yapılıyordu. 

    2.4 ve 2.2 çekirdeklerinde de genel organizasyon "fs/inode.c" dosyasında 2.6 çekirdeğindeki gibiydi:

    static LIST_HEAD(inode_in_use);
    static LIST_HEAD(inode_unused);
    static struct list_head *inode_hashtable;

    Linux'un öğrenci ödevi gibi olan ilkel 0.01 versiyonunda zaten çekirdeğin bir heap sistemi ve bir inode önbellek 
    sistemi yoktu. Sistemde bütün inode elemanları global bir dizide tutuluyordu ve oradan boş olanlar tahsis ediliyordu. 
    Bu inode dizisi de 32 elemanlı çok küçük bir diziydi. Eğer inode dizisindeki tüm inode elemanları kullanılıyorsa 
    çekirdek panic oluşturup çöküyordu. Aşağıda bu versiyondaki "fs/inode.c" dosyası içersindeki get_empty_inode fonksiyonunu 
    görüyorsunuz:

    #define NR_INODE 32

    struct m_inode inode_table[NR_INODE]={{0,},};

    struct m_inode * get_empty_inode(void)
    {
        struct m_inode * inode;
        int inr;

        while (1) {
            inode = NULL;
            inr = last_allocated_inode;
            do {
                if (!inode_table[inr].i_count) {
                    inode = inr + inode_table;
                    break;
                }
                inr++;
                if (inr>=NR_INODE)
                    inr=0;
            } while (inr != last_allocated_inode);
            if (!inode) {
                for (inr=0 ; inr<NR_INODE ; inr++)
                    printk("%04x: %6d\t",inode_table[inr].i_dev,
                        inode_table[inr].i_num);
                panic("No free inodes in mem");
            }
            last_allocated_inode = inr;
            wait_on_inode(inode);
            while (inode->i_dirt) {
                write_inode(inode);
                wait_on_inode(inode);
            }
            if (!inode->i_count)
                break;
        }
        memset(inode,0,sizeof(*inode));
        inode->i_count = 1;
        return inode;
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                            34. Ders 16/11/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Peki inode önbelleğinden inode nesneleri nasıl çıkarılmaktadır? Daha önceden de belirttiğimiz gibi bir dentry 
    nesnesi dentry önbelleğinde bulunuyorsa ona ilişkin inode nesnesi de inode önbelleğinde bulunmaktadır. Çünkü dentry
    nesneleri inode nesnelerinin nesne sayacını (inode yapısındaki i_count elemanı) artırmaktadır. Bir inode nesnesinin 
    inode önbelleğinden atılabilmesi için onun referans sayacının 0'a düşmüş olması gerekir. (Zaten inode nesnesi LRU 
    listesine referans sayacı 0 olduğunda girmektedir.) Bellek baskısı oluştuğunda çekirdek yine super_block nesnelerinden 
    hareketle LRU listelerini sondan itibaren dolaşıp belirlenen miktarda inode nesnesini önbellekten atmaktadır. Çekirdek 
    bir inode nesnesi "kirli (dirty)" durumdaysa onu inode önbelleğinden atmamaktadır. Çünkü kirli inode elemanları başka 
    bir çekirdek thread'i tarafından flush edilmektedir. Benzer biçimde o anda inode nesnesi kilitliyse yine o inode elemanı 
    da inode önbelleğinden atılmamaktadır. Özetle inode nesnesinin inode önbelleğinden atılmasının koşulları 
    şunlardır:

    1) Inode nesnesinin referans sayacı (yani i_count elemanı) 0 olmalıdır. (Zaten nesne LRU listesindeyse onun referans 
    sayacı 0 olmak zorundadır. Ancak bu durumun bazı istisnaları olabilmektedir.)
    2) Inode nesnesi kirli (dirty) olmamalıdır. (Inode nesnesi kirli olduğu halde inode önbelleğinde bulunabilmektedir.)
    3) Inode nesnesi o anda kilitli (locked) olmamalıdır.

    Güncel çekirdeklerde LRU listesindeki inode nesnelerinin inode önbelleğinden atılıp atılmayacağına "fs/inode.c" 
    dosyası içerisindeki inode_lru_isolate fonksiyonunda karar verilmektedir:

    static enum lru_status inode_lru_isolate(struct list_head *item,
        struct list_lru_one *lru, spinlock_t *lru_lock, void *arg)
    {
        struct list_head *freeable = arg;
        struct inode *inode = container_of(item, struct inode, i_lru);

        if (!spin_trylock(&inode->i_lock))
            return LRU_SKIP;

        /*
        * Referenced or dirty inodes are still in use. Give them another pass
        * through the LRU as we canot reclaim them now.
        */
        if (atomic_read(&inode->i_count) ||
            (inode->i_state & ~I_REFERENCED)) {
            list_lru_isolate(lru, &inode->i_lru);
            spin_unlock(&inode->i_lock);
            this_cpu_dec(nr_unused);
            return LRU_REMOVED;
        }

        /* recently referenced inodes get one more pass */
        if (inode->i_state & I_REFERENCED) {
            inode->i_state &= ~I_REFERENCED;
            spin_unlock(&inode->i_lock);
            return LRU_ROTATE;
        }

        if (inode_has_buffers(inode) || !mapping_empty(&inode->i_data)) {
            inode_pin_lru_isolating(inode);
            spin_unlock(&inode->i_lock);
            spin_unlock(&lru->lock);
            if (remove_inode_buffers(inode)) {
                unsigned long reap;
                reap = invalidate_mapping_pages(&inode->i_data, 0, -1);
                if (current_is_kswapd())
                    __count_vm_events(KSWAPD_INODESTEAL, reap);
                else
                    __count_vm_events(PGINODESTEAL, reap);
                mm_account_reclaimed_pages(reap);
            }
            inode_unpin_lru_isolating(inode);
            return LRU_RETRY;
        }

        WARN_ON(inode->i_state & I_NEW);
        inode->i_state |= I_FREEING;
        list_lru_isolate_move(lru, &inode->i_lru, freeable);
        spin_unlock(&inode->i_lock);

        this_cpu_dec(nr_unused);
        return LRU_REMOVED;
    }

    Güncel çekirdeklerde bellek baskısı oluştuğunda inode nesnelerinin atılmasına ilişkin çağrı zinciri aşağıdaki gibidir:
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Dentry önbelleğinden bir dentry nesnesinin atıldığını varsayalım. Peki buna ilişkin inode nesnesinin bu olay nedeniyle 
    referans sayacı 0'a düşmüşse o inode nesnesi de inode önbelleğinden o sırada atılacak mıdır? İşte dentry önbelleğinin 
    shrink edilmesi ile inode önbelleğinin shrink edilmesi farklı zamanlarda yapılmaktadır. Dolayısıyla çekirdek bir dentry 
    nesnesini dentry önbelleğinden atarken o sırada ona ilişkin inode nesnesini koşullar sağlanmış olsa bile inode önbelleğinden 
    atmaya çalışmamaktadır. inode önbelleğinin shrink edilmesi farklı bir periyotta yapılmaktadır. 

    Şimdi de bu önbellek sisteminin nasıl işletildiğini daha iyi anlayabilmek için birkaç örnek senaryo üzerinde durmak 
    istiyoruz. Varsayalım ki "sample.txt" isiml bir dosya open fonksiyonuyla açılıp hemen close fonksiyonuyla kapatılmış olsun. 
    Bu sırada dentry ve inode önbelleklerine ilişkin şu olaylar gerçekleşecektir:

    1) Dosya açılırken "yol ifadesinin çözümlenmesi (pathame resolution)" sırasında ilgili dosyanın dentry nesnesi dentry 
    önbelleğinde yoksa dentry önbelleğine alınacaktır. Dosya açılmasından dolayı yaratılan dosya nesnesi (yani struct file 
    nesnesi) bu dentry nesnesine referans ettiği için dentry nesnesinin referans sayacı 1 artırılmaktadır. 

    2) Dosya açılırken ilgili dosyanın inode bilgileri inode önbelleğinde aranacak, yoksa inode önbelleğine yerleştirilecektir. 
    Bu inode nesnesi dentry nesnesi tarafından referans edildiği için onun da referans sayacı 1 artırılacaktır.

    3) Dosya kapatıldığında dosya nesnesi yok edilirken dentry nesnesinin referans sayacı 1 eksiltilecektir. Eğer dentry 
    nesnesinin referans sayacı 0'a düşerse çekirdek onu dentry LRU listesinin başına yerleştirecektir. Tabii bu dentry 
    nesnesinin referans sayacı düşürülürken ona ilişkin inode nesnesinin de referans sayacı düşürülmektedir. O inode 
    nesnesinin de referans sayacı eğer 0'a düşerse o inode nesnesi de inode LRU listesinin başına yerleştirilecektir. 

    4) Eğer dentry nesnesi ve ona ilişkin inode nesnesi LRU listelerine aktarılmışsa artık zamanla bellek baskısı 
    oluştuğunda listenin sonundan itibaren boşaltım yapılırken bunlar bu önbelleklerden tamamen atılacaktır. 

    Burada bir noktaya dikkat ediniz: Dosya nesnesi dentry nesnesinin referans sayacını 1 artırmaktadır, dentry nesnesi 
    de inode nesnesinin referans sayacını 1 artırmaktadır. 

    Şimdi de dosya nesnesi oluşturmayan bir işlem yapıldığında önbellek bağlamında neler olduğuna bakalım. Örneğin 
    bir dosyanın bilgileri stat fonksiyonuyla elde edilmek istensin. Bu durumda önbellek bağlamında sırasıyla şunlar 
    olacaktır:

    1) İlgili dosyaya ilişkin dentry nesnesi dentry önbelleğinde aranacak, bulunamazsa oluşturularak dentry önbelleğine 
    yerleştirilecektir. Bu işlem dentry nesnesinin referans sayacını 1 artıracaktır.

    2) Dentry nesnesine ilişkin inode nesnesi inode önbelleğinde aranacak, bulunamazsa oluşturularak inode önbelleğine 
    yerleştirilecektir. Bu işlem inode nesnesinin referans sayacını 1 artıracaktır.

    3) Inode nesnesi içerisindeki bilgiler kullanıcı modundaki prosese teslim edilecektir. 

    4) İşlem bittikten sonra dentry nesnesinin referans sayacı 1 eksiltilecektir. Eğer nesnenin referans sayacı 0'a 
    düşerse nesne dentry LRU listesinin başına yerleştirilecektir. Tabii bu sırada inode nesnesinin referans sayacı da 
    1 eksiltilecektir. Eğer inode referans sayacı da 0'a düşerse inode nesnesi de inode LRU listesinin başına 
    yerleştirilecektir.

    6) Bellek baskısı oluştuğunda eğer bu nesneler kullanılmamışlarsa ve listenin sonlarına doğru da ilerlemişlerse
    bu önbelleklerden atılacaklardır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Çekirdek inode önbellek sistemi hakkında da bazı bilgileri "proc" dosya sistemi yoluyla dış dünyaya iletmektedir. 
    Bunları da açıklamak istiyoruz:

    - "/proc/sys/fs/inode-state" dosyası inode önbelleği hakkında bilgi veren temel bir dosyadır. Bu dosya okunduğunda 
    aşağıdaki formatta bilgiler verilmektedir:

    $ cat /proc/sys/fs/inode-state
    42418	532	0	0	0	0	0

    Buradaki birinci sütunda (örneğimizdeki 424128) inode önbelleğindeki toplam inode nesnelerinin sayısı, ikinci sütunda 
    ise LRU listesindeki kullanılmayan inode nenelerinin sayısı (örnğimizde 532) rapor edilmektedir. Kullanılmayan inode
    nesneleri referans sayacı (yani i_count elemanı) 0 olan nesnelerdir. Bazı istisnalar dışında zaten kullanılmayan 
    inode nesneleri LRU listelerine alınmaktadır. Diğer sütunlar güncel çekirdeklerde kullanılmamaktadır. Bu sütunlarda 
    hep 0 bulunmaktadır. Bu dosya okunduğunda çekirdekte "fs/inode.c" dosyasında bulunan aşağıdaki fonksiyon çağrılmaktadır:

    static int proc_nr_inodes(const struct ctl_table *table, int write, void *buffer,
			  size_t *lenp, loff_t *ppos)
    {
        inodes_stat.nr_inodes = get_nr_inodes();
        inodes_stat.nr_unused = get_nr_inodes_unused();
        return proc_doulongvec_minmax(table, write, buffer, lenp, ppos);
    }

    - "/proc/sys/fs/inode-nr" dosyası da yukarıdaki iki sütunu vermektedir:

    $ cat /proc/sys/fs/inode-nr
    42421	532

    - Yine dilimli tahsisat sistemi yoluyla inode önbellek tahsisat dilimleri hakkında bilgiler "/proc/slabinfo" 
    dosyasından elde edilebilmektedir. Örneğin:

    $ sudo cat /proc/slabinfo | grep inode
    isofs_inode_cache        46     46    688   23    4 : tunables    0    0    0 : slabdata      2      2      0
    btrfs_inode               0      0   1088   30    8 : tunables    0    0    0 : slabdata      0      0      0
    fscrypt_inode_info        0      0    128   32    1 : tunables    0    0    0 : slabdata      0      0      0
    mqueue_inode_cache       34     34    960   34    8 : tunables    0    0    0 : slabdata      1      1      0
    fuse_inode               72     72    896   36    8 : tunables    0    0    0 : slabdata      2      2      0
    ecryptfs_inode_cache      0      0   1024   32    8 : tunables    0    0    0 : slabdata      0      0      0
    fat_inode_cache          20     20    792   20    4 : tunables    0    0    0 : slabdata      1      1      0
    squashfs_inode_cache      0      0    704   23    4 : tunables    0    0    0 : slabdata      0      0      0
    ext4_inode_cache      14013  14013   1184   27    8 : tunables    0    0    0 : slabdata    519    519      0
    hugetlbfs_inode_cache    50     50    648   25    4 : tunables    0    0    0 : slabdata      2      2      0
    sock_inode_cache       1498   1521    832   39    8 : tunables    0    0    0 : slabdata     39     39      0
    tracefs_inode_cache    1584   1584    672   24    4 : tunables    0    0    0 : slabdata     66     66      0
    proc_inode_cache       5060   5060    712   23    4 : tunables    0    0    0 : slabdata    220    220      0
    shmem_inode_cache      1965   1980    792   20    4 : tunables    0    0    0 : slabdata     99     99      0
    inode_cache           19289  19350    640   25    4 : tunables    0    0    0 : slabdata    774    774      0
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dikkatimizi süper blok nesnelerine ve dosya sistemlemlerinin kaydettirilmesine (file system registration) 
    çevirelim. Daha önceden de belirttiğimiz mount edilen her dosya sistemi için Linux çekirdeği süper blok nesnesi 
    denilen bir nesne oluşturmaktadır. Süper blok nesneleri çekirdekte "include/fs.h" dosyası içerisindeki super_block 
    yapısıyla temsil edilmektedir. Güncel çekirdeklerde super_block yapısı şöyle bildirilmiştir:

    struct super_block {
    struct list_head	        s_list;		                /* Keep this first */
        dev_t			        s_dev;		                /* search index; _not_ kdev_t */
        unsigned char		    s_blocksize_bits;
        unsigned long		    s_blocksize;
        loff_t			        s_maxbytes;	                /* Max file size */
        struct file_system_type	*s_type;
        const struct super_operations	*s_op;
        const struct dquot_operations	*dq_op;
        const struct quotactl_ops	    *s_qcop;
        const struct export_operations *s_export_op;
        unsigned long		            s_flags;
        unsigned long		            s_iflags;	        /* internal SB_I_* flags */
        unsigned long		            s_magic;
        struct dentry		            *s_root;
        struct rw_semaphore	            s_umount;
        int			                    s_count;
        atomic_t		                s_active;
    #ifdef CONFIG_SECURITY
        void                    *s_security;
    #endif
        const struct xattr_handler * const *s_xattr;
    #ifdef CONFIG_FS_ENCRYPTION
        const struct fscrypt_operations	*s_cop;
        struct fscrypt_keyring	*s_master_keys;         /* master crypto keys in use */
    #endif
    #ifdef CONFIG_FS_VERITY
        const struct fsverity_operations *s_vop;
    #endif
    #if IS_ENABLED(CONFIG_UNICODE)
        struct unicode_map *s_encoding;
        __u16 s_encoding_flags;
    #endif
        struct hlist_bl_head    s_roots;	            /* alternate root dentries for NFS */
        struct list_head	    s_mounts;	            /* list of mounts; _not_ for fs use */
        struct block_device	    *s_bdev;	            /* can go away once we use an accessor for @s_bdev_file */
        struct file		        *s_bdev_file;
        struct backing_dev_info *s_bdi;
        struct mtd_info		    *s_mtd;
        struct hlist_node	    s_instances;
        unsigned int		    s_quota_types;	        /* Bitmask of supported quota types */
        struct quota_info	    s_dquot;	            /* Diskquota specific options */

        struct sb_writers	    s_writers;

        /*
        * Keep s_fs_info, s_time_gran, s_fsnotify_mask, and
        * s_fsnotify_info together for cache efficiency. They are frequently
        * accessed and rarely modified.
        */
        void			        *s_fs_info;	            /* Filesystem private info */

        /* Granularity of c/m/atime in ns (cannot be worse than a second) */
        u32			s_time_gran;
        /* Time limits for c/m/atime in seconds */
        time64_t		   s_time_min;
        time64_t		   s_time_max;
    #ifdef CONFIG_FSNOTIFY
        u32			s_fsnotify_mask;
        struct fsnotify_sb_info	*s_fsnotify_info;
    #endif

        /*
        * q: why are s_id and s_sysfs_name not the same? both are human
        * readable strings that identify the filesystem
        * a: s_id is allowed to change at runtime; it's used in log messages,
        * and we want to when a device starts out as single device (s_id is dev
        * name) but then a device is hot added and we have to switch to
        * identifying it by UUID
        * but s_sysfs_name is a handle for programmatic access, and can't
        * change at runtime
        */
        char			s_id[32];	        /* Informational name */
        uuid_t			s_uuid;		        /* UUID */
        u8			s_uuid_len;	            /* Default 16, possibly smaller for weird filesystems */

        /* if set, fs shows up under sysfs at /sys/fs/$FSTYP/s_sysfs_name */
        char			s_sysfs_name[UUID_STRING_LEN + 1];

        unsigned int		s_max_links;

        /*
        * The next field is for VFS *only*. No filesystems have any business
        * even looking at it. You had been warned.
        */
        struct mutex s_vfs_rename_mutex;	/* Kludge */

        /*
        * Filesystem subtype.  If non-empty the filesystem type field
        * in /proc/mounts will be "type.subtype"
        */
        const char *s_subtype;

        const struct dentry_operations *s_d_op; /* default d_op for dentries */

        struct shrinker *s_shrink;	        /* per-sb shrinker handle */

        /* Number of inodes with nlink == 0 but still referenced */
        atomic_long_t s_remove_count;

        /* Read-only state of the superblock is being changed */
        int s_readonly_remount;

        /* per-sb errseq_t for reporting writeback errors via syncfs */
        errseq_t s_wb_err;

        /* AIO completions deferred from interrupt context */
        struct workqueue_struct *s_dio_done_wq;
        struct hlist_head s_pins;

        /*
        * Owning user namespace and default context in which to
        * interpret filesystem uids, gids, quotas, device nodes,
        * xattrs and security labels.
        */
        struct user_namespace *s_user_ns;

        /*
        * The list_lru structure is essentially just a pointer to a table
        * of per-node lru lists, each of which has its own spinlock.
        * There is no need to put them into separate cachelines.
        */
        struct list_lru		s_dentry_lru;
        struct list_lru		s_inode_lru;
        struct rcu_head		rcu;
        struct work_struct	destroy_work;

        struct mutex		s_sync_lock;	/* sync serialisation lock */

        /*
        * Indicates how deep in a filesystem stack this SB is
        */
        int s_stack_depth;

        /* s_inode_list_lock protects s_inodes */
        spinlock_t		s_inode_list_lock ____cacheline_aligned_in_smp;
        struct list_head	s_inodes;	    /* all inodes */

        spinlock_t		s_inode_wblist_lock;
        struct list_head	s_inodes_wb;	/* writeback inodes */
    } __randomize_layout;

    Görüldüğü gibi yapı oldukça büyüktür ve bazı elemanlar çeşitli konfigürasyon seçenekleriyle yapıya dahil edilmektedir. 
    2.6 çekirdeklerinde super_block yapısı şöyleydi:

    struct super_block {
        struct list_head	s_list;		                /* Keep this first */
        dev_t			    s_dev;		                /* search index; _not_ kdev_t */
        unsigned long		s_blocksize;
        unsigned char		s_blocksize_bits;
        unsigned char		s_dirt;
        loff_t			    s_maxbytes;	                /* Max file size */
        struct file_system_type	*s_type;
        const struct super_operations	*s_op;
        const struct dquot_operations	*dq_op;
        const struct quotactl_ops	    *s_qcop;
        const struct export_operations  *s_export_op;
        unsigned long		    s_flags;
        unsigned long		    s_magic;
        struct dentry		    *s_root;
        struct rw_semaphore	    s_umount;
        struct mutex		    s_lock;
        int			            s_count;
        int			            s_need_sync;
        atomic_t		        s_active;
    #ifdef CONFIG_SECURITY
        void                    *s_security;
    #endif
        struct xattr_handler	**s_xattr;

        struct list_head	    s_inodes;	            /* all inodes */
        struct hlist_head	    s_anon;		            /* anonymous dentries for (nfs) exporting */
        struct list_head	    s_files;
        /* s_dentry_lru and     s_nr_dentry_unused are protected by dcache_lock */
        struct list_head	    s_dentry_lru;	        /* unused dentry lru */
        int			            s_nr_dentry_unused;	    /* # of dentry on lru */

        struct block_device	    *s_bdev;
        struct backing_dev_info *s_bdi;
        struct mtd_info		    *s_mtd;
        struct list_head	    s_instances;
        struct quota_info	    s_dquot;	            /* Diskquota specific options */

        int			            s_frozen;
        wait_queue_head_t	    s_wait_unfrozen;

        char s_id[32];				                    /* Informational name */

        void 			        *s_fs_info;	/* Filesystem private info */
        fmode_t			        s_mode;

        /*
        * The next field is for VFS *only*. No filesystems have any business
        * even looking at it. You had been warned.
        */
        struct mutex            s_vfs_rename_mutex;	    /* Kludge */

        /* Granularity of c/m/atime in ns.
        Cannot be worse than a second */
        u32		                s_time_gran;

        /*
        * Filesystem subtype.  If non-empty the filesystem type field
        * in /proc/mounts will be "type.subtype"
        */
        char *s_subtype;

        /*
        * Saved mount options for lazy filesystems using
        * generic_show_options()
        */
        char                    *s_options;
    };

    2.4 ve 2.2 çekirdeklerinde ise super_block yapısı şöyleydi:

    struct super_block {
        struct list_head	        s_list;		        /* Keep this first */
        kdev_t			            s_dev;
        unsigned long		        s_blocksize;
        unsigned char		        s_blocksize_bits;
        unsigned char		        s_dirt;
        unsigned long long	        s_maxbytes;	        /* Max file size */
        struct file_system_type	    *s_type;
        struct super_operations	    *s_op;
        struct dquot_operations	    *dq_op;
        struct quotactl_ops	        *s_qcop;
        unsigned long		        s_flags;
        unsigned long		        s_magic;
        struct dentry		        *s_root;
        struct rw_semaphore	        s_umount;
        struct semaphore	        s_lock;
        int			                _count;
        atomic_t		            s_active;

        struct list_head	        s_dirty;	                /* dirty inodes */
        struct list_head	        s_locked_inodes;            /* inodes being synced */
        struct list_head	        s_files;

        struct block_device	        *s_bdev;
        struct list_head	        s_instances;
        struct quota_info	        s_dquot;	                /* Diskquota specific options */

        union {
            struct minix_sb_info	minix_sb;
            struct ext2_sb_info	    ext2_sb;
            struct ext3_sb_info	    ext3_sb;
            struct hpfs_sb_info	    hpfs_sb;
            struct ntfs_sb_info	    ntfs_sb;
            struct msdos_sb_info	msdos_sb;
            struct isofs_sb_info	isofs_sb;
            struct nfs_sb_info	    nfs_sb;
            struct sysv_sb_info	    sysv_sb;
            struct affs_sb_info	    affs_sb;
            struct ufs_sb_info	    ufs_sb;
            struct efs_sb_info	    efs_sb;
            struct shmem_sb_info	shmem_sb;
            struct romfs_sb_info	romfs_sb;
            struct smb_sb_info	    smbfs_sb;
            struct hfs_sb_info	    hfs_sb;
            struct adfs_sb_info	    adfs_sb;
            struct qnx4_sb_info	    qnx4_sb;
            struct reiserfs_sb_info	reiserfs_sb;
            struct bfs_sb_info	    bfs_sb;
            struct udf_sb_info	    udf_sb;
            struct ncp_sb_info	    ncpfs_sb;
            struct usbdev_sb_info   usbdevfs_sb;
            struct jffs2_sb_info	jffs2_sb;
            struct cramfs_sb_info	cramfs_sb;
            void			        *generic_sbp;
        } u;

    Linux'un ilkel 0.01 versiyonunda super_block yapısı da oldukça minimalistti:

    struct super_block {
        unsigned short      s_ninodes;
        unsigned short      s_nzones;
        unsigned short      s_imap_blocks;
        unsigned short      s_zmap_blocks;
        unsigned short      s_firstdatazone;
        unsigned short      s_log_zone_size;
        unsigned long       s_max_size;
        unsigned short      s_magic;
    /* These are only in memory */
        struct buffer_head  * s_imap[8];
        struct buffer_head  * s_zmap[8];
        unsigned short      s_dev;
        struct m_inode      * s_isup;
        struct m_inode      * s_imount;
        unsigned long       s_time;
        unsigned char       s_rd_only;
        unsigned char       s_dirt;
    };
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir dosya sistemi biraz karmaşık bir süreçle çekirdek tarafında kullanılabilir hale gelmektedir. Şimdi bu süreçlerin 
    üzerinde duracağız. 
    
    Süper blok nesneleri dosya sistemi mount edilirken oluşturulmaktadır. Mount işlemi bir dosya sisteminin dizin ağacındaki 
    bir dizine iliştirilmesi anlamına gelmektedir. mount işlemi tipik olarak komut satırında "mount" isimli komut ile 
    (tabii biz komut diyoruz ama aslında "mount" bir programdır) yapılmaktdır. Komutun yalın genel biçimi şöyledir:

    mount [-t <dosya_sisteminin_tür_ismi] <blok_aygıt_dosyası> <mount_noktası>

    Burada mount noktası dosya sisteminin kökünün monte edileceği dizini belirtmektedir. Örneğin:

    $ mount -t ext4 /dev/sbd1 /home/kaan

    Mount işlemi sırasında dosya sisteminin tür ismi hiç belirtilmeyebilir. Bu durumda tür otomatik belirlenmeye çalışılır. 
    Tabii dosya sisteminin türü otomatik olarak Belirlenemezse error olulmaktadır. "mount" programı aslında Linux sistemlerinde 
    "mount" isimli kütüphane fonksiyonunu çağırarak işlemini yapmaktadır. Bu fonksiyonun parametrik yapısı şöyledir:

     #include <sys/mount.h>

     int mount(const char *source, const char *target, const char *filesystemtype, unsigned long mountflags,
                 const void *_Nullable data);

     "mount" komutu ve mount fonksiyonu POSIX standartlarında bulunmamaktadır. Linux'ta mount kütüphane fonksiyonu sys_mount 
     sistem fonksiyonunu çağırmaktadır. Yani aslında mount kütüphane fonksiyonunun tek yaptığı şey mount sistem fonksiyonunu 
     çağırmaktır:

    ┌────────────────────────┐
    │    "mount" komutu      │
    │    (komut satırı)      │
    └───────────┬────────────┘
                │
                ▼
    ┌────────────────────────┐
    │  mount() kütüphane     │
    │  fonksiyonu (glibc)    │
    │  (kullanıcı modu)      │
    └───────────┬────────────┘
                │  Çekirdek moduna geçiş yapılıyor
                ▼
    ┌───────────────────────────────┐
    │ sys_mount() sistem fonksiyonu │
    │         (Çekirdek modu)       │
    └───────────────────────────────┘

    Mount edilmiş olan dosya sistemi komut satırında "umount" komutuyla unmount edilebilmektedir. Komutun yalın genel 
    biçimi şöyledir:

    umount <mount_noktası ya da blok_aygıt_dosyası>

    Örneğin:

    $ umount /home/kaan

    umount komutu umount ya da umount2 kütüphane fonksiyonlarını çağırarak işlemini yapmaktadır. umount fonksiyonunun 
    prototipi şöyledir:

    #include <sys/mount.h>

    int umount(const char *target);
    int umount2(const char *target, int flags);

    umount ve umount2 kütüphane fonksiyonları da sırasıyla sys_umount ve sys_umount2 sistem fonksiyonlarını çağırmaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    UNIX/Linux sistemlerinde disk gibi blok transferi yapan aygıtlara "blok aygıtları (block devices)" denilmektedir. 
    Blok aygıtları "blok aygıt sürücüleri (block device drivers)" tarafından yönetilmektedir. Öreğin blok aygıtı bir 
    diski belirtiyorsa biz bu blok aygıtını open fonksiyonuyla açıp oradan read fonksiyonuyla okuma yaptığımızda aslında 
    diskten okuma yapmış oluruz. Anımsayacağınız gibi blok aygıt sürücüleri Linux'un "sayfa önbelliği (page cache)" ile 
    ilişkili biçimde çalışmaktadır. Yani blok aygıtlarından okunan bilgiler önce sayfa önbelleiğine aktarılıp aygıt 
    okumaları minimize edilmeye çalışılmaktadır. Linux blok aygıtlarından yalnızca "sayfa önbelleğinde olmayan" sektörleri 
    talep etmektedir. 

    Bir blok aygıtının içerisinde bir dosya sistemi bulunmak zorunda değildir. Bir blok aygıtına dosya sistemi kurmak 
    için dosya sisteminin metadata bilgilerinin o blok aygıtında oluşturulması gerekir. Buna da genel olarak "formatlama"
    denilmektedir. Linux'ta formatlama işlemi için özel programlar bulundurulmuştur. Bu programlar "mkfs.xxx" biçiminde 
    isimlendirilmiştir. Örneğin ext2 formatlaması için "mkfs.ext2" programı, ext4 formatlaması için "mkfs.ext4" programı
    Microsoft'un fat dosya sistemi için ise "mkfs.fat" programı bulunmaktadır. Bu programlar blok aygıt dosyasını ya da 
    normal bir dosyayı parametre olarak alıp dosya sistemini içi boş bir biçimde oluturmaktadır. Örneğin:

    $ sudo mkfs.ext4 /dev/sda2
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir dosya sistemini çekirdeğin kullanabilmesi için onun çekirdeğe register ettilmesi gerekmektedir. Çekirdek register 
    ettirilen tüm dosya sistemlerini bir tek bağlı listede (single linked list) tutmaktadır. Mount işlemi de register 
    ettirilen bu bilgiler kullanılarak gerçekleştirilmektedir. 

    Çekirdekte register ettirilmiş olan tüm dosya sistemleri file_systems iisimli global bir tek bağlı listede tutulmaktadır:

    static struct file_system_type *file_systems;

    Bu bağlı listede file_system_type yapısı türünden nesneler bulunmaktadır. file_systems değişkeni ilk düğümün yerini 
    tutmaktadır. 

    file_systems ──▶ [file_system_type] ──▶ [file_system_type] ──▶ [file_system_type] ──▶ ... ──▶ [file_system_type] ──▶ NULL
                         nesnesi                 nesnesi                 nesnesi                        nesnesi

    Linux çekirdeğinde çok uzun süredir dosya sistemleri "fs/filesystem.c" dosyası içerisinde register_filesystem isimli 
    fonksiyonla register ettirilmektedir. Güncel çekirdeklerde bu fonksiyon şöyle yazılmıştır:

    int register_filesystem(struct file_system_type * fs)
    {
        int res = 0;
        struct file_system_type ** p;

        if (fs->parameters &&
            !fs_validate_description(fs->name, fs->parameters))
            return -EINVAL;

        BUG_ON(strchr(fs->name, '.'));
        if (fs->next)
            return -EBUSY;
        write_lock(&file_systems_lock);
        p = find_filesystem(fs->name, strlen(fs->name));
        if (*p)
            res = -EBUSY;
        else
            *p = fs;
        write_unlock(&file_systems_lock);
        return res;
    }

    EXPORT_SYMBOL(register_filesystem);

    Burada find_filesystem fonksiyonu eğer register ettirilmek istenen dosya sistemi bağlı listede yoksa eklemenin yapılacağı 
    son düğümün next elemanının adresiyle, register ettirilmek istenen dosya sistemi zaten bağlı listede varsa -BUSY errno 
    değeriyle geri dönmektedir. Bu fonksiyon da şöyle yazılmıştır:

    static struct file_system_type **find_filesystem(const char *name, unsigned len)
    {
        struct file_system_type **p;
        for (p = &file_systems; *p; p = &(*p)->next)
            if (strncmp((*p)->name, name, len) == 0 &&
                !(*p)->name[len])
                break;
        return p;
    }

    Fonksiyonun eğer ilgili düğüm bulunamazsa son düğümün next göstericisinin adresiyle bulunursa NULL adresle geri döndüğüne 
    dikkat ediniz. 

    register_filesystem fonksiyonu export edilmiş olduğu için çekirdek modülleri tarafından da çağrılabilmektedir. Dosya 
    sistemini bu fonksiyonla register ettirecek kişi önce file_system_type yapısı türünden statik ömürlü bir nesne oluşturup 
    onun adresini register_filesystem fonksiyonuna geçirmelidir. file_system_type yapısı güncel çekirdeklerde "include/fs.h" 
    dosyası içerisinde şöyle bildirilmiştir:

    struct file_system_type {
        const char *name;
        int fs_flags;
    #define FS_REQUIRES_DEV		    1 
    #define FS_BINARY_MOUNTDATA	    2
    #define FS_HAS_SUBTYPE		    4
    #define FS_USERNS_MOUNT		    8	        /* Can be mounted by userns root */
    #define FS_DISALLOW_NOTIFY_PERM	16	        /* Disable fanotify permission events */
    #define FS_ALLOW_IDMAP         32           /* FS has been updated to handle vfs idmappings. */
    #define FS_RENAME_DOES_D_MOVE	32768	    /* FS will handle d_move() during rename() internally. */
        int (*init_fs_context)(struct fs_context *);
        const struct fs_parameter_spec *parameters;
        struct dentry *(*mount) (struct file_system_type *, int,
                const char *, void *);
        void (*kill_sb) (struct super_block *);
        struct module *owner;
        struct file_system_type *next;
        struct hlist_head       fs_supers;

        struct lock_class_key   s_lock_key;
        struct lock_class_key   s_umount_key;
        struct lock_class_key   s_vfs_rename_key;
        struct lock_class_key s_writers_key[SB_FREEZE_LEVELS];

        struct lock_class_key   i_lock_key;
        struct lock_class_key   i_mutex_key;
        struct lock_class_key   invalidate_lock_key;
        struct lock_class_key   i_mutex_dir_key;
    };

    Programcı bu yapının tüm elemanlarını doldurmak zorunda değildir. Yalnızca gerekli birkaç elemanını doldurması 
    yeterlidir. Diğer elemanlar çekirdek tarafından gerektiğinde kullanılmaktadır. Eskiden bu yapı daha sade bir görümündeydi. 
    Örneğin 2.6 çekirdeklerinde şöyleydi:

    struct file_system_type {
        const   char *name;
        int     fs_flags;
        int     (*get_sb) (struct file_system_type *, int,
                    const char *, void *, struct vfsmount *);
        struct dentry *(*mount) (struct file_system_type *, int,
                const char *, void *);
        void    (*kill_sb) (struct super_block *);
        struct module *owner;
        struct file_system_type * next;
        struct list_head fs_supers;

        struct lock_class_key s_lock_key;
        struct lock_class_key s_umount_key;
        struct lock_class_key s_vfs_rename_key;

        struct lock_class_key i_lock_key;
        struct lock_class_key i_mutex_key;
        struct lock_class_key i_mutex_dir_key;
        struct lock_class_key i_alloc_sem_key;
    };

    2.4 ve 2.2 çekirdeklerinde de şöyleydi:

    struct file_system_type {
        const   char *name;
        int     fs_flags;
        struct super_block *(*read_super) (struct super_block *, void *, int);
        struct  module *owner;
        struct  file_system_type * next;
        struct  list_head fs_supers;
    };

    Linux'un ilkel 0.01 versiyonunda zaten dosya sisteminin register ettirilmesi diye bir kavram da yoktu.

    register_filesystem fonksiyonu ile register ettirilmiş olan dosya sistemi unregister_filesystem fonksiyonuyla 
    unregister ettirilebilir. Tipik olarak register_filesystem fonksiyonu çekirdek nodülünün init fonksiyonunda 
    unregister_filesystem fonksiyonu ise çekirdek modülünün exit fonksiyonunda çağrılmalıdır. unregister_filesystem 
    fonksiyonu file_system_type nesnesini bağlı listedne silmektedir. Güncel çekirdeklerde bu fonksiyon "fs/filesystem.c"
    dosyasında aşağıdaki gibi tanımlanmıştır:

    int unregister_filesystem(struct file_system_type * fs)
    {
        struct file_system_type ** tmp;

        write_lock(&file_systems_lock);
        tmp = &file_systems;
        while (*tmp) {
            if (fs == *tmp) {
                *tmp = fs->next;
                fs->next = NULL;
                write_unlock(&file_systems_lock);
                synchronize_rcu();
                return 0;
            }
            tmp = &(*tmp)->next;
        }
        write_unlock(&file_systems_lock);

        return -EINVAL;
    } 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Güncel Linux çekirdeklerinde file_system_type yapısının eğer yüksek seviyeli çekirdek fonksiyonlarından faydalanılacaksa 
    dosya sistemini yazanlar tarafından doldurması gereken elemanları şunlardır:

    static struct file_system_type myfs_type = {
        .owner = THIS_MODULE,
        .name = "myfs",
        .mount = myfs_mount,
        .kill_sb = myfs_kill_super,
    };

    Yapının owner elemanına THIS_MODULE değeri atanmalıdır. Her dosya sisteminin bir ismi vardır. Yapının name elemanı 
    bu ismi belirtmektedir. Dosya sistemi mount edilmek istendiğinde çekirdek file_systems bağlı listesinde arama 
    yaparak bizim yerleştirdiğimiz yapı nesnesini bulur ve onun mount elemanında belirtilen fonksiyonu çağırır. Yani 
    bu yapının mount elemanına "dosya sistemimiz mount edildiğinde çağrılacak fonksiyonu" yerleştirmeliyiz. Dosya 
    sistemi unmount edilirken eğer super_block nesnesinin referans sayacı 0'a düşerse kill_sb elemanında belirtilen  
    fonksiyon çağrılmaktadır. O halde dosya sistemini gerçekleştiren sistem programcısı yapının mount elemanına ve 
    kill_sb elemanına uygun parametrik yapılara sahip fonksiyonların adreslerini yerleştirmesi gerekir. Örneğimizde 
    yapının bu elemanlarına myfs_mount ve myfs_kill_super fonksiyonlarının adresleri yerleştirilmiştir. Yeni çekirdeklerde 
    bir süredir yapının mıunt elemanına alternatif olarak init_fs_context isimli bir eleman daha bulundurulmaktadır. 
    init_fs_context elemanı da bir fonksiyon göstericisidir ve mount işlemi sırasında çağrılmaktadır. init_fs_context 
    elemanı mount elemanına daha modern bir alternatiftir. Sistem programcısı yapınn ya mount elemanını ya da init_fs_context
    elemanını kullanır. Eğer yapının bu iki elemanı da set edilmişse çekirdek yalnızca mount işlemi sırasında init_fs_context  
    elemanında belirtilen fonksiyonu çağırmaktadır. Her ne kadar init_fs_context elemanı ile yeni bir alternatif oluşturulmuşsa 
    da mount elemanı "deprecated" yapılmamıştır. Yani eski mount elemanının kullanımı devam etmektedir ve bu eleman 
    init_fs_context elemanından daha kolay bir kullanım sunmaktadır.
        
    Peki bu fonksiyonların içinde sistem programcısı ne yapmalıdır?
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        35. Ders 22/11/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    file_system_type yapısının mount elemanına adresi yerleştirilecek fonksiyonun parametrik yapısı şöyle olmalıdır:

    static struct dentry *myfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)
    {
        /* ... */
    }

    Fonksiyonun birinci parametresi register ettirilmiş olan file_system_type nesnesinin adresini belirtmektedir. sys_mount 
    sistem fonksiyonu file_systems bağlı listesinde belirtilen isimdeki file_system_type nesnesini bularak onun adresini bu 
    fonksiyonun birinci parametresine geçirmektedir. Fonksiyonun diğer parametreleri sys_mount sistem fonksiyonundan gelen 
    parametrelerdir. Bir dosya sistemi mount edilirken dosya sisteminin içinde bulunduğu blok aygıtına ilişkin blok dosyasın 
    belirtildiğini anımsaynız. İşte bu blok aygıt dosyası fonksiyonun üçüncü parametresine, mount bayrakları ikinci parametresine
    ve mount edilecek sisteme ilişkin özel bilgiler de dördüncü parametresine geçirilmektedir. 
    
    file_system_type yapısının mount elemanına girilen fonksiyon aşağıdaki işlemleri yapmalıdır:

    1) Bir super_block nesnesi oluşturup bunun içini doldurmalıdır.
    2) Mount edilecek dosya sisteminin kök dizinine ilişkin inode ve dentry nesnelerini yaratıp bunları inode ve dentry 
    önbelleklerine yerleştirmelidir.
    3) Fonksiyon kök dizine ilişkin dentry nesnesinin adresiyle geri dönmelidir. 
        
    AslındaLinux çekirdeğinde mount edilecek dosya sistemi için super_block nesnesini oluşturan daha yüksek seviyeli fonksiyonlar 
    da bulunmaktadır. Bu amaçla kullanılan iki önemli çekirdek fonksiyonu mount_nodev ve mount_bdev isimli fonksiyonlarıdır. 
    Bu fonksiyonlar export edilmiştir. mount_nodev ve mount_bdev fonksiyonlarının prototipleri "include/linux/fs.h" dosyası 
    içerisinde şöyle bildirilmiştir:

    struct dentry *mount_nodev(struct file_system_type *fs_type, int flags, void *data,
            int (*fill_super)(struct super_block *, void *, int))
    
    struct dentry *mount_bdev(struct file_system_type *fs_type, int flags, const char *dev_name, void *data,
            int (*fill_super)(struct super_block *, void *, int))

    Bu fonksiyonların tanımlamaları "fs/super.c" dosyası içerisinde yapılmıştır. 

    mount_nodev fonksiyonu ramdisk gibi disk tabanlı olmayan dosya sistemlerinin gerçekleştirilmesinde, mount_bdev fonksiyonu
    ise disk tabanlı dosya sistemlerinin gerçekleştirilmesinde kullanılmaktadır. 

    Çekirdeğin en yeni versiyonlarında artık mount_bdev, mount_nodev gibi fonskiyonlar kaldırılmıştır. Artık mount 
    işlemi sırasında file_system_type yapısının init_fs_context elemanındaki fonksiyon çağrılmaktadır. (Geçmişe doğru 
    uyumu korumak için en yeni versiyonalarda da mount elemanı muhafaza edilmiştir. Ancak bu eleman NULL adres içeriyorsa 
    yapının init_fs_context elemanına başvurulmaktadır.) En yeni çekirdeklerde mount_bdev ve mount_nodev fonksiyonlarının 
    işlevlerini get_tree_bdev fonksiyonu almıştır. Mount sırasında file_system_type yapısının mount elemanı yerine 
    alternatif init_fs_context elemanının da kullanılmaya başlanması 2019 başlarında Linux 5.1'e ile başlamıştır. Bu yeni 
    init_fs_context elemanına context oluşturan bir fonksiyon girilmektedir. Bu fonksiyon içerisinde de context işlemlerine 
    yönelik fonksiyonlar set edilmektedir. Örneğin:

    static const struct fs_context_operations myfs_context_ops = {
        .get_tree = myfs_get_tree,
    };

    static int myfs_init_fs_context(struct fs_context *fc)
    {
        fc->ops = &myfs_context_ops;

        return 0;
    }

    static struct file_system_type myfs_type = {
        .owner = THIS_MODULE,
        .name = "myfs",
        .init_fs_context = myfs_fs_init_context,
        .kill_sb = myfs_kill_super,
    };

    static int myfs_get_tree(struct fs_context *fc)
    {
        return get_tree_bdev(fc, myfs_fill_super);
    }

    Burada mount işlemi sırasında artık myfs_fs_init_context fonksiyonundan hareketle myfs_context_ops fonksiyonu 
    çağrılacaktır. İşte bu fonksiyon içerisinde de get_tree_bdev fonksiyonu çağrılmıştır. Biz izleyen paragraflarda
    daha çok eski arayüz olan mount_bdev ve mount_nodev fonksiyonlarını temel alacağız. 

    Çekirdek süper blok nesnelerini file_system_type yapısının fs_super elemanındaki bağlı listede tutmaktadır. Yani 
    her  dosya sistemi türü için o dosya sistemi türünden disk bölümlerinin mount edilmesi sonucunda elde edilen süper 
    blok nesneleri o dosya sistemi türüne ilişkin file_system_type yapısının içeisinde tutulmkatdır:
    
    struct file_system_type {
        /* ... */
        
        struct hlist_head       fs_supers;
        
        /* ... */
    };

    Bu bağlı listenin düğümlerini super_block yapısının s_list elemanı oluşturmaktadır. 

    struct super_block {
        /* ... */
        
        struct list_head       s_list;        

        /* ... */
    };

    Ayrıca Linux çekirdeği tüm süper blok nesnelerini de "fs/super.c" dosyası içerisindeki super_blocks isimli bağlı 
    listede tutmaktadır:

    static LIST_HEAD(super_blocks);

    Böylece çekirdek gerektiğinde hem belli bir dosya sistemine ilişkin süper blok nesnelerine hem de sistemdeki tüm 
    süper blok nesnelerine bu bağlı listeler yoluyla erişebilmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    O halde disk tabanlı bir dosya sistemini çekirdeğin yüksek seviyeli mount_bdev fonksiyonunu kullanarak yazacak olan 
    sistem programcıları mount fonksiyonunun içerisinde doğrudan bu fonksiyonu çağırırlar. Örneğin:

    static struct dentry *myfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)
    {
         return mount_bdev(fs_type, flags, dev_name, data, myfs_fill_super);
    }

    Burada mount fonksiyonunun parametrelerinin mount_bdev fonksiyonuna doğrudan aktarıldığını görüyorsunuz. Ancak 
    mount_bdev fonksiyonunun son parametresi dosya sistemini gerçekleştiren programcı tarafından oluşturulup fonksiyona 
    verilmektedir. Bu son parametre bir callback fonksiyondur ve süper block nesnesinin içinin sistem programcısı tarafından 
    doldurulmasına olanak sağlamaktadır. mount_bdev fonksiyonu kabaca bizim için şunları yapmaktadır:

    1) dev_name parametresindeki aygıt dosyasını kullanarak blok aygıt sürücüsüne erişir.

    2) Daha önce bu disk bölümü için bir süper blok nesnesi oluşturulmuş mu diye kontrol eder ve oluşturulmamışsa süper 
    blok nesnesini oluşturur. 

    3) Oluşturduğu super_block nesnesinin s_bdev ve s_dev elemanlarına değerlerini atar. 

    4) Sistem programcısının son parametreye geçirdiği fill_super fonksiyonunu çağırarak oluşturduğu süper blok nesnesinin 
    içinin sistem programcısı tarafından doldurulmasını sağlar. 

    Yukarıda da belirttiğimiz gibi artık en yen çekirdeklerde mount_bdev fonksiyonu kaldırılmıştır. mount_bdev fonksiyonunun 
    işlevi get_tree_bdev fonksiyonu tarafından yapılmaktadır. Bu yeni arayüzün kullanımı biraz daha zordur. Çünkü burada
    iki aşamalı işlem yapılmaktadır. Önce file_system_type yapısının init_fs_context elemanına bir fonksiyon girilmekte
    o fonksiyonda context fonksiyonları set edilmektedir. Ancak eski arayüzü yeni biçime dönüştürmek zor değildir. 
    Yeniden bu arayüzdeki genel yapıyı anımsatmak istiyoruz:

    static const struct fs_context_operations myfs_context_ops = {
        .get_tree = myfs_get_tree,
    };

    static int myfs_init_fs_context(struct fs_context *fc)
    {
        fc->ops = &myfs_context_ops;

        return 0;
    }

    static struct file_system_type myfs_type = {
        .owner = THIS_MODULE,
        .name = "myfs",
        .init_fs_context = myfs_fs_init_context,
        .kill_sb = myfs_kill_super,
    };

    static int myfs_get_tree(struct fs_context *fc)
    {
        return get_tree_bdev(fc, myfs_fill_super);
    }

    mount_nodev fonksiyonu da benzer işlemleri yapmaktadır. Ancak bir blok aygıtı söz konusu olmadığı için bazı aşamalar
    farklı bir biçimde gerçekleştirilmektedir. mount_nodev fonksiyonunu da en yeni çekirdeklerde kaldırılmıştır. Bunun 
    yerine get_tree_nodev fonksiyonu lullanılmaktadır. 

    Sistem programcısının mount fonksiyonun mount_bdev ya da mount_nodev fonksiyonlarının geri dönüş değeriyle geri 
    döndürüldüğüne dikkat ediniz:

    static struct dentry *myfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)
    {
         return mount_bdev(fs_type, flags, dev_name, data, myfs_fill_super);
    }

    Peki o halde mount edilecek dosya sisteminin kök dizinine ilişkin inode ve dentry nesneleri nerede yaratılmaktadır?
    İşte aslında bu nesneleri sistem programcısı fill_super fonksiyonunun içerisinde yaratmaktadır. fill_super fonksiyonu 
    içerisinde minimal olarak sistem programcısı super_block yapısının aşağıdaki elemanlarını doldurmalıdır :

    - Yapının unsigned long türünden s_magic elemanı dosya sistemini belirten sihirli bir sayı ile doldurulmalıdır.

    - Yapının super_operations türüden s_ops elemanı süper blok işlemlerinde çağrılacak fonksiyonları içerecek biçimde 
    doldurulmalıdır. super_operations fonksiyon göstericilerinden oluşan büyük bir yapıdır. Ancak sistem programcısı bu 
    yapının hepsini değil yalnızca bazı elemanlarını doldurabilir. super_operations yapısı şöyle bildirilmiştir:

    struct super_operations {
        struct inode *(*alloc_inode)(struct super_block *sb);
        void (*destroy_inode)(struct inode *);

        void (*dirty_inode) (struct inode *);
        int (*write_inode) (struct inode *, int);
        void (*drop_inode) (struct inode *);
        void (*delete_inode) (struct inode *);
        void (*put_super) (struct super_block *);
        void (*write_super) (struct super_block *);
        int (*sync_fs)(struct super_block *sb, int wait);
        int (*freeze_fs) (struct super_block *);
        int (*unfreeze_fs) (struct super_block *);
        int (*statfs) (struct dentry *, struct kstatfs *);
        int (*remount_fs) (struct super_block *, int *, char *);
        void (*clear_inode) (struct inode *);
        void (*umount_begin) (struct super_block *);

        int (*show_options)(struct seq_file *, struct vfsmount *);
        int (*show_stats)(struct seq_file *, struct vfsmount *);
    #ifdef CONFIG_QUOTA
        ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t);
        ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
    #endif
        int (*bdev_try_to_free_page)(struct super_block*, struct page*, gfp_t);
    };

    - Yapının s_blocksize ve s_blocksize_bits elemanları sırasıyla dosya sistemindeki bir bloğun büyüklüğünü ve bu büyüklüğün 
    2 tabanına göre logaritma değerini tutacak biçimnde doldurulmalıdır. Aslında bu elemanların doldurulması için zaten 
    çekirdeğin içerisinde sb_set_blocksize isimli bir fonksiyon bulunmaktadır:

    int sb_set_blocksize(struct super_block *sb, int size); 

    - Yapının s_fs_info elemanı dosya sistemine özgü süper blok bilgilerini tutan yapı nesnesini gösterecek biçimde set 
    edilmelidir. Her dosya sisteminin o dosya sistemine özgü süper blok bilgileri de bulunmaktadır. Bu konuyu izleyen 
    paragraflarda açıklayacağız. 

    - Sistem programcının fill_super fonksiyonu içerisinde kök dizine ilişkin inode ve dentry nesnelerini yaratması 
    ve bu yaratımları yaptıktan sonra da super_block yaoısının s_root elemanına kök dizine ilişkin dentry nesnesinin 
    adresini yerleştirmesi gerekir. Güncel çekirdeklerin içerisinde bir inode nesnesi yaratılmışsa onu gösterecek bir 
    dentry nesnesinin yaratılmasını sağlayan d_make_root isimli export edilmiş bir çekirdek fonksiyonu da bulunmaktadır:

    struct dentry *d_make_root(struct inode *root_inode); 

    İşte aslında mount_bdev ve mount_nodev fonksiyonları fill_super fonksiyonu tarafından super_block yapısının s_root 
    elemanına yerleştirilen dentry nesnesinin adresiyle geri dönmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıdaki süreçler size biraz karşık gibi gözükebilir. Burada işlemlerin hangi adımlardan geçilerek gerçekleştirildiğine 
    ilişkin özet bir açıklama yapmak istiyoruz:

    sys_mount sistem fonksiyonu ---> file_system_type içeisindeki mount fonksiyonu çağrılır ---> mount_bdev çekirdek 
    fonksiyonu çağrılır ---> mount_bdev süper blok nesnesini yaratarak verilen fill_super fonksiyonunu çağırır ---> 
    fill_super fonksiyonu içerisinde kök dizine ilişkin inode ve dentry nesneleri yaratılır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıdaki aşamalarda yalnızca bir dosya sisteminin mount edilebilmesi için gerekli olan minimal işlemleri 
    açıkladık. Peki ya mount işleminden sonra mount edilen kök dizine geçilip burada dosya yaratılmak istense ya da 
    dizin yaratılmak istense yeni gerçekleştirilen dosya sistemi bu işlemleri nasıl yapabilecektir? İşte bu tür işlemler 
    hep çekirdeğin sistem programcısı tarafından yazılmış olan fonksiyonları çağrılarak yapılmaktadır. İzleyen paragraflarda 
    bu mekanizma üzerinde temel açıklamaları yapacapız. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'un dosya sistemine "sanal dosya sistemi (virtual file system)" denilmektedir. Bu isim dosya sğistemine nesne
    yönelimli prgramlama tekniğindeki soyutlamayı ve sanal fonksiyonları çağrıştırdığı için verilmiştir. Bilindiği gibi 
    nesne yönelimli programlama tekniğinde kodda değişebilecek olan öğeler belirlenir. Bu değişebilecek öğelere doğrudan 
    değil arayüz oluşturan sanal fonksiyonlar yoluyla erişilir. İşte Linux'un sanal dosya sisteminde de dosya sistemi 
    ile ilgili genel işlemler yapan kodlar çekirdek içerisinde bulunmaktadır. Çekirdek dosya sistemine göre değişebilecek 
    işlemler söz konusu olduğunda bu işlemleri kendisi yapmaz dosya sistemini gerçekleştirenler tarafından yazılmış olan f
    onksiyonları çağırarak onlara yaptırır. Tabii C Programlama Dili nesne yönelimli bir dil değildir. Bu nedenle C'de 
    sanal fonksiyon çağırma mekanizmasının fonksiyon göstericileri ile manuel bir biçimde oluşturulması gerekmektedir. 
    Bu durumda rogramcı kendi dosya sistemini yazarken kendi dosya sistemine ilişkin işlemleri yapan fonksiyonların 
    adreslerini çekirdeğin belirlediği birtakım yapı nesnelerinin içerisine yerleştirir. Çekirdek de dosya sistemine bağlı 
    işlemler sırasında bu fonksiyonları çağırır. 

    Şimdi siz "mademki Linux'un dosya sistemi nesne yönelimli teknikle örtüşüyor o halde neden dosya sistemini C yerine 
    C++ gibi nesne yönelimli bir dille yazmamışlar" diye merak edebilirsiniz. C++ C'den daha yüksek seviyeli bir dildir. 
    Dolayısıyla C++ derleyicisi tarafından üretilen kodlar C derleyicisi tarafından üretilen kodlar kadar etkin değildir. 
    İşte her ne kadar Linux'un dosya sistemi nesne yönelimli programlama tekniğini oldukça çağrıştırıyor olsa da hiçbir 
    zaman gerçekleştiriminin C++'ta yapılması düşünülmemiştir. C++ drleyicilerinin fonksiyon isimlerini değiştirmesi 
    (name mangling), C++'ta farklı parametrik yapılara ilişkin aynı isimli fonksiyonların bulunabilmesi, exception 
    mekanizması gibi pek çok özellik çekirdek kodlaması için bir handikap oluşturmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux'un sanal dosya sistemindeki sanal fonksiyon sistemi çekirdeğin daha önce sözünü ettiğimiz dört yapısına da 
    yansıtılmıştır. Çekirdeğin bu dört yapısında da xxx_operations türünden yapı elemanları bulunmaktadır. xxx_operations 
    yapıları fonksiyon göstericilerinden oluşmaktadır. Bu fonksiyon göstericilerine adresleri yerleştirilen fonksiyonlar 
    çekirdek tarafından belirli noktalarda çağrılmaktadır. Aşağıda xxx_operations elemanlarının bu yapılar içerisindei 
    görünümleri veriyoruz:

    struct super_block {
        /* ... */

        const struct super_operations	*s_op;

        /* ... */
    };

    struct inode {
        /* ... */

        const struct inode_operations	*i_op;

        /* ... */
    };

    struct dentry {
        /* ... */

        const struct dentry_operations *d_op;

        /* ... */
    };

    struct file {
        /* ... */

        const struct file_operations	*f_op;

        /* ... */
    };

    Görüldüğü gübi süper blok nesneleri için super_operations, dentry nesneleri için dentry_operations, inode nesneleri
    için inode_operations ve dosya nesneleri için de file_operations yapıları bulnmaktadır. Buradaki xxx_operations yapıları 
    yukarıda da belirttiğimiz gibi aslında fonksiyon göstericilerinden oluşmaktadır. Güncel çekirdeklerde super_operations 
    yapısı şöyle bildirilmiştir:

    struct super_operations {
        struct inode *(*alloc_inode)(struct super_block *sb);
        void (*destroy_inode)(struct inode *);
        void (*free_inode)(struct inode *);

        void (*dirty_inode) (struct inode *, int flags);
        int (*write_inode) (struct inode *, struct writeback_control *wbc);
        int (*drop_inode) (struct inode *);
        void (*evict_inode) (struct inode *);
        void (*put_super) (struct super_block *);
        int (*sync_fs)(struct super_block *sb, int wait);
        int (*freeze_super) (struct super_block *, enum freeze_holder who, const void *owner);
        int (*freeze_fs) (struct super_block *);
        int (*thaw_super) (struct super_block *, enum freeze_holder who, const void *owner);
        int (*unfreeze_fs) (struct super_block *);
        int (*statfs) (struct dentry *, struct kstatfs *);
        int (*remount_fs) (struct super_block *, int *, char *);
        void (*umount_begin) (struct super_block *);

        int (*show_options)(struct seq_file *, struct dentry *);
        int (*show_devname)(struct seq_file *, struct dentry *);
        int (*show_path)(struct seq_file *, struct dentry *);
        int (*show_stats)(struct seq_file *, struct dentry *);
    #ifdef CONFIG_QUOTA
        ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t);
        ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
        struct dquot __rcu **(*get_dquots)(struct inode *);
    #endif
        long (*nr_cached_objects)(struct super_block *,
                    struct shrink_control *);
        long (*free_cached_objects)(struct super_block *,
                        struct shrink_control *);
        void (*shutdown)(struct super_block *sb);
    };

    inode_operations yapısı şöyle bildirilmiştir:

    struct inode_operations {
        struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int);
        const char * (*get_link) (struct dentry *, struct inode *, struct delayed_call *);
        int (*permission) (struct mnt_idmap *, struct inode *, int);
        struct posix_acl * (*get_inode_acl)(struct inode *, int, bool);

        int (*readlink) (struct dentry *, char __user *,int);

        int (*create) (struct mnt_idmap *, struct inode *,struct dentry *,
                umode_t, bool);
        int (*link) (struct dentry *,struct inode *,struct dentry *);
        int (*unlink) (struct inode *,struct dentry *);
        int (*symlink) (struct mnt_idmap *, struct inode *,struct dentry *,
                const char *);
        struct dentry *(*mkdir) (struct mnt_idmap *, struct inode *,
                    struct dentry *, umode_t);
        int (*rmdir) (struct inode *,struct dentry *);
        int (*mknod) (struct mnt_idmap *, struct inode *,struct dentry *,
                umode_t,dev_t);
        int (*rename) (struct mnt_idmap *, struct inode *, struct dentry *,
                struct inode *, struct dentry *, unsigned int);
        int (*setattr) (struct mnt_idmap *, struct dentry *, struct iattr *);
        int (*getattr) (struct mnt_idmap *, const struct path *,
                struct kstat *, u32, unsigned int);
        ssize_t (*listxattr) (struct dentry *, char *, size_t);
        int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
                u64 len);
        int (*update_time)(struct inode *, int);
        int (*atomic_open)(struct inode *, struct dentry *,
                struct file *, unsigned open_flag,
                umode_t create_mode);
        int (*tmpfile) (struct mnt_idmap *, struct inode *,
                struct file *, umode_t);
        struct posix_acl *(*get_acl)(struct mnt_idmap *, struct dentry *,
                        int);
        int (*set_acl)(struct mnt_idmap *, struct dentry *,
                struct posix_acl *, int);
        int (*fileattr_set)(struct mnt_idmap *idmap,
                    struct dentry *dentry, struct fileattr *fa);
        int (*fileattr_get)(struct dentry *dentry, struct fileattr *fa);
        struct offset_ctx *(*get_offset_ctx)(struct inode *inode);
    } ____cacheline_aligned;

    dentry_operaions yapısı şöyle bildirilmiştir:

    struct dentry_operations {
        int (*d_revalidate)(struct inode *, const struct qstr *,
                    struct dentry *, unsigned int);
        int (*d_weak_revalidate)(struct dentry *, unsigned int);
        int (*d_hash)(const struct dentry *, struct qstr *);
        int (*d_compare)(const struct dentry *,
                unsigned int, const char *, const struct qstr *);
        int (*d_delete)(const struct dentry *);
        int (*d_init)(struct dentry *);
        void (*d_release)(struct dentry *);
        void (*d_prune)(struct dentry *);
        void (*d_iput)(struct dentry *, struct inode *);
        char *(*d_dname)(struct dentry *, char *, int);
        struct vfsmount *(*d_automount)(struct path *);
        int (*d_manage)(const struct path *, bool);
        struct dentry *(*d_real)(struct dentry *, enum d_real_type type);
        bool (*d_unalias_trylock)(const struct dentry *);
        void (*d_unalias_unlock)(const struct dentry *);
    } ____cacheline_aligned;
    
    file_operations yapısı ise şöyle bildirilmiştir:

    struct file_operations {
        struct module *owner;
        fop_flags_t fop_flags;
        loff_t (*llseek) (struct file *, loff_t, int);
        ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
        ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
        ssize_t (*read_iter) (struct kiocb *, struct iov_iter *);
        ssize_t (*write_iter) (struct kiocb *, struct iov_iter *);
        int (*iopoll)(struct kiocb *kiocb, struct io_comp_batch *,
                unsigned int flags);
        int (*iterate_shared) (struct file *, struct dir_context *);
        __poll_t (*poll) (struct file *, struct poll_table_struct *);
        long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
        long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
        int (*mmap) (struct file *, struct vm_area_struct *);
        int (*open) (struct inode *, struct file *);
        int (*flush) (struct file *, fl_owner_t id);
        int (*release) (struct inode *, struct file *);
        int (*fsync) (struct file *, loff_t, loff_t, int datasync);
        int (*fasync) (int, struct file *, int);
        int (*lock) (struct file *, int, struct file_lock *);
        unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
        int (*check_flags)(int);
        int (*flock) (struct file *, int, struct file_lock *);
        ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
        ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
        void (*splice_eof)(struct file *file);
        int (*setlease)(struct file *, int, struct file_lease **, void **);
        long (*fallocate)(struct file *file, int mode, loff_t offset,
                loff_t len);
        void (*show_fdinfo)(struct seq_file *m, struct file *f);
    #ifndef CONFIG_MMU
        unsigned (*mmap_capabilities)(struct file *);
    #endif
        ssize_t (*copy_file_range)(struct file *, loff_t, struct file *,
                loff_t, size_t, unsigned int);
        loff_t (*remap_file_range)(struct file *file_in, loff_t pos_in,
                    struct file *file_out, loff_t pos_out,
                    loff_t len, unsigned int remap_flags);
        int (*fadvise)(struct file *, loff_t, loff_t, int);
        int (*uring_cmd)(struct io_uring_cmd *ioucmd, unsigned int issue_flags);
        int (*uring_cmd_iopoll)(struct io_uring_cmd *, struct io_comp_batch *,
                    unsigned int poll_flags);
        int (*mmap_prepare)(struct vm_area_desc *);
    } __randomize_layout;

    Şimdi aklınıza "ben bir dosya sistemini gerçekleştireceksem bütün bu sanal fonksiyonları yazmak zorunda mıyım?" 
    sorusu gelebilir. İşte aslında dosya sistemlerini gerçekleştirenler bu fonksiyonların hepsini yazmak zorunda 
    değillerdir. Bazı fonksiyonlar için zaten çekirdekte bulunan hazır fonksiyonları kullanabilirler. Bazıarını 
    boş bırakabilirler. Ancak çekirdek boş bırakılan fonksğyonları kullanacak bir işlem yapmaya çalıştığında ilgili 
    sistem fonksiyonları başarısızlıkla geri dönmektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        36. Ders 23/11/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    super_block yapısı içerisindeki super_operations yapısı içerisinde bulunan fonksiyonlar dosya sistemi ile ilgili 
    işlemler yapılırken çağrılmaktadır. Örneğin ne zaman bir inode nesnsi tahsis edilecek olsa çekirdek bu işlem için 
    dosya sistemine ilişkin super_block nesnesinin super_operations yapısı içerisindeki alloc_inode fonksiyonu çağırmaktadır.
    Örneğin Linux çekirdeğindeki "fs/inode.c" içerisinde bulunan çekirdeğin inode tahsisatını yaptığı alloc_inode fonksiyonu 
    şöyle yazılmıştır:

    struct inode *alloc_inode(struct super_block *sb)
    {
        const struct super_operations *ops = sb->s_op;
        struct inode *inode;

        if (ops->alloc_inode)
            inode = ops->alloc_inode(sb);
        else
            inode = alloc_inode_sb(sb, inode_cachep, GFP_KERNEL);

        if (!inode)
            return NULL;

        if (unlikely(inode_init_always(sb, inode))) {
            if (ops->destroy_inode) {
                ops->destroy_inode(inode);
                if (!ops->free_inode)
                    return NULL;
            }
            inode->free_inode = ops->free_inode;
            i_callback(&inode->i_rcu);
            return NULL;
        }

        return inode;
    }

    Fonksiyondaki aşağıdaki kısma dikkat ediniz:

    if (ops->alloc_inode)
        inode = ops->alloc_inode(sb);
    else
        inode = alloc_inode_sb(sb, inode_cachep, GFP_KERNEL);

    Burada görüldüğü gibi önce super_operations içerisindeki alloc_inode fonksiyon göstericisinin NULL olup olmadığına
    bakılmış, eğer bu gösterici NULL değilse bu göstericinin gösterdiği fonksiyon çağrılmıştır. Bu gösterici NULL ise
    alloc_inode_sb isimli default bir fonksiyon çağrıldığını görüyorsunuz.

    Buradaki tasarmın nesne yönelimli programlama tekniğindeki çokbiçimliliğe (polymoprhism) çok benzediğine dikkat 
    ediniz.  Örneğin çokbiçimli davranış için C++'ta tipik olarak taban sınıfta bir sanal fonksiyon bulundurulur. Bu 
    fonksiyon türemiş sınıflarda override edilir. Eğer türemiş sınıflarda bu fonksiyon override edilmediyse taban 
    sınıftaki fonksiyon çağrılır. İşte Linux çekirdeğinde de adeta böyle yapılmıştır.

    Eğer Linux'un dosya sistemi C++'ta nesne yönelimli bir biçimde gerçekleştirilecek olsaydı super_block, inode, 
    dentry ve file yapıları birer sınıf haline getirilip bunlara ilişkin xxx_operations fonksiyonları da bu sınıfların 
    sanal fonksiyonları yapılırdı. Ancak daha önceden de belirttiğimiz gibi çekirdek kodlaması için C++ uygun bir dil
    olarak görülmemektedir.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi sıfırdan bir dosya sistemi oluştururken sistem programcısının adım adım ne yapması gerektiğini ve çekirdeğin 
    bu dosya sistemini nasıl işlettiğini açıklayalım:

    1) Sistem programcısı kendi dosya sistemi için file_system_type türünden bir yapı nesnesi oluşturur ve register_file_system 
    fonksiyonuyla bu yapı nesnesinin file_systems isimli global bağlı listeye eklenmesini sağlar. Sistem programcısı 
    kendi oluşturduğu bu file_system_type nesnesinin mount ve sb_kill elemanlarına kendi fonksiyonlarının adreslerini 
    yerleştirmektedir.

    2) Dosya sistemi mount edildiğinde sys_mount sistem fonksiyonu dosya sistemine ilişkin file_system_type nesnesini 
    file_systems bağlı listesinden bularak o nesnenin içerisindeki mount fonksiyonunu çağırmaktadır. Artık sistem 
    programcısının mount fonksiyonu çalıştırılacaktır. 

    3) Sistem programcısının yazdığı mount fonksiyonu tipik olarak çekirdeğin içerisindeki yüksek seviyeli mount_bdev
    fonksiyonunu çağırır. Bu çekirdek fonksiyonu super_block nesnesini yaratır, bunu ilgili bağlı listelere ekler,  
    ancak içini -birkaç eleman dışında- doldurmaz. super_block nesnesinin içinin doldurulması mount_bdev fonksiyonuna 
    geçirilen fill_super fonksiyonu tarafından yapılmaktadır. 

    4) Sistem rogramcısının yazdığı fill_super fonksiyonu kendi dosya sisteminden hareketle çeşitli bilgileri elde 
    ederek süper blok nesnesinin içini doldurur. Tabii sistem programcısının super_block nesnesinin içini doldurabilmesi 
    için diskteki kendi dosya sistemine ilişkin metadata bilgilerini okuması gerekir. Aslında sistem programcısı
    kendi dosya sistemine ilişkin metadata bilgilerine başka işlemlerde de gereksinim duymaktadır. Bu nedenle sistem 
    programcısı tipik olarak önce kendi dosya sisteminin metadata bilgilerini kendine özgü bir yapı nesnesine yerleştir. 
    Sonra da bu yapı nesnesini de süper blok nesnesine liştirir. Bu iliştirme işlemi için super_block yapısının s_fs_info 
    elemanı kullanılmaktadır:

    struct super_block {
        /* ... */

        void	*s_fs_info;	        /* Filesystem private info */

        /* ... */
    };

    Böylece super_block yapısı hem her dosya sisteminde söz konusu olabilecek genel bilgileri hem de sistem programcısının 
    oluşturduğu dosya sistemine ilişkin özel bilgileri tutar hale gelmektedir. 

    5) Sistem programcısı fill_super fonksiyonu içerisinde super_block nesnesinin içini doldururken aynı zamanda kendi
    dosya sisteminin kök dizinine ilişkin inode nesnesini ve dentry nesnesini de oluşturmak zorundadır. Sistem programcısının 
    kendi dosya sistemine ilişkin dentry nesnesinin adresini super_block yapısının s_root elemanına yerleştirmesi gerekmektedir. 
    Artık bu noktada çekirdek mount edilen dosya sisteminin kök dizinine ilişkin inode nesnesine ve denty nesnesine erişebilir 
    duruma gelmiştir. inode ve dentry yapılarının içerisinde inode_operations ve dentry_operations elemanlarının bulunduğunu 
    ve burada belirtilen fonksiyonların inode ve dentry işlemlerini yaptığını anımsayınız. 

    6) Şimdi örneğin kullanıcının sistem programcısının oluşturduğu mount edilen dizine geçip orada bir dizin yaratmak 
    istedğini düşünelim. Bu işlem aslında sys_mkdir sistem fonksiyonuyla yapılmaktadır. İşte bu sistem fonksiyonu çağrıldığında 
    güncel çeekirdeklerde bir dizi çağırma sonucunda çekirdekteki vfs_mkdir fonksiyonu çağrılmaktadır. Bu fonksiyonun 
    kodları aşağıdaki gibidir:

    struct dentry *vfs_mkdir(struct mnt_idmap *idmap, struct inode *dir,
			 struct dentry *dentry, umode_t mode)
    {
        int error;
        unsigned max_links = dir->i_sb->s_max_links;
        struct dentry *de;

        error = may_create(idmap, dir, dentry);
        if (error)
            goto err;

        error = -EPERM;
        if (!dir->i_op->mkdir)
            goto err;

        mode = vfs_prepare_mode(idmap, dir, mode, S_IRWXUGO | S_ISVTX, 0);
        error = security_inode_mkdir(dir, dentry, mode);
        if (error)
            goto err;

        error = -EMLINK;
        if (max_links && dir->i_nlink >= max_links)
            goto err;

        de = dir->i_op->mkdir(idmap, dir, dentry, mode);
        error = PTR_ERR(de);
        if (IS_ERR(de))
            goto err;
        if (de) {
            dput(dentry);
            dentry = de;
        }
        fsnotify_mkdir(dir, dentry);
        return dentry;

    err:
        dput(dentry);
        return ERR_PTR(error);
    }
    EXPORT_SYMBOL(vfs_mkdir);

    Burada dizinin içinde yaratılacağı dizine ilişkin (yani üst dizine ilişkin) inode nesnesinin inode_operations
    elemanındaki mkdir fonksiyonunun çağrıldığını görüyorsunuz:

    error = -EPERM;
    if (!dir->i_op->mkdir)
        goto err;

    /* ... */

    de = dir->i_op->mkdir(idmap, dir, dentry, mode);

    /* ... */

    err:
        dput(dentry);
        return ERR_PTR(error);

    Yani aslında dizinin yaratılma işlemi dosya sistemini gerçekleştiren sistem programcısı tarafından yazılan inode_operations 
    yapısındaki mkdir fonksiyonu tarafından gerçekleştirilmektedir. Burada inode_operations içerisindeki mkdir elemanında 
    NULL adres varsa (yani bu fonksiyon girilmemişse) fonksiyonun -EPERM errno değeri ile geri döndürüldüğüne dikkat ediniz. 
    Burada inode_operations içerisindeki mkdir fonksiyonunun geri dönüş değerine dikkat ediniz. Bu fonksiyon dizini yaratıp 
    dizinin inode ve dentry nesnelerini oluşturup yaratılan dizinin dentry nesnesiyle geri dönmelidir. 
    
    O halde dosya sistemini oluşturan sistem programcısı inode_operations nesnesini de oluşturup bu kök inode nesnesinin 
    i_op elemanına yerleştirmelidir. Tabii aslında her inode nesnesi için ayrı bir inode_operations nesnesinin oluşturulmasına
    gerek yoktur. Dosya sistemini gerçekleştiren sistem programcısı bundan bir tane oluşturur. Tüm inode nesnelerinin i_op 
    elemanı aslında aynı inode_operations nesnesini gösterir durumda olur. 

    Peki sistem programcısı tarafından gerçekleştirilen ve mount edilen dosya sisteminin kök dizininde bir dosya yaratılmak 
    istense ve bu dosya üzerinde işlemler yapılmak istense arka planda neler olacaktır? Biz bir dosya açıldığında çekirdeğin 
    neler yaptığını açıklamştık. Çekirdek önce bir dosya betimleyicisi tahsis edip sonra bir dosya nesnesi yaratıp onun 
    içerisini dolduruyordu. Peki dosya nesnesinin (yani struct file yapısının) f_op elemanındaki file_operations fonksiyonları 
    ne zaman devreye girmektedir? İşte çekirdek dosya işlemleri yapılırken bu file_operations yapısının içerisine yerleştirilen 
    fonksiyonlarını uygun yerlerde çağırmaktadır. (Tabii yukarıda da belirttiğimiz gibi aslında bu yapının da tüm elemanları 
    mutlak anlamda doldurulmak zorunda değildir.) Örneğin Linux'ta karakter aygıt sürücüleri file yapısının f_op elemanını 
    değiştirip dosya işlemleri sırasında kendi fonksiyonlarının çağrılmasını sağlamaktadır. Ancak dosya sistemleri 
    gerçekleştirilirken file_operations nesnesi sistem programcısı tarafından i_fop elemanına yerleştirilmekte ve 
    çekirdek tarafından buradan alınarak file yapısının f_op elemanına yerleştirilmektedir. Yani file_operations 
    yapısı içerisindeki fonksiyonları da yine dosya sistemini gerçekleştiren sistem programcısı tanımlamaktadır.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Linux çekirdeğinin kodlamasında tür ve uzunluk belirten bazı typedef tür isimleri kullanılmıştır. Çekirdeğin içsel 
    kodlarında beli uzunlukta tamsayı türleri "include/linux/types.h" dosyası içerisinde aşağıdaki isimlerle typedef 
    edilmiştir:

    typedef unsigned char      __u8;
    typedef signed char        __s8;
    typedef unsigned short     __u16;
    typedef signed short       __s16;
    typedef unsigned int       __u32;
    typedef signed int         __s32;
    typedef unsigned long long __u64;
    typedef signed long long   __s64;

    typedef __u8    u8;
    typedef __s8    s8;
    typedef __u16   u16;
    typedef __s16   s16;
    typedef __u32   u32;
    typedef __s32   s32;
    typedef __u64   u64;
    typedef __s64   s64;

    typedef unsigned long ulong;
    typedef unsigned int  uint;
    typedef unsigned short ushort;

    Endian biligisinin de vurgulandığı türler "include/uapi/linux/types.h" dosyası içerisinde aşağıdaki gibi typedef edilmiştir:

    typedef __u16 __bitwise __le16;
    typedef __u16 __bitwise __be16;
    typedef __u32 __bitwise __le32;
    typedef __u32 __bitwise __be32;
    typedef __u64 __bitwise __le64;
    typedef __u64 __bitwise __be64;

    Burada işaretsiz tamsayı türlerinin bulnmadığına dikkat ediniz. 

    C'ye C99 ile eklenen intN_t ve uintN_t tür isimleri (öreğin int32_t gibi uint32_t gibi tür isimleri) Linux çekirdek 
    kodlarında kullanılmamaktadır. Zaten bunların bildirimleri C'ye özgü <stdint.h> içerisindedir. Linux çekirdeğinde de
    C'nin herhangi bir standart başlık dosyası kullanılmamaktadır. Bu nedenle çekirdek kodlamaları yapılırken yukarıda 
    belirttiğimiz typedef türlerini tercih etmelisiniz.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi biz bazı konuları görmemiş olsak da yukarıda verdiğimiz bilgilerin teorik düzeyde kalmaması için basit bir 
    dosya sistemi tasarlayıp onu gerçekleştirmeye çalışalım. Aynı zamanda da Linux'un sanal dosya sistemine ilişkin 
    bazı önemli fonksiyonları da bu çalışma eşliğinde açıklayalım. Böyle bir örnek Linux'un sanal dosya sisteminin nasıl 
    çalıştığının büyük resim olarak anlaşılmasını sağlayacaktır. Ancak bir dosya sistemini gerçekleştirirken bizim bir 
    diske gereksinimiz olacaktır. Neyse ki Linux'ta bir dosyayının bir disk gibi kullanılmasını sağlayan ismine "loop" 
    denilen aygıt sürücüler bulunmaktadır. Bu "loop" aygıtları sayesinde bir dosyayı disk gibi kullanarak denemelerimizi 
    kolay bir biçimde yapabileceğiz. O halde önce bu "loop" aygıt sürücüsünün nasıl kullanıldığını açıklayalım. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    "loop" aygıt sürücülerine ilişkin aygıt dosyaları Linux'ta "/dev" dizinin altında bulunmaktadır:

    $ ls /dev/loop* -l
    brw-rw---- 1 root disk  7,   0 Kas 23 12:22 /dev/loop0
    brw-rw---- 1 root disk  7,   1 Kas 23 12:22 /dev/loop1
    brw-rw---- 1 root disk  7,   2 Kas 23 12:22 /dev/loop2
    brw-rw---- 1 root disk  7,   3 Kas 23 12:22 /dev/loop3
    brw-rw---- 1 root disk  7,   4 Kas 23 12:22 /dev/loop4
    brw-rw---- 1 root disk  7,   5 Kas 23 12:22 /dev/loop5
    brw-rw---- 1 root disk  7,   6 Kas 23 12:22 /dev/loop6
    brw-rw---- 1 root disk  7,   7 Kas 23 12:22 /dev/loop7
    crw-rw---- 1 root disk 10, 237 Kas 23 12:22 /dev/loop-control

    Görüldüğü buradaki sistemde majör numaraları aynı olan minör numaraları farklı olan 7 loop aygıtı bulunmaktadır. 
    Bir loop aygıtını kullanıma hazır hale getirmek için önce onun kullanacağı bir dosyanın oluşturulması gerekir. Linux'ta
    komut satırında içi 0'larla dolu olan bir dosya "dd (disk dump)" komutuyla oluşturulabilir. "dd" komutu aslında iki 
    dosyayı blok blok kopyalamaktadır. Komutta "if (input file)" komut satırı argümanı kaynak dosyayı, "of (output file)" 
    komut satırı argümanı ise hedef dosyayı belirtmektedir. Blok uzunluğu "bs (block size)" komut satırı argümanıyla 
    kopyalanacak blok sayısı ise "count" argümanıyla belirtilmektedir. "bs" argümanı kullanılmayabilir. Bu durumda default 
    blok büyüklüğü 512 alınmaktadır. Eğer "count" argümanı kullanılmazsa bu durumda tüm kaynak dosya kopyalanana kadar 
    işlemlere devam edilmektedir. Linux'ta "/dev/zero" aygıt sürücüsü "okunduğunda hep 0 byte'ını veren bir aygıt sürücüdür. 
    Bu bilgiler eşliğinde içi 0'larla dolu 10MB civarında bir dosya şöyle oluşturulabilir:

    $ dd if=/dev/zero of=mydisk.dat bs=512 count=20480

    Bu komutla elimizde içi sıfırlarla dolu 10MB'lık bir dosya elde etmiş olacağız.

    Dosyayı oluşturduktan sonra onun loop aygıtı tarafından disk gibi kullanılmasını sağlamalıyız. Bu işlem "losetup" 
    programıyla yapılmaktadır. losetup programı şöyle kullanılmaktadır:

    losetup <loop_aygıt_dosyası> <disk_olarak_kullanılacak_dosya>

    Örneğin:

    $ sudo losetup /dev/loop0 mydisk.dat

    "/dev/loop" aygıtına erişebilmek için programın sudo ile çalıştırılması gerekmektedir. 
    
    Artık elimizde "mydisk.dat" dosyasını kullanan "loop0" isimli bir blok aygıtı bulunmaktadır. Sistemdeki blok aygıtlarını 
    "lsblk" komutu ile görüntülediğimizde bu dosyayı da görmeliyiz:

    $ lsblk
    NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
    loop0    7:0    0    10M  0 loop 
    sda      8:0    0   120G  0 disk 
    ├─sda1   8:1    0     1M  0 part 
    ├─sda2   8:2    0   513M  0 part /boot/efi
    └─sda3   8:3    0 119,5G  0 part /
    sr0     11:0    1   2,8G  0 rom  /media/kaan/Linux Mint 22.1 Cinnamon 64-bit

    Artık "/dev/loop0" dosyasını bir disk gibi kullanabiliriz. Bu diske yazma yaptığımızda bu işlemden yalnızca bu 
    dosya etilenecektir. Örneğin biz bu diskimizi ext2 dosya sistemiyle formatlayalım:

    $ sudo mkfs.ext2 /dev/loop0
    mke2fs 1.47.0 (5-Feb-2023)
    Discarding device blocks: bitti                            
    Creating filesystem with 2560 4k blocks and 2560 inodes

    Allocating group tables: bitti                            
    Düğüm tabloları yazılıyor: bitti                            
    Süperblokların ve dosya sisteminin hesap bilgileri yazılıyor: bitti

    Şimdi de bu dosya sistemini mount edelim. Tabii bunun için önce mount noktası olarsak kullanılacak içi boş bir dizin 
    oluşturmamız gerekir. (Aslında mount noktası içi dolu bir dizin de olabilir. Bu durumda mount işleminden sonra artık
    o dizinin içeriğine unmount yapılana kadar erişilemez.):
    
    $ mkdir ext2-test
    
    mount işlemi şöyle yapılabilir:

    $ sudo mount /dev/loop0 ext2-test

    Artık biz "ext2-test" dizinine geçtiğimizde yeni bir kök dizine geçmiş oluruz. Burada mount sonrasında mount noktasına
    ilişkin dizinin (örneğimizdeki "ext2-test") sahibi ve grubu "root" olacaktır. Tabii isterseniz "chown" komutuyla 
    bunu değiştirebilirsiniz:

    $ sudo chown kaan:study ext2-test

    Eğer dizininin sahibini "root" olarak bırakırsanız bu dizinde giriş yaratmak için hep "sudo" komutunu kullanmak 
    zorunda kalırsınız.

    Peki bu işlemlerin hepsi nasıl geri alınacaktır? İşte geri alımları sırasıyla tertsen yapmak gerekir. Önce unmount 
    işlemi yapılmalıdır:

    $ sudo umount ext2-test

    Bundan sonra "loop" aygıtının dosyayla ilişkisinin kesilerek onun blok aygıtı durumundan çıkartılması gerekir. Bunun 
    için "losetup -d" komutu kullanılmaktadır:

    $ sudo losetup -d /dev/loop0

    Artık "lsblk" komutunda loop0 aygıtını görmememiz gerekir:

    $ lsblk
    NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
    sda      8:0    0   120G  0 disk 
    ├─sda1   8:1    0     1M  0 part 
    ├─sda2   8:2    0   513M  0 part /boot/efi
    └─sda3   8:3    0 119,5G  0 part /
    sr0     11:0    1   2,8G  0 rom  /media/kaan/Linux Mint 22.1 Cinnamon 64-bit

    Tabii burada oluşturduğumuz "mydisk.dat" dosyası kalmaya devam etmektedir. Biz o dosyayı yine "losetup" ile blok 
    aygıtı gibi kullanabiliriz. İşlemlerimize kaldığımız yerden devam edebiliriz:
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        37. Ders 29/11/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Sıfırdan bir dosya sistemi tasarlanacaksa öncelikle dosya sisteminin disk organizasyonu üzerinde belirlemelerin 
    yapılamsı gerekir. Her dosya sisteminin bir metadata alanı bir de data alanı vardır. Metadata alanında dosya 
    sistemine ilişkin parametrik bilgiler ve önemli bölümlerin bilgileri bulundurulur. Data alanı dosyaların içindeki 
    bilgilerin depolandığı alandır. Bugün kullandığımız dosya sistemleri evrim süreci içerisinde sürekli iyileştirilmiş 
    ve bugünkü durumlarına getirilmiştir. Dolayısıyla örneğin ext4 gibi bir dosya sisteminin ya da FAT32 gibi bir dosya 
    sisteminin gerçekleştirimini bir proje gibi yapmak gerekir. Yani bunun için belli bir süre düzenli belli bir zamanın,
    ayrılması gerekir. Biz burada oldukça basit bir dosya sistemini gerçekleştirmeye çalışacağız. Burada örneğini vereceğimiz 
    dosya sistemi tamamen bu kurs için örnek oluşturmak amacıyla tasarlanmış bir sistemdir. Yani dünyada böyle bir dosya 
    sistemi  mevcut değildir. Biz bu dosya sistemine "simplefs" ismini vereceğiz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    simplefs dosya sisteminin disk organizasyonu şöyledir:

    +------------------+
    |  Süper Blok      | (Blok 0)
    +------------------+
    | Inode Bitmap     | (Blok 1)
    +------------------+
    | Data Bitmap      | (Blok 2)
    +------------------+
    |  Inode Blok      | (Blok 3 ... N + 2)
    +------------------+
    |    Kök Dizin     | (Blok N+3 ... M) 
    |                  | 
    |  Data Blokları   | (Blok N+4 ... M)
    +------------------+
    
    Simplefs dosya sistemimizin ilk bloğu (yani ilk 4096 byte'ı) süper bloktur. Burada dosya sistemimize ilişkin önemli 
    parametrik bilgiler tutulmaktadır. Daha önce de belirttiğimiz gibi UNIX/Linux sistemlerinde her dosyanın bilgileri 
    diskte "inode blok (inode block)" denilen bir grup bloktaki inode elemanlarında tutulmaktadır. Daha önceden de belirttiğimiz 
    gibi "inode blok" aşağıdaki gibi inode elemanlarından oluşmaktadır:

    
           İnode Blok          
    ╠═══════════════════════╣
    ║ inode elemanı         ║  0
    ╟───────────────────────╢
    ║ inode elemanı         ║  1
    ╟───────────────────────╢
    ║ inode elemanı         ║  2
    ╟───────────────────────╢
    ║ inode elemanı         ║  3
    ╟───────────────────────╢
    ║    . . . .            ║
    ╟───────────────────────╢
    ║ inode elemanı         ║  n
    ╚═══════════════════════╝

    Her dosya ve dizin için dosya sisteminin diskte bir inode elemanını tahsis etmesi gerekir. Boş bir inode elemanının 
    belirlenebilmesi için inode tabanlı dosya sistemlerinde genellikle bir "inode bitmap" kullanılmaktadır. Bu "inode 
    bitmap" içerisindeki her bit ilgili inode elemanının boş mu dolu olduğunu göstermektedir. Dosya sistemimizde inode 
    bitmap bir blok yer kapladığına göre toplamda 4096 * 8 = 32768 inode elemanın durumunu tutabilmektedir. 
    
    Inode tabanlı dosya sistemlerinde "data blok" içerisindeki blokların da "boş mu dolu mu olduğu bilgisi" benzer biçimde 
    bir "data bitmap" ile tutulmaktadır. Bu "data bitmap" içersindeki her bit diskteki data bloğunun boş mu dolumu olduğu 
    bilgisini tutmaktadır. 

    Pek çok dosya sisteminin disk organizasyonunda dizinler de birer dosyaymış gibi ele alınmaktadır. Dolayısıyla her dizin
    data blokğunda bir blokyer kaplamaktadır. Genellikle kök dizin belli bir yerde bulundurulur. Kök dizinin yeri de süper 
    blokta belirtilir. Tabii kök dizin için en uygun yer data bloklarının ilk bloğudur. 

    Simplefs dosya sistemimizde her dosya ve dizin en fazla bir blok (yani default olarak 4096 byte) uzunluğunda olabilmektedir. 
    Bu kısıtı koyduğumuz zaman artık bizim dosyanın bloklarının yerlerini tutmamıza gerek kalmaz. simplefs dosya sistemimizdeki 
    inode elemanlarının sayısı formatlama sırasında belirlenebilmektedir. Ancak "data bitmap" ve "inode bitmap" 4096 * 8 = 32768 
    bitten oluştuğuna göre dosya sistemi de en fazla 32768 * 4096 = 134MB büyüklükte bir diski desteklemektedir. 
    
    Diskimizin süper blok bilgileri aşağıdaki gibidir:
    
    struct simplefs_disk_super_block {
        __le32 magic;              /* 0x464D4953 ("SIMF") */
        __le32 block_size;         /* 4096 */
        __le32 inode_count;        /* Toplam inode */
        __le32 block_count;        /* Toplam blok */
        __le32 free_inodes;        /* Serbest inode */
        __le32 free_blocks;        /* Serbest blok */
        __le32 inode_table_block;  /* İnode table başlangıcı (3) */
        __le32 inode_table_size;   /* İnode table boyutu */
        __le32 data_block_start;   /* Data block başlangıcı */
        __u8   padding[4060];      /* 4096'ya tamamlama */
    };

    Buradaki __le32 türü "little endian 4 byte'lık tamsayı türünü temsil etmektedir". Buradaki "little endian" belirlemesi ile
    derleyici özel bir işlem uygulamaz. Yalnızca dosya sistemini gerçekleştirenler için okunabilirliği artırmaktadır. Yani 
    burada söylenmek istenen şey "makineniz big endian olsa bile bu bilgiler diskte little endian biçiminde tutulmaktadır" 
    bilgisidir. 

    Her dosya sisteminde bir "sihirli sayı (magic number") bulundurulur. Bizim simplefs dosya sistemimizdeki sihirli 
    sayı süper blokta 0x53494D46 ("SIMF") biçiminde tutulmaktadır. Süper blok içerisindeki block_size elemanı her zaman
    4096 biçimindedir. İleride bu dosya sistemini farklı blok uzunluklarıyla da çalışabilir hale getirebilirsiniz. Ancak
    bizim dosya sistemimizde bir blok her zaman 4096 byte'tır. Süper bloktaki inode_count elemanı inode elemanlarının 
    toplam sayısını belirtmektedir. Disk bölümü içerisinde toplamda buradaki sayıdan daha fazla dosya ve dizin bulunamaz. 
    Çünkü her dosya ve dizin için bir inode elemanı kulanımaktadır. block_count alanında ise diskteki tüm blokların 
    sayısı tutulmaktadır. Bu bloklara metadata blokları da dahildir. free_inodes ve free blocks elemanları kulanılmayan 
    inode elemanlarının ve blokların sayısını belirtmektedir. Inode tablosunun yeri inode_table_block alanında tutulmaktadır. 
    Dosya sistemimizde burada her zaman 3 değeri bulunacaktır. Ancak siz bu dosya sistemini iyileştirmek isterseniz geleceğe 
    uyum için bu alanı bulundurmaktayız. Inode elemanlarının sayısı formatlama sırasında belirtilmektedir. Dolayısıyla inode 
    bloktaki blok sayısı da değişebilmektedir. Inode bloğun bir blok uzunlukta olmayabileceğine dikkat ediniz. data_block_start
    alanında data bloğun başlangıç blok numarası bulunmaktadır. Data bloğun ilk bloğunda kök dizinin içeriğinin bulunduğunu 
    anınsayınız. Bu alanların anlamlarını aşağıda özet bir tablo halinde de vermek istiyoruz:

    ┌───────────────────────┬────────┬───────────────────────────────────────────────┐
    │ Alan                  │ Tür    │ Açıklama                                      │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ magic                 │ le32   │ Sihirli sayı                                  │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ block_size            │ le32   │ Her zaman 4096                                │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ inode_count           │ le32   │ Format sırasında belirlenir                   │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ block_count           │ le32   │ Device size / 4096                            │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ free_inodes           │ le32   │ Dinamik güncellenir                           │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ free_blocks           │ le32   │ Dinamik güncellenir                           │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ inode_table_block     │ le32   │ Her zaman 3                                   │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ inode_table_size      │ le32   │ (inode_count + 63) / 64  (round up)           │
    ├───────────────────────┼────────┼───────────────────────────────────────────────┤
    │ data_block_start      │ le32   │ 3 + inode_table_size + 1                      │
    └───────────────────────┴────────┴───────────────────────────────────────────────┘

    izleyen paragraflarda da göreceğimiz gibi bir inode elemanı inode blokta 64 byte yer kaplamaktadır. Dolayısıyla 
    bir blokta 64 inode elemanı bulunmaktadır. Yukarıdaki tabloda inode_table_size açıklamasında nde elemanlarının 
    sayısı 64'e doğru yukarı yuvarlanmıştır. 

    Disk simplfs dosya sistemi ile formatlanırken inode sayısı da formatlama sırasında belirtilmektedir. Örneğin:

    $ ./mkfs.simplfs /dev/loop0 512
 ----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Inode bloğun inode elemanlarından oluştuğunu ve simplefs dosya sistemimizde bu bloğun değişken uzunlukta olabileceğini 
    belirtmiştik. Simplfs dosya sisteminde diskteki bir inode elemanın formatı aşağıdaki gibidir:

    struct simplefs_disk_inode {
        __le32 mode;            /* File type + permissions */
        __le32 uid;             /* Owner user ID */
        __le32 gid;             /* Owner group ID */
        __le32 size;            /* File size in bytes */
        __le32 nlink;           /* Hard link count */
        __le32 blocks;          /* Block count (0 or 1) */
        __le32 block_no;        /* Data block number */
        __le32 ctime;           /* Creation time */
        __le32 mtime;           /* Modification time */
        __le32 atime;           /* Access time */
        __u8   padding[24];     /* Padding to 64 bytes */
    };

    inode bloğun mode alanında dosyanın tür bilgisi ve erişim hakları turulmaktadır. Burada Linux'un uyguladığı bitsel
    organizasyonu kullanacağız. Bu orgainzasyon şömyledir:

       31 ... 12    11 10  9     8    7    6    5    4    3    2    1    0
      ┌──────────┬─────────────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
      │ reserved │ file type   │UID │GID │STK │ UR │ UW │ UX │ GR │ GW │ GX │
      └──────────┴─────────────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴

    UID = setuid
    GID = setgid
    STK = sticky bit
    UR  = user read
    UW  = user write
    UX  = user execute
    GR  = group read
    GW  = group write
    GX  = group execute

    inode elemanının uid ve gid elemanları dosyaya ilişkin kullanıcı ve grup id'lerini belirtmektedir. size alanı 
    dosyanın uzunluğunu nlink alanı hard link sayacını belirtmektedir. Dosyalar simplfs sistemimizde bir blok yer 
    kapladığı için yalnızca tek bir bloğun yeri tutulacaktır. inode elemanının block_no alanı dosyanın bulunduğu 
    bloğu belirtmektedir. inode elemanının ctime, mtime ve atime alanları sırasıyla inode bilgilerinin son güncelleme 
    zamanını, dosyanın son değiştirilme zamanını ve dosyanın sn okuma zamanını belirtmektedir. Buradaki zamanlar
    01/01/1970'ten geçen saniye sayısı biçiminde tutulmaktadır. Biz burada her ne kadar "dosya" terimini kullanıdysak 
    da dizileri de kastetmekteyiz. Çünkü dizinlerin de inode elemanları vardır 

    Inode tabanlı dosya sistemlerinde inode bloktaki ilk inode elemanı "reserved" bırakılmaktadır. Bu inode elemanının 
    inode numarası 0'dır. (Eski sistemlerde 0 inode elemanı değeri "başarısızlık" anlamında kullanbiliyordu) Linux'un 
    ext dosya sistemlerinde ilk 2 inode elemanı "reserved" yapılmıştır. Biz simpls dosya sistemimizde yalnızca 0 numaralı
    inode elemanıno "reserved" yapacağız. Bizim dosya sistemimizde kök dizinin bilgileri her zaman 1 numaralı inode 
    elemanında bulunacaktır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Simplefs dosya sistemimizdeki her dizin girişi eşit uzunluktadır. (ext dosya sistemlerinde bunların eşit uzunlukta 
    olmadığını anımsayınız.) Simpls dosya sistemimizdeki dizin girişlerinin formatı şöyledir:

    #define SIMPLEFS_FILENAME_MAXLEN        32

    struct simplefs_disk_dir_entry {
        __le32 inode;                           /* İnode number */
        char name[SIMPLEFS_FILENAME_MAXLEN];    /* File name */
    };

    Dizin girişinin ilk alanında ilgili dosyann inode numarası bulunmaktadır. Dosya ismi her zaman 32 byte yer 
    kaplamaktadır. Dosya isminin sonunda null karakter vardır. Bi,z genel olarak dizin girişlerindeki isimlerin sonuna 
    null padding uygulayacağız. Bu durumda dosya isimleri en fazla 31 karakter olabilmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Bir dosya sistemini gerçekleştirirken ilk yapacağımız işlemlerden biri "formatlama programının" yazılmasıdır. 
    Formatlama programı disk bölümünü (yani blok aygıtını) metadata alanlarını oluşturarak kullanıma hazır hale getirmektedir. 
    Dosya sistemi aygıt sürücümüz bu metada alanlarını kullanacaktır. Biz formatlama programımıza "mkfs.simplefs" ismini 
    vereceğiz. 
    
    Simplefs dosya sisteminin formatlanması sırasında şu işlemler yaoılmalıaıdr:

    1) Süper blok bilgileri oluşturulup blok aygıtının ilk bloğuna yazılmalıdır. 
    
    2) Inode bitmap'ta bizim 0 ve 1 numaralı inode elemanlarınının bitlerini 1 yapmamız gerekir. Anımsayacağınız gibi 
    0 nunaralı inode elemanı "reserved" durumdaydı, 1 numaralı inode elemanı ise kök dizine ilişkindi.

    3) Data bitmap'in de ilk duruma getirilmesi gerekir. 0 numaralı data bloğu kök dizini içerdiği için onun bitinin 1
    yapılması gerekir. 

    4) Inode bloğun da ilk durumuna getirilmesi gerekir. Inode bloğun ilk inode elemanı 0'lar içerecek biçimde boş 
    bırakılmalıdır. Ancak sonraki inode elemanı (yani 1 numaralı inode elemanı) kök dizin bilgilerini tutacak biçimde
    güncellenmelidir.)

    5) Sıra kök dizindeki dizin girişlerinin başlangıçtaki durumunu oluşturmaya gelmiştir. Kök dizinde "." ve ".." isimli
    iki dizin girişinin bulundurulması gerekir. Bu dizin girişlerinin inode nuaralarının yine kök dizinin inode numarasını
    (örneğimizde 1) içermesi gerekmektedir. 

    Format programımızda önce komut satırı argümanları kontrol edilmiştir:

    int n_inodes = DEF_NUMBER_OF_INODES;
    int fd;

    if (argc > 3) {
        fprintf(stderr, "too many arguments!\n");
        fprintf(stderr, "usage: mkfs.simplefs <devce_path> [number_of_inodes]\n");
        exit(EXIT_FAILURE);
    }
    if (argc == 1) {
        fprintf(stderr, "too few arguments!\n");
        fprintf(stderr, "usage: mkfs.simplefs <devce_path> [number_of_inodes]\n");
        exit(EXIT_FAILURE);
    }

    if (argc == 3) {
        n_inodes = atoi(argv[2]);
        if (n_inodes < MIN_NUMBER_OF_INODES || n_inodes > MAX_NUMBER_OF_INODES) {
            fprintf(stderr, "incorrect number of inodes!. Nuber of indodes must be bewtween 50 and 32768...\n");
            exit(EXIT_FAILURE);
        }
    }

    Sonra aygıt dosyası open fonksiyonuyla açılmıştır:

    if ((fd = open(argv[1], O_WRONLY)) == -1)
        exit_sys(argv[1]);

    Süper bloğun yazılması write_super_block fonksiyonu tarafından yapılmaktadır:

    int write_super_block(int fd, int n_inodes, struct simplefs_disk_super_block *sbd)
    {
        uint64_t size;

        sbd->magic = SIMPLEFS_MAGIC;
        sbd->block_size = SIMPLEFS_BLOCK_SIZE;
        sbd->inode_count = n_inodes;
        /*
        if (ioctl(fd, BLKGETSIZE64, &size) == -1)
            return -1;
        */
        if ((size = lseek(fd, 0, SEEK_END)) == -1)
            return -1;

        sbd->block_count = size / SIMPLEFS_BLOCK_SIZE;
        sbd->free_inodes = n_inodes - 2;
        sbd->inode_table_block = 3;
        sbd->inode_table_size = (n_inodes + INODE_SIZE - 1) / INODE_SIZE;
        sbd->data_block_start = 3 + sbd->inode_table_size;
        sbd->free_blocks = sbd->block_count - sbd->data_block_start - 1;

        lseek(fd, 0, SEEK_SET);
        if (write(fd, sbd, sizeof(*sbd)) != (ssize_t)sizeof(*sbd)) 
            return -1;

        return 0;
    }

    Burada diskin uzunluğunu bulmak için dosya göstericisini sona çekip onun konumunu aldık. Aslında aynı işlem blok 
    aygıt sürücülerine BLKGETSIZE64 ioctl komutu gönderilerek de yapılabilmektedir. write_super_block fonksiyonunun 
    ikinci parametresinin inode elemanlarının sayısını belirttiğine dikkat ediniz. 

    Süper bloğu oluşturduktan sonra inode bitmap ve data bitmap blokları oluşturulmalıdır. Inode bitmap içeisindeki 
    her bit bir inode elemanının dolu mu boş mu olduğunu tutmaktadır. Başlangıçta ilk iki inde elemanı doludur. Anımsayacağınız
    gibi zaten 0'ıncı inode elemanı pek çok inode tabanlı dosya sisteminde hiç kullanılmamaktadır. Biim dosya sistemimizde
    kök dizinin bilgileri 1 numaralı inode elemanındadır. Inode bitmap'in ilk iki bitini 1'leyip geri kalan bitlerini 
    0'lamak için en pratik yöntem önce içi sıfırlarla dolu bir dizi almak sonra dizinin ilk elemanının düşük anlamlı iki 
    bitini 0'lamaktır. Bu işlem formatlama programımızda şöyle yapılmıştır:

    unsigned char bitmap[SIMPLEFS_BLOCK_SIZE] = {0};
    /* ... */

    bitmap[0] = 0x03;           /* first two bits in inode bitmap must be 1 */
    if (write(fd, bitmap, SIMPLEFS_BLOCK_SIZE) != SIMPLEFS_BLOCK_SIZE) {
        fprintf(stderr, "cannot write inode bitmap!..\n");
        exit(EXIT_FAILURE);
    }

    Data bitmap'in yalnızca ilk biti 1 olmalıdır. Çünkü diskin data bloğunda yalnızca ilk blok (orada kök dizinin olduğunu 
    anımsayınız) doludur. Bu işlem de fırmat programımızda şöyle yapılmıştır:

    bitmap[0] = 0x01;       /* first bit in data bitmap must be 1 */
    if (write(fd, bitmap, SIMPLEFS_BLOCK_SIZE) != SIMPLEFS_BLOCK_SIZE) {
        fprintf(stderr, "cannot write data bitmap!..\n");
        exit(EXIT_FAILURE);
    }   

    Sıra inode tablosunun yazılmasına gelmiştir. inode tablosundaki ilk inode elemanının boş olması gerektiğini anımsayınız. 
    Sonraki inode elemanı (yani 1 numaralı inode elemanı) kök dizinine ilişkin inode elemanı olmalıdır. Format programımızda 
    inode tablosu write_inode_table isimli bir fonksiyonla ilk haline getirilmiştir:

    int write_inode_table(int fd, struct simplefs_disk_super_block *sbd)
    {
        unsigned char buf[SIMPLEFS_BLOCK_SIZE] =  {0};
        struct simplefs_disk_inode inoded =  {0};
        time_t curtime;
        off_t pos;

        pos = lseek(fd, 0, SEEK_CUR);
        for (int i = 0; i < sbd->inode_table_size; ++i)
            if (write(fd, buf, SIMPLEFS_BLOCK_SIZE) != SIMPLEFS_BLOCK_SIZE)
                return -1;
        
        lseek(fd, pos, SEEK_SET);
        if (write(fd, &inoded, sizeof(inoded)) != (ssize_t)sizeof(inoded))
            return -1;

        inoded.mode = S_IFDIR | S_IRWXU | S_IRWXG | S_IRWXO;
        inoded.uid = 0;
        inoded.gid = 0;
        inoded.size = SIMPLEFS_BLOCK_SIZE;
        inoded.nlink = 3;
        inoded.blocks = 1;
        inoded.block_no = sbd->data_block_start;

        curtime = time(NULL);
        inoded.ctime = curtime;
        inoded.mtime = curtime;
        inoded.atime = curtime;

        if (write(fd, &inoded, sizeof(inoded)) != (ssize_t)sizeof(inoded))
            return -1;

        return 0;
    }

    Artık son olarak kök dizinin girişlerinin oluşturulmaıs gerekir. Kök dizinin ilk girişinin "." ve ".." dizinlerinden
    oluşması zorunludur. Bu dizin girişlerini kök dizine ilişkin bloğun ilk iki girişine yazmak için formatlama 
    programımızda için write_dentries fonksiyonu kullanılmıştır:

    int write_dentries(int fd, struct simplefs_disk_super_block *sbd)
    {
        struct simplefs_disk_dir_entry de = {0};

        lseek(fd, sbd->data_block_start * SIMPLEFS_BLOCK_SIZE, SEEK_SET);
        de.inode = 2;
        strcpy(de.name,  ".");
        if (write(fd, &de, SIMPLEFS_DENTRY_LEN) != SIMPLEFS_DENTRY_LEN)
            return -1;

        de.inode = 2;
        strcpy(de.name,  "..");
        if (write(fd, &de, SIMPLEFS_DENTRY_LEN) != SIMPLEFS_DENTRY_LEN)
            return -1;

        return 0;
    }

    Formatlama programımızın tamamı "mkfs.simplefs.c" ismiylebir bütün olarak aşağıda verilmiştir. 
----------------------------------------------------------------------------------------------------------------------*/

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <time.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include <linux/fs.h>
#include <linux/types.h>

#define SIMPLEFS_BLOCK_SIZE         4096
#define DEF_NUMBER_OF_INODES        1024
#define MAX_NUMBER_OF_INODES        (4096 * 8)
#define MIN_NUMBER_OF_INODES        50
#define INODE_SIZE                  64
#define SIMPLEFS_FILENAME_MAXLEN    32
#define SIMPLEFS_DENTRY_LEN         36
#define SIMPLEFS_MAGIC           0x53494D46

struct simplefs_disk_super_block {
    __le32 magic;              /* 0x464D4953 ("SIMF") */
    __le32 block_size;         /* 4096 */
    __le32 inode_count;        /* Toplam inode */
    __le32 block_count;        /* Toplam blok */
    __le32 free_inodes;        /* Serbest inode */
    __le32 free_blocks;        /* Serbest blok */
    __le32 inode_table_block;  /* İnode table başlangıcı (3) */
    __le32 inode_table_size;   /* İnode table boyutu */
    __le32 data_block_start;   /* Data block başlangıcı */
    __u8   padding[4060];      /* 4096'ya tamamlama */
};

struct simplefs_disk_inode {
    __le32 mode;            /* File type + permissions */
    __le32 uid;             /* Owner user ID */
    __le32 gid;             /* Owner group ID */
    __le32 size;            /* File size in bytes */
    __le32 nlink;           /* Hard link count */
    __le32 blocks;          /* Block count (0 or 1) */
    __le32 block_no;        /* Data block number */
    __le32 ctime;           /* Creation time */
    __le32 mtime;           /* Modification time */
    __le32 atime;           /* Access time */
    __u8   padding[24];     /* Padding to 64 bytes */
};

struct simplefs_disk_dir_entry {
    __le32 inode;                           /* İnode number */
    char name[SIMPLEFS_FILENAME_MAXLEN];    /* File name */
};

int write_super_block(int fd, int n_inodes, struct simplefs_disk_super_block *sbd);
int write_inode_table(int fd, struct simplefs_disk_super_block *sbd);
int write_dentries(int fd, struct simplefs_disk_super_block *sbd);
void exit_sys(const char *msg);

/* usage: mkfs.simplefs <devce_path> [number_of_inodes] */

int main(int argc, char *argv[])
{
    int n_inodes = DEF_NUMBER_OF_INODES;
    int fd;
    unsigned char bitmap[SIMPLEFS_BLOCK_SIZE] = {0};
    struct simplefs_disk_super_block sbd = {0};

    if (argc > 3) {
        fprintf(stderr, "too many arguments!\n");
        fprintf(stderr, "usage: mkfs.simplefs <devce_path> [number_of_inodes]\n");
        exit(EXIT_FAILURE);
    }
    if (argc == 1) {
        fprintf(stderr, "too few arguments!\n");
        fprintf(stderr, "usage: mkfs.simplefs <devce_path> [number_of_inodes]\n");
        exit(EXIT_FAILURE);
    }

    if (argc == 3) {
        n_inodes = atoi(argv[2]);
        if (n_inodes < MIN_NUMBER_OF_INODES || n_inodes > MAX_NUMBER_OF_INODES) {
            fprintf(stderr, "incorrect number of inodes!. Nuber of indodes must be bewtween 50 and 32768...\n");
            exit(EXIT_FAILURE);
        }
    }

    if ((fd = open(argv[1], O_WRONLY)) == -1)
        exit_sys(argv[1]);

    if (write_super_block(fd, n_inodes, &sbd) == -1)
        exit_sys("write_super_block");

    bitmap[0] = 0x03;       /* first two bits in inode bitmap must be 1 */
    if (write(fd, bitmap, SIMPLEFS_BLOCK_SIZE) != SIMPLEFS_BLOCK_SIZE) {
        fprintf(stderr, "cannot write inode bitmap!..\n");
        exit(EXIT_FAILURE);
    }

    bitmap[0] = 0x01;       /* first bit in data bitmap must be 1 */
    if (write(fd, bitmap, SIMPLEFS_BLOCK_SIZE) != SIMPLEFS_BLOCK_SIZE) {
        fprintf(stderr, "cannot write data bitmap!..\n");
        exit(EXIT_FAILURE);
    }   
        
    if (write_inode_table(fd, &sbd) == 1) {
       fprintf(stderr, "cannot write inode table!..\n");
        exit(EXIT_FAILURE);
    }

    if (write_dentries(fd, &sbd) == 1) {
        fprintf(stderr, "cannot write inode table!..\n");
        exit(EXIT_FAILURE);
    }
    
    printf("mkfs.simplefs completed successfully...\n");

    return 0;
}

int write_super_block(int fd, int n_inodes, struct simplefs_disk_super_block *sbd)
{
    uint64_t size;

    sbd->magic = SIMPLEFS_MAGIC;
    sbd->block_size = SIMPLEFS_BLOCK_SIZE;
    sbd->inode_count = n_inodes;
    /*
    if (ioctl(fd, BLKGETSIZE64, &size) == -1)
        return -1;
    */
    if ((size = lseek(fd, 0, SEEK_END)) == -1)
        return -1;

    sbd->block_count = size / SIMPLEFS_BLOCK_SIZE;
    sbd->free_inodes = n_inodes - 2;
    sbd->inode_table_block = 3;
    sbd->inode_table_size = (n_inodes + INODE_SIZE - 1) / INODE_SIZE;
    sbd->data_block_start = 3 + sbd->inode_table_size;
    sbd->free_blocks = sbd->block_count - sbd->data_block_start - 1;

    lseek(fd, 0, SEEK_SET);
    if (write(fd, sbd, sizeof(*sbd)) != (ssize_t)sizeof(*sbd)) 
        return -1;

    return 0;
}

int write_inode_table(int fd, struct simplefs_disk_super_block *sbd)
{
    unsigned char buf[SIMPLEFS_BLOCK_SIZE] =  {0};
    struct simplefs_disk_inode inoded =  {0};
    time_t curtime;
    off_t pos;

    pos = lseek(fd, 0, SEEK_CUR);
    for (int i = 0; i < sbd->inode_table_size; ++i)
        if (write(fd, buf, SIMPLEFS_BLOCK_SIZE) != SIMPLEFS_BLOCK_SIZE)
            return -1;
    
    lseek(fd, pos, SEEK_SET);
    if (write(fd, &inoded, sizeof(inoded)) != (ssize_t)sizeof(inoded))
        return -1;

    inoded.mode = S_IFDIR | S_IRWXU | S_IRWXG | S_IRWXO;
    inoded.uid = 0;
    inoded.gid = 0;
    inoded.size = SIMPLEFS_BLOCK_SIZE;
    inoded.nlink = 3;
    inoded.blocks = 1;
    inoded.block_no = sbd->data_block_start;

    curtime = time(NULL);
    inoded.ctime = curtime;
    inoded.mtime = curtime;
    inoded.atime = curtime;

    if (write(fd, &inoded, sizeof(inoded)) != (ssize_t)sizeof(inoded))
        return -1;

    return 0;
}

int write_dentries(int fd, struct simplefs_disk_super_block *sbd)
{
    struct simplefs_disk_dir_entry de = {0};

    lseek(fd, sbd->data_block_start * SIMPLEFS_BLOCK_SIZE, SEEK_SET);
    de.inode = 2;
    strcpy(de.name,  ".");
    if (write(fd, &de, SIMPLEFS_DENTRY_LEN) != SIMPLEFS_DENTRY_LEN)
        return -1;

    de.inode = 2;
    strcpy(de.name,  "..");
    if (write(fd, &de, SIMPLEFS_DENTRY_LEN) != SIMPLEFS_DENTRY_LEN)
        return -1;

    return 0;
}

void exit_sys(const char *msg)
{
    perror(msg);

    exit(EXIT_FAILURE);
}

/*----------------------------------------------------------------------------------------------------------------------
                                        39. Ders 06/12/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de "simplefs" dosya sistemimiz için gereken asıl dosya sistemi aygıt sürücüsünü yazalım. Yazıma iskelet bir 
    çekirdek modülü ile başlayacağız:

    /* simplefs.c */

    #include <linux/module.h>
    #include <linux/kernel.h>
    #include <linux/fs.h>

    MODULE_LICENSE("GPL");
    MODULE_AUTHOR("Kaan Aslan");
    MODULE_DESCRIPTION("simplefs");

    static int __init simplefs_module_init(void)
    {
        printk(KERN_INFO "simplefs module init\n");

        return 0;
    }

    static void __exit simplefs_module_exit(void)
    {
        printk(KERN_INFO "simplefs module exit\n");
    }

    module_init(simplefs_module_init);
    module_exit(simplefs_module_exit);

    Dosya sisyemi aygıt sürücüsü için ilk yaacağımız şey file_system_type türünden bir blobal nesne alıp onu çekirdek
    modülünün init fonksiyonunda register_filesystem fonksiyonu ile register ettirmektir:

    static struct file_system_type simplefs_type = {
        .owner = THIS_MODULE,
        .name = "simplefs",
        .mount = simplefs_mount,
        .kill_sb = simplefs_kill_sb,
        .fs_flags = FS_REQUIRES_DEV,
    };

    static int __init simplefs_module_init(void)
    {
        int result;

        if ((result = register_filesystem(&simplefs_type)) != 0)
            return result;

        printk(KERN_INFO "simplefs module init\n");

        return 0;
    }

    Anımsayacağınız gibi kullanıcı dosya sistemimizi mount etmeye çalıştığında file_system_type yapısının mount elemanına
    yerleştirdiğimiz fonksiyon çağrılacaktır. Biz bu fonksiyon içerisinde çekirdeğin daha yüksek seviyeli mount_bdev 
    fonksiyonunu çağırarak mount işlemlerini bu fonksiyona yaptırabiliriz. Yine anımsayacağınız gibi en yeni çekirdeklerde
    artık mount_bdev fonksiyonu yerine get_tree_bdev fonksiyonu kullanılıyordu. Biz simplefs dosya sistemimizde önce 
    mount_bdev fonksiyonunu kullanacağız. Sonra bunu get_tree_bdev fonksiyonunu kullanacak biçimde değiştireceğiz. 
    Simplfs dosya sistemimizdeki simple_fs_mount fonksiyonu şöyle yazılmıştır:

    static struct dentry *simplefs_mount(struct file_system_type *type, int flags, const char *dev, void *data)
    {
        return mount_bdev(type, flags, dev, data, simplefs_fill_super);
    }

    Burada gerekli işlemlerin çoğu zaten mount_bdev fonksiyonu tarafından yapılmaktadır. Bizim mount_bdev fonksiyonu 
    tarafından oluşturulan süper blok nesnesini doldurmamız gerekir. mount_bdev bu doldurma işlemi için bizim 
    simplefs_fill_super fonksiyonumuzu çağırmaktadır. Peki bu süper blok nesnesinin çini nasıl doldurmalıyız?

    static int simplefs_fill_super(struct super_block *sb, void *data, int silent)
    {
	    /* ... */

	    return 0;
    }

    Bizim super_block yapısı ile temsil edilen süper blok nesnesinin tüm elemanlarını doldurmamıza gerek yoktur. 
    Ancak yapının aşağıdaki elemanlarını mutlaka doldurmamız gerekir:

    s_magic: Bu elemana bizim dosya sistemimize ilişkin kendi belirlediğimiz 4 byte'lık bir sihirli sayı atamamız gerekir. 
    Dosya sistemimizdeki sihirli sayı şöyledir:

    #define SIMPLEFS_MAGIC           0x53494D46  
    /* ... */

    sb->s_magic = SIMPLEFS_MAGIC;

    s_blocksize: super_block yapısının s_bloksize ve s_blocksize_bits elemanlarına dosya sisteminde bir bloğun byte 
    uzunluğu ve onun log2 değeri yazılmalıdır. Bu işlem için sb_set_blocksize donksiyonundan faydalanılabilir. Örneğin:

    sb_set_blocksize(sb, SIMPLEFS_BLOCKSIZE);

    Bu fonksiyon başarı durumununda bizim ikinci parameetreye geçtiğimiz blok uzunluğuna başarısızlık durumun 0 değeine
    geri dönmektedir. Ancak fonksiyon normakl bir işleyişte başarısız olamayacağı için hat kontrolünün yapılmasına gerek 
    yoktur. 

    s_maxbytes: Bu elemana bir dosyada bulunabilecek maksimum byte byte sayısı yerleştirilmelidir. Bizim simplefs dosya
    sistemimizde bir dosyada en fazla SIMPLEFS_BLOCKSIZE kadar byte tutulabilmektedir. Bu elemana şöyle değer atayabiliriz:

    sb->s_maxbytes = SIMPLEFS_BLOCK_SIZE;

    s_op: super_block yapısının s_op elemanına çekirdek tarafından çeşitli durumlarda çağrılacak fonksiyonların adresleri 
    yerleştirilmelidir. Biz çekirdeğin bu çokbiçimli davranışından daha önce bahsetmiştir. Bu elemana super_operations
    isimli bir yapı nesnesinin adresi yerleştirilmelidir. Anımdanacağı gibi bu super_operations yapısı fonksiyon 
    göstericilerinden oluşmaktadır. Ancak bu yapının da tüm elemanlarının doldurulması gerekmez. Bizim simplefs dosya
    sistemimizde bu yapının aşağıdaki elemanlarına yerleştrme yapacağız:

    static const struct super_operations simplefs_super_ops = {
        .alloc_inode = simplefs_alloc_inode,
        .free_inode = simplefs_free_inode,
        .write_inode = simplefs_write_inode,
        .evict_inode = simplefs_evict_inode,
        .statfs = simple_statfs,
    };

    Bu elemanlara girilecek fonksiyonların parametrik yapısı aşağıdaki gibi olmalıdır:

    static struct inode *simplefs_alloc_inode(struct super_block *sb);
    static void simplefs_free_inode(struct inode *inode);
    static int simplefs_write_inode(struct inode *inode, struct writeback_control *wbc);
    static void simplefs_evict_inode(struct inode *inode);

    İşte süper blok nesnesinin s_op elemanına super_operations nesnesinin adresinin yerleştirilmesi gerekmektedir:

    sb->s_op = &simplefs_super_ops;

    Biz bu fonksiyonların ne zaman çağrılacağını ve bunların içerisinde ne yapılması gerektiğini izeleyen paragraflarda 
    açıklayacağız. 

    s_fs_info: Çekirdek super blok nesnelerini super_block isimli bir yapıyla temsil etmektedir. Ancak her dosya sisteminin 
    kendine özgü bir süper blok formatı da vardır. İşte çekirdeğin super_block yapısından hareketle sistem programcısının 
    kendi dosya sistemine ilişkin süper blok bilgilerine erişebilmesi gerekir. super_block yapısının s_fs_info elemanı 
    bunun için kullanılmaktadır. Bizim bu noktada dosya sistemimize ilişkin süper blok bilgilerini diskten okuyup 
    bunu bir yapı nesnesi içerisinde depolamamız gerekir. Ancak aslında bizim yalnızca kendi dosya sistemimize ilişkin 
    diskteki süper blok bilgilerine değil aynı zamanda onun yönetimine yönelik bilgileri de bulundurmamız gerekmektedir. 
    Bunun için dosya sistemi tasarımcıları tipik olarak kendi dosya sistemlerinin süper blok yönetimine ilişkin bir yapı 
    oluşturup diskteki süper blok bilgilerini bu yapı nesnesinin içerisinde içerisinde saklamaktadır. super_block yapısının 
    s_fs_info elemanına da bu nesnenin adresini yerleştirmektedir. Biz simplefs dosya sistemimizdeki süper blok yönetimi 
    için aşağıdaki gibi bir yapı oluşturabiliriz:

    struct simplefs_super_block {
        struct simplefs_disk_super_block *sbd;
        struct buffer_head *sb_bh;
        struct buffer_head *inode_bitmap_bh;
        struct buffer_head *data_bitmap_bh;
        unsigned long *inode_bitmap;
        unsigned long *data_bitmap;
        spinlock_t lock;
    };

    Burada yapının sbd elemanı bizim diskte tuttuğumuz süper blok bilgilerini göstermektedir. Biz zaten formatlama programınd
    bu yapıyı aşağıdaki gibi oluşturmuştuk:

    struct simplefs_disk_super_block {
        __le32 magic;              /* 0x464D4953 ("SIMF") */
        __le32 block_size;         /* 4096 */
        __le32 inode_count;        /* Toplam inode */
        __le32 block_count;        /* Toplam blok */
        __le32 free_inodes;        /* Serbest inode */
        __le32 free_blocks;        /* Serbest blok */
        __le32 inode_table_block;  /* İnode table başlangıcı (3) */
        __le32 inode_table_size;   /* İnode table boyutu */
        __le32 data_block_start;   /* Data block başlangıcı */
        __u8   padding[4060];      /* 4096'ya tamamlama */
    };

    Buradaki super_block yapısının s_fs_info elemanını için oluşan durumu aşağıdaki gibi betimleyebiliriz:

    super_block
    ...
    s_fs_info --------> simple_fs_super_block
    ...                 sbd    ---------------------> simple_fs_disk_super_block
                        ...     

    Artık çekirden bize super_block nesnesini verdiğinde biz kendi sistemimize ilişkin tüm süper blok bilgilerine 
    eriyor olacağız. simplefs_super_block yapısının sbd dışındaki diğer elemanalrını izleyen paragraflarda açıklayacağız. 

    s_root: super_block yapısının bu elemanına bizim kök dizine ilişkin dentry nesne adresini yerleştirmemiz gerekir. 
    Tabii dentry nesnesinin elde edilmesi için önce inode nesnesinin elde edilmesi gerekir. Bu işlemlerin nasıl 
    yapılacağı izleyen paragraflarda açıklanmaktadır, 

    Biz burada bir dosya sistemini oluşturabilmek için super_block yapısının minimal hangi elemanlarının doldurulması 
    gerektiğini açıkladık. Aslında isteğe bağlı olarak yapının diğer başka elemanları da doldurulabilir. 

    Biz yukarıdaki bazı süreçlerle doldurulacak elemanlar dışında simplefs_fill_uper fonksiyonumuzun ilk kısmını şöyle 
    oluşturabiliriz:

    static int simplefs_fill_super(struct super_block *sb, void *data, int silent)
    {
        sb->s_magic = SIMPLEFS_MAGIC;
        sb_set_blocksize(sb, SIMPLEFS_BLOCK_SIZE);
        sb->s_maxbytes = SIMPLEFS_BLOCK_SIZE;
        sb->s_op = &simplefs_super_ops;
        sb->s_flags |= SB_NOATIME;

        /* ... */

        return 0;
    }

    sb->s_flags |= SB_NOATIME işlemini dosya sisteminin basit tutulmasını sağlamak amacıyla uyguladık. Bu bayrak 
    dosyaların erişim zamanlarının güncellenmesini engelleyecektir. İzleyen paragraflarda suğer_block yapısının 
    diğer elemanlarının nasıl doldurulacağınzı açıklayacağız.
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Önce super_block yapısın s_fs_info elemanını doldurmaya çalışalım. Anımsanacağı gibi bu eleman bizim dosya sistemimizdeki
    süper blok işlemlerini yönetecek simplefs_super_block nesnesinin adresini tutmaktadır. Bizim önce bu nesneyi çekirdeğin
    heap sisteminde tahsis etmemiz gerekir. Biz çekirdeğin heap sistemini henüz incelemedik. Burada kzalloc isimli 
    çekirdek fonksiyonuyla bu tahsisatı yapacağız. kzalloc fonksiyonu kmalloc fonksiyonun tahis edilen alanı sıfırlayan 
    bir biçimidi. Tahisatı şöyşe yapabiliriz:

    struct simplefs_super_block *sfs_sb;
    /* ... */

	if ((sfs_sb = kzalloc(sizeof(struct simplefs_super_block), GFP_KERNEL))== NULL) {
		printk(KERN_INFO "cannot allocate simplefs super block!..\n");
		return -ENOMEM;
	}

    Biz içi sıfırlarla dolduurulmuş simplefs_super_block türündne bir nesneyi tahsis etmiş olduk. Bu nesnenin içini 
    doldurmamız gerekir. Bu nesne bizim kendi dosya sistemimizin süper blok işlemleri için faydalanacağımız elemanlardan
    oluşmaktadır. Biz önce yapının sbd elemanını dolduralım. Bu eleman anımsayacağınız gibi bizim dosya sistemimizin 
    diskteki süper blok bilgilerini tutmaktadır. O halde bizim bu noktada diskin süper bloğunu (yani 0 numaralı 
    bloğunu) okumamız gerekir. Bu işlemi yapmanın klasik yolu çekirdeğin biraz daha yüksek seviyeli sb_bread fonksiyonunu
    kullanmaktır. Çekirdğein sb_bread fonksiyonu süer blok içerisindeki blok aygıtı bilgilerinden ve blok uzunluğundan 
    faydalanarak diskin belirttiğimiz numaralı bloğunu okumaktadır. sb_bread fonksiyonun prototipi şöyledir:

    struct buffer_head *sb_bread(struct super_block *sb, sector_t block);

    Fonks,yonun birinci parametresi super_block nesnesinin adresini ikinci parametresi ise okunacak bloğun numarasını 
    almaktadır. (Tabii bizim bu fonksiyonu çağırmadan önce super_block nesnesine blok büyüklüğünü yerleştirmemiz 
    gerekir.) Fonksiyon okunan bloğu çekirdekte temsil eden buffer_head nesnesinin adresine geri dönmektedir. Biz 
    buffer_head yapısını ver organizasyonunu bellek yönetimi kısmında göreceğiz. Linux'ta bu buffer_head tasarımına 
    yeni ve modern bir alternatif de eklenmiştir. Buna "bio" sistemi de denilmektedir. Ancak bu buffer_head Sistemi
    çekirdek atılabilecek bir yapı değildir. Çünkü pek çok dosya sistemi halen bu yapıyı kullanmaktadır. Güncel 
    çekirdeklerde buffer_head yapısı "include/linux/buffer_headf.h" dosyasında şöyle bildirilmiştir:

    struct buffer_head {
        unsigned long b_state;		            /* buffer state bitmap (see above) */
        struct buffer_head *b_this_page;/* circular list of page's buffers */
        union {
            struct page *b_page;	            /* the page this bh is mapped to */
            struct folio *b_folio;	            /* the folio this bh is mapped to */
        };

        sector_t b_blocknr;		                /* start block number */
        size_t b_size;			                /* size of mapping */
        char *b_data;			                /* pointer to data within the page */

        struct block_device *b_bdev;
        bh_end_io_t *b_end_io;		            /* I/O completion */
        void *b_private;		                /* reserved for b_end_io */
        struct list_head b_assoc_buffers;       /* associated with another mapping */
        struct address_space *b_assoc_map;	    /* mapping this buffer is
                                                associated with */
        atomic_t b_count;		                /* users using this buffer_head */
        spinlock_t b_uptodate_lock;	/* Used by the first bh in a page, to
                        * serialise IO completion of other
                        * buffers in the page */
    };

    Okunan bloğun bilgileri yapının b_data elemanından elde edilmektedir. Bizim bu aşamada bu buffer_head tasarımını 
    bilmemize gerek yoktur. 
    
    sb_bread fonksiyonu başarısızlık durumunda NULL adrese geri dönmektedir. Biz simplefs_fill_super fonksiyonumuzda 
    diskimizin süper bloğunu şöyle okuyabiliriz:

    if ((sfs_sb->sb_bh = sb_bread(sb, 0)) == NULL)
		goto EXIT1;

    Görüldüğü gibi biz diskimizin süper bloğunu kendi süper blok yönetminde kullanacağımız simplefs_super_block yapının 
    sb_bh elemanına yerleştirdik. Çünkü bu bloktan elde ettiğimiz disk süper blok bilgilerinin bulunduğu bellek alanının 
    da yaşıyor olması gerekmektedir. 

    if ((sfs_sb->sb_bh = sb_bread(sb, 0)) == NULL) {
		printk(KERN_INFO "cannot read  simplefs disk super block!..\n");
		retval = -EINVAL;
		goto EXIT1;
	}
	
    Şimdi artık biz diskimizin süper bloğunu okuduk. Kendi disk süper blok bilgilerimizi simplefs_super_block nesnensinin 
    sbd elemanına yerleştirebiliriz:

    struct simplefs_disk_super_block *sbd;
    /* ... */

    sbd = (struct simplefs_disk_super_block *) sfs_sb->sb_bh->b_data;
	sfs_sb->sbd = sbd;

    Biz burada disk süper blok bilgilerine erişirken birden fazla -> operatörü kullanmamak için sbd isminde bir gösterici 
    kullandık. Aslında derleyiciler optimizasyon seçenekleri açıksa birden fazla -> operatörünü zaten optimize etmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        40. Ders 07/12/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Eğer Linux çekirdeği için yazılan kodların hem "little endian" hem de "big endian" makinelerde sorunsuz çalışması
    isteniyorsa diskten okunan bilgilerin endian dönüştürmesine sokulması gerekir. Biz simplefs dosya sistemimizde 
    diskteki bilgilerin "little endian" olduğunu varsaymıştık. Buradaki veri yapısı bir yapıya map edildiğinde orada 
    bilgi "little endian" biçiminde oluşacaktır. Eğer kodun çalışacağı makine "big endian" ise sorun oluşacaktır. İşte 
    bunun için Linux çekirdeğinde "endian" dönüştürmesi yapan yardımcı fonksiyonlar bulundurulmuştur. Bu fonksiyonların 
    listesini aşağıda veriyoruz:

    le16_to_cpu
    le32_to_cpu
    le64_to_cpu
    be16_to_cpu
    be32_to_cpu
    be64_to_cpu

    Bu fonksiyonların (aslında birer makro olarak yazılmıştır) başında "le" öneki parametrenin "litle endian" olduğunu 
    "be" öneki ise "big endian" olduğunu belirtmektedir. Örneğin le32_to_cpu fonksiyonu parametre olarak 32 bitlik 
    işaretsiz "little endian" bir değeri parametre olarak alır. Eğer o anda çalışılan CPU "litte endian" ise onu 
    dönüştürmez, "big endian" ise onu "big endian" formata dönüştürür. Bu fonksiyonların parametresi gösterici olan 
    biçimleri de vardır:

    le16_to_cpup
    le32_to_cpup
    le64_to_cpup
    be16_to_cpup
    be32_to_cpup
    be64_to_cpup

    Tabii yukarıdakilern bir de ters biçimleri bulunmaktadır:

    cpu_to_le16
    cpu_to_le32
    cpu_to_le64
    cpu_to_be16
    cpu_to_be32
    cpu_to_be64

    cpu_to_le16p
    cpu_to_le32p
    cpu_to_le64p
    cpu_to_be16p
    cpu_to_be32p
    cpu_to_be64p
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Diskimizdeki süper bloğu okudaktan sonra ilk yapacağımız şey sihirli sayı karşılaştırmasıdır. Sihirli karşılaştırması 
    ile diskimizin simplfs dosya sistemiyle formatlanıp formatlanmadığını kesin olarak anlayamayabiliriz. Ancak olasılığı 
    azaltırız. Örneğin:

    if (le32_to_cpu(sbd->magic) != SIMPLEFS_MAGIC) {
		printk(KERN_INFO "invalid magic number for simpls: %08X\n", sbd->magic);
		retval = -EINVAL;
		goto EXIT2;
	}

    Sihirli sayı tesadüfen de (olasılık oldukça az) doğrulanabilir. Olasılığı azaltmak için dosya sistemine ilişkin 
    başka değerler de kontrol edilebilir. Örneğin bizim dosya sistemimizde blok büyüklükleri sabit 406 byte'tır. 
    Bu karşılaştırmayı da yapabiliriz. Örneğin bizim dosya sistemimizde inode tablosunun blok numarası her zaman 3'tür. 
    bu karşılaştırmayı da yapabiliriz:

    if (le32_to_cpu(sbd->block_size) != SIMPLEFS_BLOCK_SIZE) {
		printk(KERN_INFO "invalid block size for simpls: %08X\n", sbd->block_size);
		retval = -EINVAL;
		goto EXIT2;
	}
	if (le32_to_cpu(sbd->inode_table_block) != 3) {
		printk(KERN_INFO "invalid inode table for simpls: %08X\n", sbd->inode_table_block);
		retval = -EINVAL;
		goto EXIT2;
	}

    Şimdi sıra "inode bitmap" ve "data bitmap" bloklarının okunarak simplefs_super_block yapısının içerisinde saklanmasına 
    gelmeiştir. simple_fs_super_block yapısını anımsatmak istiyoruz:

    struct simplefs_super_block {
        struct simplefs_disk_super_block *sbd;
        struct buffer_head *sb_bh;
        struct buffer_head *inode_bitmap_bh;
        struct buffer_head *data_bitmap_bh;
        unsigned long *inode_bitmap;
        unsigned long *data_bitmap;
        spinlock_t lock;
    };

    Bizim önce indeo bitmap'i diskten okuyup onun buffer_head adresini tapının inode_bitmap_bh elemanına sonra da 
    onun bilgilerin bulunduğu yerin adresini de yapının inode_bitmap elemanına yerleştirmemiz gerekir. Aynı şeyi tabii
    data bitmap için de yapmalıyız. Data bitmap'i diskten okuyup onun buffer head adresini yapının data_bitmap_bh 
    elemanına, onun bilgilerinin bulunduğu yerin adresini de yapının data_bitmap elemanına yerleştirmemiz gerekir. 
    Bu işlemleri şöyle yapabiliriz:

    #define SIMPLEFS_INODE_BITMAP_LOCATION		    1
    #define SIMPLEFS_DATA_BITMAP_LOCATION		    2
    /* ... */

    if ((sfs_sb->inode_bitmap_bh = sb_bread(sb, SIMPLEFS_INODE_BITMAP_LOCATION)) == NULL) {
		printk(KERN_INFO "cannot read simplefs inode bitmap!..\n");
		retval = -EIO;
		goto EXIT2;
	}
	sfs_sb->inode_bitmap = (unsigned long *) sfs_sb->inode_bitmap_bh->b_data;

	if ((sfs_sb->data_bitmap_bh = sb_bread(sb, SIMPLEFS_DATA_BITMAP_LOCATION)) == NULL) {
		printk(KERN_INFO "cannot read simplefs data bitmap!..\n");
		retval = -EINVAL;
		goto EXIT3;
	}
	sfs_sb->data_bitmap = (unsigned long *) sfs_sb->data_bitmap_bh->b_data;
	
    Burada simplefs_super_block yapısının inode_bitmap ve data_bitmap elemanlarının neden char * değil de unsiegned long *
    türünden olduğunu merak edebilirsiniz. Linux çekirdeğindeki bitmap'ler üzerinde işlem yapan yardımcı çekirdek 
    fonksiyonları genel olarak bitmap adreslerini unsigned long olarak almaktadır. İzleyen paragraflarda Linux çekirdeğindeki
    bitmap'ler üzerinde işlem yapan fonksiyonları gözden geçireceğiz. 

    Yukarıdaki işlemlerdne sonra fill_super blok fonksiyonumuzda geldiğimiz yere kadarki kısmı özetleme amacıyla 
    vermek istiyoruz:

    static int simplefs_fill_super(struct super_block *sb, void *data, int silent)
    {
        struct simplefs_super_block *sfs_sb;
        struct simplefs_disk_super_block *sbd;
        struct inode *root_inode;
        int retval;
        
        sb->s_magic = SIMPLEFS_MAGIC;
        sb_set_blocksize(sb, SIMPLEFS_BLOCK_SIZE);
        sb->s_maxbytes = SIMPLEFS_BLOCK_SIZE;
        sb->s_op = &simplefs_super_ops;
        sb->s_flags |= SB_NOATIME;

        if ((sfs_sb = kzalloc(sizeof(struct simplefs_super_block), GFP_KERNEL))== NULL) {
            printk(KERN_INFO "cannot allocate simplefs super block!..\n");
            return -ENOMEM;
        }
        sb->s_fs_info = sfs_sb;
        spin_lock_init(&sfs_sb->slock);

        if ((sfs_sb->sb_bh = sb_bread(sb, 0)) == NULL) {
            printk(KERN_INFO "cannot read  simplefs disk super block!..\n");
            retval = -EINVAL;
            goto EXIT1;
        }
        
        sbd = (struct simplefs_disk_super_block *) sfs_sb->sb_bh->b_data;
        sfs_sb->sbd = sbd;

        if (le32_to_cpu(sbd->magic) != SIMPLEFS_MAGIC) {
            printk(KERN_INFO "invalid magic number for simpls: %08X\n", sbd->magic);
            retval = -EINVAL;
            goto EXIT2;
        }
        if (le32_to_cpu(sbd->block_size) != SIMPLEFS_BLOCK_SIZE) {
            printk(KERN_INFO "invalid block size for simpls: %08X\n", sbd->block_size);
            retval = -EINVAL;
            goto EXIT2;
        }
        if (le32_to_cpu(sbd->inode_table_block) != 3) {
            printk(KERN_INFO "invalid inode table for simpls: %08X\n", sbd->inode_table_block);
            retval = -EINVAL;
            goto EXIT2;
        }

        if ((sfs_sb->inode_bitmap_bh = sb_bread(sb, SIMPLEFS_INODE_BITMAP_LOCATION)) == NULL) {
            printk(KERN_INFO "cannot read simplefs inode bitmap!..\n");
            retval = -EIO;
            goto EXIT2;
        }
        sfs_sb->inode_bitmap = (unsigned long *) sfs_sb->inode_bitmap_bh->b_data;

        if ((sfs_sb->data_bitmap_bh = sb_bread(sb, SIMPLEFS_DATA_BITMAP_LOCATION)) == NULL) {
            printk(KERN_INFO "cannot read simplefs data bitmap!..\n");
            retval = -EINVAL;
            goto EXIT3;
        }
        sfs_sb->data_bitmap = (unsigned long *) sfs_sb->data_bitmap_bh->b_data;

        /* ... */

    EXIT3:
        brelse(sfs_sb->inode_bitmap_bh);
    EXIT2:
        brelse(sfs_sb->sb_bh);
    EXIT1:
        kfree(sfs_sb);

        return retval;
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi artık fil_super fonksiyonumuzda sıra kök dizine inode elemanını ve dentry nesnesini oluşturmaya gelmiştir. 
    Bu aşamada durum biraz daha karmaşık hale gelecektir. Bizim diskteki bir inode elemanını okuyacak bir fonksiyon 
    yazmamız gerekir. Çünkü kök dizinin değil ileride belli bir inode numarasına ilişkin inode elemanının da diskten 
    okunması gerekecektir. Biz de buaşamda belli bir inode numarasına ilişkin inode elemanını diskten okuyan aşağıdaki 
    paramterik yapıya sahip bir fonksiyon yazacağız:

    static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
    {
        /* ... */
    }

    Fonksiyonun birinci parametresi çekirdeğin süper blok nesne adresini, ikinci parametresi ise inode numarasını 
    almaktadır. Fonksiyon başarı durumunda diskten okunan inode nesnesinin adresine, başarısızlık durumunda ise başarısızlığı
    belirten geçersiz adrese geri dönmektedir. Linux çekirdeğinde adrese geri dönen fonksiyonlar istenirse aynı zamanda
    errno değerini de tutabilmektedir. Çok yüksek adreslerin bir bölümü aslında Linux çekirdeğinde geçerli bir adres
    belirtmemektedir. İşte bu biçimdeki Linux için geçersiz adresler aslında errno değerini barındırmaktadır. Bu işlemler
    için çekirdekte inline fonksiyon bulundurulmuştur: IS_ERR, PTR_ERR, ERR_PTR makroları. IS_ERR fonksiyonu bir adres 
    bilgisini alır. Onun Linux için geçerli bir adres olup olmadığına bakar. Eğer adres geçersizse onun içerisine depolanmış 
    olan negatif errno değeri PTR_ERR fonksiyonuyla elde edilmektedir. İçerisinde negatif errno değerini barındıran bir adres 
    bilgisi de ERR_PTR fonksiyonuyla oluşturulmaktadır. Linux çekirdeğindeki başarısızlık ve errno değerlerinin hangi 
    biçimlerde bulunduğunu aşağıda maddeler halinde özetliyoruz:

    1) Bir çekirdek fonksiyonu tamsayı türlerine ilişkin bir değere geri dönüyorsa geri dönüş değerinin negatif olması 
    fonksiyonun başarısız olduğunu gösterir. Bu negatif değerin pozitiflisi errno değerini vermektedir.

    2) Bir çekirdek fonksiyonu bir adrese geri dönüyorsa fonksiyonun başarısı verilen adresin değerine bağlıdır. Eğer 
    verilen adres çok bir büyük bir adresse fonksiyon başarısız olmuştur. Bu kontrol IS_ERR inline fonksiyonuyla yapılmaktadır.

    3) Adrese geri dönen çekirdek fonksiyonu eğer başarısızsa negatif errno değeri PTR_ERR inline fonksiyonuyla elde edilir.

    4) Biz negatif bir errno değerini bir adres gibi geri döndüreceksek bunun için ERR_PTR inline fonksiyonunu kullanmalıyız.

    Biz de çekirdek kodlaması yaparken  çekirdekte uygulanan bu biçime (convention) uymalıyız.

    Biz simplefs_iget fonksiyonunun içini yazdığımızı varsayarsak fill_super fonksiyonumuz içerisinde kök dizine ilişkin
    inode elemanını diskten şöyle okuyabiliriz:

    #define SIMPLEFS_ROOT_INO					1
    /* ... */
    struct inode *root_inode;
    /* ... */

    root_inode = simplefs_iget(sb, SIMPLEFS_ROOT_INO);
	if (IS_ERR(root_inode)) {
		printk(KERN_INFO "cannot read root inode!..\n");
		retval = PTR_ERR(root_inode);
		goto EXIT4;
	}
    /* ... */

    EXIT4:
        brelse(sfs_sb->data_bitmap_bh);
    EXIT3:
        brelse(sfs_sb->inode_bitmap_bh);
    EXIT2:
        brelse(sfs_sb->sb_bh);
    EXIT1:
        kfree(sfs_sb);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi dosya sistemimiz için diskten bir inode elemenını okuyan simplefs_iget fonksiyonun içini yazalım. Anımsayacağınız
    gibi inode nesneleri çekirdek tarafındna bir önbellek içeriisnde saklanıyordu. Bizim dosya sistemimize ilişkin inode
    elemanları da yine bu önbellek içeriisnde saklanacaktır. Dolayısıyla biz bir inode nesnesini elde etmek istediğimizde
    önce inode önbelleğine bakıp eğer talep ettiğimiz inode elemanı zaten inode önbelleği içerisinde varsa hiç disk 
    okuması yapmadan onu önbellekten alıp geri döndürmeliyiz. Eğer talep ettiğimiz inode nesnesi inode belleğinde yoksa
    bu durumda gerçek disk okuması yapmalıyız. 

    Çekirdekte bir inode nesnesini inode belleğinden alan yoksa onu tahsis edip adresini bize veren iget_locked yüksek 
    seviyeli bir fonksiyon bulunmaktadır. Eskidne bu fonksiyonun ismi iget biçimindeydi. Sonra ismi iget_locked olarak 
    değiştirildi. iget_locked fonksiyonu "fs/inode.c" dosyasında aşağıdaki gibi tanımlanmıştır:

    struct inode *iget_locked(struct super_block *sb, unsigned long ino)
    {
        struct hlist_head *head = inode_hashtable + hash(sb, ino);
        struct inode *inode;
    again:
        inode = find_inode_fast(sb, head, ino, false);
        if (inode) {
            if (IS_ERR(inode))
                return NULL;
            wait_on_inode(inode);
            if (unlikely(inode_unhashed(inode))) {
                iput(inode);
                goto again;
            }
            return inode;
        }

        inode = alloc_inode(sb);
        if (inode) {
            struct inode *old;

            spin_lock(&inode_hash_lock);
            /* We released the lock, so.. */
            old = find_inode_fast(sb, head, ino, true);
            if (!old) {
                inode->i_ino = ino;
                spin_lock(&inode->i_lock);
                inode->i_state = I_NEW;
                hlist_add_head_rcu(&inode->i_hash, head);
                spin_unlock(&inode->i_lock);
                spin_unlock(&inode_hash_lock);
                inode_sb_list_add(inode);

                /* Return the locked inode with I_NEW set, the
                * caller is responsible for filling in the contents
                */
                return inode;
            }

            /*
            * Uhhuh, somebody else created the same inode under
            * us. Use the old inode instead of the one we just
            * allocated.
            */
            spin_unlock(&inode_hash_lock);
            destroy_inode(inode);
            if (IS_ERR(old))
                return NULL;
            inode = old;
            wait_on_inode(inode);
            if (unlikely(inode_unhashed(inode))) {
                iput(inode);
                goto again;
            }
        }
        return inode;
    }
    EXPORT_SYMBOL(iget_locked);

    Burada önce inode nesnesi inode önbelleğinde aranmış eğer bulunanmışsa yeni bir inode nesnesi alloc_inode fonksiyonuyla 
    oluşturularak verilmiştir. Tabii oluşturulan bu inode nesnesi de aynı zamanda inode belleğine terleştirilmiştir. 
    Çekirdekte yeni bir inode nesnesinin tahsis edilmesi inode_alloc fonksiyonu tarafından yapılmaktadır. Ancak bu 
    fonksiyon da aslında tahsisatı çokbiçimli olarak super_block nesnesine yerleştirdiğimiz super_operations nesnesi 
    içeisindeki alloo_inode fonksiyonunu çağırarak yapmaktadır. Yani sonuç olarak aslında iget_locked fonksiyonu 
    inode nesnesini inode önbelleğinde bulmazsa bizim süper blok nesnesine yerleştirdiğimiz fonksiyonu çağırarak 
    tahsisatı bize yaptırmaktadır. O halde bizim super_operations nesnesine yerleştiriğimiz alloc_inde fonksiyonunda 
    kendi inode nesnemizi tahsis etmemiz gerekir. Çekirdekteki "fs/inode.c" dosyası içerisinde bulunan alloc_inode 
    fonksiyonu aşağıdaki gibi tanımanmıştır:

    struct inode *alloc_inode(struct super_block *sb)
    {
        const struct super_operations *ops = sb->s_op;
        struct inode *inode;

        if (ops->alloc_inode)
            inode = ops->alloc_inode(sb);
        else
            inode = alloc_inode_sb(sb, inode_cachep, GFP_KERNEL);

        if (!inode)
            return NULL;

        if (unlikely(inode_init_always(sb, inode))) {
            if (ops->destroy_inode) {
                ops->destroy_inode(inode);
                if (!ops->free_inode)
                    return NULL;
            }
            inode->free_inode = ops->free_inode;
            i_callback(&inode->i_rcu);
            return NULL;
        }

        return inode;
    }

    Burada önce super_operations nesnesi içerisindeki alloc_inode fonksiyon göstericisinin NULL olup olmadığına 
    bakılmış eğer bu elemana bir fonksiyon adresi yerleştirilmişse o fonksiyon çağrılmıştır:

    if (ops->alloc_inode)
        inode = ops->alloc_inode(sb);
    else
        inode = alloc_inode_sb(sb, inode_cachep, GFP_KERNEL);

    Eğer super_operations nesnesinin alloc_inode elemanı NULL ise bu durumda inode tahsisatı çekirdek içerisindeki 
    alloc_inode_sb fonksiyonuna yaptırılmıştır. Abcak dosya sistemlerinde çoğu kez her dosya sisteminin kendi inode 
    nesnesini kendisinin tahsis etmesi gerekmektedir. Bunun nedenin izleyen paragraflarda anlayacaksınız. 

    O halde bizim simplefs_iget fonksiyonumuzda inode nesnesini şöyle elde edebiliriz:

    struct inode *inode;
    /* ... */

	if ((inode = iget_locked(sb, ino)) == NULL) 
		return ERR_PTR(-ENOMEM);

    Tabii medemki iget_locked inode nesnesini inode bulamazsa bizim süper bloğumuza ilişkin alloc_inode fonksiyonunu 
    çağırmaktadır, o halde bizim bu noktada artık inode_alloc fonksiyonumuzun içini yazmamız gerekir. Bizim süper 
    bloğuna yerleştirdiğimiz alloc_inode fonksiyonu şöyleydi:
    
    static struct inode *simplefs_alloc_inode(struct super_block *sb)
    {
        /* ... */
    }

    Görüldüğü gibi fonksiyon tahsis edilen inode nesnesiyle geri dönmektedir. Ancak burada bazı ayrıntılar vardır. 
    Aslında sistem programcısı bu tahsisat fonksiyonunda struct inode türünden bir nesne tahsis etmemektedir. Sistem 
    programcısı bu fonksiyonda kendi dosya sistemine ilişkin inode nesnesini tahsis eder fakat onu sanki struct inode 
    nesnesiymiş gibi geri döndürür. Dosya sistemlerinin standart inode yapısından farklı inode temsilleri olabilir. 
    Bu nedenle her dosya sistemi aslında kendine özgü bir inode yapısı oluşturmaktadır. Bu durumu nesne yönelimli 
    programlama telniğindeki taban sınıf türemiş sınıf olgusuna benzetebiliriz. Bu bağlamda struct inode adeta bir 
    taban sınıf gibidir. Sistem programcısının kendisinin oluşturduğu inode yapısı da adeta türemiş sınıf gibidir. 
    Biz simplefs dosya sistemimiz için aşağıdaki gibi bir inode yapısı oluşturabiliriz:

    struct simplefs_inode {
        __u32 block_no;
        struct inode vfs_inode;
    };

    Görüldüğü gibi aslında bizim kendi inode yapımız çekirdeğin inode yapısını da tutmaktadır. Böylece sistem programcısı 
    alloc_inode fonksiyonu içerisinde aslında struct inode türünden değil struct simplefs_inode türünden nesne tahsis 
    eder ancak nesnenin vfs_inode elemanın adresiyle geri döner. Böylece çekirdek ne zaman bize bizim bir inode nesnemizin
    adresini verse container_of makrosuyla biz kendi inode nesnemize erişebiliriz. Dosya sistemimie ilişkin simplefs_inode 
    yapısı içerisinde standart inode nesnesinin yanı sıra aynı zamanda ilgili inode elemanının diskteki hangi blokta 
    tutulduğu bilgisinin de bulunduğuna dikkat ediniz:

    simplefs_inode
    ┌────────────┐
    │  block_no  │
    ├────────────┤  ──────────▶ fonksiyonu döndürüyor adres
    │ vfs_inode  │
    └────────────┘

    Pekiyi biz alloc_inode fonksiyonumuz içerisinde simplefs_inode nesnesini nasıl tahsis edeceğiz? İşte aslında bu konu
    ileride alacağımız "dilimli tahsisat sistemi (slab allocator)" denilen çekirdek tahsisat mekanizması ile ilgildir. 
    Çekirdek içeisinde dinamik tahsisatlar hızlı yapıslın diye hep aynı büyüklükte bloklardan oluşan ve ismine "dilim
    (slab)" denilen ayrık heap alanları oluşturulabilmektedir. Ayrıc Linux çekirdeği başlangıçta bazı sabit uzunluklu 
    bloklar barındıran hazır dilimler de oluşturmaktadır. Çekirdeğin kmalloc gibi kzalloc gibi genel tahsisat fonksiyonları
    bu hazır dilimlerden tahsisat yapmaktadır. Biz simplefs_inode nesnelerimizi kmalloc gibi kzalloc gibi fonksiyonlarla
    tahsis edebiliriz. Ancak tam olarak simplefs_inode kadar uzunlukta bloklara sahip olan ayrı bir dilimli tahsisat 
    nesnesi oluşturmak daha etkin bir yöntemdir. Biz dilimli tahsisat nesnelernin nasıl oluşturulacağını ileride göreceğiz. 
    Ancak burada bu işlemi yapan fonksiyon çağrısını vermekle yetineceğiz:

    static struct kmem_cache *simplefs_inode_cachep;
    /* ... */

    simplefs_inode_cachep = kmem_cache_create("simplefs_inode_cache", sizeof(struct simplefs_inode),
            0, SLAB_HWCACHE_ALIGN,  NULL);

    Fonksiyon başarı durumunda dilimli tahsisat sistemine ilişkin bilgilerin saklandığı kmem_cache türündne yapı
    nesnesinin adresine, başarısızlık durumunda NULL adrese geri dönmektedir. Başarısızlık durumunda modülün init 
    fonksiyonunun -ENOMEM errno değeri ile geri döndürülmesi uygun olur. 

    Biz yarattığımız bu dilimli tahsisat nesnesinden blok tahsisatını kmem_cache_alloc fonksiyonuyla yaparız. Örneğin:

    struct simplefs_inode *inode_simpls;
    /*  ... */
    
    inode_simplefs = kmem_cache_alloc(simplefs_inode_cachep, GFP_KERNEL);

    Fonksiyon başarısızlık durumunda NULL adrese geri dönmektedir. 

    kmem_cache_alloc fonksiyonuyla tahsis edilen alan kmeme_cache_free fonksiyonuyla serbest bırakılmaktadır:

    kmem_cache_free(simplefs_inode_cachep, inode_simplefs);

    kmem_cache_create fonksiyonu ile yaratılmış olan dilimli tahsisat nesnesi kmem_cache_destroy fonksiyonuyla yok 
    edilmektedir:

    kmem_cache_destroy(simplefs_inode_cachep);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        41. Ders 13/12/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Yukarıda da belirttiğimiz gibi iget_locked çekirdek fonksiyonu eğer belirtilen numaralı inode nesnesi inode önbelleği 
    içerisinde varsa doğrudan onu oradan alıp geri dönmektedir. Ancak belirtilen numaralı inode nesnesi inode önbelleğinde
    yoksa bu durumda bizim super_operations nesnesine yerleştirdiğimiz alloc_inode fonksiyonu ile tahsisat yapılmaktadır. 
    O halde bizim bu noktada dosya sistemimize ilişkin inode nesnesini tahsis edecek fonksiyonu yazmamız gerekir. Bu 
    fonksiyonu şöyle yazabiliriz:

    static struct inode *simplefs_alloc_inode(struct super_block *sb)
    {
        struct simplefs_inode *inode_simplefs;

        if ((inode_simplefs = kmem_cache_alloc(simplefs_inode_cachep, GFP_KERNEL)) == NULL) 
            return NULL;
        inode_init_once(&inode_simplefs->vfs_inode);

        return &inode_simplefs->vfs_inode;
    }

    inode nesnesi tahsis edildikten sonra onun içerisindeki elemanlara inode_init_once fonksiyonu ile ilkdeğerlerin 
    verildiğine dikkat ediniz. 

    Tabii bu inode tahsisatını serbest bırakan super_operations fonksiyonun da bu noktada yazılması gerekir:

    static void simplefs_free_inode(struct inode *inode) 
    {
        struct simplefs_inode *inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
        
        kmem_cache_free(simplefs_inode_cachep, inode_simplefs);
    }

    simplefs_inode yaımızın block_no elemanına henüz bir değer atamadık. Bu eleman inode elemanın bulunduğu disk bloğunun 
    numarasını tutmaktadır. inode nesnesinin içi inode elemanı diskten elde edilerek doldurulacaktır. simplefs_iget 
    fonksiyonumuzun içi henüz şu durumdadır:
    
    static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
    {
        struct inode *inode;

        if ((inode = iget_locked(sb, ino)) == NULL) 
            return ERR_PTR(-ENOMEM);

        /* ... */

        return NULL;
    }

    Artık biz inode nesnesini tahsis etmiş olduk. Şimdi diskte ilgili inode elemanını bulup onun içerisindeki bilgilerden
    hareketle bu nesnenin için doldurmamız gerekir. Bunu da bir fonksiyona yaptıralım. Bu işlemi yapan fonksiyonumuzun 
    parametrik yapısı şöyledir:
   
    static struct simplefs_disk_inode *simplefs_get_inode_disk(struct super_block *sb, 
                unsigned long ino, struct buffer_head **bhpp)
    {
        /* ... */        
    }

    Fonksiyonun birinci parametresi süper blok nesnesinin adresini, ikinci parametresi inode numarasını almaktadır. 
    Fonksiyon diskteki inode elemanı ile geri dönmektedir. Ancak fonksiyonun geri döndürdüğü adresini fonksiyon çıkışında 
    da yaşıyor olması gerekir. Bu adres okunan bloktaki bir yer olduğu için bloğun da referans sayacının artırılarak 
    page cache içerisinde muhafaza edilmesi gerekmektedir. Bu nedenle okunan bloğun buffer_head adresi ayrıca dışarıya
    iletilmiştir. Biz fonksiyonun son parametresine bir buffer_head göstericisinin adresini geçiririz. Fonksiyon da 
    buffer_head nesnesinin adresini bu göstericiye yerleştirir. 

    Bizim simplefs_get_inode_disk fonksiyonunda öncelikle elde etmek istediğimiz inode elemanının diskimizin hangi 
    bloğunda ve o bloğun hangi offset'in olduğunu belirlememiz gerekir. Bu belirlemeyi şöyle yapabiliriz:

    #define SIMPLEFS_DISK_INODE_SIZE	sizeof(struct simplefs_disk_inode)

    block_no = ino * SIMPLEFS_DISK_INODE_SIZE / SIMPLEFS_BLOCK_SIZE;
	block_offset = ino * SIMPLEFS_DISK_INODE_SIZE % SIMPLEFS_BLOCK_SIZE;

    Ancak burada bir noktaya dikkat ediniz. Inode elemanlarının diskte bir blokta başlayıpo diğer bloktan devam etmesi 
    iyi bir tasarım değildir. Bunun engellenmesi için inode elemanlarının uzunluğunun blok uzunluğuna tam bölünmesi 
    gerekir. Bizim simplefs dosya sistemimizde diskteki inode elemanlarımız 64 byte uzunluktadır. Bu değer de 4096'ya
    tam bölünebilmektedir. Ancak eğer blok uzunluğu inode elemanlarının diskteki uzunluğuna tam bölünemeseydi her 
    bloğun sonunda kullanılmayan alan oluşurdu. Dolayısıyla yukarıdaki hesap da hatalı olurdu. Bu durumda (bizim 
    tasarımıızda geçerli değil) inode elemanın blok numarası ve blok offseti şöyle elde edilebilirdi:

    #define SIMPLEFS_INODE_TABLE_LOCATION		3
    #define SIMPLEFS_DISK_INODE_SIZE	        sizeof(struct simplefs_disk_inode)
    #define SIMPLEFS_DISK_INODE_PER_BLOCK     	(SIMPLEFS_BLOCKSIZE / SIMPLEFS_DISK_INODE_SIZE)

    block_no = SIMPLEFS_INODE_TABLE_LOCATION + ino / SIMPLEFS_INODES_PER_BLOCK;
    block_offset = (ino % SIMPLEFS_DISK_INODE_PER_BLOCK) * SIMPLEFS_DISK_INODE_SIZE;

    ino numaralı inode elemanının diskimizin hangi bloğunda ve hangi offset'inde olduğunu belirledikten sonra artık 
    bloğu sb_bread fonksiyonu ile okuyabiliriz:

    if ((bh = sb_bread(sb, block_no)) == NULL) 
        return ERR_PTR(-EIO);

    Artık simplefs_get_inode_disk fonksiyonumuzu tamamlayabiliriz:

    static struct simplefs_disk_inode *simplefs_get_inode_disk(struct super_block *sb, 
            unsigned long ino, struct buffer_head **bhpp)
    {
        int block_no, block_offset;
        struct buffer_head *bh;
        struct simplefs_disk_inode *disk_inode;
        
        block_no = SIMPLEFS_INODE_TABLE_LOCATION + ino * SIMPLEFS_DISK_INODE_SIZE / SIMPLEFS_BLOCK_SIZE;
        block_offset = ino * SIMPLEFS_DISK_INODE_SIZE % SIMPLEFS_BLOCK_SIZE;

        if ((bh = sb_bread(sb, block_no)) == NULL) 
            return ERR_PTR(-EIO);

        disk_inode = (struct simplefs_disk_inode *)(bh->b_data + block_offset);
        *bhpp = bh;

        return disk_inode;
    }

    Artık diskten inode elemanını okumuş durumdayız. Şimdi bu bilgilerden hareketle simplefs_iget fonksiyonunda 
    inode nesnesinin içini doldurmamız gerekir. Aşağıda simplefs_iget fonksiyonunun yeni durumunu veriyoruz:

    #define SIMPLEFS_SB(sb)						((struct simplefs_super_block *)((sb->s_fs_info)))
    #define SIMPLEFS_DISK_SB(sb)				(SIMPLEFS_SB((sb))->sbd)

    static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
    {
        struct inode *inode;
        struct simplefs_disk_inode *disk_inode;
        struct buffer_head *bh;
        struct simplefs_disk_super_block *simplefs_sbd;

        simplefs_sbd = SIMPLEFS_DISK_SB(sb);
        if (ino >= simplefs_sbd->inode_count) 
            return ERR_PTR(-EINVAL);

        if ((inode = iget_locked(sb, ino)) == NULL) 
            return ERR_PTR(-ENOMEM);

        if (!(inode->i_state & I_NEW))
            return inode;

        disk_inode = simplefs_get_inode_disk(sb, ino, &bh);
        if (IS_ERR(disk_inode)) {
            iget_failed(inode);
            return (struct inode *)disk_inode;
        }

        /* ... */

        return inode;
    }

    Burada önce super_block nesnesindne hareketle dosya sistemimize ilişkin simplefs_disk_super_block nesnesine 
    SIMPLEFS_DISK_SB makrosu ile erişilmiştir. Bizim dosya sistemimizde olabilecek en büyük inode değeri zaten bu 
    yapının içerisindeki inode_count elemanında bulunuyordu. Böylece simplefs_iget fonksiyonu içerisinde yanlışlıkla 
    olmayan bir inode elemanı okunmak istenirse fonksiyon hemen başarısızlıkla geri dönmektedir:

    simplefs_sbd = SIMPLEFS_DISK_SB(sb);
    if (ino >= simplefs_sbd->inode_count) 
        return ERR_PTR(-EINVAL);

    Anımsayacağınız gibi iget_locked fonksiyonu eğer inode nesnesi diskte inode önbelleğinde varsa onu alıyor, yoksa 
    tahsis edip bize veriyordu. İşte bizim inode nesnesinin yeni tahsis edilmiş olup olmadığını anlamamız gerekir. 
    Çekirdekteki kodlar eğer inode elemanı yeni tahsis edilmişse inode yapısının i_state elemanında I_NEW bayrağını
    set etmektedir. Biz de fonksiyonda bu bayrağı kontrol ettik:

    if (!(inode->i_state & I_NEW))
            return inode;
    
    Artık akış ancak yeni tahsis edilmiş bir inode nesnesi söz konusuysa aşağıya geçecektir. Fonksiyonumuzda bundan 
    sonra diskten inode elemanı okunmuştur:

    disk_inode = simplefs_get_inode_disk(sb, ino, &bh);
    if (IS_ERR(disk_inode)) {
        iget_failed(inode);
        return (struct inode *)disk_inode;
    }

    Artık tahsis edilen inode nesnesinin elemanları diskten alınan inode bilgileri ile doldurulmalıdır. Tabii biz 
    itahsis ettiğimiz inode nesnesinin tüm elemanlarını doldurmak zorunda değiliz. Yalnızca dosya sistemimize 
    yönelik bazı elemanların doldurmamız yeterli olacaktır. Doldurma işlemini şöyle yapabiliriz:

    inode->i_mode = le32_to_cpu(disk_inode->mode);
    i_uid_write(inode, le32_to_cpu(disk_inode->uid));
    i_gid_write(inode, le32_to_cpu(disk_inode->gid));
    inode->i_size = le32_to_cpu(disk_inode->size);
    set_nlink(inode, le32_to_cpu(disk_inode->nlink));
    inode->i_blocks = le32_to_cpu(disk_inode->blocks);
    inode->i_size = le32_to_cpu(disk_inode->size);
    inode_set_atime(inode, le32_to_cpu(disk_inode->atime), 0);
	inode_set_mtime(inode, le32_to_cpu(disk_inode->mtime), 0);
    inode_set_ctime(inode, le32_to_cpu(disk_inode->ctime), 0);

    inode nesnesinin elemanları doldurulurken bazı sarma çekirdek fonksiyonlarının da kullanıldığına dikkat ediniz. 
    i_uid_write ve i_gid_write fonksiyonları dosyanın kullanıcı id'si ve grup id'si değerlerini inode nesnesi içerisine
    yerleştirmektedir. Burada bu işlem için neden fonksiyon çağrıldığını meraj edebilirsiniz. Daha önce de belirttiğimiz 
    gibi Linux çekirdeklerine belli bir süreden sonra çeşitli alt sistemler için "isim alanları (name spaces)" eklenmiştir. 
    Belli bir kullanıcı id'si ve grup id'si o isim alanına ilişkin kullanıcı id'si ve grup id'sine dönüştürülmektedir. 
    Linux çekirdeği farklı isim alanlarında aynı id'ler bulunabilmesine izin veriyor olsa da aslında kendi içerisinde 
    bu id'leri ayırmaktadır. Bu fonksiyonlar isim alanlarındaki id'leri çekirdeğin kullandığı id'lere dönüştürmektedir. 
    Aslında default isim alanında bulunuluyorsa bu çekirdekler aynı id'değerlerini inode nesnesinin içerisine yerleştirmektedir. 
    Örneğimizde dosyanın hard link sayscı da set_nlink fonksiyonuyla set edilmiştir. Çünkü hard link sayacının 
    atomik bir biçimde birkaç durum gözetilerek yerleştirilmesi gerekmektedir. Linux çekirdeklerinde dosyanın tarih 
    zaman bilgilerinin tutulma biçimi versiyonlarla üç kez değiştirilmiştir. Eskiden yalnızca 01/01/1970'ten geçen 
    saniye sayısı tutuluyordu. Sonra timespec yapısı kullanılmaya başlandı. Bu yapıyu kullanıcı modu uygulamalarıdan
    da anımsayacksınız. Bu timespec yapısı hem 01/01/1970'ten geçen saniye sayısını hem de bu saniyedne sonraki nano 
    saniye sayısını tutuyordu. Nihayet yeni çekirdeklerde artık tarih ve zaman bilgisi inode nesnesinde iki ayrı yapı 
    elemanıyla tuutlmaktadır. Çekirdekteki bu değişmlerden etkilenmek istenmiyorsa tarih zama bilgisi doğrudan değil 
    çekirdeğin içerisindeki inode_set_atime, inode_set_mtime, inode_set_ctime fonksiyonlarıyla set edilmelidir. 
    Bu fonksiyonlar uzunca bir süredir çekirdeklerde bulunmaktadır ve set işlmeini çekirdeğin o versiyonundaki formata 
    göre yapmaktadır. 

    inode nesnesinin doldurulurken inode nesnesinin belirttiği dosyanın bilgilerinin hangi data block içerisinde 
    olduğu da saklanmalıdır. Çünkü biz ileride inode nesnesinden hareketle dosyaya ilişkin bilgilere ulaşmak 
    isteyeceğiz. Bu bilgi zaten diskteki inode elemanında bulunuyordu. O halde bu bilginin inode nesnesine yerleştirilmesi 
    şöyle yapılabilir:

    struct simplefs_inode *inode_simplefs;
    /* ... */

    inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
	inode_simplefs->block_no = disk_inode->block_no;

    inode nesne adresinin bizim simplefs_inode yapımızın içerisindeki vfs_inode isimli elemanın adresi olduğunu anımsayınız.
    Biz container_of makrosuyla yukarı çıkarak asıl nesnemizin adresini elde etmekteyiz. 

    simplefs_iget fonksiyonundan çıkarken nesne sayacını artırdığımız iki nesneyi de geri bırakmamız gerekir:

    brelse(bh);
    unlock_new_inode(inode);
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        42. Ders 14/12/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Artık simplefs_iget fonksiyonunda inode nesnesimizin içini de doldurmuş olduk. Fonksiyonda geldiğimiz yer şöyledir:

    static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
    {
        struct inode *inode;
        struct simplefs_disk_inode *disk_inode;
        struct buffer_head *bh;
        struct simplefs_disk_super_block *simplefs_sbd;
        struct simplefs_inode *inode_simplefs;
        
        simplefs_sbd = SIMPLEFS_DISK_SB(sb);
        if (ino >= simplefs_sbd->inode_count) 
            return ERR_PTR(-EINVAL);

        if ((inode = iget_locked(sb, ino)) == NULL) 
            return ERR_PTR(-ENOMEM);

        if (!(inode->i_state & I_NEW))
            return inode;

        disk_inode = simplefs_get_inode_disk(sb, ino, &bh);
        if (IS_ERR(disk_inode)) {
            iget_failed(inode);
            return (struct inode *)disk_inode;
        }

        inode->i_mode = le32_to_cpu(disk_inode->mode);
        i_uid_write(inode, le32_to_cpu(disk_inode->uid));
        i_gid_write(inode, le32_to_cpu(disk_inode->gid));
        inode->i_size = le32_to_cpu(disk_inode->size);
        set_nlink(inode, le32_to_cpu(disk_inode->nlink));
        inode->i_blocks = le32_to_cpu(disk_inode->blocks);
        inode->i_size = le32_to_cpu(disk_inode->size);
        inode_set_atime(inode, le32_to_cpu(disk_inode->atime), 0);
        inode_set_mtime(inode, le32_to_cpu(disk_inode->mtime), 0);
        inode_set_ctime(inode, le32_to_cpu(disk_inode->ctime), 0);

        inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
        inode_simplefs->block_no = disk_inode->block_no;

        /* ... */

        brelse(bh);
        unlock_new_inode(inode);

        return inode;
    }

    Şimdi bizim son olarak inode nesnesine ilişkin inode_operations ve file_operations adreslerini oluşturmamız gerekmektedir. 
    Çekirdek izleyen paragraflarda da ayrıntılandıracağımız gibi inode kavramına ilişkin işlemler yapılırken inode_operations
    fonksiyonarını dosyaların bilgileri üzerinde işlemler yapılırken file_operations fonksiyonlarını çağırmaktadır. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Anımsanacağı gibi inode yapısının içerisinde iki önemli fonksiyon göstericilerinden oluşan i_op ve i_fop isminde 
    iki eleman bulunuyordu:

    struct inode {
        /* ... */

        const struct inode_operations	*i_op;

        union {
            const struct file_operations	*i_fop;	            /* former ->i_op->default_file_ops */
            void (*free_inode)(struct inode *);
        };

        /* ... */
    };

    Ne zaman bir inode nesnesine ilişkin disk işlemi yapılacak olsa çekirdek inode nesnesinin i_op elemanından hareketle 
    inode_operations yapısının ilgili fonksiyonunu çağırmaktadır. Örneğin inode elemanı bir dizine ilişkin olsun. Bu 
    dizinde de bir dosya aranacak olsun. Linux çekirdeği bizim dosya sistemimizi bilmediğine göre arama işlemi için 
    bizim bir fonksiyonumuzu çağırması gerekir. Ya da örneğin bu dizinde yeni bir dosya yaratılacak olsun. Linux çekirdeği
    bizim dizin organizasyonumunuz bilmediğine göre bu işlem için de bizim bir fonksiyonumuzu çağırması gerekir. Bir dizin 
    içerisindeki bir dosyanın isminin değiştirilmek istendiğini düşünelim. Linux çekirdeği bizim dosya sistemimize ilişkin 
    disk organizasyonunu bilmediğinde yine bu işlem için bizim bir fonksiyonumuzu çağırması gerekir. İşte inode_operations 
    fonksiyonları bu fonksiyonları barındırmaktadır. Biz daha önce bu operations yapılarından kısaca bahsetmiştik. Güncel 
    Linux çekirdeklerinde inode_operations yapısını yeniden anımsatmak istiyoruz:

    struct inode_operations {
        struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int);
        const char * (*get_link) (struct dentry *, struct inode *, struct delayed_call *);
        int (*permission) (struct mnt_idmap *, struct inode *, int);
        struct posix_acl * (*get_inode_acl)(struct inode *, int, bool);

        int (*readlink) (struct dentry *, char __user *,int);

        int (*create) (struct mnt_idmap *, struct inode *,struct dentry *,
                umode_t, bool);
        int (*link) (struct dentry *,struct inode *,struct dentry *);
        int (*unlink) (struct inode *,struct dentry *);
        int (*symlink) (struct mnt_idmap *, struct inode *,struct dentry *,
                const char *);
        struct dentry *(*mkdir) (struct mnt_idmap *, struct inode *,
                    struct dentry *, umode_t);
        int (*rmdir) (struct inode *,struct dentry *);
        int (*mknod) (struct mnt_idmap *, struct inode *,struct dentry *,
                umode_t,dev_t);
        int (*rename) (struct mnt_idmap *, struct inode *, struct dentry *,
                struct inode *, struct dentry *, unsigned int);
        int (*setattr) (struct mnt_idmap *, struct dentry *, struct iattr *);
        int (*getattr) (struct mnt_idmap *, const struct path *,
                struct kstat *, u32, unsigned int);
        ssize_t (*listxattr) (struct dentry *, char *, size_t);
        int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
                u64 len);
        int (*update_time)(struct inode *, int);
        int (*atomic_open)(struct inode *, struct dentry *,
                struct file *, unsigned open_flag,
                umode_t create_mode);
        int (*tmpfile) (struct mnt_idmap *, struct inode *,
                struct file *, umode_t);
        struct posix_acl *(*get_acl)(struct mnt_idmap *, struct dentry *,
                        int);
        int (*set_acl)(struct mnt_idmap *, struct dentry *,
                struct posix_acl *, int);
        int (*fileattr_set)(struct mnt_idmap *idmap,
                    struct dentry *dentry, struct file_kattr *fa);
        int (*fileattr_get)(struct dentry *dentry, struct file_kattr *fa);
        struct offset_ctx *(*get_offset_ctx)(struct inode *inode);
    } ____cacheline_aligned;

    Burada pek çok fonksiyon göstericisinin bulunduğunu görüyorsunuz. Tabii daha önceden de belirttiğimiz gibi 
    aslında sistem programcısının bütün bu göstericiler için fonksiyonları oluşturmasına mutlak anlamda gerek yoktur. 
    Dosya sisteminin desteklediği işlemlere yönelik fonksiyonların yazılması yeterli olmaktadır. 

    Burada önemli bir noktanın üzerinde durmak istiyoruz. Bir inode nesnesi bir dizine ya da bir dosyaya ilişkin 
    olabilir. Bir dizin için yapılacak işlemlerle dosya için yaılacak işlemler tamamen farklıdır. Dolayısıyla inode 
    nesnesine inode_operations fonksiyonlarını atarken onun bir dizin mi dosya mı olduğuna yönelik kontrol de yapmalısınız. 
    Örneğin:

    if (S_ISDIR(inode->i_mode)) {
        inode->i_op = &simplefs_dir_inode_ops;
        /* ... */
    }
    else {
        inode->i_op = &simplefs_file_inode_ops;
        /* ... */
    }

    Burada S_ISDIR makrosuyla dosyanın erişim haklarında onun bir dizin olup olmadığı bilgisine bakılmaktadır. Duruma 
    göre farklı yapılar inode nesnesinin i_op elemanına atanmıştır. Buradaki simplefs_dir_inode_ops ve simplefs_file_inode_ops 
    nesneleri global düzeyde tanımlanmıştır. Peki biz simplefs dosya sistemimizi test edilebilir hale getirmek için 
    inode_operations yapısının minimal hangi elemanlarına fonksiyon yerleştirmeliyiz? Bizim dizin üzerinde yapacağımız 
    işlemlere göre minimal olarak aşağıdaki fonksiyonları yazmamız gerekir:

    static const struct inode_operations simplefs_dir_inode_ops = {
        .lookup = simplefs_lookup,
        .create = simplefs_create,
        .mkdir = simplefs_mkdir,
        .unlink = simplefs_unlink,
        .rmdir = simplefs_rmdir,
    };

    lookup fonksiyonu bizim dosya sistemimize ilişkin yol ifadeleri çözümlenirken çağrılmaktadır. create fonksiyonu 
    ilgili dizinde yeni bir dizin girişi yaratılırken çağrılmaktadır. mkdir, unlink ve rmdir fonksiyonlaır ise bizim 
    dizinimizde sırasıyla dizin yaratılırkeni bir dizin girişi silinirken ve bir dizin silinirklen çağrılmaktadır. 
    Biz dosya sistemimizde test amaçlı önce minimal olarak lookup fonksiyonunu yazacağız. Dolayısıyla bizim 
    inode_operations nesnesimiz başlangıçta şöyle olacaktır:

    static const struct inode_operations simplfs_dir_inode_ops = {
        .lookup = simplefs_lookup,
    };    
    
    lookup fonksiyonunun parametrik yapısı şöyle olmalıdır:

    static struct dentry *simplefs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags)
    {
        /* ... */
    }

    Fonksiyonun birinci parametresi dizine ilişkin inode nesnesinin adresini almaktadır. İkinci parametre çekirdek 
    tarafından oluşturulmuş olan ve içerisine aranacak dosyanın isminin yerleştirildiği dentry nesnesinin adresini 
    almaktadır. Yani fonksiyon birinci parametresiyle belirtilen dizinde ikinci parametresiyle belirtilen dosyayı 
    aramalıdır. Fonksiyonun üçüncü parametresi arama işleminin bazı ayrıntılarına ilişkin bayrakları belirtmektedir. 
    Bu bayraklar aramanın hangi amaçla yapıldığı hakkında ek bilgi vermektedir. eğer bu fonksiyon NULL ile geri 
    döndürülürse fonksiyona geçirilen ve çekirdek tarafındna yaratılmış olan dentry nesnesi çekirdek tarafından 
    kullanılır. Ancak bazı durumlarda zaten bu inode nesnesine ilişkin dentry nesneleri de bulunuyor olabilir. Bu 
    durumda fonksiyonu zaten bulunan dentry nesnesi ile geri döndürmek gerekir. 

    Bizim simplfs dosya sistemimizdeki lookup fonksiyonumuz ne yapmalıdır? İşte bizim bu lookup fonksiyonu içerisinde
    dizinimize ilişkin disk bloğunu elde edip oradaki dentry girişlerinde arama yapmamız gerekir. Dosya sistemimizin 
    disk tarafındaki dizin girişleri şöyle organize edilmişti:

    #define SIMPLEFS_FILENAME_MAXLEN    		32

    struct simplefs_disk_dir_entry {
        __le32 inode;                           /* inode number */
        char name[SIMPLEFS_FILENAME_MAXLEN];    /* File name */
    };

    Daha önceden de belirttiğimiz gibi aslında dizinler birer dosya gibidir. Dizin dosyalarının içerisinde dosya 
    bilgileri olarak dizin girişlerine ilişkin bilgiler bulunmaktadır. Yani bizim dizinimize ilişkin blok içerisinde 
    yukarıdaki formatta kayıtlar bulunmaktadır. Buradaki dosya isminin sonunda null karakterin bulunduğunu anımsatmak 
    istiyoruz. O halde dosya sistemimize ilişkin lookup fonksiyonu şöyle yazılabilir:

    static struct dentry *simplefs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags)
    {
        struct super_block *sb = dir->i_sb;
        struct simplefs_inode *inode_simplefs = container_of(dir, struct simplefs_inode, vfs_inode);
        struct buffer_head *bh;
        struct simplefs_disk_dir_entry *de;
        struct inode *inode = NULL;
        int i;
        
        if (dentry->d_name.len > SIMPLEFS_FILENAME_MAXLEN - 1)
            return ERR_PTR(-ENAMETOOLONG);
        
        if ((bh = sb_bread(sb, inode_simplefs->block_no)) == NULL)
            return ERR_PTR(-EIO);
        
        de = (struct simplefs_disk_dir_entry *)bh->b_data;
        for (i = 0; i < SIMPLEFS_MAX_DIR_ENTRIES; ++i) {
            if (de[i].inode == 0)			/* deleted entry */
                continue;
            if (strcmp(de[i].name, dentry->d_name.name) == 0) {
                inode = simplefs_iget(sb, le32_to_cpu(de[i].inode));
                if (IS_ERR(inode)) 
                    return (struct dentry *)inode;
                break;
            }
        }
       
        brelse(bh);

        return d_splice_alias(inode, dentry);
    }

    Burada önce aranacak dosya ismi için bir uzunluk kontrolü yaptık:

    if (dentry->d_name.len > SIMPLEFS_FILENAME_MAXLEN - 1)
            return ERR_PTR(-ENAMETOOLONG);

    lookup fonksiyonumuzda daha sonra dizine ilişkin blok okunmuştur:

    if ((bh = sb_bread(sb, inode_simplefs->block_no)) == NULL)
        return ERR_PTR(-EIO);

    Tabii bu blok zaten sayfa önbelleiğinde varsa gerçek bir okuma yapılmayacaktır. Fonksiyonumuzda sonra doprusal 
    arama yapılmıştır:

    de = (struct simplefs_disk_dir_entry *)bh->b_data;
    for (i = 0; i < SIMPLEFS_MAX_DIR_ENTRIES; ++i) {
        if (de[i].inode == 0)			/* deleted entry */
            continue;
        if (strcmp(de[i].name, dentry->d_name.name) == 0) {
            inode = simplefs_iget(sb, le32_to_cpu(de[i].inode));
            if (IS_ERR(inode)) {
				brelse(bh);			
				return (struct dentry *)inode;
			}
			break;
		}
	}
    
    Bu arama sırasında ilgili dizin girişindeki inode elemanının 0 olması bu dizin girişinin silinmiş olduğu anlamına 
    gerektedir. Bu nedenle bu dizin girişi aramada geçilmiştir. Bizim dosya sistemimizde her dentry elemanı 36 byte 
    uzunluğundadır. Yukarıdaki döngüde biz dizin bloğunun sonuna kadar her dizin girişini gözden geçirmekteyiz. Tabii 
    bu işlem bir zaman kaybına yol açacaktır. ext2 gibi ext4 gibi dosya sistemlerinde dizin girişlerinin sonu da 
    bir biçimde bilinmektedir. Böylece o dosya sistemlerinde tüm blok ya da bloklar sonuna kadar gözden geçirilmemektedir.  
    Burada biz bloğun sonuna kadar gözden geçirme zorunluluğunu ortadan kaldırmak için simplefs_disk_dir_entry yapısının
    içerisine bir "son giriş mi" bayrağı tutabiliriz.

    Yukarıda kodda eğer dizin girişi bulunmuşsa ona ilişkin inode nesnesi daha önce yazmış olduğumuz simplefs_iget 
    fonksiyonuyla elde edilmiştir. Elde edilen bu inode nesnesinin fonksiyonun parametresiyle bize verilen dentry 
    nesnesine bağlanması gerekmektedir. Bu bağlantıyı yapmak için çekirdek içerisinde d_add ve d_splice_alias 
    fonksiyonları bulundurulmuştur. d_splice_alias eğer bu inode nesnesine referans eden dentry önbelleğinde zaten bir 
    dentry nesnesi varsa bu fonksiyon zaten olan nesnenin adresini geri döndürmektedir. Eğer böyle bir durumun 
    olmadığından eminsek bağlamayı daha hızlı biçimde d_add fonksiyonuyla yapıp lookup fonksiyonundan NULL adresle 
    geri dönebiliriz. 
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıda dosya sistemimizdeki inode_operations yapısına eğer inode elemanı bir dizin belirtiyorsa ona minimal 
    olarak bir lookup fonksiyonu yerleştirdik. Ancak inode elemanının bir dosya belirtmesi durumunda inode_operations
    yapısına başka fonksiyonların yerleştirilmesi gerekebilmektedir. Biz hızlı bir test ortamı oluşturmak için 
    buraya şimdilik içi boş bir inode_operations nesnesi atayacağız:

    static const struct inode_operations simplefs_file_inode_ops = {
        NULL
    };
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
                                        43. Ders 20/12/2025 - Cumartesi
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz yukarıda inode nesnesi oluşturan simplefs_iget isimli bir fonksiyon yazdık. Aslında gerçekleştirime biz 
    simplefs_fill_super fonksiyonundan başlamıştık. Bu simplefs_iget fonksiyonunu simplefs_fill_super fonksiyonundan 
    çağırmıştık:

    /* ... */

    root_inode = simplefs_iget(sb, SIMPLEFS_ROOT_INO);
	if (IS_ERR(root_inode)) {
		printk(KERN_INFO "cannot read root inode!..\n");
		retval = PTR_ERR(root_inode);
		goto EXIT4;
	}

    Şimdi simplefs_fill_super fonksiyonuna geri dönüp kaldığımız yerden devam edelim. simplefs_iget ile ilgili bazı 
    eklemeleri de daha sonra yapacağız. Bizim simplefs_fill_super fonksiyonunda, mount edilen kök dizine ilişkin inode 
    nesnesini oluşturduktan sonra, bu inode nesnesine referans eden bir kök dentry nesnesi de oluşturup onun adresini 
    super_block nesnesnin s_root elemanına yerleştirmemiz gerekir. Bu işlem tek hamlede d_make_root isimli yüksek seviyeli 
    çekirdek fonksiyonuyla yapılabilmektedir. Fonksiyon şöyle yazılmıştır:

    struct dentry *d_make_root(struct inode *root_inode)
    {
        struct dentry *res = NULL;

        if (root_inode) {
            res = d_alloc_anon(root_inode->i_sb);
            if (res)
                d_instantiate(res, root_inode);
            else
                iput(root_inode);
        }
        return res;
    }
    EXPORT_SYMBOL(d_make_root);

    d_make_root fonksiyonu kök inode nesnesini alıp, dentry nesnesi yaratarak bu inode nesnesiyle dentry nesnesini 
    ilişkilendirmekte ve tarattığı dentry nesnesi ile geri döndürmektedir. Biz simplefs_fill_super fonksiyonumuzda bu 
    fonksiyonu şöyle çağırabiliriz:

    if ((sb->s_root = d_make_root(root_inode)) == NULL) {
		retval = -ENOMEM;
		goto EXIT4;
	}
   
    d_make_root fonksiyonunun başarısız durumunda iput(toot_inode) işlemini de yaptığına dikkat ediniz. 

    Bilindiği gibi bir fonksiyonda çeşitli noktalarda tahsisatlar yapılıyorsa bunların geri alımları "ters sırada goto 
    tekniği kullanılarak" yapılmalıdır. Bu yöntem boşaltım işlemlerinde kod tekrarını ortadan kaldırmaktadır. Biz de 
    simplefs_fill_super fonksiyonumuzda bu tekniği kullandık:

    EXIT4:
        brelse(sfs_sb->data_bitmap_bh);
    EXIT3:
        brelse(sfs_sb->inode_bitmap_bh);
    EXIT2:
        brelse(sfs_sb->sb_bh);
    EXIT1:
        kfree(sfs_sb);

    Bu durumda geldiğimiz noktaya kadar simplefs_fill_super fonksiyonun kodu şöyledir:

    static int simplefs_fill_super(struct super_block *sb, void *data, int silent)
    {
        struct simplefs_super_block *sfs_sb;
        struct simplefs_disk_super_block *sbd;
        struct inode *root_inode;
        int retval;
        
        sb->s_magic = SIMPLEFS_MAGIC;
        sb_set_blocksize(sb, SIMPLEFS_BLOCK_SIZE);
        sb->s_maxbytes = SIMPLEFS_BLOCK_SIZE;
        sb->s_op = &simplefs_super_ops;
        sb->s_flags |= SB_NOATIME;

        if ((sfs_sb = kzalloc(sizeof(struct simplefs_super_block), GFP_KERNEL))== NULL) {
            printk(KERN_INFO "cannot allocate simplefs super block!..\n");
            return -ENOMEM;
        }
        sb->s_fs_info = sfs_sb;
        spin_lock_init(&sfs_sb->slock);

        if ((sfs_sb->sb_bh = sb_bread(sb, 0)) == NULL) {
            printk(KERN_INFO "cannot read  simplefs disk super block!..\n");
            retval = -EINVAL;
            goto EXIT1;
        }
        
        sbd = (struct simplefs_disk_super_block *) sfs_sb->sb_bh->b_data;
        sfs_sb->sbd = sbd;

        if (le32_to_cpu(sbd->magic) != SIMPLEFS_MAGIC) {
            printk(KERN_INFO "invalid magic number for simpls: %08X\n", sbd->magic);
            retval = -EINVAL;
            goto EXIT2;
        }
        if (le32_to_cpu(sbd->block_size) != SIMPLEFS_BLOCK_SIZE) {
            printk(KERN_INFO "invalid block size for simpls: %08X\n", sbd->block_size);
            retval = -EINVAL;
            goto EXIT2;
        }
        if (le32_to_cpu(sbd->inode_table_block) != 3) {
            printk(KERN_INFO "invalid inode table for simpls: %08X\n", sbd->inode_table_block);
            retval = -EINVAL;
            goto EXIT2;
        }

        if ((sfs_sb->inode_bitmap_bh = sb_bread(sb, SIMPLEFS_INODE_BITMAP_LOCATION)) == NULL) {
            printk(KERN_INFO "cannot read simplefs inode bitmap!..\n");
            retval = -EIO;
            goto EXIT2;
        }
        sfs_sb->inode_bitmap = (unsigned long *) sfs_sb->inode_bitmap_bh->b_data;

        if ((sfs_sb->data_bitmap_bh = sb_bread(sb, SIMPLEFS_DATA_BITMAP_LOCATION)) == NULL) {
            printk(KERN_INFO "cannot read simplefs data bitmap!..\n");
            retval = -EINVAL;
            goto EXIT3;
        }
        sfs_sb->data_bitmap = (unsigned long *) sfs_sb->data_bitmap_bh->b_data;

        root_inode = simplefs_iget(sb, SIMPLEFS_ROOT_INO);
        if (IS_ERR(root_inode)) {
            printk(KERN_INFO "cannot read root inode!..\n");
            retval = PTR_ERR(root_inode);
            goto EXIT4;
        }

        if ((sb->s_root = d_make_root(root_inode)) == NULL) {
            retval = -ENOMEM;
            goto EXIT4;
        }

        return 0;

    EXIT4:
        brelse(sfs_sb->data_bitmap_bh);
    EXIT3:
        brelse(sfs_sb->inode_bitmap_bh);
    EXIT2:
        brelse(sfs_sb->sb_bh);
    EXIT1:
        kfree(sfs_sb);

        return retval;
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Biz henüz dosya sistemimizin gerçekleştirimini bitirmedik. Ancak bu aşamada artık çekirdek modülümüzü derleyip 
    onu insmod komutuyla yükleyip basit bir test uygulayabiliriz. Çünkü bu tür projelerde test aşamasına ne kadar erken 
    geçilirse o kadar iyidir. Derlemeyi oluşturduğumuz make dosyasını kullanarak şöyle yapabiliriz:

    $ make file=simplefs

    Elde ettiğimiz "simplefs.ko" dosyasını şöyle yükleyebiliriz:

    $ sudo insmod simplefs.ko

    Biz modülümüzü yüklediğimizde modülümüzün init fonksiyonu çağrılacaktır. Anımsayacağınız gibi init fonksiyonu içeriisnde 
    dosya sistemni register ettirmiştik. Register ettrilen dosya sistemleri "/proc/filesystems" dosyası yoluyla görüntülenebilmektedir.

    $ cat /proc/filesystems
    nodev	sysfs
    nodev	tmpfs
    nodev	bdev
    nodev	proc
    nodev	cgroup
    nodev	cgroup2
    nodev	cpuset
    nodev	devtmpfs
    nodev	configfs
    nodev	debugfs
    nodev	tracefs
    nodev	securityfs
    nodev	sockfs
    nodev	bpf
    nodev	pipefs
    nodev	ramfs
    nodev	hugetlbfs
    nodev	devpts
            ext3
            ext2
            ext4
            squashfs
            vfat
    nodev	ecryptfs
            fuseblk
    nodev	fuse
    nodev	fusectl
    nodev	efivarfs
    nodev	mqueue
    nodev	pstore
            btrfs
    nodev	autofs
    nodev	binfmt_misc
            iso9660
            simplefs

    Burada listenin sonunda "simplefs" dosya sistemimizin de bulunduğunu görüyorsunuz. Bu listede başında "nodev" 
    bulunan dosya sistemleri "disk tabanlı olmayan", diğerleri ise "disk tabanlı olan" dosya sistemlerini belirtmektedir. 
    Başka bir deyişle "nodev" içeren dosya sistemleri bir blok aygıtı kullanmamakta, "nodev" içermeyen dosya sistemeri 
    bir blok aygıtı kullanmaktadır. 

    Peki bizim dosya sistemimiz için yazdığımız kodlar ne zaman devreye girecektir? İşte yukarıda da belirttiğimiz 
    gibi aslında her şey mount işlemiyle devreye sokulmaktadır. O halde bizim test için mount işlemi de yapmamız gerekir. 
    mount işlemi için önce mount noktası olabilecek bir dizin yaratmalıyız. Örneğin:

    $ mkdir simplefs-root

    mount işlemi yapılırken bizim dosya sistemimiz henüz otomatik tespit edilmeye elverişli değildir. Bu nedenle mount 
    komutunda "-t simplefs" komut satırı argümanını da bulundurmalıyız. Tabii önce loop aygıtını ayarlamayı da unutmayınız. 
    mount işlemi için yapılacak işlemleri şöyle listeleyebiliriz:

    $ sudo losetup /dev/loop0 simplefs.dat
    $ sudo insmod simplefs.ko
    $ sudo mount -t simplefs /dev/loop0 simplefs-root

    Artık mount işlemini yaptık. Ancak mount dizinine geçtiğimizde "ls" komutunu kullanamayız. Çünkü "ls" komutu dosya
    sistemimize ilişkin henüz yazmadığımız file_operations yapısındaki iterate_shared ya da iterate fonksiyonlarının 
    çağrılmasına yol açacaktır. Ancak biz dosya sistemimiz için lookup fonksiyonunu yazmıştık. Bu fonksiyon sayesinde 
    yol ifadeleri bizim dizinimizde çözülürken sorun oluşmayacaktır. Unmount işlemi sırasında file_system_type nesnesindeki 
    kill_sb fonksiyonu çağrılmaktadır. Biz henüz bu fonksiyonun da içini yazmadık. Eğer bu fonksiyonun içi yazılmazsa
    unmount işlemi sırasında bazı geri alımlar yapılammış olur. Her ne kadar bu durum çekirdeğn çökmesine neden olmayacak 
    olsa da "bellek sızıntısının (memory leak)" oluşmasına yol açacaktır. kill_sb fonksiyonunda bizim fill_super fonksiyonunda 
    yaptığımız işlemleri geri almamız gerekir. simplefs_kill_sb fonksiyonunda geri alımları şöyle yapabilirix:
   
    static void simplefs_kill_sb(struct super_block *sb)
    {
        struct simplefs_super_block *sfs_sb = sb->s_fs_info;

        kill_block_super(sb);
        
        if (sfs_sb) {
            if (sfs_sb->data_bitmap_bh)
                brelse(sfs_sb->data_bitmap_bh);
            if (sfs_sb->inode_bitmap_bh)
                brelse(sfs_sb->inode_bitmap_bh);
            if (sfs_sb->sb_bh)
                brelse(sfs_sb->sb_bh);
            kfree(sfs_sb);
        }
                
        printk(KERN_INFO "unmount super block...\n");
    }
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Aşağıda geldiğimiz noktaya kadarki simplfs dosya sistemimizin kodlarını bir bütün olarak veriyoruz. Bu koddaki 
    eksiklikleri sonraki paragraflarda tamamlayacağız. Ancak dosya sistemimiz bu haliyle mount edilebilecektir. Dosya 
    sistemimizi aşağıdaki gibi derleyebiliriz:

    $ make file=simplefs

    Diskimizi temsil eden "simplefs.dat" dosyasını loop0 aygıtı gibi gösterebiliriz:    

    $ sudo losetup /dev/loop0 simplefs.dat

    Mount işlemi için bir mount dizini yaratmamız gerekir:

    $ mkdir simplefs-root

    Artık dosya sistemimize ilişkin çekirdek modülünü yükleyebiliriz:

    $ sudo insmod simplefs.ko

    Mount işlemini de şöyle yapabiliriz:

    $ sudo mount /dev/loop0 simplefs-root

    lsblk komutunda artık dosya sistemimizin mount edildiğini görebileceğiz:

    $ lsblk
    NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
    loop0    7:0    0    10M  0 loop /home/kaan/Study/LinuxKernel/FileSystem/Simplefs/simplefs-root
    sda      8:0    0   120G  0 disk 
    ├─sda1   8:1    0     1M  0 part 
    ├─sda2   8:2    0   513M  0 part /boot/efi
    └─sda3   8:3    0 119,5G  0 part /
    sr0     11:0    1   2,8G  0 rom  /media/kaan/Linux Mint 22.1 Cinnamon 64-bit

    Tüm mount noktalarını "mount" komutuyla komutunun argümansız kullanımıyla da elde edebilirdik:

    $ mount

    ....
    binfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,nosuid,nodev,noexec,relatime)
    tmpfs on /run/user/1000 type tmpfs (rw,nosuid,nodev,relatime,size=808388k,nr_inodes=202097,mode=700,uid=1000,gid=1000,inode64)
    gvfsd-fuse on /run/user/1000/gvfs type fuse.gvfsd-fuse (rw,nosuid,nodev,relatime,user_id=1000,group_id=1000)
    /dev/loop0 on /home/kaan/Study/LinuxKernel/FileSystem/Simplefs/simplefs-root type simplefs (rw,relatime)

    Bu bilgi de aslında "/proc/mounts" dosyasından elde edilmektedir. 

    Biz henüz dosya sistemimizde "ls" gibi komutların çalışmasını sağlayacak fonksiyonları yazmadık. Dolayısıyla mount
    ettiğimiz dosya sistemi için "ls" komutunu uygularsak hata ortaya çıkacaktır:

    $ ls simplefs-root
    ls: 'simplefs-root' dizini açılamıyor: Böyle bir aygıt ya da adres yok

    Ancak dosya sistemimiz için lookup fonksiyonunu yazdık. Bu lookup fonksiyonu yol ifadeleri çözülürken dizin içerisindeki
    dosya aramaları sırasında kullanılmaktadır. Yani biz bu haliyle dosya sistemimize ilişkin bir giriş için yol ifadesi 
    belirtebiliriz. Henüz dosya sistemimizin kökünde yalnızca iki dosya bulunduğunu anımsayınız. Bunlar "." ve ".." 
    dosyalarıdır. Biz test amacıyla ".." dizini için "ls" komutunu kullanabiliriz. ".." dizin girişi bizim dosya sistemimizde l
    ookup fonksiyonumuz tarafından bulunacaktır. "ls" komutu da üst dizinde listeleme yapacağı için aşağıdaki komut 
    çalışacaktır:

    $ ls simplefs-root/..
    load      mkfs.simplefs    modules.order   simplefs.c    simplefs.ko   simplefs.mod.c  simplefs.o     unload
    Makefile  mkfs.simplefs.c  Module.symvers  simplefs.dat  simplefs.mod  simplefs.mod.o  simplefs-root

    Burada lookup fonksiyonumuzun ".." dizin girişini bulup verebildiğini görüyoruz. 

    Kodumuzda şimdilik bir senktronizasyon gerekmemiştir. Ancak dosya sistemimiz için eksik olan fonksiyonları yazarken 
    bazı yerlerde senkronizasyon uygulamamız gerekecektir. 
----------------------------------------------------------------------------------------------------------------------*/

/* simplefs.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/buffer_head.h>

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("simplefs");

#define SIMPLEFS_BLOCK_SIZE         		4096
#define SIMPLEFS_MAGIC           			0x53494D46
#define SIMPLEFS_INODE_BITMAP_LOCATION		1
#define SIMPLEFS_DATA_BITMAP_LOCATION		2
#define SIMPLEFS_INODE_TABLE_LOCATION		3
#define SIMPLEFS_ROOT_INO					1
#define SIMPLEFS_FILENAME_MAXLEN    		32
		
struct simplefs_disk_super_block {
    __le32 magic;              /* 0x464D4953 ("SIMF") */
    __le32 block_size;         /* 4096 */
    __le32 inode_count;        /* Toplam inode */
    __le32 block_count;        /* Toplam blok */
    __le32 free_inodes;        /* Serbest inode */
    __le32 free_blocks;        /* Serbest blok */
    __le32 inode_table_block;  /* İnode table başlangıcı (3) */
    __le32 inode_table_size;   /* İnode table boyutu */
    __le32 data_block_start;   /* Data block başlangıcı */
    __u8   padding[4060];      /* 4096'ya tamamlama */
};

struct simplefs_super_block {
	struct simplefs_disk_super_block *sbd;
	struct buffer_head *sb_bh;
	struct buffer_head *inode_bitmap_bh;
	struct buffer_head *data_bitmap_bh;
	unsigned long *inode_bitmap;
	unsigned long *data_bitmap;
	spinlock_t lock;
};

struct simplefs_inode {
    __u32 block_no;
    struct inode vfs_inode;
};

struct simplefs_disk_inode {
    __le32 mode;            /* File type + permissions */
    __le32 uid;             /* Owner user ID */
    __le32 gid;             /* Owner group ID */
    __le32 size;            /* File size in bytes */
    __le32 nlink;           /* Hard link count */
    __le32 blocks;          /* Block count (0 or 1) */
    __le32 block_no;        /* Data block number */
    __le32 ctime;           /* Creation time */
    __le32 mtime;           /* Modification time */
    __le32 atime;           /* Access time */
    __u8   padding[24];     /* Padding to 64 bytes */
};

struct simplefs_disk_dir_entry {
    __le32 inode;                           /* İnode number */
    char name[SIMPLEFS_FILENAME_MAXLEN];    /* File name */
};

#define SIMPLEFS_MAX_DIR_ENTRIES			(SIMPLEFS_BLOCK_SIZE / sizeof(struct simplefs_disk_dir_entry))

#define SIMPLEFS_SB(sb)						((struct simplefs_super_block *)((sb->s_fs_info)))
#define SIMPLEFS_DISK_SB(sb)				(SIMPLEFS_SB((sb))->sbd)

#define SIMPLEFS_DISK_INODE_SIZE			sizeof(struct simplefs_disk_inode)
#define SIMPLEFS_DISK_INODE_PER_BLOCK     	(SIMPLEFS_BLOCKSIZE / SIMPLEFS_DISK_INODE_SIZE)

static struct dentry *simplefs_mount(struct file_system_type *type, int flags, const char *dev, void *data);
static int simplefs_fill_super(struct super_block *sb, void *data, int silent);
static void simplefs_kill_sb(struct super_block *sb);
static struct inode *simplefs_alloc_inode(struct super_block *sb);
static void simplefs_free_inode(struct inode *inode);
static int simplefs_write_inode(struct inode *inode, struct writeback_control *wbc);
static void simplefs_evict_inode(struct inode *inode);
static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino);
static struct simplefs_disk_inode *simplefs_get_inode_disk(struct super_block *sb, 
		unsigned long ino, struct buffer_head **bhp);
 static struct dentry *simplefs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags);

static struct file_system_type simplefs_type = {
    .owner = THIS_MODULE,
    .name = "simplefs",
    .mount = simplefs_mount,
    .kill_sb = simplefs_kill_sb,
    .fs_flags = FS_REQUIRES_DEV,
};

static const struct super_operations simplefs_super_ops = {
	.alloc_inode = simplefs_alloc_inode,
	.free_inode = simplefs_free_inode,
	.write_inode = simplefs_write_inode,
	.evict_inode = simplefs_evict_inode,
	.statfs = simple_statfs,
};

static const struct inode_operations simplefs_dir_inode_ops = {
	.lookup = simplefs_lookup,
};    

static const struct inode_operations simplefs_file_inode_ops = {
	NULL
};

static struct kmem_cache *simplefs_inode_cachep;

static int __init simplefs_module_init(void)
{
	int result;

	if ((simplefs_inode_cachep = kmem_cache_create("simplefs_inode_cache", sizeof(struct simplefs_inode),
            0, SLAB_HWCACHE_ALIGN, NULL)) == NULL) {
		printk(KERN_ERR "cannot allocate slab cache\n");
		return -ENOMEM;
	}

	if ((result = register_filesystem(&simplefs_type)) != 0) {
		printk(KERN_ERR "cannot register file system\n");
		goto EXIT;
	}

	printk(KERN_INFO "simplefs module init\n");

    return 0;
EXIT:
	kmem_cache_destroy(simplefs_inode_cachep);

	return result;
}

static void __exit simplefs_module_exit(void)
{
	unregister_filesystem(&simplefs_type);
	kmem_cache_destroy(simplefs_inode_cachep);

	printk(KERN_INFO "simplefs module exit\n");
}

static struct dentry *simplefs_mount(struct file_system_type *type, int flags, const char *dev, void *data)
{
    return mount_bdev(type, flags, dev, data, simplefs_fill_super);
}

static int simplefs_fill_super(struct super_block *sb, void *data, int silent)
{
	struct simplefs_super_block *sfs_sb;
	struct simplefs_disk_super_block *sbd;
	struct inode *root_inode;
	int retval;
	
	sb->s_magic = SIMPLEFS_MAGIC;
	sb_set_blocksize(sb, SIMPLEFS_BLOCK_SIZE);
	sb->s_maxbytes = SIMPLEFS_BLOCK_SIZE;
	sb->s_op = &simplefs_super_ops;
    sb->s_flags |= SB_NOATIME;

	if ((sfs_sb = kzalloc(sizeof(struct simplefs_super_block), GFP_KERNEL))== NULL) {
		printk(KERN_INFO "cannot allocate simplefs super block!..\n");
		return -ENOMEM;
	}
	sb->s_fs_info = sfs_sb;
	spin_lock_init(&sfs_sb->slock);

	if ((sfs_sb->sb_bh = sb_bread(sb, 0)) == NULL) {
		printk(KERN_INFO "cannot read  simplefs disk super block!..\n");
		retval = -EINVAL;
		goto EXIT1;
	}
	
	sbd = (struct simplefs_disk_super_block *) sfs_sb->sb_bh->b_data;
	sfs_sb->sbd = sbd;

	if (le32_to_cpu(sbd->magic) != SIMPLEFS_MAGIC) {
		printk(KERN_INFO "invalid magic number for simpls: %08X\n", sbd->magic);
		retval = -EINVAL;
		goto EXIT2;
	}
	if (le32_to_cpu(sbd->block_size) != SIMPLEFS_BLOCK_SIZE) {
		printk(KERN_INFO "invalid block size for simpls: %08X\n", sbd->block_size);
		retval = -EINVAL;
		goto EXIT2;
	}
	if (le32_to_cpu(sbd->inode_table_block) != 3) {
		printk(KERN_INFO "invalid inode table for simpls: %08X\n", sbd->inode_table_block);
		retval = -EINVAL;
		goto EXIT2;
	}

	if ((sfs_sb->inode_bitmap_bh = sb_bread(sb, SIMPLEFS_INODE_BITMAP_LOCATION)) == NULL) {
		printk(KERN_INFO "cannot read simplefs inode bitmap!..\n");
		retval = -EIO;
		goto EXIT2;
	}
	sfs_sb->inode_bitmap = (unsigned long *) sfs_sb->inode_bitmap_bh->b_data;

	if ((sfs_sb->data_bitmap_bh = sb_bread(sb, SIMPLEFS_DATA_BITMAP_LOCATION)) == NULL) {
		printk(KERN_INFO "cannot read simplefs data bitmap!..\n");
		retval = -EINVAL;
		goto EXIT3;
	}
	sfs_sb->data_bitmap = (unsigned long *) sfs_sb->data_bitmap_bh->b_data;

	root_inode = simplefs_iget(sb, SIMPLEFS_ROOT_INO);
	if (IS_ERR(root_inode)) {
		printk(KERN_INFO "cannot read root inode!..\n");
		retval = PTR_ERR(root_inode);
		goto EXIT4;
	}

	if ((sb->s_root = d_make_root(root_inode)) == NULL) {
		retval = -ENOMEM;
		goto EXIT4;
	}

	return 0;

EXIT4:
	brelse(sfs_sb->data_bitmap_bh);
EXIT3:
	brelse(sfs_sb->inode_bitmap_bh);
EXIT2:
	brelse(sfs_sb->sb_bh);
EXIT1:
	kfree(sfs_sb);

	return retval;
}

static void simplefs_kill_sb(struct super_block *sb)
{
   	struct simplefs_super_block *sfs_sb = sb->s_fs_info;
    
    kill_block_super(sb);

    if (sfs_sb) {
        if (sfs_sb->data_bitmap_bh)
            brelse(sfs_sb->data_bitmap_bh);
        if (sfs_sb->inode_bitmap_bh)
            brelse(sfs_sb->inode_bitmap_bh);
        if (sfs_sb->sb_bh)
            brelse(sfs_sb->sb_bh);
        kfree(sfs_sb);
    }
     
	printk(KERN_INFO "unmount super block...\n");
}

static struct inode *simplefs_alloc_inode(struct super_block *sb)
{
	struct simplefs_inode *inode_simplefs;

	if ((inode_simplefs = kmem_cache_alloc(simplefs_inode_cachep, GFP_KERNEL)) == NULL) 
		return NULL;
    inode_init_once(&inode_simplefs->vfs_inode);

	return &inode_simplefs->vfs_inode;
}

static void simplefs_free_inode(struct inode *inode) 
{
	struct simplefs_inode *inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
    
    kmem_cache_free(simplefs_inode_cachep, inode_simplefs);
}

static int simplefs_write_inode(struct inode *inode, struct writeback_control *wbc)
{
	return 0;
}

static void simplefs_evict_inode(struct inode *inode)
{

}

static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
{
	struct inode *inode;
	struct simplefs_disk_inode *disk_inode;
	struct buffer_head *bh;
	struct simplefs_disk_super_block *simplefs_sbd;
	struct simplefs_inode *inode_simplefs;

	simplefs_sbd = SIMPLEFS_DISK_SB(sb);
	if (ino >= simplefs_sbd->inode_count) 
		return ERR_PTR(-EINVAL);

	if ((inode = iget_locked(sb, ino)) == NULL) 
		return ERR_PTR(-ENOMEM);

	 if (!(inode->i_state & I_NEW))
        return inode;

	disk_inode = simplefs_get_inode_disk(sb, ino, &bh);
	if (IS_ERR(disk_inode)) {
		iget_failed(inode);
		return (struct inode *)disk_inode;
	}

	inode->i_mode = le32_to_cpu(disk_inode->mode);
    i_uid_write(inode, le32_to_cpu(disk_inode->uid));
    i_gid_write(inode, le32_to_cpu(disk_inode->gid));
    inode->i_size = le32_to_cpu(disk_inode->size);
    set_nlink(inode, le32_to_cpu(disk_inode->nlink));
    inode->i_blocks = le32_to_cpu(disk_inode->blocks);
    inode->i_size = le32_to_cpu(disk_inode->size);
    inode_set_atime(inode, le32_to_cpu(disk_inode->atime), 0);
	inode_set_mtime(inode, le32_to_cpu(disk_inode->mtime), 0);
    inode_set_ctime(inode, le32_to_cpu(disk_inode->ctime), 0);

	inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
	inode_simplefs->block_no = disk_inode->block_no;

	if (S_ISDIR(inode->i_mode)) {
        inode->i_op = &simplefs_dir_inode_ops;
        /* ... */
    }
    else {
        inode->i_op = &simplefs_file_inode_ops;
        /* ... */
    }
	brelse(bh);
    unlock_new_inode(inode);
	
	return inode;
}

static struct simplefs_disk_inode *simplefs_get_inode_disk(struct super_block *sb, 
            unsigned long ino, struct buffer_head **bhpp)
{
	int block_no, block_offset;
	struct buffer_head *bh;
	struct simplefs_disk_inode *disk_inode;
	
	block_no = SIMPLEFS_INODE_TABLE_LOCATION + ino * SIMPLEFS_DISK_INODE_SIZE / SIMPLEFS_BLOCK_SIZE;
	block_offset = ino * SIMPLEFS_DISK_INODE_SIZE % SIMPLEFS_BLOCK_SIZE;

	if ((bh = sb_bread(sb, block_no)) == NULL) 
        return ERR_PTR(-EIO);

	disk_inode = (struct simplefs_disk_inode *)(bh->b_data + block_offset);
	*bhpp = bh;

	return disk_inode;
}

static struct dentry *simplefs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags)
{
    struct super_block *sb = dir->i_sb;
    struct simplefs_inode *inode_simplefs = container_of(dir, struct simplefs_inode, vfs_inode);
    struct buffer_head *bh;
    struct simplefs_disk_dir_entry *de;
    struct inode *inode = NULL;
    int i;
    
    if (dentry->d_name.len > SIMPLEFS_FILENAME_MAXLEN)
        return ERR_PTR(-ENAMETOOLONG);
    
    if ((bh = sb_bread(sb, inode_simplefs->block_no)) == NULL)
        return ERR_PTR(-EIO);
    
    de = (struct simplefs_disk_dir_entry *)bh->b_data;
    for (i = 0; i < SIMPLEFS_MAX_DIR_ENTRIES; ++i) {
        if (de[i].inode == 0)			/* deleted entry */
            continue;
        if (strcmp(de[i].name, dentry->d_name.name) == 0) {
            inode = simplefs_iget(sb, le32_to_cpu(de[i].inode));
            if (IS_ERR(inode)) {
				brelse(bh);			
				return (struct dentry *)inode;
			}
			break;
		}
	}
    
    brelse(bh);

	return d_splice_alias(inode, dentry);
}

module_init(simplefs_module_init);
module_exit(simplefs_module_exit);

# Makefile

 obj-m += ${file}.o

all:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} modules
clean:
	make -C /lib/modules/$(shell uname -r)/build M=${PWD} clean

/*----------------------------------------------------------------------------------------------------------------------
                                            44. Ders 28/12/2025 - Pazar
----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------
    Şimdi de dosya sistemimize dizin listesinin elde edilebilmesi için gereken fonksiyonları yerleştirelim. Anımsanacağı
    gibi "ls" programı dizini opendir POSIX fonksiyonuyla açıp readdir POSIX fonksiyonuyla dizin girişlerini sırasıyla 
    okumakta ve sonunda da dizini closedir POSIX fonksiyonuyla kapatmaktadır. Güncel Linux çekirdeklerinde readdir POSIX 
    fonksiyonu sys_getdents64 isimli sistem fonksiyonunu çağırmaktadır. sys_getdents64 fonksiyonu talep edilen kadar 
    miktardaki dizin girişini kullanıcı alanındaki tampona kopyalamaktadır. İşte sys_getdents64 fonksiyonu bu işlemlerin
    yapılmasını belli bir boktada file_operations yapısının iterate ya da iterate_shared fonksiyonunu çağırarak dosya 
    sistemine havale etmektedir. Yeni çekirdeklerdeki bu çağırma silsilesi şöyledir:

    SYSCALL_DEFINE3(getdents64, ...)            [fs/readdir.c]
    │
    └─→ iterate_dir(file, ctx)                  [fs/readdir.c]
            │
            ├─→ down_read(&inode->i_rwsem)      [Okuma kilidi]
            │
            ├─→ file->f_op->iterate_shared(file, ctx)  [Dosya sistemi callback]
            │       │
            │       └─→ ext4_readdir()                 [fs/ext4/dir.c]
            │       └─→ simplefs_readdir()             [Sizin implementasyonunuz]
            │
            └─→ up_read(&inode->i_rwsem)    [Kilidi serbest bırak]
    
    sys_getdents64 fonksiyonun parametrik yapısı şöyledir:

    long sys_getdents64(unsigned int fd, struct linux_dirent64 __user *dirent, unsigned int count);

    Fonksiyonun birinci parametresi dizine ilişkin dosya betimleyicisini, ikinci parametresi dizin girişlerinin yerleştirileceği
    tamponun adresini ve üçüncü parametresi de bu tamponun uzunluğunu belirtmektedir. readdir POSIX fonksiyonu sys_getdents64 
    sistem fonksiyonunu çağırarak bir grup dizin girişini tamponlamakta ve bu tampondaki girişleri vermektedir.
    
    Yukarıda da belirttiğimiz gibi sistemin düzgün çalışması için bizim de dizine ilişkin inode nesnesinin i_fop elemanına 
    yerleştirdiğimiz file_operations nesnesinde iterate ya da iterate_shared fonksiyonlarını girmiş olmamız gerekir. Bu işlemi 
    simplefs_iget fonksiyonu içerisinde aşağıdaki gibi yapabiliriz:

    static const struct file_operations simplefs_dir_inode_fops = {
        .iterate_shared = simplefs_iterate_shared
    };

    static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
    {
        /* ... */

        if (S_ISDIR(inode->i_mode)) {
            inode->i_op = &simplefs_dir_inode_ops;
            inode->i_fop = &simplefs_dir_inode_fops;        /* BU SATIRI YENİ EKLEDİK */
            /* ... */
        }
        else {
            inode->i_op = &simplefs_file_inode_ops;
            /* ... */
        } 
        
        /* ... */
    }

    file_operations yapısının iterate fonksiyonu dizin girişi dolaşılırken güncelleme yapıldığı durumda, iterate_shared 
    fonksiyonu ise güncelleme yapılmadığı durumda kullanılmaktadır. Biz dosya sistemimizde iterate_shared fonksiyonunu 
    kullanacağız. file_operations yapısının iterate_shared elemanına gireceğimiz fonksiyonun parametrik yapısı şöyle 
    olmalıdır:

    static int simplefs_iterate_shared (struct file *file, struct dir_context *ctx)
    {
        /* ... */
    }

    Fonksiyonun birinci parametresi dosya nesnesinin adresini belirtmektedir. Biz bu parametreden hareketle dizine 
    ilişkin inode nesnesine erişebiliriz. Fonksiyonun ikinci parametresi dir_context isimli bir yapı türünden nesnenin 
    adresini belirtmektedir. Bu parametrenin gösterdiği nesne çekirdek tarafından tahsis edilmiş durumdadır. dir_context 
    yapısı şöyle bildirilmiştir:

    struct dir_context {
        filldir_t actor;
        loff_t pos;
    };

    Yapının actor elemanını biz doğrudan kullanmayacağız. pos elemanı kaçıncı dizin girişinin elde edilmek istendiğini 
    belirtmektedir. Örneğin bu fonksiyon çağrıldığında eğer pos == 0 ise bizim "." girişini, pos == 1 ise ".." girişini 
    vermemiz gerekir. Fonksiyon başarı durumunda 0 değerine, başarısızlık durumunda negatif errno değerine geri 
    döndürülmelidir. 

    Bizim simplefs_iterate_shared fonksiyonunda önce file nesnesinden hareketle dizine ilişkin inode nesnesine ve 
    oaradan da kendi dosya sistemimize ilişkin inode bilgilerine erişmemiz gerekir. Anımsacağı gibi file yapısının 
    f_inode elemanı dosya nesnesine ilişkin inode nesnesinin adresini tutmaktadır. Çekirdek içerisinde "include/linux.fs.h" 
    dosyası içerisinde bu elemana geri fönen file_inode isimli bir inline fonksiyon da bulundurulmuştur:

    static inline struct inode *file_inode(const struct file *f)
    {
        return f->f_inode;
    }

    İleride birtakım değişiklerin olabileceği fikriyle elemana doğrudan erişmek yerine bu fonksiyonla erişmeyi tercih 
    edebilirsiniz:

    struct inode *inode;
	struct simplefs_inode *inode_simplefs;

    inode = file_inode(file);
	inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);

    Fonksiyonda güvenlik amacıyla bazı kontrolleri de yapabiliriz. Örneğin:

    if (ctx->pos >= SIMPLEFS_MAX_DIR_ENTRIES) 
        return 0;

    if (inode_simplefs->block_no == 0) {
        printk(KERN_INFO "unallocated disk block for directory!..\n");
        return 0;
    }

    Biz artık bu fonksiyonda dizin girişlerini ctx->pos indeksinden başlayarak tek tek dolaşmalıyız. Daha önce lookup 
    fonksiyonunda kendi dosya sistemimiz için dizin girişlerini dolaşan bir döngü oluşturmuştuk. Oradaki döngüye benzer 
    bir döngü ile işlemlerimizi yapabiliriz. Dizin girişlerini dolaşırken bizim dir_context yapısındaki pos elemanını 
    her yinelemede artırmamız gerekir. Kullanıcı modundaki readdir fonksiyonu sys_getdents64 sistem fonksiyonunu 
    çağırdığını söylemiştik. Bu sistem fonksiyonu çağrılırken fonksiyona bir tamponun adresi ve uzunluğunu parametre 
    olarak geçirilmektedir. Böylece bir dizin girişi bile okunacak olsa aslında birden fazla giriş okunup tamponlanmaktadır. 
    sys_getdents64 sistem fonksiyonu neticede bizim dosya sistemimizdeki iterate_shared fonksiyonu çağıracaktır. 
    iterate_shared fonksiyonu içerisinde sistem programsının her dizin girişini dir_emit isimli bir fonksiyonla kullanıcı 
    moduna iletmesi gerekmektedir. Kullanıcı modundaki tampon dolduğunda bizim iterate_shared fonksiyonunu sonlandırmamız 
    gerekir. Tanponun dolduğu dir_emit fonksiyonunun geri dönüş değeriyle anlaşılmaktadır. dir_emit fonksiyonu mevcut 
    çekirdeklerde şöyle yazılmıştır:

    static inline bool dir_emit(struct dir_context *ctx, const char *name, int namelen, u64 ino, unsigned type)
    {
        return ctx->actor(ctx, name, namelen, ctx->pos, ino, type);
    }

    Fonksiyonun birinci parametresi dir_context nesnesinin adresini, ikinci parametresi dizin girişinin dosya ismini
    (ismin sonunda null karakerin bulunması zorunlu değildir), üçüncü parametresi dosya isminin uzunluğunu dördüncü 
    parametresi inode numarasını ve beşici parametresi de bulun girişin dosya türünü belirtmektedir. 
----------------------------------------------------------------------------------------------------------------------*/

/* simplefs.c */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/fs.h>
#include <linux/buffer_head.h>

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kaan Aslan");
MODULE_DESCRIPTION("simplefs");

#define SIMPLEFS_BLOCK_SIZE         		4096
#define SIMPLEFS_MAGIC           			0x53494D46
#define SIMPLEFS_INODE_BITMAP_LOCATION		1
#define SIMPLEFS_DATA_BITMAP_LOCATION		2
#define SIMPLEFS_INODE_TABLE_LOCATION		3
#define SIMPLEFS_ROOT_INO					1
#define SIMPLEFS_FILENAME_MAXLEN    		32
		
struct simplefs_disk_super_block {
    __le32 magic;              /* 0x464D4953 ("SIMF") */
    __le32 block_size;         /* 4096 */
    __le32 inode_count;        /* Toplam inode */
    __le32 block_count;        /* Toplam blok */
    __le32 free_inodes;        /* Serbest inode */
    __le32 free_blocks;        /* Serbest blok */
    __le32 inode_table_block;  /* İnode table başlangıcı (3) */
    __le32 inode_table_size;   /* İnode table boyutu */
    __le32 data_block_start;   /* Data block başlangıcı */
    __u8   padding[4060];      /* 4096'ya tamamlama */
};

struct simplefs_super_block {
	struct simplefs_disk_super_block *sbd;
	struct buffer_head *sb_bh;
	struct buffer_head *inode_bitmap_bh;
	struct buffer_head *data_bitmap_bh;
	unsigned long *inode_bitmap;
	unsigned long *data_bitmap;
	spinlock_t slock;
};

struct simplefs_inode {
    __u32 block_no;
    struct inode vfs_inode;
};

struct simplefs_disk_inode {
    __le32 mode;            /* File type + permissions */
    __le32 uid;             /* Owner user ID */
    __le32 gid;             /* Owner group ID */
    __le32 size;            /* File size in bytes */
    __le32 nlink;           /* Hard link count */
    __le32 blocks;          /* Block count (0 or 1) */
    __le32 block_no;        /* Data block number */
    __le32 ctime;           /* Creation time */
    __le32 mtime;           /* Modification time */
    __le32 atime;           /* Access time */
    __u8   padding[24];     /* Padding to 64 bytes */
};

struct simplefs_disk_dir_entry {
    __le32 inode;                           /* İnode number */
    char name[SIMPLEFS_FILENAME_MAXLEN];    /* File name */
};

#define SIMPLEFS_MAX_DIR_ENTRIES			(SIMPLEFS_BLOCK_SIZE / sizeof(struct simplefs_disk_dir_entry))

#define SIMPLEFS_SB(sb)						((struct simplefs_super_block *)((sb->s_fs_info)))
#define SIMPLEFS_DISK_SB(sb)				(SIMPLEFS_SB((sb))->sbd)

#define SIMPLEFS_DISK_INODE_SIZE			sizeof(struct simplefs_disk_inode)
#define SIMPLEFS_DISK_INODE_PER_BLOCK     	(SIMPLEFS_BLOCKSIZE / SIMPLEFS_DISK_INODE_SIZE)

static struct dentry *simplefs_mount(struct file_system_type *type, int flags, const char *dev, void *data);
static int simplefs_fill_super(struct super_block *sb, void *data, int silent);
static void simplefs_kill_sb(struct super_block *sb);
static struct inode *simplefs_alloc_inode(struct super_block *sb);
static void simplefs_free_inode(struct inode *inode);
static int simplefs_write_inode(struct inode *inode, struct writeback_control *wbc);
static void simplefs_evict_inode(struct inode *inode);
static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino);
static struct simplefs_disk_inode *simplefs_get_inode_disk(struct super_block *sb, 
		unsigned long ino, struct buffer_head **bhp);
static struct dentry *simplefs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags);
static int simplefs_iterate_shared (struct file *file, struct dir_context *ctx);

static struct file_system_type simplefs_type = {
    .owner = THIS_MODULE,
    .name = "simplefs",
    .mount = simplefs_mount,
    .kill_sb = simplefs_kill_sb,
    .fs_flags = FS_REQUIRES_DEV,
};

static const struct super_operations simplefs_super_ops = {
	.alloc_inode = simplefs_alloc_inode,
	.free_inode = simplefs_free_inode,
	.write_inode = simplefs_write_inode,
	.evict_inode = simplefs_evict_inode,
	.statfs = simple_statfs,
};

static const struct inode_operations simplefs_dir_inode_ops = {
	.lookup = simplefs_lookup,
};    

static const struct inode_operations simplefs_file_inode_ops = {
	NULL
};

static const struct file_operations simplefs_dir_inode_fops = {
	.iterate_shared = simplefs_iterate_shared
};

static struct kmem_cache *simplefs_inode_cachep;

static int __init simplefs_module_init(void)
{
	int result;

	if ((simplefs_inode_cachep = kmem_cache_create("simplefs_inode_cache", sizeof(struct simplefs_inode),
            0, SLAB_HWCACHE_ALIGN, NULL)) == NULL) {
		printk(KERN_ERR "cannot allocate slab cache\n");
		return -ENOMEM;
	}

	if ((result = register_filesystem(&simplefs_type)) != 0) {
		printk(KERN_ERR "cannot register file system\n");
		goto EXIT;
	}

	printk(KERN_INFO "simplefs module init\n");

	return 0;
EXIT:
	kmem_cache_destroy(simplefs_inode_cachep);

	return result;
}

static void __exit simplefs_module_exit(void)
{
	unregister_filesystem(&simplefs_type);
	kmem_cache_destroy(simplefs_inode_cachep);

	printk(KERN_INFO "simplefs module exit\n");
}

static struct dentry *simplefs_mount(struct file_system_type *type, int flags, const char *dev, void *data)
{
    return mount_bdev(type, flags, dev, data, simplefs_fill_super);
}

static int simplefs_fill_super(struct super_block *sb, void *data, int silent)
{
	struct simplefs_super_block *sfs_sb;
	struct simplefs_disk_super_block *sbd;
	struct inode *root_inode;
	int retval;
	
	sb->s_magic = SIMPLEFS_MAGIC;
	sb_set_blocksize(sb, SIMPLEFS_BLOCK_SIZE);
	sb->s_maxbytes = SIMPLEFS_BLOCK_SIZE;
	sb->s_op = &simplefs_super_ops;
	sb->s_flags |= SB_NOATIME;

	if ((sfs_sb = kzalloc(sizeof(struct simplefs_super_block), GFP_KERNEL))== NULL) {
		printk(KERN_INFO "cannot allocate simplefs super block!..\n");
		return -ENOMEM;
	}
	sb->s_fs_info = sfs_sb;
	spin_lock_init(&sfs_sb->slock);

	if ((sfs_sb->sb_bh = sb_bread(sb, 0)) == NULL) {
		printk(KERN_INFO "cannot read  simplefs disk super block!..\n");
		retval = -EINVAL;
		goto EXIT1;
	}

	sbd = (struct simplefs_disk_super_block *) sfs_sb->sb_bh->b_data;
	sfs_sb->sbd = sbd;

	if (le32_to_cpu(sbd->magic) != SIMPLEFS_MAGIC) {
		printk(KERN_INFO "invalid magic number for simpls: %08X\n", sbd->magic);
		retval = -EINVAL;
		goto EXIT2;
	}
	if (le32_to_cpu(sbd->block_size) != SIMPLEFS_BLOCK_SIZE) {
		printk(KERN_INFO "invalid block size for simpls: %08X\n", sbd->block_size);
		retval = -EINVAL;
		goto EXIT2;
	}
	if (le32_to_cpu(sbd->inode_table_block) != 3) {
		printk(KERN_INFO "invalid inode table for simpls: %08X\n", sbd->inode_table_block);
		retval = -EINVAL;
		goto EXIT2;
	}

	if ((sfs_sb->inode_bitmap_bh = sb_bread(sb, SIMPLEFS_INODE_BITMAP_LOCATION)) == NULL) {
		printk(KERN_INFO "cannot read simplefs inode bitmap!..\n");
		retval = -EIO;
		goto EXIT2;
	}
	sfs_sb->inode_bitmap = (unsigned long *) sfs_sb->inode_bitmap_bh->b_data;

	if ((sfs_sb->data_bitmap_bh = sb_bread(sb, SIMPLEFS_DATA_BITMAP_LOCATION)) == NULL) {
		printk(KERN_INFO "cannot read simplefs data bitmap!..\n");
		retval = -EINVAL;
		goto EXIT3;
	}
	sfs_sb->data_bitmap = (unsigned long *) sfs_sb->data_bitmap_bh->b_data;

	root_inode = simplefs_iget(sb, SIMPLEFS_ROOT_INO);
	if (IS_ERR(root_inode)) {
		printk(KERN_INFO "cannot read root inode!..\n");
		retval = PTR_ERR(root_inode);
		goto EXIT4;
	}

	if ((sb->s_root = d_make_root(root_inode)) == NULL) {
		retval = -ENOMEM;
		goto EXIT4;
	}

	return 0;

EXIT4:
	brelse(sfs_sb->data_bitmap_bh);
EXIT3:
	brelse(sfs_sb->inode_bitmap_bh);
EXIT2:
	brelse(sfs_sb->sb_bh);
EXIT1:
	kfree(sfs_sb);

	return retval;
}

static void simplefs_kill_sb(struct super_block *sb)
{
   	struct simplefs_super_block *sfs_sb = sb->s_fs_info;

	kill_block_super(sb);
    
    if (sfs_sb) {
        if (sfs_sb->data_bitmap_bh)
            brelse(sfs_sb->data_bitmap_bh);
        if (sfs_sb->inode_bitmap_bh)
            brelse(sfs_sb->inode_bitmap_bh);
        if (sfs_sb->sb_bh)
            brelse(sfs_sb->sb_bh);
        kfree(sfs_sb);
    }
     
	printk(KERN_INFO "unmount super block...\n");
}

static struct inode *simplefs_alloc_inode(struct super_block *sb)
{
	struct simplefs_inode *inode_simplefs;

	if ((inode_simplefs = kmem_cache_alloc(simplefs_inode_cachep, GFP_KERNEL)) == NULL) 
		return NULL;
	inode_init_once(&inode_simplefs->vfs_inode);

	return &inode_simplefs->vfs_inode;
}

static void simplefs_free_inode(struct inode *inode) 
{
	struct simplefs_inode *inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
    
    kmem_cache_free(simplefs_inode_cachep, inode_simplefs);
}

static int simplefs_write_inode(struct inode *inode, struct writeback_control *wbc)
{
	return 0;
}

static void simplefs_evict_inode(struct inode *inode)
{
	truncate_inode_pages_final(&inode->i_data);
    clear_inode(inode);
}

static struct inode *simplefs_iget(struct super_block *sb, unsigned long ino)
{
	struct inode *inode;
	struct simplefs_disk_inode *disk_inode;
	struct buffer_head *bh;
	struct simplefs_disk_super_block *simplefs_sbd;
	struct simplefs_inode *inode_simplefs;

	simplefs_sbd = SIMPLEFS_DISK_SB(sb);
	if (ino >= simplefs_sbd->inode_count) 
		return ERR_PTR(-EINVAL);

	if ((inode = iget_locked(sb, ino)) == NULL) 
		return ERR_PTR(-ENOMEM);

	 if (!(inode->i_state & I_NEW))
        return inode;

	disk_inode = simplefs_get_inode_disk(sb, ino, &bh);
	if (IS_ERR(disk_inode)) {
		iget_failed(inode);
		return (struct inode *)disk_inode;
	}

	inode->i_mode = le32_to_cpu(disk_inode->mode);
    i_uid_write(inode, le32_to_cpu(disk_inode->uid));
    i_gid_write(inode, le32_to_cpu(disk_inode->gid));
    inode->i_size = le32_to_cpu(disk_inode->size);
    set_nlink(inode, le32_to_cpu(disk_inode->nlink));
    inode->i_blocks = le32_to_cpu(disk_inode->blocks);
    inode->i_size = le32_to_cpu(disk_inode->size);
    inode_set_atime(inode, le32_to_cpu(disk_inode->atime), 0);
	inode_set_mtime(inode, le32_to_cpu(disk_inode->mtime), 0);
    inode_set_ctime(inode, le32_to_cpu(disk_inode->ctime), 0);

	inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);
	inode_simplefs->block_no = disk_inode->block_no;

	if (S_ISDIR(inode->i_mode)) {
        inode->i_op = &simplefs_dir_inode_ops;
		inode->i_fop = &simplefs_dir_inode_fops;
        /* ... */
    }
    else {
        inode->i_op = &simplefs_file_inode_ops;
        /* ... */
    }
	brelse(bh);
    unlock_new_inode(inode);
	
	return inode;
}

static struct simplefs_disk_inode *simplefs_get_inode_disk(struct super_block *sb, 
            unsigned long ino, struct buffer_head **bhpp)
{
	int block_no, block_offset;
	struct buffer_head *bh;
	struct simplefs_disk_inode *disk_inode;
	
	block_no = SIMPLEFS_INODE_TABLE_LOCATION + ino * SIMPLEFS_DISK_INODE_SIZE / SIMPLEFS_BLOCK_SIZE;
	block_offset = ino * SIMPLEFS_DISK_INODE_SIZE % SIMPLEFS_BLOCK_SIZE;

	if ((bh = sb_bread(sb, block_no)) == NULL) 
        return ERR_PTR(-EIO);

	disk_inode = (struct simplefs_disk_inode *)(bh->b_data + block_offset);
	*bhpp = bh;

	return disk_inode;
}

static struct dentry *simplefs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags)
{
    struct super_block *sb = dir->i_sb;
    struct simplefs_inode *inode_simplefs = container_of(dir, struct simplefs_inode, vfs_inode);
    struct buffer_head *bh;
    struct simplefs_disk_dir_entry *de;
    struct inode *inode = NULL;
    int i;
    
    if (dentry->d_name.len > SIMPLEFS_FILENAME_MAXLEN)
        return ERR_PTR(-ENAMETOOLONG);
    
    if ((bh = sb_bread(sb, inode_simplefs->block_no)) == NULL)
        return ERR_PTR(-EIO);
    
    de = (struct simplefs_disk_dir_entry *)bh->b_data;
    for (i = 0; i < SIMPLEFS_MAX_DIR_ENTRIES; ++i) {
        if (de[i].inode == 0)			/* deleted entry */
            continue;
        if (strcmp(de[i].name, dentry->d_name.name) == 0) {
            inode = simplefs_iget(sb, le32_to_cpu(de[i].inode));
            if (IS_ERR(inode)) {
				brelse(bh);			
				return (struct dentry *)inode;
			}
			break;
		}
	}
    
    brelse(bh);

	return d_splice_alias(inode, dentry);
}

static int simplefs_iterate_shared(struct file *file, struct dir_context *ctx)
{
	struct inode *inode;
	struct simplefs_inode *inode_simplefs;
	struct buffer_head *bh;
	struct simplefs_disk_dir_entry *de;
	unsigned long ino;
	int i;

	inode = file_inode(file);
	inode_simplefs = container_of(inode, struct simplefs_inode, vfs_inode);

	if (ctx->pos >= SIMPLEFS_MAX_DIR_ENTRIES) 
        return 0;

    if (inode_simplefs->block_no == 0) {
        printk(KERN_INFO "unallocated disk block for directory!..\n");
        return 0;
    }

	if ((bh = sb_bread(inode->i_sb, inode_simplefs->block_no)) == NULL) {
		printk(KERN_INFO "cannot read directory block from disk!..\n")  ;
        return -EIO;
    }
    de = (struct simplefs_disk_dir_entry *)bh->b_data;

	for (i = ctx->pos; i < SIMPLEFS_MAX_DIR_ENTRIES; ++i) {
        if (de[i].inode == 0) {
            /* Move position forward even for empty entries */
            ctx->pos = i + 1;
            continue;
        }
        ino = le32_to_cpu(de[i].inode);
		
		if (!dir_emit(ctx, de[i].name, strlen(de[i].name), ino, DT_UNKNOWN)) {
            brelse(bh);
            return 0;
        }
        
        ctx->pos = i + 1;
    }
	brelse(bh);

	return 0;
}

module_init(simplefs_module_init);
module_exit(simplefs_module_exit);

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/

/*----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------*/





